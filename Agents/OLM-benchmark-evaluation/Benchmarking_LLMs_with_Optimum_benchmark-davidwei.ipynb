{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGNbcKn-b5P"
      },
      "source": [
        "This notebook shows how to use optimum-benchmark to benchmark LLMs. It  focuses on benchmarking for Phi-3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow56uKbJ0IGO"
      },
      "source": [
        "We need to install the following packages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUJH8FMq-Yx_",
        "outputId": "7a41cb94-1df3-4323-8807-e8930638d244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/optimum-benchmark.git\n",
            "  Cloning https://github.com/huggingface/optimum-benchmark.git to /tmp/pip-req-build-8wpm6fcw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-benchmark.git /tmp/pip-req-build-8wpm6fcw\n",
            "  Resolved https://github.com/huggingface/optimum-benchmark.git to commit 347e13ca9f7f904f55669603cfb9f0b6c7e8672c\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: transformers in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (4.41.2)\n",
            "Requirement already satisfied: accelerate in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (0.31.0)\n",
            "Requirement already satisfied: datasets in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (2.19.2)\n",
            "Requirement already satisfied: hydra-core in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (1.3.2)\n",
            "Requirement already satisfied: omegaconf in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (2.3.0)\n",
            "Requirement already satisfied: psutil in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (4.11.0)\n",
            "Requirement already satisfied: flatten-dict in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (0.4.2)\n",
            "Requirement already satisfied: colorlog in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (6.8.2)\n",
            "Requirement already satisfied: pandas in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (2.2.2)\n",
            "Requirement already satisfied: nvidia-ml-py in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from optimum-benchmark==0.2.1) (12.555.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from accelerate->optimum-benchmark==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from accelerate->optimum-benchmark==0.2.1) (23.2)\n",
            "Requirement already satisfied: pyyaml in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from accelerate->optimum-benchmark==0.2.1) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from accelerate->optimum-benchmark==0.2.1) (2.3.1)\n",
            "Requirement already satisfied: huggingface-hub in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from accelerate->optimum-benchmark==0.2.1) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from accelerate->optimum-benchmark==0.2.1) (0.4.3)\n",
            "Requirement already satisfied: filelock in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->optimum-benchmark==0.2.1) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from datasets->optimum-benchmark==0.2.1) (3.9.5)\n",
            "Requirement already satisfied: six<2.0,>=1.12 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from flatten-dict->optimum-benchmark==0.2.1) (1.16.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from hydra-core->optimum-benchmark==0.2.1) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from pandas->optimum-benchmark==0.2.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from pandas->optimum-benchmark==0.2.1) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from pandas->optimum-benchmark==0.2.1) (2024.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from transformers->optimum-benchmark==0.2.1) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from transformers->optimum-benchmark==0.2.1) (0.19.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from aiohttp->datasets->optimum-benchmark==0.2.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from aiohttp->datasets->optimum-benchmark==0.2.1) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from aiohttp->datasets->optimum-benchmark==0.2.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from aiohttp->datasets->optimum-benchmark==0.2.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from aiohttp->datasets->optimum-benchmark==0.2.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from aiohttp->datasets->optimum-benchmark==0.2.1) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from requests>=2.32.1->datasets->optimum-benchmark==0.2.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from requests>=2.32.1->datasets->optimum-benchmark==0.2.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from requests>=2.32.1->datasets->optimum-benchmark==0.2.1) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from requests>=2.32.1->datasets->optimum-benchmark==0.2.1) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (1.12.1)\n",
            "Requirement already satisfied: networkx in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/miniconda/envs/benchmark/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->optimum-benchmark==0.2.1) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -m pip install git+https://github.com/huggingface/optimum-benchmark.git\n",
        "!pip install vllm\n",
        "!pip install optimum-benchmark[vllm]"      
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO6NNcn90ZNd"
      },
      "source": [
        "Define the configuration for optimum-benchmark.\n",
        "\n",
        "Here we benchmark for inference, using different batch sizes, Mistral 7B loaded as fp16.\n",
        "If you run this notebook on Google Colab, you will need the A100 only for this part. The following benchmarks would run on the T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jun 11 08:38:13 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA H100 NVL                Off | 00000001:00:00.0 Off |                    0 |\n",
            "| N/A   36C    P0              60W / 400W |      7MiB / 95830MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a7YIfxXma5C2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 10:59:23,322\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Starting benchmark in isolated process\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:25,391\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Setting torch.distributed cuda device to 0\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:25,394\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:25,415\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.3.0 available.\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:25,591\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating pytorch backend\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:25,591\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmarking a Transformers model\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
            "/opt/miniconda/envs/benchmark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,330\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Using automodel class AutoModelForCausalLM\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,330\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Seeding pytorch backend with seed 42\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,335\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Using AutoModel AutoModelForCausalLM\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,335\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating backend temporary directory\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,335\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading model with random weights\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,336\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model directory\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,336\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating no weights model state dict\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,353\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving no weights model safetensors\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,354\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Saving no weights model pretrained config\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,354\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Loading Transformers model using device context manager for fast initialization\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,673\u001b[0m][\u001b[34mpytorch\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Turning on model's eval mode\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,675\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating inference scenario\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,675\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Distributing batch size across processes\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,675\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,675\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generating Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,675\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,676\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating Text Generation kwargs with default values\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,676\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing Text Generation report\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,676\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for Inference\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:26,676\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up backend for Text Generation\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:31,795\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation memory tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:31,795\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking RAM memory of process [16093]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:31,795\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking VRAM memory of CUDA devices [0]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:31,796\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking Allocated/Reserved memory of 1 Pytorch CUDA devices\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 1283.334144 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 18063.753216 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 17439.916032 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max reserved memory: 16735.272960 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max allocated memory: 16223.365632 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 1283.334144 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 18063.753216 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 17439.916032 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max reserved memory: 16735.272960 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max allocated memory: 16593.348096 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Per-Token Text Generation latency tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 10:59:42,367\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking latency using Pytorch CUDA events\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 10\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 1.321285 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.132129 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000218 s (0.17%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.132190 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.132358 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.132422 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.132473 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,006\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 10\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 30.310179 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 3.031018 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.001790 s (0.06%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 3.031375 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 3.032804 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 3.032832 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 3.032854 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ per_token latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 989\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 31.468159 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.031818 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.012761 s (40.11%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.030585 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.030818 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.030870 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,007\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.031151 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,008\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill throughput: 3875.014301 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,008\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode throughput: 32.662295 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,008\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ per_token throughput: 31.428595 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,012\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmark completed successfully\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,012\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Destroying torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:00:14,031\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting rank process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:00:24,214\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Sending outputs to main process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:00:24,214\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting isolated process\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, PyTorchConfig  \n",
        "from optimum_benchmark.logging_utils import setup_logging  \n",
        "  \n",
        "# setup_logging(level=\"INFO\", handlers=[\"console\"])  \n",
        "if __name__ == \"__main__\":  \n",
        "    launcher_config = TorchrunConfig(nproc_per_node=1)  \n",
        "      \n",
        "    # 设置推理配置，包括批处理大小和序列长度等参数  \n",
        "    scenario_config = InferenceConfig(  \n",
        "        latency=True,  \n",
        "        memory=True,  \n",
        "        warmup_runs=5,  \n",
        "        input_shapes={  \n",
        "            \"batch_size\": 1,  # 设置批处理大小  \n",
        "            \"sequence_length\": 512  # 设置序列长度  \n",
        "        },  \n",
        "        generate_kwargs={  \n",
        "            \"max_new_tokens\": 100,  # 默认生成的新标记的最大数量  \n",
        "            \"min_new_tokens\": 100,  # 默认生成的新标记的最小数量  \n",
        "            \"num_beams\": 1,  # 默认束搜索数量  \n",
        "            \"temperature\": 1.0,  # 默认温度  \n",
        "            \"top_k\": 50,  # 默认Top-k采样  \n",
        "            \"top_p\": 0.9,  # 默认Top-p采样  \n",
        "            \"do_sample\": True  # 启用采样模式 \n",
        "        }  \n",
        "    ) \n",
        "      \n",
        "    # 定义 PyTorch 后端配置  \n",
        "    backend_config = PyTorchConfig(  \n",
        "        model=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
        "        device=\"cuda\",  \n",
        "        device_ids=\"0\",  # 确保设备ID为字符串  \n",
        "        no_weights=True  \n",
        "    )  \n",
        "      \n",
        "    # 定义基准测试配置  \n",
        "    benchmark_config = BenchmarkConfig(  \n",
        "        name=\"pytorch_Phi-3-mini-4k-instruct\",  \n",
        "        scenario=scenario_config,  \n",
        "        launcher=launcher_config,  \n",
        "        backend=backend_config,  \n",
        "    )  \n",
        "      \n",
        "    # 运行基准测试  \n",
        "    benchmark_report = Benchmark.launch(benchmark_config)  \n",
        "      \n",
        "    # 在终端中记录基准测试结果  \n",
        "    benchmark_report.log()  # or print(benchmark_report)  \n",
        "      \n",
        "    # 将工件转换为字典或数据帧  \n",
        "    benchmark_config.to_dict()  # or benchmark_config.to_dataframe()  \n",
        "      \n",
        "    # 将工件保存到磁盘为 JSON 或 CSV 文件  \n",
        "    benchmark_report.save_csv(\"benchmark_report_pytorch_Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n",
        "    benchmark_report.save_json(\"benchmark_report_pytorch_Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "      \n",
        "    # 或者将它们合并到一个单一的工件中  \n",
        "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)  \n",
        "    benchmark.save_json(\"benchmark_pytorch_Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "    benchmark.save_csv(\"benchmark_pytorch_Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:04:28,373\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Starting benchmark in isolated process\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:30,484\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Setting torch.distributed cuda device to 0\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:30,488\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing torch.distributed process group\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:30,723\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.3.0 available.\u001b[0m\n",
            "INFO 06-11 11:04:30 base.py:41] Allocating vllm backend\n",
            "INFO 06-11 11:04:30 base.py:60] \t+ Benchmarking a Transformers model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda/envs/benchmark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-11 11:04:31 base.py:74] \t+ Using automodel class AutoModelForCausalLM\n",
            "INFO 06-11 11:04:31 base.py:78] \t+ Seeding vllm backend with seed 42\n",
            "INFO 06-11 11:04:31 backend.py:23] \t+ Creating backend temporary directory\n",
            "INFO 06-11 11:04:31 backend.py:27] \t+ Loading no weights model\n",
            "INFO 06-11 11:04:31 backend.py:90] \t+ Creating no weights model\n",
            "INFO 06-11 11:04:31 backend.py:59] \t+ Creating no weights model directory\n",
            "INFO 06-11 11:04:31 backend.py:61] \t+ Creating no weights model state dict\n",
            "INFO 06-11 11:04:31 backend.py:63] \t+ Saving no weights model safetensors\n",
            "INFO 06-11 11:04:31 backend.py:66] \t+ Saving no weights model pretrained config\n",
            "INFO 06-11 11:04:31 backend.py:68] \t+ Saving no weights model pretrained processor\n",
            "INFO 06-11 11:04:31 backend.py:71] \t+ Loading no weights model from /tmp/tmpimi46wzo/no_weights_model\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:31,343\u001b[0m][\u001b[34maccelerate.utils.modeling\u001b[0m][\u001b[32mINFO\u001b[0m] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "INFO 06-11 11:04:31 backend.py:76] \t+ Saving no weights model\n",
            "INFO 06-11 11:04:39 backend.py:82] \t+ Modifying generation config for fixed length generation\n",
            "INFO 06-11 11:04:39 backend.py:86] \t+ Saving new pretrained generation config\n",
            "INFO 06-11 11:04:39 backend.py:94] \t+ Loading no weights model\n",
            "INFO 06-11 11:04:39 config.py:1130] Casting torch.float32 to torch.float16.\n",
            "INFO 06-11 11:04:39 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
            "INFO 06-11 11:04:39 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/tmp/tmpimi46wzo/no_weights_model', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=42, served_model_name=/tmp/tmpimi46wzo/no_weights_model)\n",
            "INFO 06-11 11:04:40 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n",
            "INFO 06-11 11:04:40 selector.py:51] Using XFormers backend.\n",
            "INFO 06-11 11:04:40 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n",
            "INFO 06-11 11:04:40 selector.py:51] Using XFormers backend.\n",
            "INFO 06-11 11:04:41 model_runner.py:146] Loading model weights took 7.1183 GB\n",
            "INFO 06-11 11:04:41 gpu_executor.py:83] # GPU blocks: 1865, # CPU blocks: 682\n",
            "INFO 06-11 11:04:42 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 06-11 11:04:42 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 06-11 11:04:46 model_runner.py:924] Graph capturing finished in 4 secs.\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,166\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating inference scenario\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,167\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Distributing batch size across processes\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,167\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,167\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generating Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,167\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,167\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating Text Generation kwargs with default values\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,167\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing Text Generation report\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,168\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for Inference\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,168\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up backend for Text Generation\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,662\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation memory tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,662\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking RAM memory of process [17329]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:49,662\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking VRAM memory of CUDA devices [0]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,321\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,321\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 6085.128192 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,321\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 21886.861312 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,321\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 21254.635520 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,321\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,322\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 6085.128192 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,322\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 21886.861312 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,322\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 21254.635520 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,322\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation latency tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:04:57,322\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking latency using CPU performance counter\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 378\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 9.958938 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.026346 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000212 s (0.80%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.026328 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.026469 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.026500 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.027580 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 31\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,427\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 9.259893 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.298706 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000405 s (0.14%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.298628 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.299288 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.299376 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.299385 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill throughput: 19433.397122 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,428\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode throughput: 164.040766 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,431\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmark completed successfully\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,431\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Destroying torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:05:17,447\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting rank process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:05:29,670\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Sending outputs to main process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:05:29,671\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting isolated process\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Limit gpu_memory_utilization in vLLM\n",
        "import os  \n",
        "import torch  \n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer  \n",
        "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, VLLMConfig  \n",
        "from optimum_benchmark.logging_utils import setup_logging  \n",
        "\n",
        "# setup_logging(level=\"INFO\", handlers=[\"console\"])  \n",
        "if __name__ == \"__main__\":  \n",
        "    launcher_config = TorchrunConfig(nproc_per_node=1)  \n",
        "      \n",
        "    # 设置推理配置，包括batch size、sequence length等参数  \n",
        "    scenario_config = InferenceConfig(  \n",
        "        latency=True,  \n",
        "        memory=True,  \n",
        "        warmup_runs=5,  \n",
        "        input_shapes={  \n",
        "            \"batch_size\": 1,  # 默认批处理大小  \n",
        "            \"sequence_length\": 512,  # 默认序列长度 \n",
        "            \"max_sequence_length\": 4096\n",
        "        },  \n",
        "        generate_kwargs={  \n",
        "            \"max_new_tokens\": 50,  # 默认生成的新标记的最大数量  \n",
        "            \"min_new_tokens\": 50,  # 默认生成的新标记的最小数量  \n",
        "            \"num_beams\": 1,  # 默认束搜索数量  \n",
        "            \"temperature\": 1.0,  # 默认温度  \n",
        "            \"top_k\": 50,  # 默认Top-k采样  \n",
        "            \"top_p\": 0.9,  # 默认Top-p采样  \n",
        "            \"do_sample\": True  # 启用采样模式 \n",
        "        }  \n",
        "    )  \n",
        "      \n",
        "    # 定义 vllm_config  \n",
        "    vllm_config = VLLMConfig(  \n",
        "        model=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
        "        device=\"cuda\",  \n",
        "        device_ids=\"0\",  # 确保设备ID为字符串  \n",
        "        no_weights=True,  \n",
        "        gpu_memory_utilization=0.20\n",
        "    )  \n",
        "      \n",
        "    benchmark_config = BenchmarkConfig(  \n",
        "        name=\"vllm_phi3_mini_4k_instruct\",  \n",
        "        scenario=scenario_config,  \n",
        "        launcher=launcher_config,  \n",
        "        backend=vllm_config,  \n",
        "    )  \n",
        "      \n",
        "    benchmark_report = Benchmark.launch(benchmark_config)  \n",
        "      \n",
        "    # log the benchmark in terminal  \n",
        "    benchmark_report.log()  # or print(benchmark_report)  \n",
        "      \n",
        "    # convert artifacts to a dictionary or dataframe  \n",
        "    benchmark_config.to_dict()  # or benchmark_config.to_dataframe()  \n",
        "      \n",
        "    # save artifacts to disk as json or csv files  \n",
        "    benchmark_report.save_csv(\"benchmark_reportvllm.Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n",
        "    benchmark_report.save_json(\"benchmark_reportvllm.Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "      \n",
        "    # or merge them into a single artifact  \n",
        "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)  \n",
        "    benchmark.save_json(\"benchmarkvllm.Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "    benchmark.save_csv(\"benchmarkvllm.Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:09:18,896\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Starting benchmark in isolated process\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:20,987\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Setting torch.distributed cuda device to 0\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:20,990\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing torch.distributed process group\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:21,219\u001b[0m][\u001b[34mdatasets\u001b[0m][\u001b[32mINFO\u001b[0m] - PyTorch version 2.3.0 available.\u001b[0m\n",
            "INFO 06-11 11:09:21 base.py:41] Allocating vllm backend\n",
            "INFO 06-11 11:09:21 base.py:60] \t+ Benchmarking a Transformers model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda/envs/benchmark/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-11 11:09:21 base.py:74] \t+ Using automodel class AutoModelForCausalLM\n",
            "INFO 06-11 11:09:21 base.py:78] \t+ Seeding vllm backend with seed 42\n",
            "INFO 06-11 11:09:21 backend.py:23] \t+ Creating backend temporary directory\n",
            "INFO 06-11 11:09:21 backend.py:27] \t+ Loading no weights model\n",
            "INFO 06-11 11:09:21 backend.py:90] \t+ Creating no weights model\n",
            "INFO 06-11 11:09:21 backend.py:59] \t+ Creating no weights model directory\n",
            "INFO 06-11 11:09:21 backend.py:61] \t+ Creating no weights model state dict\n",
            "INFO 06-11 11:09:21 backend.py:63] \t+ Saving no weights model safetensors\n",
            "INFO 06-11 11:09:21 backend.py:66] \t+ Saving no weights model pretrained config\n",
            "INFO 06-11 11:09:21 backend.py:68] \t+ Saving no weights model pretrained processor\n",
            "INFO 06-11 11:09:21 backend.py:71] \t+ Loading no weights model from /tmp/tmplvhtfiqb/no_weights_model\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:21,814\u001b[0m][\u001b[34maccelerate.utils.modeling\u001b[0m][\u001b[32mINFO\u001b[0m] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\u001b[0m\n",
            "INFO 06-11 11:09:22 backend.py:76] \t+ Saving no weights model\n",
            "INFO 06-11 11:09:30 backend.py:82] \t+ Modifying generation config for fixed length generation\n",
            "INFO 06-11 11:09:30 backend.py:86] \t+ Saving new pretrained generation config\n",
            "INFO 06-11 11:09:30 backend.py:94] \t+ Loading no weights model\n",
            "INFO 06-11 11:09:30 config.py:1130] Casting torch.float32 to torch.float16.\n",
            "INFO 06-11 11:09:30 config.py:1151] Downcasting torch.float32 to torch.float16.\n",
            "INFO 06-11 11:09:30 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/tmp/tmplvhtfiqb/no_weights_model', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=42, served_model_name=/tmp/tmplvhtfiqb/no_weights_model)\n",
            "INFO 06-11 11:09:30 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n",
            "INFO 06-11 11:09:30 selector.py:51] Using XFormers backend.\n",
            "INFO 06-11 11:09:30 selector.py:139] Cannot use FlashAttention-2 backend due to sliding window.\n",
            "INFO 06-11 11:09:30 selector.py:51] Using XFormers backend.\n",
            "INFO 06-11 11:09:31 model_runner.py:146] Loading model weights took 7.1183 GB\n",
            "INFO 06-11 11:09:32 gpu_executor.py:83] # GPU blocks: 12977, # CPU blocks: 682\n",
            "INFO 06-11 11:09:33 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 06-11 11:09:33 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 06-11 11:09:37 model_runner.py:924] Graph capturing finished in 4 secs.\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,041\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - Allocating inference scenario\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,041\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Distributing batch size across processes\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,041\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Creating input generator\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,041\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Generating Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,041\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing Text Generation inputs\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,042\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Updating Text Generation kwargs with default values\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,042\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Initializing Text Generation report\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,042\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Preparing backend for Inference\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,042\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Warming up backend for Text Generation\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,535\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation memory tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,536\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking RAM memory of process [18257]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:40,536\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking VRAM memory of CUDA devices [0]\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,221\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 6088.892416 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 91814.297600 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 91182.071808 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode memory:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max RAM: 6089.269248 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max global VRAM: 91814.297600 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mmemory\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t- max process VRAM: 91182.071808 (MB)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34minference\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Running Text Generation latency tracking\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:09:48,222\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Tracking latency using CPU performance counter\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,345\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,345\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 379\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,345\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 9.958572 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,345\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.026276 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,345\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000201 s (0.77%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,345\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.026256 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.026402 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.026436 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.027318 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode latency:\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ count: 31\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ total: 9.272923 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ mean: 0.299127 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ stdev: 0.000345 s (0.12%)\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p50: 0.299020 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p90: 0.299618 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p95: 0.299694 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,346\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t\t+ p99: 0.299866 s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,347\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ prefill throughput: 19485.523696 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,347\u001b[0m][\u001b[34mlatency\u001b[0m][\u001b[32mINFO\u001b[0m] - \t\t+ decode throughput: 163.810265 tokens/s\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,349\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Benchmark completed successfully\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,350\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Destroying torch.distributed process group\u001b[0m\n",
            "[RANK-PROCESS-0][\u001b[36m2024-06-11 11:10:08,369\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting rank process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:10:20,205\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Sending outputs to main process\u001b[0m\n",
            "[ISOLATED-PROCESS][\u001b[36m2024-06-11 11:10:20,205\u001b[0m][\u001b[34mtorchrun\u001b[0m][\u001b[32mINFO\u001b[0m] - \t+ Exiting isolated process\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Do not limit gpu_memory_utilization in vLLM\n",
        "import os  \n",
        "import torch  \n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer  \n",
        "from optimum_benchmark import Benchmark, BenchmarkConfig, TorchrunConfig, InferenceConfig, VLLMConfig  \n",
        "from optimum_benchmark.logging_utils import setup_logging  \n",
        "\n",
        "# setup_logging(level=\"INFO\", handlers=[\"console\"])  \n",
        "if __name__ == \"__main__\":  \n",
        "    launcher_config = TorchrunConfig(nproc_per_node=1)  \n",
        "      \n",
        "    # 设置推理配置，包括batch size、sequence length等参数  \n",
        "    scenario_config = InferenceConfig(  \n",
        "        latency=True,  \n",
        "        memory=True,  \n",
        "        warmup_runs=5,  \n",
        "        input_shapes={  \n",
        "            \"batch_size\": 1,  # 默认批处理大小  \n",
        "            \"sequence_length\": 512,  # 默认序列长度 \n",
        "            \"max_sequence_length\": 4096\n",
        "        },  \n",
        "        generate_kwargs={  \n",
        "            \"max_new_tokens\": 50,  # 默认生成的新标记的最大数量  \n",
        "            \"min_new_tokens\": 50,  # 默认生成的新标记的最小数量  \n",
        "            \"num_beams\": 1,  # 默认束搜索数量  \n",
        "            \"temperature\": 1.0,  # 默认温度  \n",
        "            \"top_k\": 50,  # 默认Top-k采样  \n",
        "            \"top_p\": 0.9,  # 默认Top-p采样  \n",
        "            \"do_sample\": True  # 启用采样模式 \n",
        "        }  \n",
        "    )  \n",
        "      \n",
        "    # 定义 vllm_config  \n",
        "    vllm_config = VLLMConfig(  \n",
        "        model=\"microsoft/Phi-3-mini-4k-instruct\",  \n",
        "        device=\"cuda\",  \n",
        "        device_ids=\"0\",  # 确保设备ID为字符串  \n",
        "        no_weights=True,  \n",
        "    #    gpu_memory_utilization=0.20\n",
        "    )  \n",
        "      \n",
        "    benchmark_config = BenchmarkConfig(  \n",
        "        name=\"vllm_phi3_mini_4k_instruct\",  \n",
        "        scenario=scenario_config,  \n",
        "        launcher=launcher_config,  \n",
        "        backend=vllm_config,  \n",
        "    )  \n",
        "      \n",
        "    benchmark_report = Benchmark.launch(benchmark_config)  \n",
        "      \n",
        "    # log the benchmark in terminal  \n",
        "    benchmark_report.log()  # or print(benchmark_report)  \n",
        "      \n",
        "    # convert artifacts to a dictionary or dataframe  \n",
        "    benchmark_config.to_dict()  # or benchmark_config.to_dataframe()  \n",
        "      \n",
        "    # save artifacts to disk as json or csv files  \n",
        "    benchmark_report.save_csv(\"benchmark_reportvllm.Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n",
        "    benchmark_report.save_json(\"benchmark_reportvllm.Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "      \n",
        "    # or merge them into a single artifact  \n",
        "    benchmark = Benchmark(config=benchmark_config, report=benchmark_report)  \n",
        "    benchmark.save_json(\"benchmarkvllm.Phi-3-mini-4k-instruct.json\")  # 保存为 JSON 文件  \n",
        "    benchmark.save_csv(\"benchmarkvllm.Phi-3-mini-4k-instruct.csv\")  # 保存为 CSV 文件  \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
