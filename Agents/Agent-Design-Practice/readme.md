# Lessons from Building Manus

***Refer to：https://medium.com/@peakji/context-engineering-for-ai-agents-lessons-from-building-manus-71883f0a67f2***

## 1. KV-Cache 缓存复用优化

### 🌟 介绍与目的

KV-Cache（Key-Value Cache，键值缓存）是大模型推理过程中的一种加速机制。它的本质是“记住已经算过的东西，不再白白重算一遍”。如果能充分利用KV-Cache，可以极大降低响应时间和服务器成本，直接让AI和用户体验“飞起来”。

### 🎯 要解决的核心问题

- **上下文越来越长**：Agent带着越来越多“历史”往前推，每一步都要把所有历史喂进大模型。
- **推理单价上升**：每一步的输入token上万，输出却可能只有10个，传一堆历史只为模型“回忆”，成本惊人。
- **缓存失效风险**：只要上下文哪怕一丁点不同（哪怕是时间戳、空格、无关顺序），所有缓存都没了，真金白银的算力全作废。

### 🔎 常见误区与致命陷阱

**最大陷阱就是在系统提示、首条上下文里放“动态内容”**，比如：

- 精确到秒的时间戳
- 会话/用户唯一ID
- 随机种子、顺序号
- 工具描述顺序每次都不一样（JSON无序）

这些都会让“明明99%内容一样”的两个请求，缓存根本无法复用，每次都变成全新长文本推理。

**现实案例**

> 之前有团队为“让AI能说出当天日期”，在system prompt里塞`"当前时间:{datetime.now()}"`，结果缓存命中率从98%→<5%，每月云账多几万美元。

### 🛠️ 正确工程实现

1. **严格锁定提示前缀内容，一字不变**：

   ```
   SYSTEM_PROMPT = "你是专业AI助手。请严格遵循JSON格式回复。"
   # 千万别放时间、session-id等
   ```

   

2. **动态信息用“工具调用”获取**：

   - 用户要时间，像function-calling这样让模型自己发起 get_time 工具调用，再把`{"now": "2025-01-01T12:00:00"}`这样的结果，追加到最后一条上下文里。
   - 这不会影响前缀，不破坏缓存。

3. **所有长内容（如工具JSON、schema）序列化时必须保证顺序唯一**：

   ```
   import json
   tool_schema = json.dumps(tools, sort_keys=True)
   ```

   

4. **上下文只能“追加”，禁止插空、删除、回滚**

   - 永远只能一行一行往后接，不能回到前面“补一刀”。

5. **多并发或分布式时，用session-id让同一请求总是命中同一worker**（不然缓存跨worker无效）。

### ✅ 成功效果

- 有效缓存命中率能从10%提升到95%+
- 基本保证无论历史多长，推理毫秒响应，单步成本降一两个数量级
- 大量并发日常流量不再波动

------

## 2. Logits掩码 控制工具动态

### 🌟 介绍与目的

Logits掩码（Logits Masking）是一种“只让模型在解码时看到允许的选项，其它都相当于‘视而不见’”的技术。**不删除、不裁剪、不干扰物理内容，只在选择分数阶段就干掉不该出现的选项**。

### 🎯 要解决的核心问题

- 随着Agent能力增强，工具数量爆炸，不同场景其实只应让模型见到一小部分工具。
- 传统“每次把无用工具描述直接在上下文删除/裁剪”，会导致“前文变动”→“缓存失效”。
- 某些历史动作用了“旧工具”，你现在删了描述，模型会懵逼。

### 🔎 常见误区或错误做法

- 每遇到不同场景就先删掉几十个工具的提示，然后拼一个新prompt。这会每次都让cache命中率降到0。
- 直接物理删除内容，忽视前后文实际引用关系。

### 🛠️ 正确工程实现

1. **全量保留所有工具描述于上下文**，无论何时何地，一字都不删。

2. **用模型API的 logit_bias 功能，对所有处于“禁用状态”的工具名首token，打上极低分数（如-100）**。这样模型即使“看见”它，也不会“选中”它。

3. **正确区分单token工具和多token工具**。一般对“首token”掩码即可，极端严谨可所有token都掩码。

   示例（OpenAI API）：

   ```
   def mask_tools(allowed_prefix="browser_"):
       return {token_id: -100 for tool,token_id in all_tool_token_ids.items()
               if not tool.startswith(allowed_prefix)}
   ```

   

4. 状态切换时，只变更掩码映射表，不更动上下文。

### ✅ 成功效果

- 动态管理工具变成毫秒级，不会破坏历史缓存。
- 系统性能受“工具数量”影响极小。
- 工具引用一致性始终保持。

------

## 3. 文件系统即上下文（外化记忆）

### 🌟 介绍与目的

大模型上下文再大，总有上线（如ChatGPT上下文128K token）。用户的输入和历史、抓取网页、读大PDF很容易突破限制。文件系统即上下文，就是**让模型通过工具主动将大文本存文件、只留下便于索引的路径在上下文里**。

### 🎯 要解决的核心问题

- 遇到大文本，例如网页、PDF、代码，无法全都塞进prompt。
- 只靠摘要或切片，万一丢掉关键细节，后续任务就挂了。
- 千万不要让模型依赖“自己全记住”，让它也能“查字典”。

### 🔎 常见误区或错误做法

- 直接把长网页全文、十几页PDF塞prompt，导致超窗口或推理慢、压缩成本高。
- 压缩摘要不可逆（删了就找不回），模型忘掉重要信息。

### 🛠️ 正确工程实现

1. **定义 file_write / file_read 工具**，允许模型自己把大文本存盘，然后只生成简短的存储索引（如：“已保存至file-xxx”）。
2. **在上下文只留【路径+简要摘要】，正文本去文件“做记忆”**，几千字只消耗几十token。
3. **后续任何任务，都能通过file_read工具取回原文而不会丢失信息**。

### ✅ 成功效果

- 长链任务的上下文长度保持线性。
- 支持“任意多历史/大文件”零信息丢失。
- 推理代价恒定，体验流畅。

------

## 4. Recitation“背诵锚定”

### 🌟 介绍与目的

LLM做长任务时，注意力主要集中在输入的“尾部”；离输出越远，模型记忆力越差。“Recitation”就是让Agent**反复在上下文末尾“背诵”目标(todo)，把目标时刻推到模型记忆窗口“新鲜区”。**

### 🎯 要解决的核心问题

- 长流程、复杂规划情况下，模型开始容易后面就丢三落四。
- “lost-in-the-middle”问题，目标记不住，做事完全变成“习惯性搬砖”。

### 🔎 常见误区和失败案例

- 认为“只要目标写在很早的位置就够了”；实际上长prompt超过几千token后前面的内容几乎不起作用。
- 每步执行完不总结当下目标，模型每次只关注局部历史，目标漂移极易发生。

### 🛠️ 正确工程实现

1. **把总目标/当前计划维护到todo.md等固定“外部文件”里**，模型能查能写。
2. **每走一步就自动将todo的最新内容摘要成3-5行再贴到上下文结尾**。
3. 确保无论推进了多少步，模型推理时总能“读到”核心目标。

### ✅ 成功效果

- 目标遗忘率、跑偏率大幅下降。
- 对话长期连贯性稳定提升。
- 批处理/复杂工程流极大降本。

------

## 5. 错误留痕（自适应自我修正）

### 🌟 介绍与目的

大循环、复杂任务一定会犯错。留痕的策略就是**不掩饰失败，而要让模型时刻“看到曾经摔过的坑”——让模型的“记忆”里包含错误与其后果，实现经验累积。**

### 🎯 要解决的核心问题

- “无脑reset”导致同样的错误一犯再犯；不学习的AI=死板工程。
- 如果每轮错误都被try/except吞掉或清理，模型无法通过上下文学习“这里有坑”。

### 🔎 常见误区和反例

- 每次工具报错只做retry/回滚，不让LM看到真实堆栈/报错内容。
- 担心展示失败让上下文“污染”，反而丢掉最有价值的反馈信号。

### 🛠️ 正确工程实现

1. **遇到任何错误都原样把堆栈、报错、输入、错误类型等详细信息“追加到上下文”**，禁止精简/清理。
2. **复盘日志格式需清晰显示错误状态**，模型看到有“失败”几率会主动避开类似操作。
3. **链路生命周期结束时，保留所有错误，不因重试而清空任何trace**。

### ✅ 成功效果

- 系统性错误大幅减少同一位置重试次数。
- 工具调用越智能，后续整体成功率越高。
- 工程师debug更容易定位根因。

------

## 6. 反Few-Shot同质化（多样化防模式陷阱）

### 🌟 介绍与目的

Few-Shot本是模型“示范学习”。但批处理或流水线任务中，**过于统一/标准的上下文样例其实是给模型挖坑——它会过度模仿而忘了自我决策。**

### 🎯 要解决的核心问题

- AI容易产生“顺拐模式”，看到前面都是一样的样板行为，就无脑复制粘贴，不考虑个体差异。
- “节奏锁定”一旦发生，后面的20条输出翻车概率极高。

### 🔎 常见误区

- 追求极致规范导致上下文示例高度复制粘贴。
- 没有主动引入“模板多样化”“同义词变换”“顺序扰动”。

### 🛠️ 正确工程实现

1. **准备3-5套不同的模板，记录观测和动作时随机选用。**

2. **对于JSON等结构体要定期扰动key顺序（或补充冗余字段使格式不能死板）。**

3. **动作命令/解释里可以安排轻度“语义漂移”，比如用“取得”替代“获取”，让模型学会分辨而非机械套壳。**

   示例代码：

   ```
   templates = ["输出如下:\n{obs}", ">> {obs}", "[RESULT]\n{obs}"]
   def obs_record(obs):
       return random.choice(templates).format(obs=obs)
   ```

   

### ✅ 成功效果

- 集合稳定度显著提升，极大降低“第二次起幻觉”风险。
- 长链任务不再出现中后程“格式溃散”。

------

## 总结Tips

- 这些技术点不是“炫技”，全部是在踩坑（损失金钱or时间）的真实场景中演化出来的。
- 任意一个没做到，都有可能导致实际生产崩盘、不稳定或成本骤增。
- 六条原则互相关联，代理系统每个环节越规范，总体表现提升是“乘法”级别的。

