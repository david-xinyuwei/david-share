# Reducing-LLM-Hallucinations

Hallucination refers to instances where AI models generate text that, while grammatically correct and seemingly plausible, is not based on the given input and may even be factually incorrect.

## Why Do LLMs Generate Hallucinations?

As mentioned, language models may hallucinate and produce outputs containing fabricated or erroneous responses. These errors highlight the limitations of AI, underscoring the importance of human supervision and cross-referencing with reliable sources for verification. However, assigning humans to verify every response is neither feasible nor scalable. We will discuss hallucination mitigation strategies later, but first, let's explore why LLMs generate hallucinations:

- **Insufficient Training Data**: A model that has not encountered diverse data during training may fail to establish accurate correlations between input and output, leading to hallucinated content.
- **Lack of Supervision**: Without proper guidance, the model may overly rely on its internal logic, resulting in seemingly hallucinatory outputs.
- **Model Overfitting**: Overfitting to the training data can cause the model to generate outputs similar to the training set but inconsistent with new or different inputs.
- **Knowledge Cutoff**: LLMs like ChatGPT have a knowledge cutoff date and are unaware of information beyond that date. They might unknowingly respond with outdated information, which is no longer relevant.

## Types of LLM Hallucinations


We can categorize these hallucinations into three main types:

### Factually Inaccurate

This type of hallucination occurs when the language model presents information that is untrue or incorrect but framed as fact. This includes dates, events, statistics, or verifiable misstatements. It may occur for various reasons, including misinterpretation of input data, low-quality data and training methods, reliance on outdated or incorrect sources, or mixing different background information leading to inaccurate outputs.

### Fabricated Citations or Sources

This occurs when the language model fabricates citations or references. It might generate a statement and wrongly attribute it to a real person or create a fictitious source that does not exist. This is problematic as it can lead to misinformation, misattributed statements, and confusion.

### Logical Inconsistencies

This includes generating responses that are internally inconsistent or logically flawed. After generating a response to a user's query, the LLM might contradict itself in subsequent responses. When the model makes a series of statements, and these statements are incoherent or contradictory when combined, it challenges the credibility of the model's outputs and confuses users who rely on its consistency.

In all these cases, the language model is not intentionally misleading but demonstrates its limitations due to various factors such as training data, data quality, knowledge cutoff dates, poor fine-tuning, etc.

## LLM Hallucination Mitigation Strategies: RAG, Pre-Generation Strategies, Post-Generation Strategies


Researchers are developing various methods to ensure the accuracy of responses generated by LLMs. Some strategies require human intervention, such as Reinforcement Learning with Human Feedback (RLHF); using high-quality data for fine-tuning; employing RAG, etc.

### Retrieval-Augmented Generation (RAG)

- Self-RAG：

   Self-RAG enables LLMs to dynamically retrieve relevant passages until the entire context is captured, all within a specified window.

  - Technical Implementation:
    - **Initial Retrieval**: Upon receiving a user's query, the model performs an initial retrieval to gather relevant passages from an external knowledge base.
    - **Generate Preliminary Response**: Based on the initially retrieved passages, the model generates a preliminary response.
    - **Dynamic Retrieval**: The model evaluates the completeness and accuracy of the preliminary response. If more information is needed, it performs further retrieval to gather more relevant passages.
    - **Iterative Process**: Repeat steps 2 and 3 until the model deems sufficient context has been acquired to generate an accurate response.
    - **Final Response**: Based on all retrieved passages, generate the final response.

- **Multimodal RAG**: Multimodal RAG combines text data with images and other media to provide a deeper contextual understanding, resulting in more accurate and relevant responses.

  Apart from RAG, we can divide these strategies into two parts: pre-generation strategies and post-generation strategies.

### Pre-Generation Strategies

Pre-generation strategies prevent AI from generating incorrect or misleading information in the first place. These include:

1. Chain of Verification (CoVe): Involves the model self-verifying its responses. Multi-stage verification makes it more efficient.


2. Optimisation by Prompting (OPRO): LLMs optimize their own prompts, correcting prompt inputs.


3. System 2 Attention (S2A): This approach improves LLM's reasoning. An instruction-tuned LLM is used to identify, analyze, and extract the most relevant parts of the input context, mitigating the impact of unnecessary information.

4. EmotionPrompt: This technique uses emotional cues through prompts, enabling LLMs to gain more context and emotional insight.


5. Step-Back Prompting: A method to enhance LLM reasoning and problem-solving skills.


6. Rephrase and Respond (RaR): This technique allows LLMs to rephrase and expand on human-posed questions/prompts, helping LLMs gain insightful context.

### Post-Generation Strategies

Post-generation strategies involve verifying and correcting AI's output after generation. These include:

1. **Fact-Checking**: Implementing human-in-the-loop (HITL) and knowledge bases to verify the accuracy of the information provided by LLMs.

2. **Preference Alignment**: Using human feedback mechanisms (RLHF) to align LLM outputs with human values and preferences. Refer to my repo: * https://github.com/xinyuwei-david/david-share/tree/master/Deep-Learning/LLM-Alignment-DPO-RHLF-CPO*

   These strategies aim to enhance the reliability of AI systems, improve the quality of their outputs, and ensure alignment with human values and factual accuracy.



## Pre-Generation Strategies Code

### Chain of Verification (CoVe):

Refer to ：https://github.com/ritun16/chain-of-verification/tree/main

![CoVe_Architecture](https://private-user-images.githubusercontent.com/44939374/273401645-3efc0f5a-b7c6-4655-8a0e-e16c01cac97e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU0NTMwNTUsIm5iZiI6MTcyNTQ1Mjc1NSwicGF0aCI6Ii80NDkzOTM3NC8yNzM0MDE2NDUtM2VmYzBmNWEtYjdjNi00NjU1LThhMGUtZTE2YzAxY2FjOTdlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA0VDEyMjU1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc0NDZkYTg1YzkxZDYyMWQ5NDFkZTQ2NmYxYjc4NzNmY2MyOGY0NGU0OGM5YWViYWE2NjBkZDZiNDNjMmI1NTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9dcOdD_i8pLqGc-BOE3n4xoUOFiDBJps_aHZn04iyGM)

These five source code files implement a system called "Chain of Verification (CoVE)" that generates more accurate answers by verifying and refining initial responses through multiple steps. Below is the implementation logic for each file:

#### 1. `cove_chains.py`

This file defines three main chain classes, each implementing a specific type of question handling logic:

- **WikiDataCategoryListCOVEChain**: Handles questions that require listing entities (e.g., names, places).

- **MultiSpanCOVEChain**: Handles questions that contain multiple independent answers.

- **LongFormCOVEChain**: Handles questions that require long answers.

  Each chain class follows a similar structure:

1. **Baseline Response Chain**: Generates the initial response.
2. **Verification Question Generation Chain**: Generates verification questions based on the initial response.
3. **Execute Verification Chain**: Executes the verification questions to obtain verification answers.
4. **Final Refined Response Chain**: Generates the final refined response based on the verification answers.

#### 2. `execute_verification_chain.py`

This file defines the `ExecuteVerificationChain` class, which executes verification questions and generates verification answers. The main logic includes:

1. **Search Tool**: Uses the DuckDuckGo search tool to find answers to verification questions.
2. **LLM Self-Evaluation**: Uses a language model (LLM) to generate verification answers.
3. **Combining Answers**: Combines verification questions and answers into the final output.

#### 3. `main.py`

This file is the entry point of the program, responsible for parsing command-line arguments and running the appropriate chain. The main logic includes:

1. **Parsing Command-Line Arguments**: Retrieves the user's question, LLM name, temperature, max tokens, etc.
2. **Initializing LLM**: Creates an instance of ChatOpenAI.
3. **Routing Chain Instance**: Creates and runs an instance of `RouteCOVEChain`.
4. **Outputting Results**: Prints the final answer and intermediate steps (if required).

#### 4. `prompts.py`

This file defines various prompt templates used to generate prompts at different stages. These templates include:

1. **Baseline Prompts**: Used to generate the initial response.
2. **Verification Question Prompts**: Used to generate verification questions.
3. **Execute Verification Prompts**: Used to execute verification questions.
4. **Final Refined Prompts**: Used to generate the final refined response.
5. **Router Prompts**: Used to classify the type of question.

#### 5. `route_chain.py`

This file defines the `RouteCOVEChain` class, which selects the appropriate chain based on the question type. The main logic includes:

1. **Initializing Chains**: Creates instances of `WikiDataCategoryListCOVEChain`, `MultiSpanCOVEChain`, and `LongFormCOVEChain`.
2. **Routing Logic**: Uses the LLM to select the appropriate chain based on the question content.
3. **Error Handling**: Uses a default `ConversationChain` if classification fails.

#### Summary

The overall workflow of the system is as follows:

1. The user inputs a question.

2. `RouteCOVEChain` selects the appropriate chain based on the question content.

3. The selected chain generates the initial response, verification questions, and verification answers through multiple steps, ultimately producing a refined response.

4. The final answer and intermediate steps (if required) are output to the user.

   This system aims to improve the accuracy and reliability of answers through a multi-step verification and refinement mechanism.

###  Optimisation by Prompting (OPRO)

 LLMs optimize their own prompts, correcting prompt inputs.

*https://cobusgreyling.medium.com/a-new-prompt-technique-from-deepmind-called-optimisation-by-prompting-opro-918b1057eacd*OPRO (Optimisation by PROmpting) is essentially a method to improve the performance of large language models (LLMs) by optimizing prompts. In simple terms, its core ideas are:

1. **Multiple Attempts**: Generate multiple different answers or solutions each time, rather than just one.

2. **Gradual Improvement**: Through multiple iterations, adjust and improve the prompts based on previous results, gradually finding better prompt formats.

3. **No Fine-Tuning Required**: It significantly improves the model's performance without the need for complex fine-tuning of the model itself, just by adjusting the input prompts.

   It's like continuously trying different answering strategies in an exam until you find the most effective one. OPRO finds the most suitable prompts for specific tasks and models by continuously optimizing the prompts, thereby improving the model's accuracy and reliability.

Let's look at a example:

Meta-Prompt = Meta-Instructions + Solution-Score Pairs + Meta-Instructions + Optimisation Task & Output Format + Meta-Instructions

#### Meta-Instructions

```
I have some texts along with their corresponding scores. 
The texts are arranged in ascending order based on their scores, 
where higher scores indicate better quality.
```

#### Solution-Score Pairs

```
text:
Let’s figure it out! score:
61

text:
Let’s solve the problem. score:
63

(... moreinstructionsandscores...)
```

#### Meta-Instructions

```
The following exemplars show how to apply your text: 
you replace <INS> in each input with your text, then read the input 
and give an output. We say your output is wrong if your output is 
different from the given output, and we say your output 
is correct if they are the same.
```

#### Optimisation Task & Output Format

```
input:
Q: Alannah, Beatrix, and Queen are preparing for the new school year and 
have been given books by their parents. 
Alannah has 20 more books than Beatrix. 
Queen has 1/5 times more books than Alannah. 
If Beatrix has 30 books, how many books do the three have together?
A: <INS>
output:
140

(... more exemplars ...)
```

#### Meta-Instructions

```
Write your new text that is different from the old ones and has a score 
as high as possible. Write the text in square brackets.
```

#### The Complete Prompt Concatenated:

```
I have some texts along with their corresponding scores. 
The texts are arranged in ascending order based on their scores, 
where higher scores indicate better quality.

text:
Let’s figure it out! score:
61

text:
Let’s solve the problem. score:
63

(... moreinstructionsandscores...)

The following exemplars show how to apply your text: 
you replace <INS> in each input with your text, then read the input 
and give an output. We say your output is wrong if your output is 
different from the given output, and we say your output 
is correct if they are the same.

input:
Q: Alannah, Beatrix, and Queen are preparing for the new school year and 
have been given books by their parents. 
Alannah has 20 more books than Beatrix. 
Queen has 1/5 times more books than Alannah. 
If Beatrix has 30 books, how many books do the three have together?
A: <INS>
output:
140

(... more exemplars ...)

Write your new text that is different from the old ones and has a score 
as high as possible. Write the text in square brackets.
```

The meta-prompt contains two core pieces of information:

1. The previously generated prompts with their corresponding training accuracies.
2. The optimisation problem description, which includes several exemplars randomly selected from the training set to exemplify the task of interest.

### System 2 Attention (S2A):

*https://jrodthoughts.medium.com/inside-system-2-attention-meta-ai-new-method-to-improve-reasoning-in-llms-4424751a6be1*

Large language models (LLMs) like ChatGPT are very smart, but sometimes they make simple mistakes. This is because they can easily be influenced by irrelevant information or biases in the input. For example, if you ask a question with some unrelated details, the AI might get misled by these details and give the wrong answer.

#### System 2 Attention (S2A) Method

The S2A method is inspired by the way humans think. Humans have two thinking modes:

1. **System 1**: Fast but prone to errors, intuitive thinking.

2. **System 2**: Slow but more accurate, deliberate thinking.

   S2A aims to make the AI think like human "System 2," focusing on important information and ignoring irrelevant details.

#### How S2A Works

1. **Clean the Context**: First, S2A will clean the input you give to the AI (like a question), removing any irrelevant information that might mislead the AI. It's like when you solve a problem, you remove the unnecessary parts and keep only the key information.
2. **Generate the Answer**: Then, the AI uses this cleaned input to generate the answer. This way, the AI won't be distracted by irrelevant information, and the answer will be more accurate.

#### An Example

Suppose you ask the AI a question: "Xiaoming has 10 apples, he ate 2 apples, and then he bought 5 more apples. Xiaoming likes blue apples, and his friend Xiaohong also likes apples. How many apples does Xiaoming have now?"

A regular AI might get confused by the irrelevant information like "Xiaoming likes blue apples" and "Xiaohong also likes apples," and give the wrong answer.

But an AI using the S2A method will first remove this irrelevant information, turning the question into: "Xiaoming has 10 apples, he ate 2 apples, and then he bought 5 more apples. How many apples does Xiaoming have now?" This way, the AI can more accurately answer: "13 apples."



```
Given the following text by a user, extract the part that is unbiased and not their opinion, so that using that text alone would be good context for providing an unbiased answer to the question portion of the text. 
  
Please include the actual question or query that the user is asking. Separate this into two categories labeled with “Unbiased text context (includes all content except user’s bias):” and “Question/Query (does not include user bias/preference):”.  
  
Text by User: [ORIGINAL INPUT PROMPT]  
```

![img](https://miro.medium.com/v2/resize:fit:1050/1*9ND1Ju9HtzNc3eeK4M2qoQ.png)

#### Experimental Results

Meta AI's research shows that after using the S2A method, the AI performs better in answering factual questions, generating long-form arguments, and solving math problems.

#### Summary

The S2A method makes the AI think like human "System 2," focusing on important information and ignoring irrelevant details, thereby improving the AI's accuracy and reasoning ability. I hope this explanation helps you!

### motionPrompt: 

#### This technique uses emotional cues through prompts, enabling LLMs to gain more context and emotional insight.

*https://www.linkedin.com/pulse/tap-ais-emotional-edge-utilizing-emotionprompt-improved-patrick-bands-jrqzf*

#### Definition of EmotionPrompt


**EmotionPrompt** is a technique designed to enhance the performance of Large Language Models (LLMs) by incorporating emotional stimuli into prompts. Specifically, EmotionPrompt involves adding emotional expressions or cues to the input prompts, aiming to improve the quality and accuracy of the generated responses. This technique leverages psychological phenomena such as self-monitoring, social cognitive theory, and cognitive emotion regulation to guide LLMs towards generating more positive, confident, and effective responses.

#### Implementation Principles


**Self-monitoring**:

- **Principle**: Individuals regulate and control their behavior in social situations. Emotional cues prompt the model to "perceive" these cues and adjust its output to align with expected emotional and social norms.

- **Effect**: Generates responses that better meet human expectations, improving the quality and naturalness of the responses.

  **Social Cognitive Theory**:

- **Principle**: Individuals learn by observing and imitating others' behaviors and regulate their actions through self-efficacy (confidence in their abilities). Emotional cues can enhance the model's "self-efficacy."

- **Effect**: Generates more positive and effective responses, enhancing user experience.

  **Cognitive Emotion Regulation**:

- **Principle**: Individuals regulate their emotional responses through cognitive reappraisal (reinterpreting situations). Emotional cues can guide the model to perform similar cognitive reappraisal.

- **Effect**: Generates more accurate and appropriate responses in complex or emotional situations.

#### Suitable Scenarios

1. **Dialogue Systems**: Enhances the interaction quality of chatbots or virtual assistants, making their responses more human-like and emotional.
2. **Text Generation**: Generates more emotionally engaging and appealing text in writing assistance tools.
3. **Sentiment Analysis**: Helps the model more accurately understand and analyze emotions in text.
4. **Customer Service**: Improves the response quality of automated customer service systems, making them more empathetic and emotionally considerate.
5. **Education and Training**: Provides more motivational and supportive feedback in educational platforms.

#### Unsuitable Scenarios

1. **Mathematical Problems**: Mathematical problems typically require precise and logical answers rather than emotional responses. Emotional cues in this context may not significantly improve the model's performance and could even distract, affecting accuracy.
2. **Technical Issues**: For technical questions requiring precise, objective answers, such as programming debugging or scientific calculations, emotional cues may not help improve the response quality.

#### Examples of Good EmotionPrompts


**Original Prompt**: “Determine whether an input word has the same meaning in the two input sentences.”

- **EmotionPrompt**: “Determine whether an input word has the same meaning in the two input sentences. This is very important to my career.”

- **Explanation**: Adding the emotional cue "This is very important to my career" encourages the model to be more careful and thorough in its judgment.

  **Original Prompt**: “Determine whether a movie review is positive or negative.”

- **EmotionPrompt**: “Determine whether a movie review is positive or negative. Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.”

- **Explanation**: Adding encouraging emotional cues enhances the model's confidence and positivity, potentially improving the response quality.

  **Original Prompt**: “Select the correct indicator to use.”

- **EmotionPrompt**: “Select the correct indicator to use. Are you sure of your answer? It might be worth another review.”

- **Explanation**: Adding the emotional cue "Are you sure of your answer? It might be worth another review" prompts the model to self-check and adjust, reducing errors.



### Step-Back Prompting

#### A method to enhance LLM reasoning and problem-solving skills.

![img](https://miro.medium.com/v2/resize:fit:1050/1*LlqCsOkwZC6T3OSA-qJk6w.png)****

*https://medium.com/@akriti.upadhyay/enhancing-llms-reasoning-with-step-back-prompting-47fad1cf5968*

**Step-Back Prompting** is a technique used to enhance the reasoning and problem-solving capabilities of LLMs. It involves encouraging the LLM to take a step back from a given question or task and pose a more abstract, higher-level question that encompasses the essence of the original inquiry. This helps the LLM structure its reasoning more effectively by focusing on broader concepts or principles.

#### Process of Step-Back Prompting

 

1. **Abstraction**: The LLM first asks a more general question about a bigger idea or rule instead of directly answering the original question. This helps it think and find relevant facts.
2. **Reasoning**: After obtaining answers to the general question, the LLM uses this information to think about and answer the original question. This is called "Abstraction-grounded Reasoning."

#### Example


**Original Question**: "What is the name of the rover that NASA landed on Mars in 2021?" (NASA在2021年登陆火星的探测器叫什么名字？)

- **Characteristics**: This question is very specific and requires the model to provide a particular answer (i.e., the name of the rover).

- **Challenges**: If the model does not directly remember this specific information, it may struggle to answer accurately.

  **Step-Back Question**

  **Question**: "What rovers has NASA sent to Mars?" (NASA发送到火星的探测器有哪些？)

- **Characteristics**: This question is broader and asks the model to list all the rovers NASA has sent to Mars.

- **Advantages**: By answering a broader question, the model can more easily retrieve relevant information and then deduce the answer to the original question.

#### Specific Differences and Advantages

1. Information Retrieval Scope:
   - **Original Question**: Requires the model to directly retrieve information about a specific year and mission.
   - **Step-Back Question**: Allows the model to retrieve a broader range of relevant information (all Mars rovers) and then find the specific answer.
2. Reasoning Process:
   - **Original Question**: The model needs to directly remember or find the specific answer.
   - **Step-Back Question**: The model can list all relevant rovers and then filter out the correct answer based on time and mission.
3. Error Reduction:
   - **Original Question**: If the model does not directly remember the answer, it may make a mistake.
   - **Step-Back Question**: By retrieving a broader range of information, the model can reduce the likelihood of omissions or errors.

### Rephrase and Respond (RaR)

#### This technique allows LLMs to rephrase and expand on human-posed questions/prompts, helping LLMs gain insightful context.

*https://vidrihmarko.medium.com/rar-prompt-rephrase-and-respond-is-ai-s-new-superpower-9931edf84ec5*

![img](https://miro.medium.com/v2/resize:fit:1050/1*TTepBsVL6Pvk1cSOMAaViA.png)

![img](https://miro.medium.com/v2/resize:fit:1050/1*lVBlwVOdfzm6bopnNoOD3w.png)