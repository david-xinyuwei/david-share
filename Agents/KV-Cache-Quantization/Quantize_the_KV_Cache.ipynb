{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysuWpTLdWJto",
        "outputId": "da492f71-bc95-4807-cb8e-cdab2878d007"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers hqq bitsandbytes transformers datasets accelerate\n",
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0fe449ba18644b0b42d1e05d5b20f9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before tokenization and model generation:\n",
            "Allocated GPU memory: 5.31 GB\n",
            "Reserved GPU memory: 5.40 GB\n",
            "After tokenization:\n",
            "Allocated GPU memory: 5.31 GB\n",
            "Reserved GPU memory: 5.40 GB\n",
            "After model generation:\n",
            "Allocated GPU memory: 5.32 GB\n",
            "Reserved GPU memory: 29.34 GB\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch  \n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig  \n",
        "import os  \n",
        "  \n",
        "# Define multiple prompts for batch processing  \n",
        "base_prompts = [  \n",
        "    \"The best tomato sauce is\",  \n",
        "    \"Artificial intelligence is\",  \n",
        "    \"The future of technology is\",  \n",
        "    \"Climate change is\",  \n",
        "    \"The benefits of exercise are\",  \n",
        "    \"Healthy eating includes\",  \n",
        "    \"The importance of sleep is\",  \n",
        "    \"The universe is\",  \n",
        "    \"Machine learning is\",  \n",
        "    \"Renewable energy sources are\",  \n",
        "]  \n",
        "  \n",
        "# Multiply the prompts by 20 to increase the batch size  \n",
        "prompts = base_prompts * 20  \n",
        "  \n",
        "model_id = \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"  \n",
        "  \n",
        "bnb_config = BitsAndBytesConfig(  \n",
        "    load_in_4bit=True,  \n",
        "    bnb_4bit_quant_type=\"nf4\",  \n",
        "    bnb_4bit_compute_dtype=torch.float16,  \n",
        "    bnb_4bit_use_double_quant=True,  \n",
        ")  \n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)  \n",
        "  \n",
        "# Set padding token  \n",
        "if tokenizer.pad_token is None:  \n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  \n",
        "  \n",
        "model = AutoModelForCausalLM.from_pretrained(  \n",
        "    model_id,   \n",
        "    trust_remote_code=True,   \n",
        "    quantization_config=bnb_config,   \n",
        "    attn_implementation=\"flash_attention_2\",   \n",
        "    torch_dtype=torch.float16,   \n",
        "    device_map=\"cuda:0\"  \n",
        ")  \n",
        "  \n",
        "# Resize token embeddings to match the tokenizer's vocabulary size  \n",
        "model.resize_token_embeddings(len(tokenizer))  \n",
        "  \n",
        "def print_gpu_memory_usage():  \n",
        "    allocated_memory = torch.cuda.memory_allocated()  \n",
        "    reserved_memory = torch.cuda.memory_reserved()  \n",
        "    print(f\"Allocated GPU memory: {allocated_memory / (1024**3):.2f} GB\")  \n",
        "    print(f\"Reserved GPU memory: {reserved_memory / (1024**3):.2f} GB\")  \n",
        "  \n",
        "print(\"Before tokenization and model generation:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Tokenize the prompts with batch processing  \n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)  \n",
        "  \n",
        "print(\"After tokenization:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Generate outputs for the batch of prompts  \n",
        "outputs = model.generate(  \n",
        "    **inputs,   \n",
        "    max_new_tokens=1000,   \n",
        "    cache_implementation=\"quantized\",   \n",
        "    cache_config={\"backend\": \"HQQ\", \"nbits\": 4, \"q_group_size\": 128, \"residual_length\": 64, \"device\": model.device}  \n",
        ")  \n",
        "  \n",
        "print(\"After model generation:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Decode the outputs for each prompt in the batch  \n",
        "results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]  \n",
        "  \n",
        "# Print the results  \n",
        "#for i, result in enumerate(results):  \n",
        "#    print(f\"Result for prompt {i+1}: {result}\")  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56879cb0658f49279355935e766ae413",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before tokenization and model generation:\n",
            "Allocated GPU memory: 5.31 GB\n",
            "Reserved GPU memory: 5.40 GB\n",
            "After tokenization:\n",
            "Allocated GPU memory: 5.31 GB\n",
            "Reserved GPU memory: 5.40 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda/envs/kvcache/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After model generation:\n",
            "Allocated GPU memory: 5.32 GB\n",
            "Reserved GPU memory: 18.94 GB\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch  \n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig  \n",
        "import os  \n",
        "\n",
        "  \n",
        "# Define multiple prompts for batch processing  \n",
        "base_prompts = [  \n",
        "    \"The best tomato sauce is\",  \n",
        "    \"Artificial intelligence is\",  \n",
        "    \"The future of technology is\",  \n",
        "    \"Climate change is\",  \n",
        "    \"The benefits of exercise are\",  \n",
        "    \"Healthy eating includes\",  \n",
        "    \"The importance of sleep is\",  \n",
        "    \"The universe is\",  \n",
        "    \"Machine learning is\",  \n",
        "    \"Renewable energy sources are\",  \n",
        "]  \n",
        "  \n",
        "# Multiply the prompts by 20 to increase the batch size  \n",
        "prompts = base_prompts * 20  \n",
        "  \n",
        "model_id = \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"  \n",
        "  \n",
        "bnb_config = BitsAndBytesConfig(  \n",
        "    load_in_4bit=True,  \n",
        "    bnb_4bit_quant_type=\"nf4\",  \n",
        "    bnb_4bit_compute_dtype=torch.float16,  \n",
        "    bnb_4bit_use_double_quant=True,  \n",
        ")  \n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)  \n",
        "  \n",
        "# Set padding token  \n",
        "if tokenizer.pad_token is None:  \n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  \n",
        "  \n",
        "model = AutoModelForCausalLM.from_pretrained(  \n",
        "    model_id,   \n",
        "    trust_remote_code=True,   \n",
        "    quantization_config=bnb_config,   \n",
        "    attn_implementation=\"flash_attention_2\",   \n",
        "    torch_dtype=torch.float16,   \n",
        "    device_map=\"cuda:0\"  \n",
        ")  \n",
        "  \n",
        "# Resize token embeddings to match the tokenizer's vocabulary size  \n",
        "model.resize_token_embeddings(len(tokenizer))  \n",
        "  \n",
        "def print_gpu_memory_usage():  \n",
        "    allocated_memory = torch.cuda.memory_allocated()  \n",
        "    reserved_memory = torch.cuda.memory_reserved()  \n",
        "    print(f\"Allocated GPU memory: {allocated_memory / (1024**3):.2f} GB\")  \n",
        "    print(f\"Reserved GPU memory: {reserved_memory / (1024**3):.2f} GB\")  \n",
        "  \n",
        "print(\"Before tokenization and model generation:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Tokenize the prompts with batch processing  \n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)  \n",
        "  \n",
        "print(\"After tokenization:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Generate outputs for the batch of prompts  \n",
        "outputs = model.generate(  \n",
        "    **inputs,   \n",
        "    max_new_tokens=1000,   \n",
        "    cache_implementation=\"quantized\",   \n",
        "    cache_config={\"backend\": \"quanto\", \"nbits\": 4, \"q_group_size\": 128, \"residual_length\": 64, \"device\": model.device}  \n",
        ")  \n",
        "  \n",
        "print(\"After model generation:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Decode the outputs for each prompt in the batch  \n",
        "results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]  \n",
        "  \n",
        "# Print the results  \n",
        "#for i, result in enumerate(results):  \n",
        "#    print(f\"Result for prompt {i+1}: {result}\")  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62c2c6d62c1d4a1fab0101b4d1182c17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before tokenization and model generation:\n",
            "Allocated GPU memory: 5.31 GB\n",
            "Reserved GPU memory: 5.40 GB\n",
            "After tokenization:\n",
            "Allocated GPU memory: 5.31 GB\n",
            "Reserved GPU memory: 5.40 GB\n",
            "After model generation:\n",
            "Allocated GPU memory: 5.32 GB\n",
            "Reserved GPU memory: 55.99 GB\n"
          ]
        }
      ],
      "source": [
        "import torch  \n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig  \n",
        "import os  \n",
        "  \n",
        "  \n",
        "# Define multiple prompts for batch processing  \n",
        "base_prompts = [  \n",
        "    \"The best tomato sauce is\",  \n",
        "    \"Artificial intelligence is\",  \n",
        "    \"The future of technology is\",  \n",
        "    \"Climate change is\",  \n",
        "    \"The benefits of exercise are\",  \n",
        "    \"Healthy eating includes\",  \n",
        "    \"The importance of sleep is\",  \n",
        "    \"The universe is\",  \n",
        "    \"Machine learning is\",  \n",
        "    \"Renewable energy sources are\",  \n",
        "]  \n",
        "  \n",
        "# Multiply the prompts by 20 to increase the batch size  \n",
        "prompts = base_prompts * 20  \n",
        "  \n",
        "model_id = \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"  \n",
        "  \n",
        "bnb_config = BitsAndBytesConfig(  \n",
        "    load_in_4bit=True,  \n",
        "    bnb_4bit_quant_type=\"nf4\",  \n",
        "    bnb_4bit_compute_dtype=torch.float16,  \n",
        "    bnb_4bit_use_double_quant=True,  \n",
        ")  \n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)  \n",
        "  \n",
        "# Set padding token  \n",
        "if tokenizer.pad_token is None:  \n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  \n",
        "  \n",
        "model = AutoModelForCausalLM.from_pretrained(  \n",
        "    model_id,   \n",
        "    trust_remote_code=True,   \n",
        "    quantization_config=bnb_config,   \n",
        "    attn_implementation=\"flash_attention_2\",   \n",
        "    torch_dtype=torch.float16,   \n",
        "    device_map=\"cuda:0\"  \n",
        ")  \n",
        "  \n",
        "# Resize token embeddings to match the tokenizer's vocabulary size  \n",
        "model.resize_token_embeddings(len(tokenizer))  \n",
        "  \n",
        "def print_gpu_memory_usage():  \n",
        "    allocated_memory = torch.cuda.memory_allocated()  \n",
        "    reserved_memory = torch.cuda.memory_reserved()  \n",
        "    print(f\"Allocated GPU memory: {allocated_memory / (1024**3):.2f} GB\")  \n",
        "    print(f\"Reserved GPU memory: {reserved_memory / (1024**3):.2f} GB\")  \n",
        "  \n",
        "print(\"Before tokenization and model generation:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Tokenize the prompts with batch processing  \n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)  \n",
        "  \n",
        "print(\"After tokenization:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Generate outputs for the batch of prompts  \n",
        "outputs = model.generate(  \n",
        "    **inputs,   \n",
        "    max_new_tokens=1000,   \n",
        ")  \n",
        "  \n",
        "print(\"After model generation:\")  \n",
        "print_gpu_memory_usage()  \n",
        "  \n",
        "# Decode the outputs for each prompt in the batch  \n",
        "results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]  \n",
        "  \n",
        "# Print the results  \n",
        "#for i, result in enumerate(results):  \n",
        "#    print(f\"Result for prompt {i+1}: {result}\")  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
