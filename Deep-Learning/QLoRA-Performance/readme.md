# 微调中LoRA

**一、大白话说LoRA**

LoRA的图如下，其实现是老生常谈了：

这张图描述了一个名为“QLoRA”的参数高效微调方法，它将大型预训练模型先量化成较低精度（通常是4-bit），再在其之上训练规模很小的适配器（adapter）参数，从而在显著减少显存占用的同时，仍能对模型进行有效的微调。下面逐步解析图中的关键点：

1. 预训练权重 (Pretrained Weights)
   • 图中左下方的蓝色矩形表示原始预训练模型的权重 W，通常是维度为 R^(d×d) 或相似规模的大矩阵。
   • 这里，QLoRA 会将这些预训练权重量化到4-bit（图中左下角标明“4-bit”），从而大幅缩小模型体积并减少显存占用。

2. 适配器 (Adapter) 参数 (A 和 B)
   • 图中橙色的结构就是 LoRA / QLoRA 中用到的两组小矩阵 A 和 B。它们以16-bit的形式存储和训练（图中右侧标有“16-bit”）。
   • 适配器的参数规模远小于全部模型权重，因而训练时只需为这小部分参数保存梯度和优化器状态，极大降低了显存需求。
   • 在最初训练时，B 被初始化为 0 矩阵（图中标注“B = 0”），A 则随机初始化（A ~ N(0, σ²)）。

3. 前向计算 (Forward Pass)
   • 在图中间部分，公式 h = W x + B A x 表示在做推断或训练时，模型的输出向量 h 不仅包含原来预训练权重 W 的贡献，也会额外加上由适配器 B A 乘以输入 x 得到的增量。
   • 注意到这里 B 与 A 相乘后形成的“rank 矩阵”可以看作是对原模型的一个低秩修正，于是通过这小部分参数就可以对模型进行调整、实现微调的效果。

4. 权重合并 (Merged Weights)
   • 右上方的公式 h = (W + BA)x，以及右侧橙色的大矩形 W_merged，表示在推理阶段可以将 W 与 B A 合并成一个新的权重矩阵 W_merged 进行计算。
   • 合并之后的矩阵可以仍以4-bit的方式存储，也就是说在推断时依旧保持低内存占用。

5. 内存优势与高效微调
   • 由于量化了预训练模型（4-bit），并在训练时仅更新体量很小的适配器（16-bit），不需要对完整版权重进行回传和梯度计算，整体显存开销显著下降。
   • 右下角提到的约3.5倍的内存节省，正是因为QLoRA量化了模型并且只储存和更新那少量的适配器参数。

   简而言之，这张图展现了 QLoRA 微调的核心思路：
   (1) 将大模型的主要权重用4-bit进行量化，减少存储占用；
   (2) 在量化后的权重上叠加一个小型的可训练适配器（A 和 B），并且仅更新适配器参数；
   (3) 推理时可以将适配器与量化后的大模型进行合并，同时保持模型精度的显著提升和内存占用的大幅降低。

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/QLoRA-Performance/images/1.png)

这里我举个简单的例子来说明LoRA训练中低秩矩阵表示高秩矩阵的实现。

1. 准备一个“原始权重”矩阵 W

   在大型模型中，W 可能是几千到几万维度的矩阵。为了举例，这里就用一个 4×4 的小矩阵来代表预训练权重 W：

   假设
   W =
   [ [1, 2, 3, 4],
   [2, 3, 4, 5],
   [3, 4, 5, 6],
   [4, 5, 6, 7] ]

   此时，W 的维度是 4×4。

   ——————————————

2. 引入 LoRA 的“低秩”更新

   LoRA 的做法是：给 W 增加一个低秩的增量 BA，而不是直接加一个任意大小的新矩阵。这里 B 和 A 分别是很小的矩阵，满足维度 B ∈ R^(d×r)，A ∈ R^(r×d)，其中 r≪d。

   在这个例子：
   • d = 4（因为 W 是 4×4）
   • 选 r = 2（这是一个低秩的示例，实际中常用更小的 r 比如 8、16 等，相对于数万维要小太多）

   那么：
   • B 的维度就是 (4×2)
   • A 的维度就是 (2×4)

——————————————

3. 构造这两个低秩矩阵 (B 和 A)


为了举例，假设给定 B 和 A（在实际训练中，它们会通过反向传播来学习），这里先随便给一个数值示例：

B =
[ [ 0.1, 0.2 ],
[ 0.0, 0.3 ],
[ 0.1, 0.1 ],
[ 0.0, 0.2 ] ]

A =
[ [ 2.0, 0.0, 0.0, 1.5 ],
[ 0.0, 1.0, 2.0, 1.0 ] ]

(这些纯属举例，实际情况中会随机初始化并在训练时更新)

——————————————

4. 计算低秩更新 BA


BA 的结果还是一个 4×4 矩阵，但秩不会超过 2（因为 B、A 里间的 r=2）。

我们先把 B×A 乘起来：

• 先看 B 的第一行 [0.1, 0.2] 乘以 A：

- 乘出来会形成一个 1×4 的向量
- 例如 第一个分量 = 0.1×2.0 + 0.2×0.0 = 0.2
- 以此类推把每行都算好，就得到一个 4×4 的矩阵

具体乘法全过程在此就不逐项写了，假设结果 BA 为：

BA =
[ [ 0.2, 0.1, 0.4, 0.3 ],
[ 0.0, 0.3, 0.6, 0.3 ],
[ 0.2, 0.1, 0.4, 0.3 ],
[ 0.0, 0.2, 0.4, 0.2 ] ]

(这是根据前面给定的 B、A 进行矩阵乘法的结果示例)

——————————————

5. 得到最终的微调权重 (W + BA)


微调后，模型实际使用的权重便是：
W_merged = W + BA

也就是将 W 与 BA 对应元素相加，得到一个新的 4×4 矩阵：

W + BA =
[ [1.2, 2.1, 3.4, 4.3],
[2.0, 3.3, 4.6, 5.3],
[3.2, 4.1, 5.4, 6.3],
[4.0, 5.2, 6.4, 7.2] ]

图示理解：
• 如果没有 LoRA，微调时往往要直接对 W 中的所有元素进行更新；
• 有了 LoRA，我们并不改 W，而是只更新两个小矩阵 B、A，使 BA 的值恰好把要做的“修正”添加到 W 上；
• 当 r 比 d 小很多时，B 和 A 的参数量远小于 d×d，对于大模型来说，可以节省大量训练开销与存储。

——————————————

6. 为什么“低秩”也能取得不错的效果


• 在大模型中，W 已经包含了丰富的通用特征知识；
• 下游任务只需要在特定方向或特定子空间中，对模型进行少量的微调；
• 事实证明，用一个秩 r 较小的增量矩阵 BA，往往就能很有效地捕捉这些“修正”。
• 由于 r≪d，你只需训练相对少量的参数 (B 和 A)，就能达到改进模型的目标，从而节省显存和计算量。

——————————————
总结

通过这个 4×4 小矩阵的举例，可以理解 LoRA (以及类似的 QLoRA) 是如何用两个小的低秩矩阵 B、A 的乘积 BA 来表示对原权重 W 的“修正”。在实际的大模型中，同样的思路可以非常有效地减少对大矩阵的直接更新，让微调更轻量、更节省资源。



**二、干货来了**

![Image](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/QLoRA-Performance/images/2.png)

对着上图我说结论：

──────────────────────────────────────────────────────────

1. 不合并 Adapter（基础模型 4-bit + Adapter 16-bit）
   • 做法：先将基础模型量化到 4-bit，然后“加载”16-bit 的 LoRA/QLoRA Adapter 不合并。
   • 效果：推理时的模型主要是 4-bit，Adapter 权重是 16-bit，但因为是参数高效微调，Adapter 大小往往较小，并不影响太多显存；困惑度最好(测到 3.55)。
   • 适用场景：如果你想最大化保留微调效果，并且管理“额外的 Adapter”文件不麻烦，就用这种方式。
   • 缺点：需要在推理时显式加载 Adapter（多个场景可能得维护多个 LoRA 文件）。

   ───────────────────────────────────────────────────────

2. 合并 Adapter 后，再使用“更好”的量化(基础模型 4-bit + 已合并的 Adapter 4-bit)
   • 做法：先在 16-bit 模型中合并 Adapter ⇒ 得到一个完整的 16-bit 模型 ⇒ 接着用 AWQ/AutoRound 等更先进的算法量化到 4-bit 整体模型。
   • 效果：AWQ 量化后困惑度约 3.88，比 bitsandbytes 量化(4.33)好很多，但依然比直接加载 Adapter 的 3.55 稍差。
   • 适用场景：合并后得到单一模型文件，部署最简洁；效果也较好，显存开销还是 4-bit 的水平。
   • 缺点：较之于不合并的 3.55，困惑度还是略有回退。

   ──────────────────────────────────────────────────────────

3. 合并 Adapter 后，不再量化(基础模型 16-bit + 已合并的 Adapter 16-bit)
   • 做法：将微调好的 Adapter 并入原 16-bit 模型，然后保持 16-bit 不变，直接用于推理。
   • 效果：困惑度约 3.60，性能和不合并 Adapter 在 4-bit 模型上加载差不多；
   • 优点：一次性合并，推理时无需单独加载 Adapter，保留了大部分精度；
   • 缺点：需要 16-bit 的显存支持，推理开销远大于 4-bit。

──────────────────────────────────────────────────────────

4) 合并 Adapter 后，再用 bitsandbytes 4-bit 量化(基础模型 4-bit + 已合并的 Adapter 4-bit)

• 做法：先在 16-bit 合并 Adapter，然后再用 bitsandbytes 4-bit 量化整个模型。
• 效果：困惑度测得 4.33，基本退回到未微调前的效果。
• 适用场景：理论上合并后单模型文件更方便，但实际效果不佳，一般不推荐除非对 PPL 要求不高。

──────────────────────────────────────────────────────────
总结：
• 如果你最想要“最低的困惑度、最好效果”，采用“基础模型 4-bit + 不合并的 Adapter (16-bit)”的方式，推理时就多加一个 16-bit adapter。
• 如果你更想要“只有一个模型文件，好分发好部署”，那就最好合并 Adapter 后用 AWQ/AutoRound 等更优质的量化方式到 4-bit，性能折损相对较低(但仍高于保持 16-bit)。
• 如果显存足够，而且不想折腾任何量化，可以合并后直接跑 16-bit（更简单，但显存耗费最大）。
• 尽量避免“合并后再用 bitsandbytes 4-bit”这种思路——当前实验结果显示困惑度直接退回到 4.33，非常不划算。