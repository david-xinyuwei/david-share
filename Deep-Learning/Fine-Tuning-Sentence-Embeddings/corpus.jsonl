{"text": "Skills: SQL, Tableau, Excel, R/Python, Business Analytics,\n\nContract Type: W2\n\nDuration: 5+ Months\n\nLocation: San Diego CA (Hybrid 2 days)\n\nPay Range:$65.00 - $70.00 per hour\n\n#LP\n\nNOTE: Role open to Women Back to Work candidates with a career gap\n\nTALK to a recruiter NOW: CONTACT Ankit at 408-907-7672\n\nGrow your skills by working with the best in the industry\n\nJob Responsibilities\n\nDrive business results: You will identify and help craft the most important KPIs to monitor the effectiveness of our operations and drive automated availability of those metrics. Amidst a sea of data, you will distill the data into key storylines that create a shared understanding of opportunities and influence teams to act.Amplify reporting insights: You will demonstrate your reporting craft by reimagining the existing reporting suite of dashboards using data visualization best practices, including uniform visuals, metric definitions, and accessible data dictionaries, to provide a comprehensive view of KPI performance.Improve operational performance: You will identify areas of operational and experiential opportunity using data-driven insights and root cause analysis, providing recommendations to subject-matter experts and partnering with teams to optimize to improve customer experiences and enable more efficient business processes.Automate data capabilities: You will leverage advanced modeling techniques and self-serve tools to develop new metrics, data pipelines, and expanded capabilities to automate processes and support business decisions, establishing consistent sources of truth and enabling faster customer speed to benefit.\n\n JOB REQUIREMENTS\n\n5+ years of experience working in the analytics field Ability to tell stories with data, educate effectively, and instill confidence, motivating stakeholders to act on recommendations.Experience as a business partner for senior leaders; comfortable distilling complex data into a simple storyline.Highly proficient in SQL, Tableau, and Excel Experience with programming languages including R or Python.outstanding communication skills with both technical and non-technical colleagues\n\nCALL NOW: Ankit at 408-907-7672\n\nAbout Akraya\n\n\"Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today!\""}
{"text": "Qualifications / Skills:• 5+ years of industry experience collecting data and building data pipelines• Degree in Computer Science or related field• Expert knowledge of databases and SQL• Mastery of Python• Experience building data pipelines from end to end:o Understanding business use cases and requirements for different internal teamso Prototyping initial collection and leveraging existing tools and/or creating new toolso Building and deploying enterprise-grade data pipelineso Maintenance of such pipelines with a focus on performance and data quality• Experience working with structured, semi-structured, and unstructured data• Experience with Azure Dev Ops or other cloud provider’s technology stack• Experience with code versioning and repository software• Experience being an active member of highly functional Agile teams• Ability to think critically and creatively in a dynamic environment, while picking up new tools and domain knowledge along the way• A positive attitude and a growth mindset• Excellent programming skills coupled with an understanding of software design patterns and good engineering practices\nBonus Qualifications• Experience with Spark• Python webapp development skills (Streamlit/Flask/Django/Dash)• Experience using property, geospatial, and image data• Experience solving financial and risk domain problems"}
{"text": "experience and knowledge, you will establish policy, process, and best practices.\n\nUsing your experience and vision, you will define future work and data technologies for reporting and analytics of police activities. This involves crafting technical roadmaps and recommending data strategies for projects involving cloud and on-premises data solutions.\n\nWhile database expertise is a necessary skill for this position, AWS cloud services will be used extensively, and you will also be relied on to understand processing and server requirements, and various cloud data analytics platforms and tools.\n\nJob Responsibilities\n\nPrimary Job Functions\n\nCoordination of Seattle IT roles to support and enhance the existing Seattle Police Data Analytics Platform (DAP) Data Mart/Warehouse Solution. Monitor processes as needed, possibly requiring availability outside of normal business hours, respond to customers, and IT staff to resolve issues. Prioritize support work with other planning and development tasks.\n\nLead production support, lead infrastructure design, participate in requirements and design, lead development, unit testing, participate in regression and UAT testing, lead deployment.\n\nPlan, build, and implement improvements to AWS Cloud based data analytics systems, on-premises data stores, and analytical models. This involves communication and coordination with business owners, program managers, data scientists, data architects, and developers across IT, Seattle Police, and external consulting companies. Analyze resource needs, and feasibility of proposed changes or enhancements, build, schedule, communicate, and implement updated data products.\n\nResearch, plan, and propose future state data analytics using new technologies and Amazon Web Services. Understand industry recommended paths to future state technology use. Analyze and propose solutions that consider technical feasibility in our city environment, business cost/value, and resource needs. Participate in high level discussions and presentations with management.\n\nQualifications\n\nRequired Qualifications: \n\nEducation: Successful candidates will have 5 years’ experience and training in Computer Science, Business Information systems, Database Management, Amazon Web Services, or a related field or a similar combination of experience and education.\n\nExperience:Demonstrated ability working with data to address business needs, including development of data repositories, warehouses, operational data stores, analytics models, or related systems.\n\nWork experience with database systems, and SQL languages, database scripting, management, scheduling, optimization, integration tools.\n\nExperience recommending solution architectures and requirements for large data analytics projects, including the ability to scope processing, memory, and storage needs.\n\nExperience and knowledge of data analytics services in cloud services, including data project migrations, cloud native analytics tools and services, deployment processes, security and privacy principles, and service cost strategies. Experience working in an Agile work environment and using tools such as Jira, Confluence, SharePoint, Teams.\n\nTechnical/Professional Skills Needed To Succeed\n\nKnowledge and experience with multiple database systems and the related tool sets available, for example Oracle, SQL Server, Postgres SQL, extract, transform, load (ETL) tools, reporting, scheduling, and integration tools.\n\nAnalytics and Cloud Data: Experience with Cloud based data tools such as AWS Cloud Formation Templates, S3, Glue, Python, Spark, Athena, or other cloud data tools. Understanding of Analytics and reporting tools (i.e. O365 tools, Tableau, etc.). Strong desire to learn and explore new capabilities in this area.\n\nKnowledge of Amazon Web Service or other cloud services and processes for building and supporting data warehouses, data marts, and data stores. Skills to provide leadership and perform development work moving and merging of approximately 20 Police Department data sets into curated data sources for reports and dashboards that are used by SPD to inform operational decisions.\n\nCriminal Justice Information Services (CJIS) certification - You must pass an FBI background investigation and certification process for work on Seattle Police systems.\n\nAdditional Information\n\nDesired Qualifications\n\nEstablished experience with written, verbal, and illustrative communication. Ability to modify communication styles to communicate relevant technical content with the professionals that perform the various city business functions, as well as technical engineers, and management.\n\nDedication to Race, Social Justice, and Equity\n\nYou will be expected to take part and understand Seattle’s need to continually address race-based disparities and to end institutionalized racism. A major role will be to work with Seattle IT and departments to prioritize project work based on how it assists Seattle to address equity. This involves participation in race and social justice initiatives and crafting standard processes for using a related tool kit in all projects. To deliver accurate city performance data, this role is necessary for our city departments to understand and manage the equitable distribution of City services.\n\nSalary: The full salary range for this position is $51.89 - $77.84 per hour. The salary for this individual role has been identified as $51.89 – $77.84 per/ hour.\n\nWhy work at the City of Seattle?\n\nThe City of Seattle recognizes every City employee must play a role in ending institutional and structural racism. Our culture is the result of our behavior, our personal commitments, and the ways that we courageously share our views and encourage others to do the same. To cultivate an antiracist culture, we seek employees who will engage in the Race and Social Justice Initiative by working to dismantle racist policies and procedures, unlearn the way things have always been done, and provide equitable processes and services.\n\nBenefits: The City of Seattle offers a comprehensive benefits package including vacation, holiday, and sick leave as well as medical, dental, vision, life and long-term disability insurance for employees and their dependents. More information about employee benefits is available on the City's website at: https://www.seattle.gov/human-resources/benefits/employees-and-covered-family-members/most-employees-plans.\n\nApplication Process: For optimal consideration, we encourage you to include a cover letter and resume with your application. We encourage you to use your cover letter to discuss why you want to do this work and how you meet the qualifications for the position. Your resume should summarize the talent, experience, knowledge, and skills you bring to this work. Apply online at https://www.governmentjobs.com/careers/seattle/\n\nIf you have any questions, please contact Alfreda Wilson, at Alfreda.wilson2@seattle.gov\n\nWorkplace Environment (Telework Expectation): This position offers the flexibility of a hybrid work schedule. At this time, hybrid telework schedules have a minimum requirement of two days onsite per week. Individual schedules will be based on operational needs and agreement between the employee and their supervisor.\n\nBackground Check: This hiring process involves a background check of conviction and arrest records in compliance with Seattle's Fair Chance Employment Ordinance, SMC 14.17. Applicants will be provided an opportunity to explain or correct background information.\n\nCriminal Justice Information Services (CJIS) certification - You must pass an FBI background investigation and certification process for work on Seattle Police systems.\n\nWho may apply: This position is open to all candidates that meet the minimum qualifications. The City of Seattle values different viewpoints and life experiences. Applicants will be considered regardless of race, color, creed, national origin, ancestry, sex, marital status, disability, religious or political affiliation, age, sexual orientation, or gender identity. The City encourages people of all backgrounds to apply, including people of color, immigrants, refugees, women, LGBTQ, people with disabilities, veterans, and those with diverse life experience."}
{"text": "QualificationsBachelor's degree in Computer Science, Statistics, Mathematics, Economics, or related field. At least five years of experience as a Data Analyst in a digital media or ecommerce setting.Proficiency in SQL, Python, R, or other programming languages for data manipulation and analysis.Experience with Google Data Studio or other data visualization tools.Experience creating custom data pipelines, automated reports, and data visualizations.Expertise in web and mobile analytics platforms (e.g. Google Analytics, Adobe Analytics, AppsFlyer, Amplitude).Current understanding of internet consumer data privacy matters.Excellent communication and collaboration skills, with the ability to present findings and recommendations to both technical and non-technical stakeholders.Strong analytical skills and attention to detail, with the ability to translate complex data into actionable insights.\n\nPreferred QualificationsExperience with video delivery systems (encoding platforms, video players, video ad integration)Experience with digital media systems including content management systems, advertising systems, consent management platforms, and identity management frameworks.Experience with machine learning, statistical analysis, and predictive modeling."}
{"text": "Requirements:Bachelors degree or equivalent practical experience.5+ years of experience with application development with at least one programming language, working with data structures/algorithms.5+ years with two or more languages included but not limited to: Python, Apache, Presto, R, ML/optimization, ScalaExperience in one or more of the following areas: machine learning, recommendation systems, pattern recognition, NLP, data mining or artificial intelligencePossessing a strong experience with ML/AI algorithms and tools, deep learning and/or natural language processing."}
{"text": "requirements through to ensuring successful implementation. The team has a strong culture of internal algorithm review and collaboration. Data science works closely with engineering, product, design, and account management teams.\n\nYOUR OPPORTUNITIES\n\nWe are looking for someone who is excited to use their creativity and analytical skills to make a difference in healthcare. You will join a team building a consumer product that incentivizes healthy behavior. You will have a foundational role in this product and be responsible for building out a core capability around LLM safety and evaluation.\n\nDesign critical algorithmic components of an LLM evaluation systemGenerate insights from large corpuses of free text dataKeep up to date with the latest advances in LLM tooling and capabilitiesCurate and develop datasets needed to support your project deliverablesCollaborate with cross-functional partners in engineering, design, and product to develop solutionsGenerate and prioritize new opportunities for improvements \n\nQualifications\n\nRequired Qualifications\n\nExperience with NLP and/or LLM-based algorithms Have shipped production algorithms to customersStrong machine learning fundamentalsAbility to solicit and translate customer and business needs into requirements and an evaluation frameworkInterest in improving healthcare and working with interdisciplinary project teamsClear communication and presentation skills MS in a quantitative field (e.g. Data Science, Economics, Statistics, Engineering)5-10 years of industry experience \n\nPreferred Qualifications\n\nExperience fine-tuning LLM modelsExperience working with medical text dataPhD in a quantitative field3-5 years of industry experience\n\nWe take into account an individual’s qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company’s equity program, paid time off, including vacation and sick leave. The expected salary range for this position is $216,000 to $249,000. The actual offer will be at the company’s sole discretion and determined by relevant business considerations, including the final candidate’s qualifications, years of experience, and skillset.\n\nNuna is an"}
{"text": "Requirements:Proficiencies:Experience in designing, building, and managing data pipelines for data structuresExpertise with advanced analytics tools such as C#, Python, SQL, PL/SQL, SSIS, SSAS, and NoSQL/Hadoop-oriented databasesStrong experience with data integration technologies and popular data discovery, analytics, and BI software toolsAbility to troubleshoot complicated issues across multiple systems and effectively convey technical concepts to non-technical individualsFinancial data or legal industry data experience is a plusQualifications:Bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science, or related quantitative fieldAt least 8 years of experience in data management disciplines including data integration, modeling, optimization, and data qualityAt least 5 years of experience working in cross-functional teams and collaborating with business stakeholdersExcellent verbal and written communication skills, problem-solving abilities, and ability to work effectively in a collaborative environment\nIf you're a dynamic professional with a passion for data engineering and meet the qualifications outlined above, we encourage you to apply for this exciting opportunity."}
{"text": "Skills: Big Data, GCP, Apache Spark, Apache Beam Requirements:Bachelor's degree in Computer Science, Systems Engineering or equivalent experience.5+ years of work experience as a Big Data Engineer.3+ years of experience using Technologies such as Apache Spark, Hive, HDFS, Beam (Optional).3+ years of experience in SQL and Scala or Python.2+ years experience with software build management tools like Maven or Gradle.2+ years of experience working with Cloud Technologies such as GCP, AWS or Azure. Preferred:Data Engineering using GCP Technologies (BigQuery, DataProc, Dataflow, Composer, DataStream, etc).Experience writing data pipelines.Self-starter that identifies/responds to priority shifts with minimal supervision.Source code control management systems (e.g. SVN/Git, Github) and build tools like Maven & Gradle.Agile environments (e.g. Scrum, XP).Relational databases (e.g. SQL Server, Oracle, MySQL).Atlassian tooling (e.g. JIRA, Confluence, and Github"}
{"text": "requirements including Terms amp; Condi"}
{"text": "experience working in AI/ML, primarily with a recent emphasis on Gen AI and LLM integrated projects. You will help in the development of innovative generative techniques and advancing our research agenda. Additionally, by leveraging your knowledge of ML architecture you will contribute to end-to-end ML pipeline development, ML infrastructure, and the integration of diffusion models & generative AI features.\n\nThis role is fully remote, but ideally PST time as the hiring manager for this role is based in Los Angeles. Required Skills & Experience\n\n4 yrs. of experience in AI/ML ML Framework (PyTorch, Tensorflow, Scikit-Learn) 2 yrs. of experience in ML Infrastructure Worked on Gen AI / LLMs projects \n\nDesired Skills & Experience\n\nWorked with Diffusion Models Experience in Text-To-Video \n\nBenefits & Perks\n\nEquity Package Health Benefits Fully Remote Generous Paid-Time Off \n\nApplicants must be currently authorized to work in the US on a full-time basis now and in the future.\n\nPosted By: Connor Hart"}
{"text": "experienced and highly skilled Sr Data Engineer to join us. This role requires a seasoned professional with a deep understanding of automated data pipelines, cloud infrastructure, databases, and workflow engines. The ideal candidate will have a minimum of 5 years of technical lead experience in the medical device field and at least 7 years of experience in data engineering. Proficiency in Python and a proven track record of leading projects to completion are essential.\n\nPrimary Duties\n\nDesign, develop, and manage robust, secure, scalable, highly available, and dynamic solutions to drive business objectives. Lead the architecture and implementation of advanced cloud-based data engineering solutions, leveraging AWS technologies and best practices. Manage and optimize data pipelines, ensuring timely and accurate data availability for analytics and machine learning applications. Oversee the administration and performance tuning of databases and workflow engines. Collaborate with cross-functional teams (e.g., product management, IT, software engineering) to define data requirements, integrate systems, and implement data governance and security policies. Mentor junior data engineers and oversee the team's development efforts, promoting best practices in coding, architecture, and data management. Stay abreast of emerging technologies and trends in data engineering, cloud services, and the medical device industry to drive innovation and competitive advantage. \n\nKnowledge, Experience & Skills\n\nDegree in Computer Science, Engineering, Information Systems, or a related field. Requiring a minimum of Bachelor’s degree +7yrs of experience or a Master’s degree +5yrs of experience. Minimum of 7 years of experience in data engineering, with expertise in developing and managing automated data pipelines, AWS cloud infrastructure, databases, and workflow engines. Certifications in AWS and data engineering preferred. Experience with machine learning algorithms and data modeling techniques. At least 5 years of experience in the medical device IVD industry, with a strong understanding of FDA regulatory standards and compliance requirements. Expert proficiency in Python programming and software engineering principles. Demonstrated experience with AWS services (e.g., EC2, RDS, S3, Lambda, Glue, Redshift, Athena, EMR) and data pipeline tools (e.g., Apache Airflow, Luigi, etc). Strong knowledge of database management (Postgres and Snowflake), SQL, and NoSQL databases. Adept at queries, report writing and presenting findings Experienced in developing and maintaining ETL pipelines in a cloud environmentExperienced in Unit Testing preferred Strong analytical skills with the ability to organize, analyze, and disseminate information with attention to detail and accuracy Excellent communication and task management skills. Comfort working in a dynamic, fast-paced, research-oriented group with several ongoing concurrent projectsFull fluency (verbal and written) of the English language is a must. \n\nThe estimated salary range for this role based in California is between $148,700 and $178,400 annually. This role is eligible to receive a variable annual bonus based on company, team, and individual performance per bioMerieux’s bonus program. This range may differ from ranges offered for similar positions elsewhere in the country given differences in cost of living. Actual compensation within this range is determined based on the successful candidate’s experience and will be presented in writing at the time of the offer.\n\nIn addition, bioMérieux offers a competitive Total Rewards package that may include:\n\nA choice of medical (including prescription), dental, and vision plans providing nationwide coverage and telemedicine optionsCompany-Provided Life and Accidental Death InsuranceShort and Long-Term Disability InsuranceRetirement Plan including a generous non-discretionary employer contribution and employer match. Adoption AssistanceWellness ProgramsEmployee Assistance ProgramCommuter BenefitsVarious voluntary benefit offeringsDiscount programsParental leaves\n\nBioFire Diagnostics, LLC. is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.\n\nPlease be advised that the receipt of satisfactory responses to reference requests and the provision of satisfactory proof of an applicant’s identity and legal authorization to work in the United States are required of all new hires. Any misrepresentation, falsification, or material omission may result in the failure to receive an offer, the retraction of an offer, or if already hired, dismissal. If you are a qualified individual with a disability, you may request a reasonable accommodation in BioFire Diagnostics’ application process by contacting us via telephone at (385) 770-1132, by email at [email protected], or by dialing 711 for access to Telecommunications Relay Services (TRS)."}
{"text": "requirements, DFMA, and design for serviceability.\nActivities include BIM management, development and implementation of product and work breakdown structures, model-based QTO and program analytics, and presentation of model analysis for lessons learned and portfolio management.\nDesired qualifications include a degree in architecture or civil engineering and proficiency in Autodesk REVIT."}
{"text": "BI/DW Engineer/Analyst to help the ERP team model their Silver Zone tables and integrate their multiple ERP systems. Also to help prepare for a BigQuery migration to Databricks."}
{"text": "Requirements:\n  5+ years of experience developing AI / ML applications and data driven solutions\n  Graduate degree in Computer Science, Engineering, Statistics or a related quantitative discipline, or equivalent work experience\n  Substantial depth and breadth in NLP, Deep Learning, Generative AI and other state of the art AI / ML techniques\n\n  Deep understanding of CS fundamentals, computational complexity and algorithm design\n  Experience with building large-scale distributed systems in an agile environment and the ability to build quick prototypes\n  Excellent knowledge of high-level programming languages (Python, Java, or C++) and core data science libraries including Pandas, NumPy and other similar libraries\n  Ability to independently conduct research and independently develop appropriate algorithmic solutions to complex business problems\n  Experience mentoring junior team members\n  Excellent problem solving and communication skills\n  \n Preferred Qualifications:\n  PhD in Computer Science with an AI / ML research focus and publications in top-tier journals and conferences.  Knowledge of the healthcare domain and experience with applying AI to healthcare data\n   Experience with AWS especially in relation to ML workflows with SageMaker, serverless compute and storage such as S3 and Snowflake\n  Experience with LLMs, prompt engineering, retrieval augmented generation, model fine tuning and knowledge graphs\n\n  \n \n The Guiding Principles for success at Norstella:\n 01: Bold, Passionate, Mission-First\n We have a lofty mission to Smooth Access to Life Saving Therapies and we will get there by being bold and passionate about the mission and our clients. Our clients and the mission in what we are trying to accomplish must be in the forefront of our minds in everything we do.\n 02: Integrity, Truth, Reality\n We make promises that we can keep, and goals that push us to new heights. Our integrity offers us the opportunity to learn and improve by being honest about what works and what doesn’t. By being true to the data and producing realistic metrics, we are able to create plans and resources to achieve our goals.\n 03: Kindness, Empathy, Grace\n We will empathize with everyone's situation, provide positive and constructive feedback with kindness, and accept opportunities for improvement with grace and gratitude. We use this principle across the organization to collaborate and build lines of open communication. \n 04: Resilience, Mettle, Perseverance\n We will persevere – even in difficult and challenging situations. Our ability to recover from missteps and failures in a positive way will help us to be successful in our mission.\n 05: Humility, Gratitude, Learning\n We will be true learners by showing humility and gratitude in our work. We recognize that the smartest person in the room is the one who is always listening, learning, and willing to shift their thinking. \n \n Benefits:\n  Medical and prescription drug benefits\n  Health savings accounts or flexible spending accounts\n  Dental plans and vision benefits\n  Basic life and AD&D Benefits\n  401k retirement plan\n  Short- and Long-Term Disability\n  Maternity leave\n  Paid parental leave\n  Open Vacation Policy\n  \n Please note- all candidates must be authorized to work in the United States. We do not provide visa sponsorship or transfers. We are not currently accepting candidates who are on an OPT visa.\n \n The expected base salary for this position ranges from $160,000 to $200,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus.\n \n MMIT is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people’s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual’s abilities, skills, performance and behavior and our business requirements. MMIT operates a zero tolerance policy to any form of discrimination, abuse or harassment.\n \n Sometimes the best opportunities are hidden by self-doubt. We disqualify ourselves before we have the opportunity to be considered. Regardless of where you came from, how you identify, or the path that led you here- you are welcome. If you read this job description and feel passion and excitement, we’re just as excited about you."}
{"text": "skills to analyze and answer complex questions in the defense domain. You will collaborate with a team of scientists, engineers, analysts, data modelers and a broad range of domain experts on a wide range of problems.\n\nAs a Senior Scientist – Modeling, your emphasis will be in systems modeling, experimental design, research methods, and (most importantly) critical thinking to ensure that conclusions are supported by the available evidence. You should be comfortable critiquing research methods and conclusions and have the creativity to propose next steps. Direct experience or strong interest in a broad range of defense systems would be an advantage.\n\nYou will communicate your findings and recommendations to technical and non-technical audiences, using clear and compelling reports.\n\nResponsibilities\n\n Manage modeling projects for defense related applications in lead, individual contributor, or independent reviewer roles. Collect, integrate, and evaluate data from multiple sources. Develop and/or critique advanced predictive models, machine learning algorithms, and statistical methods to discover insights and identify opportunities. Critique and redesign experimental methods to achieve the desired objectives. Create and present reports that explain the results and implications to technical and non-technical audiences. Provide guidance and mentorship to junior data scientists and analysts in the team. Stay current on the latest developments in modeling, AI/ML, and defense domains.\n\nMinimum Requirements\n\n MA/MS degree in a related scientific field (Mathematics, Applied Mathematics, Statistics, Data Science, etc.) 10+ years of experience with 3 or more years on related programs and projects. At least 7 years of experience modeling realworld systems in Engineering, Physics, Health, or comparable systems. Active Top-Secret clearance with SCI eligibility. Hands-on experience applying a wide variety of statistical modeling techniques to real world problems. Experience using tools such as Python, R, MATLAB, SAS, SPSS or equivalent. Well-developed written and oral communication skills with ability to present complex statistical concepts to non-analytical stakeholders (Excel, Word and PowerPoint are a must). Interest and/or direct experience with a broad range of defense systems is a plus. Expertise in statistical modeling and machine learning techniques, such as supervised learning, unsupervised learning, deep learning, regression, decision trees, Bayesian inference, etc.\n\nAdditional Beneficial Experience\n\n A proven track record of designing and delivering complex IT solutions for global enterprise-scale organizations. A deep understanding of enterprise architecture framework and design patterns. Hands-on experience in designing and implementing cloud-based data-driven solutions that include artificial intelligence, machine learning, big data, and analytics components. Relevant experience in C5ISR defense systems and operations, and an understanding of the security challenges and requirements in this domain. A familiarity with the defense industry standards, regulations, and best practices, and an ability to apply them to the solutions architecture. A capability to design and implement solutions that meet the defense clients’ operational, tactical, and strategic needs, and that enhance their mission readiness and effectiveness. A proficient knowledge of various programming languages, platforms, frameworks, databases, cloud services, and software development tools. Excellent communication skills and the ability to explain technical information in layman’s terms. A customer-oriented attitude and a passion for creating innovative and effective solutions whilst understanding critical details\n\nWhat We Can Offer You\n\n We’ve been named a Best Place to Work by the Washington Post. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive benefits and learning and development opportunities. We are mission-oriented and ever vigilant in aligning our solutions with the nation’s highest priorities. For over 60 years, the principles of CACI’s unique, character-based culture have been the driving force behind our success.\n\nCompany Overview\n\nCACI is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other protected characteristic.\n\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits. Learn more here\n\nThe Proposed Salary Range For This Position Is\n\n$104,200-$229,200"}
{"text": "Requirements: US Citizen, GC Holders or Authorized to Work in the U.S.\n\nJob Description\n\nWe are looking for a passionate certified data analyst.The successful candidate will turn data into information, information into insight and insight into business decisions.The data analyst is responsible for overseeing our data systems and reporting frameworks, guaranteeing the integrity and precision of data.Data analysts are tasked with enhancing analytical and reporting functions, as well as supervising performance and quality assurance processes to pinpoint areas for enhancement.\n\nRoles And Responsibilities\n\nDriving roadmap initiatives around the Project Management functions in Clarity PPM.Conducting project assessments to determine compliance with internal project management and resource management standards.Be able to read, update and develop data dashboards, charts, and data sets in Clarity to support decision-making across departments.Detect, examine, and decode trends or patterns within intricate datasets to scrutinize data and produce actionable insights.Assist in the implementation and deployment of Clarity PPM enhancements and provide ongoing training and support.Providing support and training to new and existing tool users, both individually and in groups.Creating training materials and conducting user training.Suggesting solutions, recommendations and enhancements based on customer input and observations.Tracking progress and notifying management of issues that require escalation and assuming responsibility for resolving or coordinating the resolution of resource capacity issues and project data quality concerns.Meeting with analysts, customers, and partners to showcase value and drive adoption of the resource capacity planning processes and the Clarity PPM tool.Locate and define new process improvement opportunities.Evolve our data collection capabilities, analytics and reporting process as the business evolves and grows by optimizing strategies, implement databases, and managing security, data sources and metadata.Commissioning and decommissioning data sets as needed, while maintaining, cleaning, and organizing data sets as needed.Keeping current with industry standards, and implementing updates as needed or required.Leadership skills and the ability to connect and communicate across multiple departments.Adept at report writing and presenting findings.Ability to work under pressure and meet tight deadlines.Be able to read and update project and program level resource forecasts.Identify recurring process issues and work with manager to find solutions and initiate improvements to mitigate future recurrence.\n\nBasic Qualifications\n\nMinimum of 5 years of experience with Clarity PPM and 5-8 years in an analyst capacity.Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, etc.)Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SAS, etc)You have a high understanding of PPM disciplines, have worked in a team and covered strategic projects.Experience with Dashboard customization, configuration, user interface personalization and infrastructure management will be helpful.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail, accuracy, and actionable insights.Excellent communicator, adjusting communication styles based on your audience.Quick learner, adaptable and can thrive in new environments.Proactive, confident, and engaging; especially when it comes to large stakeholder groups.Capable of critically evaluating data to derive meaningful, actionable insights.Demonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background.\n\nOur benefits package includes:\n\nComprehensive medical benefitsCompetitive pay, 401(k)Retirement plan…and much more!\n\nAbout INSPYR Solutions\n\nTechnology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients’ business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.\n\nINSPYR Solutions provides Equal Employment Opportunities (\n\n hybrid"}
{"text": "skills and ability to lead detailed data analysis meetings/discussions.\n\nAbility to work collaboratively with multi-functional and cross-border teams.\n\nGood English communication written and spoken.\n\nNice to have;\n\nMaterial master create experience in any of the following areas;\n\nSAP\n\nGGSM\n\nSAP Data Analyst, MN/Remote - Direct Client"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Data Architect -AWS Cloud to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating them to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and subsystems on production servers\n\n• Mentor, support and manage team members\n\nYour Skills & Experience:\n\n• 8+ years of demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platform- AWS (Amazon Web Services)\n\n• Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension concepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAnnual base pay range: $117,000 - $175,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "experience of data analysis or equivalent experience (university included)Bachelor’s degree in Computer Science, Information Security, Data Analytics, or a related fieldExperience with Python for data wrangling, analysis, and visualization, leveraging libraries such as Pandas and NumPyExperience with PowerBI, Tableau, or another comparable data visualization tool to build interactive dashboardsFamiliarity with FAIR methodology and basic fundamentals of probability and statisticsFamiliarity with the NIST Cybersecurity Framework is a plusMust be able to effectively communicate to various types of audiencesAbility to think critically, solve problems, make decisions and build trust across the organizationStrong logic and reason along with problem solving skills.Ability to work independently.\n\nPlayer Benefits\n\nWe treat our team right\n\nFrom our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:\n\nAn exciting and fun environment committed to driving real growthOpportunities to build really cool products that fans loveMentorship and professional development resources to help you refine your gameBe well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another \n\nFanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to \n\nThe applicable salary range for this position is $108,000 - $135,000, which is dependent on a variety of factors including relevant experience, location, business needs and market demand. This role may offer the following benefits: medical, vision, and dental insurance; life insurance; disability insurance; a 401(k) matching program; among other employee benefits. This role may also be eligible for short-term or long-term incentive compensation, including, but not limited to, cash bonuses and stock program participation. This role includes paid personal time off and 14 paid company holidays. FanDuel offers paid sick time in accordance with all applicable state and federal laws."}
{"text": "skills, able to translate complex business requirements into sound data management and data governance solutionsWork with clients to understand data analytics requirements. Analyze data to ensure it meets specific data management, data governance, and data quality assurance requirements before processing the data within the Data Lake and data warehouseWork with the Product Managers, Database Architect or BI Architect to understand data pipeline and data life cycle processing requirements and patternsInstall and configure data sources for use by the data pipelinesWork with the Database Architect to define data management, data governance, and data quality assurance requirements to manage the entire data processing life cycleActs as a key contributor to all phases of the design and development lifecycle of analytic applications utilizing Microsoft Azure and BI technology platformsCurates data for analyses, business reviews, and operational data analysis demandsUse an agile approach to define requirements, design and develop data pipeline solutions to enable near real-time change data capture (CDC) and data consumptionProficient in applying data management, data governance, and data quality processes and tools to correlate disparate sources\nSkills & Competencies: \nStrong business acumen and proven experience in solving complex problems and creating elegant business solutionsAdept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of actionAdvanced SQL development skills, broad exposure to all language constructsExperience with Microsoft On-premise SQL Server (2017 or higher) and Azure SQL Server technologies including broad experience with SQL Server capabilities and tools (e.g., CDC, Columnstore Index, In-memory Table, SSAS Tabular, DAX, T-SQL, SSIS)Experience in Power BI, Power BI Embedded, Power BI Services, Power BI Gateway, and Power BI DataflowExperience with Azure products and services including Azure Data Lake Gen2, Azure Databricks, Azure Databricks Unity Catalog, Databricks API, Databricks Row-level security, Databricks error logging, Azure SQL Server, and Azure Analysis Services.Experience using Azure DevOps and CI/CD as well as Agile tools and processes including Git, Jenkins, Jira, and ConfluenceExperience with data integration through APIs, Web Services, SOAP, and/or REST servicesExperience with Lakehouse architecture and design for multi-tenant, OLTP data modeling, dimensional data modeling, composite modeling, data transformation, row-level security, and designing the most optimal analytical data structures for near real-time data analyticsAdditional programming experience is a plus (preferably.NET) or other languages such as Python, Scala, R.\nEducation or Prior Work Experience: \nBachelor's degree in CS10+ years of experience with designing and developing complex data analytics solutions5+ years of experience with Microsoft Big Data solutions"}
{"text": "requirements for proposed models, scorecards, and forecastsManage deliverables across multiple projects in a deadline-driven environment and maintain good communication with all model stakeholders.Work with business to review data sources, data inconsistencies, and business logic for initiating data discovery.\n\nQualifications\n\nThis role is highly technical in nature; an ideal candidate has a sound blend of Business and Data Science background with Credit Risk/Classification modeling skills3+ years of work experience in data modeling, statistical analysis requiredExperience in machine learning and regression based statistical methodology; demonstrated experience using these techniques to solve modeling problemsProficient in SQL, Python, R, or other analytical/model building programming languagesExperience working with large datasets (greater than 1 million records) and applying techniques to efficiently manage big dataPrevious experience working with credit bureau data (preferred)Previous experience in Credit Card risk modeling and analytics (preferred)Excellent written and verbal communication skills, ability to convey actionable and understandable business intelligenceA strong sense of intellectual curiosity and ability to thrive and deliver value in an entrepreneurial working environment; flexibility to take on new roles and responsibilities as initiatives evolveAbility to work in a high-performance professional environment, with quick turn-around and evolving priorities\n\nEducation\n\nRequired: Ph.D. or Master’s degree in statistics, computer science, mathematics, economics, biophysics or directly related field\n\nSome job boards have started using jobseeker-reported data to estimate salary ranges for roles. If you apply and qualify for this role, a recruiter will discuss accurate pay guidance.\n\n\n\nAt Citizens we value diversity, equity and inclusion, and treat everyone with respect and professionalism. Employment decisions are based solely on experience, performance, and ability. Citizens, its parent, subsidiaries, and related companies (Citizens) provide equal employment and advancement opportunities to all colleagues and applicants for employment without regard to age, ancestry, color, citizenship, physical or mental disability, perceived disability or history or record of a disability, ethnicity, gender, gender identity or expression (including transgender individuals who are transitioning, have transitioned, or are perceived to be transitioning to the gender with which they identify), genetic information, genetic characteristic, marital or domestic partner status, victim of domestic violence, family status/parenthood, medical condition, military or veteran status, national origin, pregnancy/childbirth/lactation, colleague’s or a dependent’s reproductive health decision making, race, religion, sex, sexual orientation, or any other category protected by federal, state and/or local laws.\n\nEqual Employment and Opportunity Employer\n\nCitizens is a brand name of Citizens Bank, N.A. and each of its respective affiliates.\n\nWhy Work for Us\n\nAt Citizens, you'll find a customer-centric culture built around helping our customers and giving back to our local communities. When you join our team, you are part of a supportive and collaborative workforce, with access to training and tools to accelerate your potential and maximize your career growth"}
{"text": "requirements. Key ResponsibilitiesData Quality Rule results to recommend Data Quality Rule modifications or, the need to investigate a Data-Related issue.Extract and analyze data to perform Root cause analysis.Document the details of RCA by Recursive Data Transformation review and determine the Root causes of defects.Improve Data Quality rules including Data Construct, Critical Data Element, Rule registration in Collibra, scripting business rules, collaborating with Technology to implement Rules, Testing implementation of rules.Collaborate with Data Owners and Process Owners to collect evidence for each phase of Data Concern Management.Maintain Data concerns/issues in HPALM (Tool in which Data concerns are logged and tracked).Minimum QualificationsStrong Data Analysis and Problem-solving skills using excel.Thrives in a collaborative environment working with cross functional teams.Strong interpersonal skills, interacting with clients both on the business side as well as technical specialists.Has worked in an agile environment.Bachelors degree in finance, Accounting, Economics, Engineering, Computer SciencePreferred Qualifications/ SkillsCollibra HPALM or similar tools in issue management.Strong communication and Presentation skill.Proficient in MS Office tools."}
{"text": "Skills: SQL, PySpark, Databricks, Azure Synapse, Azure Data Factory.\nNeed hands-on coding\nRequirements:1. Extensive knowledge of any of the big cloud services - Azure, AWS or GCP with practical implementation (like S3, ADLS, Airflow, ADF, Lamda, BigQuery, EC2, Fabric, Databricks or equivalent)2. Strong Hands-on experience in SQL and Python/PySpark programming knowledge. Should be able to write code during an interview with minimal syntax error.3. Strong foundational and architectural knowledge of any of the data warehouses - Snowflake, Redshift. Synapse etc.4. Should be able to drive and deliver projects with little or no guidance. Take ownership, become a self-learner, and have leadership qualities."}
{"text": "skills: 1 Experienced in either programming languages such as SQL and Python and/or R, big data tools such as Hadoop, or data visualization tools such as Tableau. 2 The ability to communicate effectively in writing, including conveying complex information and promoting in-depth engagement on course topics. 3 Experience working with large datasets. 4. Extensive experience in project management 5. Strong communication skills to various stakeholders in different functions and at different levels  ● Good to have skills: 1 Business context in social marketing and other market areas 2 Background with Meta, or similar companies like Amazon, Google.\n Skills: • Experienced in either programming languages such as SQL and Python and/or R, big data tools such as Hadoop, or data visualization tools such as Tableau. • The ability to communicate effectively in writing, including conveying complex information and promoting in-depth engagement on course topics. • Experience working with large datasets.  Nice to have: • Business context in social marketing and other market areas • Background with Meta, or similar companies like Amazon, Google.  Education/Experience: • Degree is mandatory • Masters in Mathematics, Statistics, a relevant technical field, or equivalent practical experience or Ph.D. Degree in a quantitative field"}
{"text": "Experience You’ll Need\n\nA PhD in statistics, mathematics, data science, machine learning, computer science, a related quantitative discipline, or equivalent work experienceDeep statistical, probabilistic, and ML expertise and intuition demonstrated by 5-7+ years of experience applying tools from those domains to answer questions in real-world datasetsStrong preference for experience working with large, experimentally generated biological datasets (microscopy, genomic, proteomic, etc) Experience independently developing and leading quantitative research projects in biology or chemistry as part of an interdisciplinary teamHigh fluency with Python, including a strong background in scientific computing using the Python numerical and data stackExperience collaboratively writing high-quality, reusable code in Python in version-controlled environmentsExperience working collaboratively in an interdisciplinary environment and communicating complex technical concepts and ideas to broad audiencesComfort and familiarity working in a cutting-edge research environment where scientific and technological setbacks and failures are likely to occur\nHow You’ll Be Supported\n\nYou will be assigned a peer trail guide to support and mentor you as you onboard and get familiar with Recursion systemsReceive real-time feedback from peers on analysis results, scientific methodology, and code quality and best practicesAbility to learn from and participate regularly in scientific brainstorming sessions and discussions with the entire Inception Labs teamOption to attend an annual conference to learn more from colleagues, network, and build your skillset\n\nThe Values That We Hope You Share\n\nWe Care: We care about our drug candidates, our Recursionauts, their families, each other, our communities, the patients we aim to serve and their loved ones. We also care about our work. We Learn: Learning from the diverse perspectives of our fellow Recursionauts, and from failure, is an essential part of how we make progress. We Deliver: We are unapologetic that our expectations for delivery are extraordinarily high. There is urgency to our existence: we sprint at maximum engagement, making time and space to recover. Act Boldly with Integrity: No company changes the world or reinvents an industry without being bold. It must be balanced; not by timidity, but by doing the right thing even when no one is looking. We are One Recursion: We operate with a 'company first, team second' mentality. Our success comes from working as one interdisciplinary team. \n\nRecursion spends time and energy connecting every aspect of work to these values. They aren’t static, but regularly discussed and questioned because we make decisions rooted in those values in our day-to-day work. You can read more about our values and how we live them every day here .\n\nMore About Recursion\n\nRecursion is a clinical stage TechBio company leading the space by decoding biology to industrialize drug discovery. Enabling its mission is the Recursion OS, a platform built across diverse technologies that continuously expands one of the world’s largest proprietary biological and chemical datasets. Recursion leverages sophisticated machine-learning algorithms to distill from its dataset a collection of trillions of searchable relationships across biology and chemistry unconstrained by human bias. By commanding massive experimental scale — up to millions of wet lab experiments weekly — and massive computational scale — owning and operating one of the most powerful supercomputers in the world, Recursion is uniting technology, biology and chemistry to advance the future of medicine.\n\nRecursion is headquartered in Salt Lake City, where it is a founding member of BioHive , the Utah life sciences industry collective. Recursion also has offices in London, Toronto, Montreal and the San Francisco Bay Area. Learn more at www.Recursion.com , or connect on X (formerly Twitter) and LinkedIn .\n\nRecursion is"}
{"text": "Requirements: \nAzure GenAI architect (understanding of vector stores and other AI components)Experience managing offshore teamsAbility to manage backlog and prioritize effectively.Qualifications:Experience: Minimum of 6-12 years of relevant experience.Education: Bachelor’s degree required.Product Management: Excellent product management skills.Agile/Scrum: Familiarity with Agile/Scrum methodologies.Cloud Platforms: Knowledge of cloud big data platforms (Azure).AI/ML: Understanding of AI/ML, including GenAI/LLM solutions"}
{"text": "skills and proficiency/expertise in analytical tools including PowerBI development, Python, coding, Excel, SQL, SOQL, Jira, and others.Must be detail oriented, focused on excellent quality deliverables and able to analyze data quickly using multiple tools and strategies including creating advanced algorithms.Position serves as a critical member of data integrity team within digital solutions group and supplies detailed analysis on key data elements that flow between systems to help design governance and master data management strategies and ensure data cleanliness.\nRequirements:5 to 8 years related experience preferred. Bachelor's degree preferred.Power BIPythonSQL/SOQLJiraExcel"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Senior Associate Data Engineer to be part of our team\n\nof top-notch technologists. You will lead and deliver technical solutions for large-scale\n\ndigital transformation projects. Working with the latest data technologies in the industry,\n\nyou will be instrumental in helping our clients evolve for a more digital future.\n\n\n\nYour Impact:\n\n• Combine your technical expertise and problem-solving passion to work closely\n\nwith clients, turning complex ideas into end-to-end solutions that transform our\n\nclients’ business\n\n• Translate clients requirements to system design and develop a solution that\n\ndelivers business value\n\n• Lead, design, develop and deliver large-scale data systems, data processing and\n\ndata transformation projects\n\n• Automate data platform operations and manage the post-production system and\n\nprocesses\n\n• Conduct technical feasibility assessments and provide project estimates for the\n\ndesign and development of the solution\n\n• Mentor, help and grow junior team members\n\n\n\nYour Skills & Experience:\n\n• Demonstrable experience in data platforms involving implementation of end to\n\nend data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platforms\n\n(Amazon Web Services, Azure or Google Cloud)\n\n• Implementation experience with column-oriented database technologies (i.e., Big\n\nQuery, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB,\n\nBigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server,\n\nOracle, MySQL)\n\n• Experience in implementing data pipelines for both streaming and batch\n\nintegrations using tools/frameworks like Glue ETL, Lambda, Google Cloud\n\nDataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle module or track level responsibilities and contributing to tasks\n\n“hands-on”\n\n• Experience in data modeling, warehouse design and fact/dimension\n\nimplementations\n\n• Experience working with code repositories and continuous integration\n\n\n\nSet Yourself Apart With:\n\n• Developer certifications for any of the cloud services like AWS, Google Cloud or\n\nAzure\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nQualifications\n\nYour Skills & Experience:\n\n• Demonstrable experience in data platforms involving implementation of end to\n\nend data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platforms\n\n(Amazon Web Services, Azure or Google Cloud)\n\n• Implementation experience with column-oriented database technologies (i.e., Big\n\nQuery, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB,\n\nBigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server,\n\nOracle, MySQL)\n\n• Experience in implementing data pipelines for both streaming and batch\n\nintegrations using tools/frameworks like Glue ETL, Lambda, Google Cloud\n\nDataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle module or track level responsibilities and contributing to tasks\n\n“hands-on”\n\n• Experience in data modeling, warehouse design and fact/dimension\n\nimplementations\n\n• Experience working with code repositories and continuous integration\n\n\n\nSet Yourself Apart With:\n\n• Developer certifications for any of the cloud services like AWS, Google Cloud or\n\nAzure\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "Experience And Skills\n\nBS/BA (or equivalent) or higher with preference to business, technology, or engineering focus.3-5 years of experience in asset management or financial services either as an investment/data technology implementation specialist or with direct relevant user, development, service, or operation experiences.Relentless desire for understanding how processes work and entrepreneurial aim to learn new skills and technologies.Strong attention to details and focus on high quality delivery.Familiarity to Aladdin infrastructure tools and process (Security Master, Green Package, Alpha, BondCalc, PMS, Dashboard, Explore, CRA, PRADA, etc.), is preferred.Basic SQL and Python experience.Shown ability to work well independently or as part of a team in an innovative, ambitious, and fast-paced environment, run multiple tasks, adapt to change, and work well under tight time restraints.A reputation as a good communicator and the ability to distill sophisticated concepts and information.Experience with financial market indices and concepts.\n\nFor California only the salary range for this position is $132,500 - $157,500. Additionally, employees are eligible for an annual discretionary bonus, and benefits including heath care, leave benefits, and retirement benefits. BlackRock operates a pay-for-performance compensation philosophy and your total compensation may vary based on role, location, and firm, department and individual performance.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock’s hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person – aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children’s educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment – the one we make in our employees. It’s why we’re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity and Affirmative Action Employer. We evaluate qualified applicants without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, disability, protected veteran status, and other statuses protected by law.\n\nWe recruit, hire, train, promote, pay, and administer all personnel actions without regard to race, color, religion, sex (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), sex stereotyping (including assumptions about a person’s appearance or behavior, gender roles, gender expression, or gender identity), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. We interpret these protected statuses broadly to include both the actual status and also any perceptions and assumptions made regarding these statuses.BlackRock will consider for employment qualified applicants with arrest or conviction records in a manner consistent with the requirements of the law, including any applicable fair chance law."}
{"text": "Experience You’ll Need \n\nA PhD in statistics, mathematics, data science, machine learning, computer science, a related quantitative discipline, or equivalent work experienceDeep statistical, probabilistic, and ML knowledge and intuition demonstrated by 3-5+ years of experience applying tools from those domains to answer questions in real-world datasetsStrong preference for experience working with large, experimentally generated biological datasets (microscopy, genomic, proteomic, etc)High fluency with Python, including a strong background in scientific computing using the Python numerical and data stackExperience collaboratively writing high-quality, reusable code in Python in version-controlled environmentsExperience working collaboratively in an interdisciplinary environment and communicating complex technical concepts and ideas to broad audiencesComfort and familiarity working in a cutting-edge research environment where scientific and technological setbacks and failures are likely to occur\nHow You’ll Be Supported\n\nYou will be assigned a peer trail guide to support and mentor you as you onboard and get familiar with Recursion systemsReceive real-time feedback from peers on analysis results, scientific methodology, and code quality and best practicesAbility to learn from and participate regularly in scientific brainstorming sessions and discussions with the entire Inception Labs teamOption to attend an annual conference to learn more from colleagues, network, and build your skillset\n\nThe Values That We Hope You Share\n\nWe Care: We care about our drug candidates, our Recursionauts, their families, each other, our communities, the patients we aim to serve and their loved ones. We also care about our work. We Learn: Learning from the diverse perspectives of our fellow Recursionauts, and from failure, is an essential part of how we make progress. We Deliver: We are unapologetic that our expectations for delivery are extraordinarily high. There is urgency to our existence: we sprint at maximum engagement, making time and space to recover. Act Boldly with Integrity: No company changes the world or reinvents an industry without being bold. It must be balanced; not by timidity, but by doing the right thing even when no one is looking. We are One Recursion: We operate with a 'company first, team second' mentality. Our success comes from working as one interdisciplinary team. \n\nRecursion spends time and energy connecting every aspect of work to these values. They aren’t static, but regularly discussed and questioned because we make decisions rooted in those values in our day-to-day work. You can read more about our values and how we live them every day here .\n\nMore About Recursion\n\nRecursion is a clinical stage TechBio company leading the space by decoding biology to industrialize drug discovery. Enabling its mission is the Recursion OS, a platform built across diverse technologies that continuously expands one of the world’s largest proprietary biological and chemical datasets. Recursion leverages sophisticated machine-learning algorithms to distill from its dataset a collection of trillions of searchable relationships across biology and chemistry unconstrained by human bias. By commanding massive experimental scale — up to millions of wet lab experiments weekly — and massive computational scale — owning and operating one of the most powerful supercomputers in the world, Recursion is uniting technology, biology and chemistry to advance the future of medicine.\n\nRecursion is headquartered in Salt Lake City, where it is a founding member of BioHive , the Utah life sciences industry collective. Recursion also has offices in London, Toronto, Montreal and the San Francisco Bay Area. Learn more at www.Recursion.com , or connect on X (formerly Twitter) and LinkedIn .\n\nRecursion is"}
{"text": "requirements (while not losing sight of the higher-priority market needs) and then keep the teams informed and involved throughout the product development process with excellent communication skills. You communicate directly and regularly with internal and external stakeholders to fully understand their workflows, mindsets, and experiences using our products. You serve as a voice of the customer, incorporating client feedback into requirements, designs, and feature roadmaps. You work closely with your User Experience and Design counterparts to deliver high-quality formulations and metrics. \n\nWhat You’ll Need\n\n Qualifications & Experience \n\n Experience with Asset Management, Inventory Management, Procurement Management, Facilities Management, or Construction Cost Estimation.  Knowledge and experience in Reliability Centered Maintenance and Preventive Maintenance.  Familiarity with standards such as ISO 55000 (Asset Management) and ISO 31000 (Risk Management).  Understanding of ESG (Environmental, Social, and Corporate Governance) principles, Energy & Sustainability in the context of asset management.  Strong evidence of keeping commitments and delivering results.  Demonstrated ability to interpret and analyze data to support decision-making.  Superb communication skills for both oral and written communication; ability to communicate confidently across all levels in the organization. \n\n Behavioural Competencies \n\n Entrepreneurial :High energy level, sense of urgency and intellectual curiosity, confident, thorough, not afraid to make decisions, oriented toward practical solutions. Passion for developing great products! Analytical: Evaluate different construction methods and options in order to determine the most cost-effective solutions or recommendations that meet the required specifications  Confident, without an ego, to have both the conviction and willingness to make decisions as well as the confidence to seek collaborative solutions  : Ability to overcome setbacks and enthusiastically persist until ambitious goals are achieved—must be resourceful, creative, and innovative.  Results-oriented team player who leads by example, holds self-accountable for performance, takes ownership, and champions efforts with enthusiasm and conviction. \n\nWho We Are\n\nBrightly, the global leader in intelligent asset management solutions enables organizations to transform the performance of their assets. Brightly’s sophisticated cloud-based platform leverages more than 20 years of data to deliver predictive insights that help users through the key phases of the entire asset lifecycle. More than 12,000 clients of every size worldwide depend on Brightly’s complete suite of intuitive software – including CMMS, EAM, Strategic Asset Management, IoT Remote Monitoring, Sustainability, and Community Engagement. Paired with award-winning training, support, and consulting services, Brightly helps light the way to a bright future with smarter assets and sustainable communities.\n\n The Brightly culture \n\nService. Ingenuity. Integrity. Together. These values are core to who we are and help us make the best decisions, manage change, and provide the foundations for our future. These guiding principles help us innovate, flourish, and make a real impact in the businesses and communities we help to thrive. We are committed to the great experiences that nurture our employees and the people we serve while protecting the environments in which we live.\n\n Together we are Brightly \n\n\n\nSiemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law.\n\nReasonable Accommodations\n\nIf you require a reasonable accommodation in completing a job application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please fill out the accommodations form by clicking on this link Accomodation for disablity form If you’re unable to complete the form, you can reach out to our AskHR team for support at 1-866-743-6367. Please note our AskHR representatives do not have visibility of application or interview status.\n\n\n\nApplicants and employees are protected under Federal law from discrimination. To learn more, Click here .\n\nPay Transparency Non-Discrimination Provision\n\nSiemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here .\n\nCalifornia Privacy Notice\n\nCalifornia residents have the right to receive additional notices about their personal information. To learn more, click here ."}
{"text": "experience for our TikTok users. \n\nE-commerce - Alliance \nThe E-commerce Alliance team aims to serve merchants and creators in the e-commerce platform to meet merchants' business indicators and improve creators' creative efficiency. By cooperating with merchants and creators, we aim to provide high-quality content and a personalized shopping experience for TikTok users, create efficient shopping tools at seller centers, and promote cooperation between merchants and creators.\n\nE-commerce - Search\nThe Search E-Commerce team is responsible for the search algorithm for TikTok's rapidly growing global e-commerce business. We use state-of-the-art large-scale machine learning technology, the cutting-edge NLP, CV and multi-modal technology to build the industry's top-class search engine to provide the best e-commerce search experience, for more than 1 billion monthly active TikTok users around the world. Our mission is to build a world where \"there is no hard-to-sell good-priced product in the world\".\n\nE-commerce - Search Growth \nThe Search Growth E-commerce team is at the forefront of developing the search recommendation algorithm for TikTok's rapidly expanding global e-commerce enterprise. Utilizing cutting-edge machine learning technology, advanced NLP, CV, recommendation, and multi-modal technology, we're shaping a pioneering engine within the industry. Our objective is to deliver the ultimate e-commerce search experience to over 1 billion active TikTok users worldwide. experience, and promote healthy ecological development \n\nQualifications\n\n Qualifications\n- Bachelor above degree in computer science or relevant areas.\n- 3+ years of experience with a solid foundation in data structure and algorithm design, and be proficient in using one of the programming languages such as Python, Java, C++, R, etc.;\n- Familiar with common machine/deep learning, causal inference, and operational optimization algorithms, including classification, regression, clustering methods, as well as mathematical programming and heuristic algorithms;\n- Familiar with at least one framework of TensorFlow / PyTorch / MXNet and its training and deployment details,as well as the training acceleration methods such as mixed precision training and distributed training;\n- Familiar with big data related frameworks and application, those who are familiar with MR or Spark are preferred\n\nPreferred Qualifications:\n- Experience in recommendation systems, online advertising, ranking, search, information retrieval, natural language processing, machine learning, large-scale data mining, or related fields.\n- Publications at KDD, NeurlPS, WWW, SIGIR, WSDM, ICML, IJCAI, AAAI, RECSYS and related conferences/journals, or experience in data mining/machine learning competitions such as Kaggle/KDD-cup etc.\n\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $145000 - $355000 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "skills and experience. You receive state-of-the-art training in a variety of domains, and lead the organization to adopting new and innovative methods to solve our clients’ hardest questions. You won’t just be a technical expert: you will intersect between data science, and commercial business understanding, across a variety of domains to provide analytically driven recommendations to our clients. \nData & Analytics is one of four key pillars to the Solutions Team at L.E.K. Consulting, alongside Market Insights, Information & Research Center and Knowledge Management. The Solutions team works together to support and enable our consulting team members to generate best in class insights for our clients. \n\nQualifications and Experience: Degree in a quantitative and/or business discipline preferred, examples include: Statistics, Computer Science, Data Science, Mathematics, Operations Research, Engineering, Economics A minimum of 4 years of experience in applied data science with a solid foundation in machine learning, statistical modeling, and analysis Strong knowledge, experience, and fluency in a wide variety of tools including Python with data science and machine learning libraries (e.g., scikit-learn, TensorFlow, PyTorch), Spark, SQL; familiarity with Alteryx and Tableau preferred Technical understanding of machine learning algorithms; experience with deriving insights by performing data science techniques including classification models, clustering analysis, time-series modeling, NLP; technical knowledge of optimization is a plus Expertise in developing and deploying machine learning models in cloud environments (AWS, Azure, GCP) with a deep understanding of cloud services, architecture, and scalable solutions. (e.g., Sagemaker, Azure ML, Kubernetes, Airflow) Demonstrated experience with MLOps practices, including continuous integration and delivery (CI/CD) for ML, model versioning, monitoring, and performance tracking to ensure models are efficiently updated and maintained in production environments Hands-on experience with manipulating and extracting information on a variety of large both structured and unstructured datasets; comfort with best data acquisition and warehousing practices Experience with commercial business analytics; experience at a consulting firm / agency is a plus Proficient Excel, PowerPoint presentation and excellent communication skills, both written and oral; ability to explain complex algorithms to business stakeholdersAbility to achieve results through others; experience and proven success record working in matrix, agile and fast-growing environments; and assertive, intellectually curious and continuously driving towards excellence. \n\nCandidates responding to this posting must currently possess eligibility to work in the United States L.E.K. Consulting is"}
{"text": "experience with speech interfaces Lead and evaluate changing dialog evaluation conventions, test tooling developments, and pilot processes to support expansion to new data areas Continuously evaluate workflow tools and processes and offer solutions to ensure they are efficient, high quality, and scalable Provide expert support for a large and growing team of data analysts Provide support for ongoing and new data collection efforts as a subject matter expert on conventions and use of the data Conduct research studies to understand speech and customer-Alexa interactions Assist scientists, program and product managers, and other stakeholders in defining and validating customer experience metrics\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nBoston, MA, USA | Seattle, WA, USA\n\nBasic Qualifications\n\n 3+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience 2+ years of data scientist experience Bachelor's degree Experience applying theoretical models in an applied environment\n\nPreferred Qualifications\n\n Experience in Python, Perl, or another scripting language Experience in a ML or data scientist role with a large technology company Master's degree in a quantitative field such as statistics, mathematics, data science, business analytics, economics, finance, engineering, or computer science\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $111,600/year in our lowest geographic market up to $212,800/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2610750"}
{"text": "QUALIFICATIONS, EXPERIENCE REQUIRED:\n· Bachelor’s Degree in Mathematics, Business or related field preferred.· Prefer 5+ years of experience in an analytics or reporting role.· Experience in the insurance or financial services preferred.· Actuarial experience· Proven ability to gather insights through data analysis.· Ability to communicate effectively with peers, executive management, agents and sales staff.· 3+ years of SQL experience· Must possess excellent organizational and time management skills and desire to work in a fast paced team environment."}
{"text": "requirements and translate them into technical solutions.Utilize common ML frameworks and algorithms to develop predictive models and analytics solutions.Demonstrate proficiency in SQL, preferably with experience in Snowflake, for data manipulation and analysis.Lead efforts in containerized environments to ensure scalability and efficiency of ML solutions.Stay updated with the latest advancements in ML/AI technologies and incorporate them into existing projects where applicable.Provide technical leadership and mentorship to junior team members.Communicate effectively with stakeholders, including presenting findings and recommendations to both technical and non-technical audiences.\n\nMust-Haves\n\n2-3 years of experience building, deploying, and supporting end-to-end ML pipelines.Minimum of 3 years of experience with Python.Good understanding of common ML frameworks and algorithms.Strong SQL skills, ideally with experience in Snowflake.\n\nDesired\n\nExperience with containerized environments and solving scalability issues.Proficiency in Spark/Databricks for big data processing.Familiarity with OpenAI / LLM models for natural language processing tasks.Experience with CICD tools such as Azure DevOps, Jenkins, GitHub, or similar platforms.\n\nThis role offers an exciting opportunity to work on cutting-edge ML/AI initiatives within a dynamic and collaborative environment. The ideal candidate will possess a combination of technical expertise, strong communication skills, and a passion for innovation in the healthcare industry. If you are a self-motivated individual with a desire to make a significant impact, we encourage you to apply for this position.\n\n#INAPR2024"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Data Engineering Architect -AWS Cloud to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating them to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and subsystems on production servers\n\n• Mentor, support and manage team members\n\nYour Skills & Experience:\n\n• 8+ years of demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platform- AWS (Amazon Web Services)\n\n• Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension concepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAnnual base pay range: $123,000 - $184,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "skills. This person would also engage in requirements and solution concept development, requiring strong analytic and communication skills.\n\n requirements  Optimally leverage the data management tool components for developing efficient solutions for data management, data storage, data packaging and integration. Develop overall design and determine division of labor across various architectural components  Deploy and customize Standard Architecture components that can be reused  Assist in development of task plans including schedule and effort estimation \n\n Skills and Qualifications: \n\n Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required  Highly experienced in Data analytical functions,  Azure cloud experience (ADLS, ADF, Synapse, Logic Apps, Event Hub)  Strong data architecture & modeling skills  10+ years’ total experience in data space, devising end to end data solutions.  6+ years’ advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns  2+ years’ experience in a data engineering, leveraging Python, Pyspark, etc. \n\n if interested please send resumes to [email protected] \n\n\n\nApex Systems is \n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\n4400 Cox Road\n\nSuite 200\n\nGlen Allen, Virginia 23060\n\nApex Systems is"}
{"text": "experience. 3+ years of experience to work on specific code in our Ads space to implement new privacy controls.\nDesigns, develops, and implements Hadoop eco-system-based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation.\nExperience/Skills Required:\nExperience in building scalable, robust applications using Big Data technologies like Hadoop, Spark, Hive, Map reduce.Proficient with SQLExperience with Scripting – Python, shell.Nice to have - Experience with MySQLNice to have – experience with GCP, BigQuery, Apache Nifi.Experience in Scala and Java is a plus\nBachelors degree in Computer Science, Information Technology, or related field and minimum 5 years experience in computer programming, software development or related"}
{"text": "requirements, and integrated management systems for our countries civilian agencies (FAA, FDIC, HOR, etc.).Our primary mission is to best serve the needs of our clients by solutioning with our stakeholder teams to ensure that the goals and objectives of our customers are proactively solutioned, such that opportunities to invest our time in developing long-term solutions and assets are abundant and move our clients forward efficiently.At DEVIS, we are enthusiastic about our research, our work and embracing an environment where all are supported in the mission, while maintaining a healthy work-life balance.\nWe are currently seeking a Data Analyst to join one of our Department of State programs. The candidate would support the Bureau of Population, Refugees, and Migration (PRM) Refugee Processing Center (RPC) in Rosslyn, VA. The ideal candidate must be well-versed in ETL services and adept at gathering business requirements from diverse stakeholders, assessing the pros/cons of ETL tools, and conducting dynamic hands-on evaluation of ETL solutions. The successful candidate will turn data into information, information into insight and insight into business decisions. Data analyst responsibilities include conducting full lifecycle analysis to include requirements, activities and design. Data Analysts will develop analysis and reporting capabilities. They will also monitor performance and quality control plans to identify improvements. ResponsibilitiesInterpret data, analyze results using statistical techniques and provide ongoing reportsDevelop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and qualityAcquire data from primary or secondary data sources and maintain databases/data systemsIdentify, analyze, and interpret trends or patterns in complex data setsFilter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problemsWork with management to prioritize business and information needsLocate and define new process improvement opportunitiesRelated duties or special projects as assigned Required Skills and QualificationsProven working experience as a data analyst or business data analystTechnical expertise regarding data models, database design development, data mining and segmentation techniquesStrong knowledge of and experience with reporting packages (Business Objects etc.), databases (SQL etc.), programming (XML, JavaScript, or ETL frameworks)Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc.)Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracyAdept at queries, report writing and presenting findings Education / CertificationsBachelor’s Degree in Mathematics, Computer Science, Information Management or Statistics Clearance RequirementsMust be a U.S. Citizen with the ability to obtain and maintain a Secret clearance \nAdditional Perks/BenefitsCompetitive salary compensation 401k Retirement Contribution Savings Plan"}
{"text": "experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.\nAscendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:Build the coolest tech for world’s leading brandsSolve complex problems – and learn new skillsExperience the power of transforming digital engineering for Fortune 500 clientsMaster your craft with leading training programs and hands-on experience\nExperience a community of change makers!Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\n *** About the Role ***  Job Title: Senior Azure Cloud Data Engineer\nKey requirements and design suitable solutions.Optimize and troubleshoot existing data pipelines for performance and reliability.Ensure data security, integrity, and compliance with best practices.Stay updated on the latest Azure cloud technologies and recommend improvements to enhance data processing capabilities.\nMinimum Qualifications:Strong expertise in Spark and Databricks for big data processing.Minimum 8 to 10+ years of proven experience as a Data Engineer with a focus on Azure cloud services.Proficiency in SQL and NoSQL databases, as well as data modeling techniques.Experience with data warehousing and analytics tools, such as Azure Synapse Analytics, Power BI, or Azure Analysis ServicesShould have knowledge in languages such as Python, Scala, or Java.Experience with data modeling, ETL processes, and data warehousing.Excellent problem-solving and communication skills.Data engineering certifications or relevant Azure certifications. \nDesired Qualifications:Nice to have experience in Mortgage / Banking domain.Azure certifications related to data engineering.Familiarity with machine learning concepts and implementations.Experience with streaming data solutions.\nLocation: 100% Remote role (Needs to work as per CST Time Zone) Salary Range: The salary for this position is between $67,000 – $117,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience and other qualifications of the successful candidate.\nThis position is eligible for commissions in accordance with the terms of the Company’s plan. Commissions for this position are estimated to be based on individual performance. Additionally, this role is also eligible for bonus based on achievement of mutually agreed KRAs.\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [12-15 days of paid vacation time] [6-8 weeks of paid parental leave after a year of service] [9 paid holidays and 2 floating holidays per calendar year] [Ascendion Learning Management System] [Tuition Reimbursement Program] Want to change the world? Let us know.Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!"}
{"text": "experience, an annualized transactional volume of $140 billion in 2023, and approximately 3,200 employees located in 12+ countries, Paysafe connects businesses and consumers across 260 payment types in over 40 currencies around the world. Delivered through an integrated platform, Paysafe solutions are geared toward mobile-initiated transactions, real-time analytics and the convergence between brick-and-mortar and online payments. Further information is available at www.paysafe.com.\n\nAre you ready to make an impact? Join our team that is inspired by a unified vision and propelled by passion.\n\nPosition Summary\n\nWe are looking for a dynamic and flexible, Senior Data Analyst, Pricing to support our global Sales and Product organizations with strategic planning, analysis, and commercial pricing efforts . As a Senior Data Analyst , you will be at the frontier of building our Pricing function to drive growth through data and AI-enabled capabilities. This opportunity is high visibility for someone hungry to drive the upward trajectory of our business and be able to contribute to their efforts in the role in our success.\n\nYou will partner with Product Managers to understand their commercial needs, then prioritize and work with a cross-functional team to deliver pricing strategies and analytics-based solutions to solve and execute them. Business outcomes will include sustainable growth in both revenues and gross profit.\n\nThis role is based in Jacksonville, Florida and offers a flexible hybrid work environment with 3 days in the office and 2 days working remote during the work week.\n\nResponsibilities\n\n Build data products that power the automation and effectiveness of our pricing function, driving better quality revenues from merchants and consumers.  Partner closely with pricing stakeholders (e.g., Product, Sales, Marketing) to turn raw data into actionable insights. Help ask the right questions and find the answers.  Dive into complex pricing and behavioral data sets, spot trends and make interpretations.  Utilize modelling and data-mining skills to find new insights and opportunities.  Turn findings into plans for new data products or visions for new merchant features.  Partner across merchant Product, Sales, Marketing, Development and Finance to build alignment, engagement and excitement for new products, features and initiatives.  Ensure data quality and integrity by following and enforcing data governance policies, including alignment on data language. \n\n  Qualifications  \n\n Bachelor’s degree in a related field of study (Computer Science, Statistics, Mathematics, Engineering, etc.) required.  5+ years of experience of in-depth data analysis role, required; preferably in pricing context with B2B & B2C in a digital environment.  Proven ability to visualize data intuitively, cleanly and clearly in order to make important insights simplified.  Experience across large and complex datasets, including customer behavior, and transactional data.  Advanced in SQL and in Python, preferred.  Experience structuring and analyzing A/B tests, elasticities and interdependencies, preferred.  Excellent communication and presentation skills, with the ability to explain complex data insights to non-technical audiences. \n\n Life at Paysafe: \n\nOne network. One partnership. At Paysafe, this is not only our business model; this is our mindset when it comes to our team. Being a part of Paysafe means you’ll be one of over 3,200 members of a world-class team that drives our business to new heights every day and where we are committed to your personal and professional growth.\n\nOur culture values humility, high trust & autonomy, a desire for excellence and meeting commitments, strong team cohesion, a sense of urgency, a desire to learn, pragmatically pushing boundaries, and accomplishing goals that have a direct business impact.\n\n \n\nPaysafe provides equal employment opportunities to all employees, and applicants for employment, and prohibits discrimination of any type concerning ethnicity, religion, age, sex, national origin, disability status, sexual orientation, gender identity or expression, or any other protected characteristics. This policy applies to all terms and conditions of recruitment and employment. If you need any reasonable adjustments, please let us know. We will be happy to help and look forward to hearing from you."}
{"text": "requirements, processes, and workflows. They will work closely with project managers, developers, and process owners to ensure business data and reporting needs are met and projects are delivered successfully. The Senior Data Analyst is responsible for independently documenting user requirements and turning them into specifications that can be effectively implemented by system developers and report developers. They will also be responsible for identifying areas for process improvement, recommending solutions to enhance operational efficiency and effectiveness, and disseminating results in meaningful ways to multiple audiences through summary reports and dashboards.\nResponsibilities\nSpecific responsibilities include:Collaborate with cross-functional teams to identify, validate, and clarify business requirements, ensuring a comprehensive understanding of needs and expectations.Methodically identify, document, and prioritize business requirements through comprehensive analysis to support informed decision-making and project planning.Evaluate the potential impact of proposed process changes, analyzing how modifications may affect existing operations and systems, while offering mitigation strategies.Document and create clear and comprehensive business process flows and detailed requirements, ensuring transparency and traceability throughout project lifecycles.Ensure that business requirements are translated into actionable solutions that align with organizational goals, leveraging expertise to bridge the gap between needs and implementable solutions.Offer technical support in handling complex processes, systems, software, and various technical elements. Develop technical documentation while interfacing with users, providing necessary support, and training as required.Apply suitable methodologies and tools for the completion of tasks, ensuring compliance with organizational standards and requirements pertinent to specific assignments.\nQualifications\nRequired Education, Experience, and QualificationsFive years of experience solving computer, business, scientific, engineering, policy/compliance, or other discipline system/process problems.Demonstrated track record of successfully analyzing data, gathering requirements, and providing valuable insights and solutions to support business decision-making.Competence in using various data analysis/reporting tools, such as Excel, Power BI, DAS, or other relevant software.Excellent communication skills to interact with cross-functional teams, articulate complex information to diverse stakeholders, and present findings in a clear and understandable manner.Bachelor’s degree in Computer Science, Information Systems, Accounting, Engineering or other applicable discipline. \nPreferred Education, Experience, and Qualifications\nConstruction Industry Experience\nTravel Requirements\n0 to 5 % of time will be spent traveling to job site(s)/office location.\nPhysical/Work Environment Requirements\nProlonged periods of sitting at a desk and working on a computer.Remaining in a stationary position, often kneeling, standing or sitting for prolonged periods.Quiet environment.Light work that includes adjusting and/or moving objects up to 20 pounds.\nBernhard is proud to be"}
{"text": "experienced data analysts/scientists.\n\nQualifications\n\nMaster's Degree and at least 3 years of relevant experience.Strong Organization and time line management skills .Experience in AI/ML modeling approaches such as: metabolic modeling, convolutional neural networks, and Gradient-weighted Class Activation Mapping.Understand all phases of the analytic process including data collection, preparation, modeling, evaluation, and deployment.\n\nAnticipated hiring range: $100,000 - $120,000 / annual\n\nTo Apply\n\nPlease visit UVA job board: https://jobs.virginia.edu and search for “R0056431”\n\nComplete An Application And Attach\n\nCover LetterCurriculum Vitae \n\nPlease note that multiple documents can be uploaded in the box.\n\nINTERNAL APPLICANTS: Please search for \"find jobs\" on your workday home page and apply using the internal job board.\n\nReview of applications will begin January 22, 2024 and continue until the position is filled.\n\nFor questions about the position, please contact: Adam Greene, Research Program Officer (arg7ef@virginia.edu) For questions about the application process, please contact: Rhiannon O'Coin (mo2r@virginia.edu)\n\nFor more information about the School of Data Science, please see www.datascience.virginia.edu\n\nFor more information about the University of Virginia and the Charlottesville community, please see www.virginia.edu/life/charlottesville and www.embarkuva.com\n\nThe selected candidate will be required to complete a background check at the time of the offer per University policy.\n\nPHYSICAL DEMANDS This is primarily a sedentary job involving extensive use of desktop computers. The job does occasionally require traveling some distance to attend meetings, and programs.\n\nThe University of Virginia, including the UVA Health System which represents the UVA Medical Center, Schools of Medicine and Nursing, UVA Physician’s Group and the Claude Moore Health Sciences Library, are fundamentally committed to the diversity of our faculty and staff. We believe diversity is excellence expressing itself through every person's perspectives and lived experiences. We are equal opportunity and affirmative action employers. All qualified applicants will receive consideration for employment without regard to age, color, disability, gender identity or expression, marital status, national or ethnic origin, political affiliation, race, religion, sex (including pregnancy), sexual orientation, veteran status, and family medical or genetic information."}
{"text": "Qualifications:\nFluency in English (native or bilingual)Proficient in at least one programming language (Python, JavaScript, HTML, C++, C# and SQL)Excellent writing and grammar skillsA bachelor's degree (completed or in progress)\nNote: Data Annotation payment is made via PayPal. We will never ask for any money from you. PayPal will handle any currency conversions from USD."}
{"text": "Qualifications:\n\nBachelor's degree required Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future Active TS/SCI Clearance Required2+ years of relevant experience within data science or analysis2+ years of relevant experience with programming languages such as Python, R, and VBA; and query languages such as SQL2+ years of relevant experience with data visualization tools, such as Tableau, Qlik, PowerBI, d3.js and other web application technologies, or equivalent2+ years of relevant experience with SQL and NoSQL database technologies such as SQL Server, Oracle SQL2+ years of relevant experience with data extraction, transformation, and loading to support advanced analytics2+ years of relevant experience with a wide range of analytics techniques, such as statistics, machine learning, natural language processing, optimization, simulation, or closely related techniquesAbility to support a hybrid to fulltime schedule onsite in the Tampa, FL area or the Rosslyn, VA area.\n\nPreferred Qualifications:\n\nStrong strategic communication skills to include presenting quantitative concepts in easy to understand formats and actively listening to identify business problems and their causesA curious, questioning attitude that leads them to look past superficial appearances to find deeper problems, insights, and solutionsThrives in fast-paced work environment with multiple stakeholdersHigh-performing team player who believes that cross-functional teams are greater than the sum of its partsExperience leveraging data analysis to construct strategic narrativesFamiliarity with cloud computing technology, especially Microsoft AzureFamiliarity with Agile project management principlesConfidence to drive assignments to completionEagerness to learn and develop\n\nInformation for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html"}
{"text": "Skills ; Apache Spark, Azure Synapse, Azure Databricks, SQL, SSIS\nOverall IT experience: 10+ yearsNeed a Sr Data Engineer who has 5+ years of experience in Azure native services with good exposure to ADF, Synapse, ADLS Gen2, Strong SQL skills, spark.Experience in analyzing/reverse engineering SSIS packages to re-platform solution on AzureDesigning Synapse tables and implementing data solutions within the Azure ecosystem.Design , develop and implement Synapse tables to support data ingestion, transformation and storage processes.Utilize Spark Scala / SQL to build scalable and efficient data pipelines within Azure Synapse.Optimize data storage, ensuring high performance and reliability in Synapse environment.Provide expertise in troubleshooting and resolving data related issues within Azure Synapse.Collaborate with cross-functional teams to understand data requirements and translate them into technical solutions.Proven experience working with Azure Synapse Analytics.Proficiency in Spark Scala/SQL for data processing and transformation.Strong understanding of data modelling concepts and database design principles within Synapse.Ability to optimize and tune Synapse tables for performance and scalability.Excellent communication skills and the ability to work collaboratively in a team environment."}
{"text": "Skills :Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric. Proven experience with SQL, namely schema design and dimensional data modellingSolid knowledge of data warehouse best practices, development standards and methodologiesStrong experience with Azure Cloud on data integration with DatabricksBe an independent self-learner with the “let’s get this done” approach and ability to work in Fast paced and Dynamic environment\nNice-to-Have Skills:Basic understanding on ML Studio, AI/ML, MLOps etc.Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge.Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes.\n\nIf interested please provide a Word version of your resume, please confirm your C2C hourly rate expectations ($)."}
{"text": "experiences for the end users of our software stack. Build compiler toolchain to translate, convert and optimize machine learning models. Define and build user-facing application programming interfaces and software packages to enable users to interact with software. Interact with customers as needed and provide engineering roadmap to assist in prioritization of deliverables.\n\nMinimum Qualifications\n\n Bachelor's degree in Computer Science, Engineering, Information Systems, or related field and 4+ years of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.\n\n\n\nOR\n\nMaster's degree in Computer Science, Engineering, Information Systems, or related field and 3+ years of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.\n\nOR\n\nPhD in Computer Science, Engineering, Information Systems, or related field and 2+ years of Hardware Engineering, Software Engineering, Systems Engineering, or related work experience.\n\nPreferred Qualifications\n\n3 years of experience as Software Engineer, Systems Engineer, Machine Learning Engineer, or related occupation\n\n\n\nSpecial Requirements: Must have prior work experience in each of the following:\n\nWriting compiler code and optimizations including graph passesPerformance optimization using hardware-software co-design.Performant in C++, Python Experience with open-source machine learning frameworks such as PyTorch or TensorflowWriting performance and scalable software stack\n\n\n\nAlthough this role has some expected minor physical activity, this should not deter otherwise qualified applicants from applying. If you are an individual with a physical or mental disability and need an accommodation during the application/hiring process, please call Qualcomm’s toll-free number found here for assistance. Qualcomm will provide reasonable accommodations, upon request, to support individuals with disabilities as part of our ongoing efforts to create an accessible workplace.\n\nQualcomm is \n\nTo all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.\n\n\n\nQualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law.\n\nPay Range\n\n$156,500.00 - $234,500.00\n\nThe above pay scale reflects the broad, minimum to maximum, pay scale for this job code for the location for which it has been posted. Even more importantly, please note that salary is only one component of total compensation at Qualcomm. We also offer a competitive annual discretionary bonus program and opportunity for annual RSU grants (employees on sales-incentive plans are not eligible for our annual bonus). In addition, our highly competitive benefits package is designed to support your success at work, at home, and at play. Your recruiter will be happy to discuss all that Qualcomm has to offer!\n\nIf you would like more information about this role, please contact Qualcomm Careers.\n\n3061060"}
{"text": "Qualifications\n Strong knowledge in Pattern Recognition and Neural NetworksProficiency in Computer Science and StatisticsExperience with Algorithms and Data StructuresHands-on experience in machine learning frameworks and librariesFamiliarity with cloud platforms and big data technologiesExcellent problem-solving and analytical skillsStrong programming skills in languages such as Python or RGood communication and collaboration skillsMaster's or PhD in Computer Science, Data Science, or a related field"}
{"text": "requirements.\n\nWe seek candidates with knowledge of and hands‐on experience with modern business intelligence software with proven ability to work independently in converting business and functional requirements into data pipelines, complex reports, data visualizations, and dashboards, and predictive analytics.\n\nThe individual will serve as a data scientist on the team, consulting with the client to develop automated data pipelines that back intuitive and user-friendly data visualization dashboards and applications with opportunities for task and team lead roles.\n\nInitially, team tasks will include importing data from various external sources into a visualization engine and developing other web-based query applications. Ultimately, the client seeks a consulting team that is software agnostic and possesses a strategic outlook that produces relevant, timely, and actionable technical advice (e.g., advice regarding financial, logistics, contract management, and related workflows).\n\nWhat You Will Need\n\nUS Citizenship and the ability to obtain and maintain a federal SECRET security clearanceBachelor’s degreeFIVE (5) or more years of experience in:Data AnalyticsBusiness AnalyticsData VisualizationData Science \nWhat Would Be Nice To Have\n\nAn Active and current secret federal security clearanceExperience in performing software development efforts with clear examples of value added to the clientExperience interacting with end-users to understand and document business and functional requirements for complex projects, as well as collaborating with technical teams across the full software development life cycleExperience developing web-based user visualizations using JavaScript or TypeScriptUnderstanding of data visualization tools (e.g., MS Power BI, Tableau, Qlik etc.). Examples of migration between multiple visualization tools is a plusDemonstrable experience with web applications. Data API integration, data analytics, artificial intelligence, big data platforms, and automating machine learning componentsExperience in any database technology (e.g., SQL Server/Azure Cosmos DB/Amazon Athena)Experience working with query languages such as SQL or PySparkExperience in at least one statistical programming language (e.g., Python, R).\n\nWhat We Offer\n\nGuidehouse offers a comprehensive, total rewards package that includes competitive compensation and a flexible benefits package that reflects our commitment to creating a diverse and supportive workplace.\n\nBenefits Include\n\nMedical, Rx, Dental & Vision InsurancePersonal and Family Sick Time & Company Paid HolidaysPosition may be eligible for a discretionary variable incentive bonusParental Leave and Adoption Assistance401(k) Retirement PlanBasic Life & Supplemental LifeHealth Savings Account, Dental/Vision & Dependent Care Flexible Spending AccountsShort-Term & Long-Term DisabilityStudent Loan PayDownTuition Reimbursement, Personal Development & Learning OpportunitiesSkills Development & CertificationsEmployee Referral ProgramCorporate Sponsored Events & Community OutreachEmergency Back-Up Childcare ProgramMobility Stipend\n\nAbout Guidehouse\n\nGuidehouse is an \n\nGuidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance including the Fair Chance Ordinance of Los Angeles and San Francisco.\n\nIf you have visited our website for information about employment opportunities, or to apply for a position, and you require an accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.\n\nGuidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse and Guidehouse will not be obligated to pay a placement fee."}
{"text": "Qualifications and Skills: Note: These are mandatory items that all candidates MUST have when applying for this position. Please ensure that your submission addresses each of these requirement items. Candidates without these required elements will not be considered.· Bachelor’s degree in a relevant discipline from an accredited institution of higher learning.· Minimum of two (2) years of experience performing tasks similar to those stated above.· Knowledge/experience in the following areas:Financial AnalysisBusiness ProcessBusiness Process EngineeringPolicy compliance and reportingManagement experienceData Analysis\nExpertise in Microsoft Excel or Power BI, and strong experience with data analysis, financial and accounting (especially federal accounting) knowledge.· Highly motivated individual who is willing to learn, work in a team environment, is self-sufficient, and willing to do independent research to solve problems.· Self-sufficient worker and analytical thinker with the ability to normalize and manipulate data within large datasets to perform in-depth analysis and resolve complex problems.· Detail-oriented with a proven ability to adapt to a dynamic organization.· Strong interpersonal and communications skills (both oral and written) with a customer service orientation and demonstrated ability to effectively interact at all levels across the organization to build successful relationships.· Excellent consultative, conflict resolution, negotiation, and facilitation skills to gain consensus in a matrixed and complex organization.· Proven problem-solving skills with demonstrated ability to think out of the box and generate creative solutions; ability to break a complex problem down into its component parts and arrive at the appropriate solution in a timely fashion.· Must be flexible and able to thrive in a time-sensitive environment to meet strict deadlines.· Positive attitude, averse to the status quo, always looking to improve current processes and procedures; ability to see opportunities for change, capitalize on them, and implement them when appropriate for the benefit of the organization.· A self-starter with a strong work ethic who sets high standards for self and others and demonstrates enthusiasm for the mission of the team.· Please note that pursuant to a government contract, this specific position requires U.S. Citizenship· Must be able to obtain Moderate Risk Background Investigation (MBI) Public Trust T3 case type or higher.\nDesired Qualifications and Skills: It is desirable that the candidate has the following qualifications:· Proficient in Microsoft Access.· Federal government experience.· Master’s or higher degree in a relevant discipline from an accredited institution of higher learning.· Bachelor’s degree in accounting, finance, economics, or business information systems.· Experience with Momentum or UFMSJob Type: Full-time\nPay: $90,000.00 - $100,000.00 per year\nBenefits:Dental insuranceHealth insurancePaid time offVision insuranceSchedule:8 hour shiftDay shift\nWork Location: Hybrid remote in Arlington, VA 22202"}
{"text": "Requirements:Proven experience as an AI Engineer, with a strong track record of developing and deploying AI solutionsExtensive knowledge of AI algorithms, machine learning techniques and deep learning frameworksProficiency in Python and other relevant programming languagesExperience with popular AI libraries and tools such as TensorFlow, PyTorch, Keras or CaffeSolid understanding of data management, data preprocessing and feature engineeringStrong analytical and problem-solving skills, with the ability to think creatively and propose innovative AI solutionsExcellent communication and collaboration skills, with the ability to work effectively in a team environment and communicate complex ideas to both technical and non-technical stakeholdersProven leadership skills, with the ability to mentor and guide junior team members\n If you are passionate about using data to drive business decisions and have experience in data analytics and ERP systems, we would love to hear from you.\n \n Thank you,\n \n Scott Kohut\n Technology Services Recruiter\n LaSalle NetworkLaSalle Network is"}
{"text": "experience as a data engineer, data architect, with strong Python and SQL knowledge. Experience with AWS services and Databricks, and ideal if they've developed data pipelines in airflow or any streaming services (Kafka, Kinesis, etc). Expert-level competency in Big Data manipulation and transformation, both within and outside of a database. Need to have competency in API creation, and Machine Learning model deployment. Experience mentoring others and can help as a field leader for newer team members.Additional Skills & QualificationsExperience building decision-support applications based on Data Science and Machine LearningExperience building effective, efficient solutions in AWS, using Terraform and/or CloudFormation to build infrastructure as codeFamiliarity with Snowflake, Airflow, and other Big Data and data pipeline frameworksEducation, training, and certifications in engineering, computer science, math, statistics, analytics, or cloud computing."}
{"text": "experience, and boosting operational efficiency. Your work will have a direct impact on crucial decisions and projects, with significant exposure to senior leadership. This position requires a great deal of independence and a quick learning curve.\nApplyingEasy Apply applications through LinkedIn will NOT be considered. We want someone who has researched the company and is really excited about joining. Please refer to the very bottom for directions on how to apply in a way that will ensure your application is considered. \nResponsibilities- Conduct research and analytics, including identification of data sources, processing, data modeling, and translating insights into actionable recommendations.- Perform ad-hoc and flexible analyses, and deliver BI projects for various business functions.- Design research and analytic frameworks within the context of overall project goals such as website conversion optimization, subscription model enhancement, and statistical analysis of company a/b tests.- Collaborate effectively with other departments, partners, and leaders to achieve project goals.- Develop and maintain documentation and processes for data integration.- Proactively plan and communicate effectively with leadership.- Build new reports using our analytics stack, which includes Snowflake, Daasity, and Looker.\nBasic Qualifications- Bachelor’s degree in Mathematics, Economics, Statistics, or related fields with a focus on data analytics and/or statistics.- Proficient in Excel (SQL or LookML proficiency is a bonus).- Demonstrates a strong ability to learn new business knowledge, business processes, and analytical tools/techniques.- Capable of staying organized and managing tasks in a fast-paced environment.- Resourceful, detail-oriented, and solution-focused.- Possesses effective communication skills and excellent analytical abilities.- Familiarity with basic eCommerce operations is advantageous.\nApplyingIn order to be considered for the role, please email david@mycarpe.com with your resume and a non-GPT/Claude written paragraph about why you'd be great for the role. \nWe are aiming to make a hiring decision for the role by April 30th, so make sure to apply ASAP to be considered."}
{"text": "Skills: 10+ years of experience in Hadoop/big data technologies.Experience with Spark/Storm/Kafka or equivalent streaming/batch processing and event based messaging.Relational and NoSQL database integration and data distribution principles experience.Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr).Experience with API development and use of JSON/XML/Hypermedia data formats.Strong development/automation skills.Experience with all aspects of DevOps (source control, continuous integration, deployments, etc.).5+ years of hands-on experience as a Scala developer (with previous Java background).Experience in Core Banking functionality for generating various hand-offs is preferred.Experience with containerization and related technologies (e.g. Docker, Kubernetes) is preferred.Comprehensive knowledge of the principles of software engineering and data analytics is preferred.Knowledge of Agile(scrum) development methodology is a plus.Cloudera/Hortonworks/AWS EMR, S3 experience a plus.Strong Communication skills.Self-Motivated.Willingness to learn.Excellent planning and organizational skills.Education:Strong academic record, ideally with a Bachelor degree in Engineering/mathematical or scientific background."}
{"text": "requirements and develop solutions that meet both technical and business objectives.Design and execute experiments to evaluate model performance, identify areas for improvement, and iteratively refine our approaches to achieve optimal results.Drive initiatives to enhance data quality, preprocessing pipelines, and feature engineering techniques to support the training and evaluation of vision transformer models.Mentor and provide guidance to junior team members, sharing your expertise and best practices to foster their professional development and accelerate their contributions to the team.Stay abreast of emerging trends and technologies in machine learning and computer vision, proactively exploring new ideas and methodologies that can be applied to our projects.\nQualifications\nAdvanced degree (Ph.D. preferred) in computer science, statistics, mathematics, or a related field, with a strong emphasis on machine learning and computer vision.Extensive experience (5+ years) working in a data science or machine learning role, with a proven track record of developing and deploying advanced models in real-world applications.Expertise in vision transformer models, including a deep understanding of their architecture, training methodologies, and practical considerations for implementation.Proficiency in deep learning frameworks such as TensorFlow, PyTorch, or JAX, along with experience working with large-scale datasets and distributed computing environments.Strong programming skills in Python, with experience writing efficient and maintainable code for data manipulation, model training, and evaluation.Excellent analytical and problem-solving abilities, with a keen attention to detail and a passion for tackling challenging problems with innovative solutions.Effective communication skills, with the ability to clearly articulate complex ideas and collaborate effectively with colleagues from diverse backgrounds and areas of expertise.Proven ability to work independently and take ownership of projects from conception to completion, while also thriving in a fast-paced, collaborative team environment."}
{"text": "experience, regulatory compliance & operational efficiencies, enabled by Google Cloud.\n\nThis position will lead integration of core data from New North America Lending platforms into Data Factory (GCP BQ), and build upon the existing analytical data, including merging historical data from legacy platforms with data ingested from new platforms. To enable critical regulatory reporting, operational analytics, risk analytics and modeling\n\nWill provide overall technical guidance to implementation teams and oversee adherence to engineering patterns and data quality and compliance standards, across all data factory workstreams. Support business adoption of data from new platform and sunset of legacy platforms & technology stack.\n\nThis position will collaborate with technical program manager, data platform enablement manager, analytical data domain leaders, subject matter experts, supplier partners, business partner and IT operations teams to deliver the Data integration workstream plan following agile framework.\n\nResponsibilities\n\nWe are looking for dynamic, technical leader with prior experience of leading data warehouse as part of complex business & tech transformation. Has strong experience in Data Engineering, GCP Big Query, Data ETL pipelines, Data architecture, Data Governance, Data protection, security & compliance, and user access enablement.\n\nKey responsibilities -\n\nThis role will focus on implementing data integration of new lending platform into Google Cloud Data Platform (Data factory), existing analytical domains and building new data marts, while ensuring new data is integrated seamlessly with historical data. Will lead a dedicated team of data engineers & analysts to understand and assess new data model and attributes, in upstream systems, and build an approach to integrate this data into factory.Will lead the data integration architecture (in collaboration with core mod platform & data factory architects) and designs, and solution approach for Data FactoryWill understand the scope of reporting for MMP (Minimal Marketable Product) launch & build the data marts required to enable agreed use cases for regulatory, analytical & operational reporting, and data required for Risk modeling. Will collaborate with Data Factory Analytical domain teams, to build new pipelines & expansion of analytical domains. Will lead data integration testing strategy & its execution within Data Factory (end-to-end, from ingestion, to analytical domains, to marts) to support use cases.Will be Data Factory SPOC for all Core Modernization program and help facilitate & prioritize backlogs of data workstreams.Ensure the data solutions are aligned to overall program goals, timing and are delivered with qualityCollaborate with program managers to plan iterations, backlogs and dependencies across all workstream to progress workstreams at required pace.Drive adoption of standardized architecture, design and quality assurance approaches across all workstreams and ensure solutions adheres to established standards.People leader for a team of 5+ data engineers and analysts. Additionally manage supplier partner team who will execute the migration planLead communication of status, issues & risks to key stakeholders\n\n\nQualifications\n\nYou'll have…..\n\nBachelor’s degree in computer science or equivalent5+ years of experience delivering complex Data warehousing projects and leading teams of 10+ engineers and suppliers to build Big Data/Datawarehouse solutions.10+ years of experience in technical delivery of Data Warehouse Cloud Solutions for large companies, and business adoption of these platforms to build analytics , insights & modelsPrior experience with cloud data architecture, data modelling principles, DevOps, security and controls Google Cloud certified - Cloud Data Engineer preferred.Hands on experience of the following:Orchestration of data pipelines (e.g. Airflow, DBT, Dataform, Astronomer).Batch data pipelines (e.g. BQ SQL, Dataflow, DTS).Streaming data pipelines (e.g. Kafka, Pub/Sub, gsutil)Data warehousing techniques (e.g. data modelling, ETL/ELT).\n\n\nEven better, you may have….\n\nMaster’s degree in- Computer science, Computer engineering, Data science or related fieldKnowledge of Ford credit business functional, core systems, data knowledge Experience in technical program management & delivering complex migration projects.Building high performance teamsManaging/or working with globally distributed teamsPrior experience in leveraging offshore development service providers.Experience in a Fintech or large manufacturing company.Very strong leadership, communication, organizing and problem-solving skills.Ability to negotiate with and influence stakeholders & drive forward strategic data transformation.Quick learner, self-starter, energetic leaders with drive to deliver results. Empathy and care for customers and teams, as a leader guide teams on advancement of skills, objective setting and performance assessments\n\n\nYou may not check every box, or your experience may look a little different from what we've outlined, but if you think you can bring value to Ford Motor Company, we encourage you to apply!\n\nAs an established global company, we offer the benefit of choice. You can choose what your Ford future will look like: will your story span the globe, or keep you close to home? Will your career be a deep dive into what you love, or a series of new teams and new skills? Will you be a leader, a changemaker, a technical expert, a culture builder...or all of the above? No matter what you choose, we offer a work life that works for you, including:\n\nImmediate medical, dental, and prescription drug coverageFlexible family care, parental leave, new parent ramp-up programs, subsidized back-up childcare and moreVehicle discount program for employees and family members, and management leasesTuition assistanceEstablished and active employee resource groupsPaid time off for individual and team community serviceA generous schedule of paid holidays, including the week between Christmas and New Year's DayPaid time off and the option to purchase additional vacation time\n\n\nFor a detailed look at our benefits, click here:\n\n2024 New Hire Benefits Summary\n\nVisa sponsorship is not available for this position.\n\nCandidates for positions with Ford Motor Company must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire.\n\nWe are"}
{"text": "experience (3+ years) in developing and deploying machine learning models, particularly in healthcare or life sciences domains. you will be responsible for developing and implementing machine learning models and algorithms, conducting data analysis and visualization, and collaborating with computational chemists to drive innovation and deliver impactful solutions in the biotechnology industry. The first project will be to perform data normalization on chemistry files. You will also contribute to the research and development of new machine learning techniques and technologies for various biotechnology projects such as computational drug discovery.QualificationsPhD in Computer Science or Machine Learning.Strong background in machine learning, deep learning, and statistical modelingProficiency in the Python programming languages.Proven experience (4+ years) in developing and deploying machine learning models, particularly in healthcare or life sciences domains is preferred.Experience with machine learning frameworks and libraries, such as TensorFlow or PyTorchHands-on experience with data preprocessing, data normalization, feature engineering, and model evaluationExperience with (ETL) Extraction, Transform, Load.Knowledge of cloud platforms, high performance computing, and distributed computing. Strong problem-solving and analytical skillsExcellent communication and collaboration abilitiesExperience in the healthcare sector is preferred."}
{"text": "requirements on time. Supports the ongoing activities in the field of Data Analytics within BMW across multiple business units. Deals with large volumes of data, understands and explores data critical to BMW’s business. Works with different BMW business units to understand the business demands with respect to data. Position Responsibilities/Accountabilities: List the major duties/accountabilities to achieve the positions key objectives.· Achieves overall optimal solutions by working closely together with teams of specialists for business processes, IT technologies, IT processes and project managers.· Analyzes business critical data and recommends improvements. · Creating IT Technical documents.· Drafting and sending IT corporate communications.· Manages multiple project and initiatives simultaneously as needed.· Steers external providers to ensure that the appropriate resources are available, and deliverables are completed at the expected levels of quality.· Supports Agile project delivery of IT deliverables.· Translates business requirements into team deliverables.· Performs other duties as assigned by BMW Line Manager or Feature Team Lead. Position Competencies:A) Education:BA/BS degree OR the equivalent of four years’ experience in an Enterprise IT environment. B) Experience: One-year experience in one of the following areas: · IT Project Management· IT Management· IT Procurement· IT Systems Analysis· Software Development· Applying data analytics techniques in a business area· Agile Project Management· Business Requirements Analysis· Business Relationship Management· Corporate Communications\nC) Training: As applicable, per training guidelines.\nD) Licenses and/or Certifications: N/A \nE) Knowledge/Skills/Abilities: Basic = less than 1 year of experience/training needed; Intermediate = 1 – 3 years of experience/some training may be needed; Advanced = 3-5 years’ experience/no training needed; Expert = 5+ years’ experience/able to train others.\n• Basic ability to work effectively in teams.• 1+ years of MS office application skills including presentational skills.• 1+ years of knowledge in conducting statistical analytics and data modeling• 1+ years of knowledge in data discovery systems: SQL, Qlik, Power BI, Tableau, etc• 1+ years of communication and interpersonal skills, ability to foster networks and partnerships, and good working knowledge of information and computer technologies.• 1+ years of analytical skills and judgment.• 1+ years of ability to lead strategic planning, change processes, results-based management and reporting.• 1+ years of ability to lead formulation, oversight of implementation, monitoring and evaluation of development projects and or processes. • 1+ years of task management (multitasking) ability.• Fluency (written & spoken) in English language skills."}
{"text": "Qualifications\n\n3 to 5 years of experience in exploratory data analysisStatistics Programming, data modeling, simulation, and mathematics Hands on working experience with Python, SQL, R, Hadoop, SAS, SPSS, Scala, AWSModel lifecycle executionTechnical writingData storytelling and technical presentation skillsResearch SkillsInterpersonal SkillsModel DevelopmentCommunicationCritical ThinkingCollaborate and Build RelationshipsInitiative with sound judgementTechnical (Big Data Analysis, Coding, Project Management, Technical Writing, etc.)Problem Solving (Responds as problems and issues are identified)Bachelor's Degree in Data Science, Statistics, Mathematics, Computers Science, Engineering, or degrees in similar quantitative fields\n\n\nDesired Qualification(s)\n\nMaster's Degree in Data Science, Statistics, Mathematics, Computer Science, or Engineering\n\n\nHours: Monday - Friday, 8:00AM - 4:30PM\n\nLocations: 820 Follin Lane, Vienna, VA 22180 | 5510 Heritage Oaks Drive, Pensacola, FL 32526 | 141 Security Drive, Winchester, VA 22602\n\nAbout Us\n\nYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:\n\n Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services\n\n\n\n\nDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market position\n\nBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."}
{"text": "skills.50% of the time candidate will need to manage and guide a team of developers and the other 50% of the time will be completing the technical work (hands on). Must have previous experience with this (i.e., technical lead)Code review person. Each spring. Coders will do developing then candidate will be reviewing code and auditing the code to ensure its meeting the standard (final eye)Migrating to a data warehouse.\nRequired Skills:Informatica, IICS data pipeline development experienceCloud Datawarehouse (Snowflake preferred), on-prem to cloud migration experience.Ability to perform peer SIT testing with other Cloud Data EngineersDatabase - MS SQL Server, Snowflake\nNice to have:Medium priority: Informatica PowerCenter (high priority)Analytical reporting - Tableau / Qlik Sense / SAS / R (migrating existing reports - mostly Tableau / moving from Qlik View to Qlik Sense)Kafka, KubernetesFinance, Lease / Loan or Automotive experience is a plus.\nCandidate can expect a panel interview with the hiring manager and members of the team.Potential for 2nd interview to be scheduled\nWFH:This person will be onsite 100 percent of the time during training. If the candidate shows they are can work independently and productively, some flexibility could be offered to work from home. This is up to the hiring manager.\nEducation:Bachelor’s Degree in Information technology or like degree plus 5 years of IT work experience.\nexperienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. During various aspects of this process, you should collaborate with co workers to ensure that your approach meets the needs of each project.To ensure success as a data engineer, you should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. A formidable data engineer will demonstrate unsatiated curiosity and outstanding interpersonal skills.\nKey accountabilities of the function Leading Operations for Assigned Systems:Designing, implementing, and operating assigned cloud technology platforms as the technical expert.Leading internal and external resources in the appropriate utilization of cloud technology platforms.Executing ITSM/ITIL processes to ensure ongoing stable operations and alignment with SLAs.Steering providers in the execution of tier 2 and 3 support tasks and SLAs.Resolving escalated support issues.Performing routine maintenance, administering access and security levels.Driving System Management & Application Monitoring.Ensuring monitoring and correct operation of the assigned system.Ensuring changes to the system are made for ongoing run and support.Ensuring consolidation of emergency activities into regular maintenance.Analyzing system data (system logs, performance metrics, performance counters) to drive performance improvement.Supporting Agility & Customer Centricity.Supporting the end user with highly available systems.Participating in the support rotation.Performing other duties as assigned by management Additional skills: special skills / technical ability etc.Demonstrated experience in vendor and partner management.Technically competent with various business applications, especially Financial Management systems.Experience at working both independently and in a team-oriented, collaborative environment is essential.Must be able to build and maintain strong relationships in the business and Global IT organization.Ability to elicit cooperation from a wide variety of sources, including central IT, clients, and other departments.Strong written and oral communication skills.Strong interpersonal skills.\nQualifications:This position requires a Bachelor's Degree in Computer Science or a related technical field, and 5+ years of relevant employment experience.2+ years of work experience with ETL and Data Modeling on AWS Cloud Databases.Expert-level skills in writing and optimizing SQL.Experience operating very large data warehouses or data lakes.3+ years SQL Server.3+ years of Informatica or similar technology.Knowledge of Financial Services industry.\nPREFERRED QUALIFICATIONS:5+ years of work experience with ETL and Data Modeling on AWS Cloud Databases.Experience migrating on-premise data processing to AWS Cloud.Relevant AWS certification (AWS Certified Data Analytics, AWS Certified Database, etc.).Expertise in ETL optimization, designing, coding, and tuning big data processes using Informatica Data Management Cloud or similar technologies.Experience with building data pipelines and applications to stream and process datasets at low latencies.Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.Sound knowledge of data management and knows how to optimize the distribution, partitioning, and MPP of high-level data structures.Knowledge of Engineering and Operational Excellence using standard methodologies.\n HKA Enterprises is a global workforce solutions firm. If you're seeking a new career opportunity or project experience, our recruiters will work to understand your qualifications, experience, and personal goals. At HKA, we recognize the importance of matching employee goals with those of the employer. We strive to seek credibility, satisfaction, and endorsement from all of our applicants. We invite you to take time and search for your next career experience with us! HKA is an"}
{"text": "requirements may change at any time.\n\nQualifications\n\n Qualification:\n• BS degree in Computer Science, Computer Engineering or other relevant majors.\n• Excellent programming, debugging, and optimization skills in general purpose programming languages\n• Ability to think critically and to formulate solutions to problems in a clear and concise way.\n\nPreferred Qualifications:\n• Experience with one or more general purpose programming languages including but not limited to: Go, C/C++, Python.\n• Good understanding in one of the following domains: ad fraud detection, risk control, quality control, adversarial engineering, and online advertising systems.\n• Good knowledge in one of the following areas: machine learning, deep learning, backend, large-scale systems, data science, full-stack.\n\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/ktJP6\n\nThis role requires the ability to work with and support systems designed to protect sensitive data and information. As such, this role will be subject to strict national security-related screening. \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $119000 - $168150 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "skills:Proficiency in Python programming languageKnowledge of natural language processing (NLP), data science, and deep learning algorithms (RNN, CNN, etc.)Ability to implement machine learning algorithms and statistical analysisStrong presentation and teaching skills to articulate complex concepts to non-technical audiencesUnderstanding of data structures and algorithms in PythonExcellent research skills, utilizing papers, textbooks, online resources, and GitHub repositoriesPotential involvement in writing and publishing academic papers\nQualifications2nd or 3rd-year undergraduate student in computer science or statisticsRequired experience: candidates must have completed at least three of the following courses: Statistics, Machine Learning, Deep Learning, AI, and Data Structures and Algorithms.GPA of 3.5 or higher.Ability to work independently and collaborativelyExcellent problem-solving and analytical skillsStrong written and verbal communication skills\nRelevant coursework projects or teaching experience as a teaching assistant (TA) in artificial intelligence, deep learning, machine learning, big data, and data analysis is beneficial. We value a strong passion for innovative technologies and a drive to learn and teach machine learning and AI."}
{"text": "requirements and guarantee that solutions are tailored to user specifications.Manage the complete lifecycle of new features, from development and testing through to deployment and ongoing maintenance.Leverage your knowledge to guide and train colleagues and contributors throughout the company.\nRequirementsDemonstrated hands-on experience or a strong interest in applying ML in robotics.Experience in roles such as Senior Software Engineer or Machine Learning Engineer.Demonstrated skills in programming languages like Python, Julia, or R, and familiarity with associated frameworks (e.g., PyTorch, TensorFlow, Pandas, Numpy).Expertise in distributed computing and handling large datasets with technologies like Spark.Proficient with cloud services (e.g., Google Cloud, AWS, Azure) and experienced in deploying scalable ML solutions (using Docker, Kubernetes, Ansible, Terraform).Capable of managing data processing and refining techniques.Proven ability to collaborate within multidisciplinary teams.Holds a Bachelor’s degree in Engineering, Computer Science, or a related technical discipline.\nThe DetailsHealth Benefits: Comprehensive health insurance including medical, dental, and vision coverage.Retirement Plans: 401(k) plan with company match to help you invest in your future.Paid Time Off: Generous vacation, public holidays, and sick leave.Professional Development: Opportunities for professional growth through workshops, courses, and conferences.Work-Life Balance: Flexible working hours and options for remote work to balance personal and professional life.Wellness Programs: Access to wellness programs and gym memberships to maintain a healthy lifestyle.Employee Assistance Programs: Confidential counseling and support for mental and emotional well-being.\nThis position offers the chance to significantly impact the development of cutting-edge robotic technologies in a fast-paced and collaborative environment. Our client values innovative thinking and diligent work ethic."}
{"text": "experienced Data Engineer to join their team!\n\nThe Ideal Candidate Will Be\n\nSeasoned data engineer with experience in data infrastructure. Well-versed in using Python, SQL, Redshift, and AWS. Competent in creating data lakes and integrating data in AWS. Experience in building, mentoring, and growing a global engineering team. Exceptional at communicating technical concepts. Experience building and operating large scale production data pipelines. A passion for data solutions and willingness to pick up new programming languages, technologies, and frameworks.\n\nRequired Skills : - Bachelor's degree in Computer Science preferred. Will consider other relevant technical experience depending on degree. - AWS - Creation of Data Lakes - Python - Redshift - Glue - Snowflake big plus\n\nRank :A3\n\nRequested Date :2024-04-03"}
{"text": "experience.You will collaborate with other team leads and stakeholders to understand and explain industry trends and develop industry expertise across our product portfolio. Additionally, you will work with our international data operations team to develop new tools to expand and enhance your team’s ability to develop industry insights at scale and to drive real change in the US P&C insurance market. You will operate within a Shared Services model, where you will be expected to manage stakeholder expectations and organize deliverables across junior staff. You will be expected to develop expertise in multiple lines of business, AGILE project management, and advanced analytics using SQL on our cloud data warehouse, being at the forefront of data analytics for the P&C industry. Your responsibilities include: Work directly with line of business stakeholders to understand business needs, gather, manipulate, and analyze data to deliver digestible insights that drive business decisions Analyze industry data for commercial lines property coverages to identify trends, anomalies, and data quality issues, ensuring product teams are aware of impacts to our analytics driven by industry experience Develop data reports and visualizations using business intelligence and analytics software Lead quarterly presentations with LOB stakeholders to discuss data analysis results, summarizing and synthesizing analysis from junior team members, and documenting key take aways from discussions Manage our data steward relationship with the insurance carriers, providing prompt and meaningful analysis of their data to ensure their compliance with regulatory requirements and the applicability of our industry benchmark analytics to their business Drive organizational change in how we deliver results both internally and externally \n\nQualifications\n\nBachelor's degree in a STEM major or with STEM coursework learned in associated majors (Actuarial Science, Computer Science, Data Engineering, Data Science, Mathematics, Applied Mathematics, Statistics, Finance, Economics) A minimum of two years P&C insurance data analysis experience with a preferable focus on Commercial Lines Property coverages Excellent SQL, Excel/BI skills with a focus on data analysis Excellent communication skills (both oral and written) are required, with a desire to improve presentation and persuasion skills Experience supervising junior team members and a strong record of building great stakeholder relationships Experience with a general purpose (C++, JAVA) or analytical (R, Python) programming language is preferred. A passion for using data and technology to understand the real world and drive change with a focus on the importance of data quality A self-starter with a commitment to innovation and pro-active problem solving You will be part of a culture that celebrates success, recognizes and rewards achievements and excellence, and provides personal and professional enrichment opportunities. Salary commiserate with experience and location. \n\nAdditional Information\n\nFor over 50 years, Verisk has been the leading data analytics and technology partner to the global insurance industry by delivering value to our clients through expertise and scale. We empower communities and businesses to make better decisions on risk, faster.\n\nAt Verisk, you'll have the chance to use your voice and build a rewarding career that's as unique as you are, with work flexibility and the support, coaching, and training you need to succeed.\n\nFor the eighth consecutive year, Verisk is proudly recognized as a Great Place to Work® for outstanding workplace culture in the US, fourth consecutive year in the UK, Spain, and India, and second consecutive year in Poland. We value learning, caring and results and make inclusivity and diversity a top priority. In addition to our Great Place to Work® Certification, we’ve been recognized by The Wall Street Journal as one of the Best-Managed Companies and by Forbes as a World’s Best Employer and Best Employer for Women, testaments to the value we place on workplace culture.\n\nWe’re 7,000 people strong. We relentlessly and ethically pursue innovation. And we are looking for people like you to help us translate big data into big ideas. Join us and create an exceptional experience for yourself and a better tomorrow for future generations.\n\nVerisk Businesses\n\nUnderwriting Solutions — provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precision\n\nClaims Solutions — supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiences\n\nProperty Estimating Solutions — offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficient\n\nExtreme Event Solutions — provides risk modeling solutions to help individuals, businesses, and society become more resilient to extreme events.\n\nSpecialty Business Solutions — provides an integrated suite of software for full end-to-end management of insurance and reinsurance business, helping companies manage their businesses through efficiency, flexibility, and data governance\n\nMarketing Solutions — delivers data and insights to improve the reach, timing, relevance, and compliance of every consumer engagement\n\nLife Insurance Solutions – offers end-to-end, data insight-driven core capabilities for carriers, distribution, and direct customers across the entire policy lifecycle of life and annuities for both individual and group.\n\nVerisk Maplecroft — provides intelligence on sustainability, resilience, and ESG, helping people, business, and societies become stronger\n\nVerisk Analytics is \n\nAll members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability.\n\nhttp://www.verisk.com/careers.html\n\nUnsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.\n\nHR CCPA Privacy Notice.pdf"}
{"text": "skills: 2-5 y of exp with data analysis/ data integrity/ data governance; PowerBI development; Python; SQL, SOQL   Requirements: • Availability to work 100% at the Client’s site in Juno Beach, FL (required); • Experience in data analysis/ data integrity/ data governance; • Experience in analytical tools including PowerBI development, Python, coding, Excel, SQL, SOQL, Jira, and others.   Responsibilities include but are not limited to the following: • Analyze data quickly using multiple tools and strategies including creating advanced algorithms; • Serve as a critical member of data integrity team within digital solutions group and supplies detailed analysis on key data elements that flow between systems to help design governance and master data management strategies and ensure data cleanliness."}
{"text": "requirements and translate them into technical specifications for ML models.- Research and experiment with state-of-the-art machine learning algorithms and techniques to improve model performance and accuracy.- Design and implement scalable ML pipelines for data preprocessing, model training, and deployment in production environments.- Utilize deep learning frameworks (e.g., TensorFlow, PyTorch) to develop and optimize neural network architectures for Predictive Maintenance use cases.- Evaluate the performance of ML models using appropriate metrics and iterate on solutions to achieve desired outcomes.- Work closely with Cloud Ops to integrate ML models into existing systems and ensure smooth deployment at scale.- Experience deploying ML models in production environments using containerization technologies (e.g., Docker, Kubernetes) is a plus.- Hands on experience on solving various ML solutions related to Sensor data such as anomaly detection, health index, remaining useful life, etc.- Solid understanding of cloud platforms such as AWS, and experience leveraging cloud services for data storage, big data computation (Spark), and deployment. Qualifications:- Master's degree in Computer Science, Engineering, Mathematics, or related field; advanced degree preferred.- 5+ years of experience in data science, with a focus on sensor data and machine learning.- Proven track record of successfully completing projects involving IoT applications, particularly in the manufacturing industry.- Strong programming skills in Python and proficiency in popular ML libraries (e.g., scikit-learn, Keras).- Experience working with deep learning frameworks such as TensorFlow or PyTorch.- Solid understanding of time series techniques and familiarity with relevant libraries.- Ability to communicate complex technical concepts effectively to both technical and non-technical stakeholders.- Excellent problem-solving skills and a passion for driving innovation through data-driven decision-making."}
{"text": "requirements, identify opportunities, and integrate generative AI solutions into products and applications.Experiment with and evaluate various training strategies, loss functions, and regularization techniques to improve the performance and stability of generative models.Stay up-to-date with the latest research advancements in generative AI and related fields, and apply cutting-edge techniques to address real-world challenges.Conduct thorough experimentation, analysis, and validation to assess the effectiveness and robustness of generative models under different conditions and scenarios.Develop tools, frameworks, and pipelines to streamline the training, evaluation, and deployment of generative AI models in production environments.Collaborate with colleagues to publish research papers, contribute to open-source projects, and participate in academic and industry conferences and workshops.Provide technical guidance and mentorship to junior team members, and actively contribute to knowledge sharing and skill development within the organization.Continuously iterate on existing solutions and explore new directions to enhance the capabilities and performance of generative AI systems, while maintaining a focus on scalability, efficiency, and reliability.Qualifications:Bachelor's, Master's, or Ph.D. degree in Computer Science, Electrical Engineering, Mathematics, or a related field.Solid understanding of machine learning principles, deep learning frameworks (e.g., TensorFlow, PyTorch), and software development fundamentals.Proficiency in programming languages such as Python, with experience in building and deploying machine learning models in production environments.Demonstrated expertise in generative models, including GANs, VAEs, and relevant architectures, with a track record of successful projects or research publications in the field.Strong analytical and problem-solving skills, with the ability to formulate and address complex research problems and engineering challenges.Excellent communication skills, with the ability to collaborate effectively in a team environment and present technical concepts to diverse audiences.Creative thinking and a passion for exploring novel ideas and pushing the boundaries of AI technology.Experience with distributed computing, cloud platforms, and GPU acceleration is a plus.Familiarity with domain-specific applications of generative AI, such as computer vision, natural language processing, audio synthesis, or creative arts, is desirable."}
{"text": "requirements Skills Required: Have Technical Documentation Skill by translating business requirements into tech specification. Understanding of the GCP ecosystem with a focus on Big Query, DataFlow. Capability of designing and coding analytical solutions for data collections Capability of developing data quality and validation routines Capability of testing data products in development procedure\n\nSkills Preferred:\n\nStrong Oral and written communication skills o Ability to write complex SQL queries needed to query & analyze data o Ability to communicate complex solution concepts in simple terms o Ability to apply multiple solutions to business problems o Ability to quickly comprehend the functions and capabilities of new technologies.\n\nExperience Required:\n\n1 years of academic/work experience with one or more of the following: o Data design, data architecture and data modeling (both transactional and analytic) o Building Big Data pipelines for operational and analytical solutions o Running and tuning queries in databases including Big Query, SQL Server, Hive or other equivalent platforms o Data Management - including running queries and compiling data for analytics o Experience with developing code in one or more languages such as Java, Python and SQL\n\nExperience Preferred:\n\n 2+ year of experience with the following: o GCP Cloud data implementation projects experience (Dataflow, AirFlow, BigQuery, Cloud Storage, Cloud Build, Cloud Run, etc.) Experience with Agile methodologies and tools such as Rally or Jira Certification: Google Professional Data Engineer Experience programming and producing working models or transformations with modern programming languages Knowledge or experience of designing and deploying data processing systems with one or more of the technologies such as Oracle, MS SQL Server, MySQL, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, HBase, Teradata, Tableau, Qlik or Other Strong team player, with the ability to collaborate well with others, to solve problems and actively incorporate input from various sources Demonstrated customer focus, with the ability to evaluate decisions through the eyes of the customer, build strong customer relationships, and create processes with customer viewpoint Strong analytical and problem-solving skills, with the ability to communicate in a clear and succinct manner and effectively evaluates information / data to make decisions Resourceful and quick learner, with the ability to efficiently seek out, learn, and apply new areas of expertise, as needed Highly self-motivated, with the ability to work independently\n\nEducation Required:\n\nBachelor’s degree in Computer Science, Computer Engineering, Information Technology, or equivalent experience\n\nEducation Preferred:\n\nMasters degree in Computer Science, Computer Engineering, Information Technology, or equivalent experience are preferred\n\nAdditional Information :\n\nTech Skill Based Assessment is mandatory. Tech Skill assessment is not fully defined yet how it will be conducted. Hybrid and Remote but Hybrid is preferred\n\n\n\nApex Systems is \n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\n4400 Cox Road\n\nSuite 200\n\nGlen Allen, Virginia 23060\n\nApex Systems is"}
{"text": "experiences, optimize operations, and revolutionize their product offerings through seamless integration, optimization, and activation of technology and data.\nWe are seeking a talented and experienced Senior Data Engineer to join our growing team. In this position, you will be responsible for the design, development, and deployment of extraction, transformation, and load (ETL) processes and methodologies to satisfy corporate data movements across various environments. \nAbout the roleProficiency in Python, Spark, Java, SQL, DataOps.live/DBT, SnowflakePrior experience supporting Data Governance initiatives desired: Data Quality, Metadata Management (Data Cataloging, Data Lineage), Master Data Management, Data SecurityProficiency in other software engineering languages and/or methodologiesStrong knowledge of working Unix Shell scriptingProvide ongoing support for the existing ETL processes, procedures, as well as the ability to design, code and deploy new ETL packages to support application and business needs.Develop an effective working relationship with Data Analysts, Business Analysts, Database Administrators, and others in designing, developing, and implementing robust data movement solutions.Demonstrate the ability to translate business rules into ETL code while adhering to best ETL development and design practices.Demonstrate proficiency in SQL language, as well as working knowledge of common transformation methodologies, techniques, and tools.\nRequired Knowledge, Skills, and Experience Undergraduate degree with 10+ years of Data Engineering experience with specific ETL Tools (e.g., DBT, Informatica, Data Stage, etc.) or big data stackExperience in leading, hiring, developing, and building data engineering team and providing them with technical direction.Experience working with cloud technologies like Snowflake on Microsoft Azure, Amazon AWS or Google GCP Experience in reviewing and building dimensional data models to improve accessibility, efficiency and quality of data.Experience in building high quality applications, data pipelines and analytics solutions ensuring data privacy and regulatory compliance. Experience working with Business Stakeholders to understand requirements and ability to translate them into scalable and sustainable solutions. Should be proficient in writing Advanced SQLs, and expertise in performance tuning of SQLs in Snowflake.Demonstrate good understanding of development processes and agile methodologies. • Demonstrate honesty, integrity, and professionalism at all times.Excellent communication, customer service, and problem-solving skills.Exercise self-control under trying or tough circumstances, situations, or under any kind of pressureActively exchange ideas with others and build on their ideas to identify new solutions. • Outstanding planning & Organizational skillsAble to work independently or collaboratively and maintain a positive attitude. \nPreferred Qualifications/Selection CriteriaBachelor of Engineering degree with 5-10+ years of Data Engineering experience with pertinent ETL tools (e.g., DBT, Informatica, Data Stage, etc.) or big data stackLead Data Warehouse/Business Intelligence development team in building Analytics solutionsLead the design of data pipeline/ETL using emerging technologies and tools.\nIf you are passionate about data engineering and have the skills to excel in this role, please submit your resume. Be sure to highlight your experience with Azure, ETL, SQL, and Python!"}
{"text": "Qualifications\n\nBachelor’s degree or equivalent experience in a scientific discipline, mathematics, applied statistics, information technology or a related field4 years of experience or equivalent competency identifying business problems, conducting data experiments, performing statistical analysis and synthesizing findings to guide business decisionsExperience leading end-to-end data analysis projects resulting in positive business changesExpertise creating data visualizations using modern business intelligence (BI) tools, highlighting insights as relevantProficiency with industry-standard statistical analysis tools, such as SAS, R or PythonExperience training and mentoring peersExperience creating data sets with advanced SQL or other querying languages and using them to develop business strategiesExperience researching data lineageAdvanced practical knowledge of standard statistical measures, probability theory and both qualitative and quantitative techniquesExperience working with data warehousesExperience data mining or using other exploratory techniquesExperience working with structured and unstructured dataExperience writing software functions to support data analysisExperience contributing to standards and processes that increase efficiency and impactConceptual knowledge of logical and physical data modelsDomain knowledge of multiple business areas\n\nPreferred Qualifications\n\nComprehensive knowledge of at least one business area and its data, including industry or other external sourcesExperience in several business areas and familiarity with associated dataConceptual knowledge of big data, machine learning, or data science\n\nJob Summary\n\nAs a Senior Data Analyst, you'll specialize in collecting, organizing and analyzing data from various sources with the purpose of turning numbers into context and recommendations. You'll paint a picture of how the business is operating using a variety of data analysis techniques and statistics. You'll integrate, transform and improve volumes of data at the project or company level for streamlined processes, greater efficiencies and more informed decision-making. The Senior Data Analyst must demonstrate leadership among peers and extremely deep knowledge of their industry and business.\n\nResponsibilities\n\nProactively identify and communicate business opportunities and recommendations using data manipulation, programming, statistics and data visualizationUse data to support and challenge business solutions with executive leadership or other stakeholdersLead design and validation for all phases of research projects, including presentation of results, guiding iterative improvementMaintain relationships and alignment among stakeholders to meet objectivesDefine, gather, and validate project data requirements and evaluate data qualityEvaluate the quality of organizational data sourcesCreate and support business intelligence tools, databases, dashboards, reports or methodsDocument technical design standards for reporting solutions to ensure their accurate developmentIdentify, document, and use exploratory data analysis to investigate assumptionsIdentify and analyze relevant, external data to monitor the competitive environmentStay informed of industry or business trends that pertain to dataMentor teammates on any of the above responsibilities\n\nBenefits And Perks\n\nOur team members fuel our strategy, innovation and growth, so we ensure the health and well-being of not just you, but your family, too! We go above and beyond to give you the support you need on an individual level and offer all sorts of ways to help you live your best life. We are proud to offer eligible team members perks and health benefits that will help you have peace of mind. Simply put: We’ve got your back. Check out our full list of Benefits and Perks.\n\nWho We Are\n\nRocket Companies® is a Detroit-based company made up of businesses that provide simple, fast and trusted digital solutions for complex transactions. The name comes from our flagship business, now known as Rocket Mortgage®, which was founded in 1985. Today, we’re a publicly traded company involved in many different industries, including mortgages, fintech, real estate and more. We’re insistently different in how we look at the world and are committed to an inclusive workplace where every voice is heard. We’re passionate about the work we do, and it shows. We’ve been ranked #1 for Fortune’s Best Large Workplaces in Financial Services and Insurance List in 2022, named #5 on People Magazine’s Companies That Care List in 2022 and recognized as #7 on Fortune’s list of the 100 Best Companies to Work For in 2022.\n\nDisclaimer\n\nThis is an outline of the primary responsibilities of this position. As with everything in life, things change. The tasks and responsibilities can be changed, added to, removed, amended, deleted and modified at any time by the leadership group.\n\nWe are proud equal opportunity employers and committed to providing an inclusive environment based on mutual respect for all candidates and team members. Employment decisions, including hiring decisions, are not based on race, color, religion, national origin, sex, physical or mental disability, sexual orientation, gender identity or expression, age, military or veteran status or any other characteristic protected by state or federal law. We also provide reasonable accommodation to qualified individuals with disabilities in accordance with state and federal law."}
{"text": "Experience, and Diversity, Inclusion & Belonging. In addition to your take-home pay, your benefits package is a major component of your total compensation at UK. These benefits include flexible work arrangements; our 200 percent match on eligible retirement contributions; competitive health, dental, and vision coverage; tuition benefits for classes at UK, and much more.\nUK, located in Lexington, KY, is the ninth largest economic company in the state. Lexington has been ranked as one of the nation's best places to live and work, and offers a vibrant community for professionals and their families. Lexington is safe, affordable, and offers excellent education opportunities, and is home to several Fortune 500 companies and the University of Kentucky. More information regarding Lexington can be found at: https://vimeo.com/302892787\n\nJob SummaryThis position will offer flexibility for a fully, on-campus schedule or a hybrid schedule with three days on-campus and two days remote.\nThe Institute for Biomedical Informatics is looking for a Data Scientist II to join our team! This position will be primarily responsible for performing statistical analysis on large healthcare data sets to uncover insights that improve patient care and outcomes, identify important relationships and trends, and communicate findings to the University and scientific community. This position has the following skills to lead meetings and oversee statistical analyses conducted by graduate research students and staff members.\n\nRequired Education / ExperienceMaster's degree and five (5) years of experience OR equivalent (below)High School diploma or GED and eleven (11) years of experienceAssociate’s degree and nine (9) years of experienceBachelor's degree and seven (7) years of experienceDoctorate degree and two (2) years of experience\nThe University of Kentucky uses equivalencies in determining if a potential applicant meets the minimum education and experience. Related experience can be substituted for education. Click here for more information about equivalencies: http://www.uky.edu/hr/employment/working-uk/equivalencies\nRequired License/Registration/CertificationNone\nPreferred Education / ExperiencePhD preferred.\n\nUniversity Community of InclusionThe University of Kentucky is committed to a diverse and inclusive workforce by ensuring all our students, faculty, and staff work in an environment of openness and acceptance. We strive to foster a community where people of all backgrounds, identities, and perspectives can feel secure and welcome. We also value the well-being of each of our employees and are dedicated to creating a healthy place to work, learn and live. In the interest of maintaining a safe and healthy environment for our students, employees, patients and visitors the University of Kentucky is a Tobacco & Drug Free campus.\nThe University of Kentucky is \nAny candidate offered a position may be required to pass pre-employment screenings as mandated by University of Kentucky Human Resources. These screenings may include a national background check and/or drug screen."}
{"text": "Qualifications To Be Successful In This Role\n\n Due to the nature of the contract requirements, US citizenship and successful passing of CGI background check is required prior to beginning work. In addition, candidates must have the ability to obtain and maintain a DHS CISA EOD/Public Trust clearance Bachelor's degree in Computer Science or data related field required and 8+ years experience Experience developing applications/utilities using Python, Java, or Scala leveraging tools like Presto, AWS Athena, Spark or AWS Glue Design and develop utilities to transform, enhance, and clean-up data in preparation for loading to target data-lake such a Redshift, Iceberg or Elasticsearch Design and develop stored procedures for data validation Parse disparate data sources including XLS, XML, JSON and CSV files and load/output to similar formats Build logic to clean-up data, ensure compliance to defined data-dictionary Research on published APIs for identified tools with an intent to extract the data using right APIs and access points Test and debug custom data extraction utilities and validate the data-feed requirements that are part of the data-pipe line Update and maintain the data extraction utilities to comply with the changes in data sources Prior experience in information technology, contracting or other related fields Experience with Agile development concepts or an interest to learn Experience in cybersecurity Experience with testing/or requirements development An aspiration to be a perpetual learner is highly desirable Experience with project coordination and administration Experience with Jira and/or Confluence Experience on complex work assignments in matrixed organizations Exposure to or general knowledge of CISA’s Continuous Diagnostics and Mitigation Program (CDM)\n\nCGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and license and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $108,600.00 - $235,200.00.\n\n#CGIFederalJob\n\n#DHSCareers\n\nTogether, as owners, let’s turn meaningful insights into action.\n\nLife at CGI is rooted in ownership, teamwork, respect and belonging. Here, you’ll reach your full potential because…\n\nYou are invited to be an owner from day 1 as we work together to bring our Dream to life. That’s why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our company’s strategy and direction.\n\nYour work creates value. You’ll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise.\n\nYou’ll shape your career by joining a company built to grow and last. You’ll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons.\n\nCome join our team—one of the largest IT and business consulting services firms in the world.\n\nQualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, or any other legally protected status or characteristics.\n\nCGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the requisition number of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a requisition number will not be returned.\n\nWe make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.\n\nAll CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held.\n\nCGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information."}
{"text": "QualificationsData Engineering and Data Modeling skillsExperience with ETL (Extract Transform Load) processesData Warehousing and Data Analytics experienceStrong problem-solving and analytical skillsProficiency in SQL and programming languages (e.g., Python, Java)Knowledge of cloud platforms (e.g., AWS, Azure) and big data technologies (e.g., Hadoop, Spark) is a plusBachelor's degree in Computer Science, Information Systems, or a related fieldExcellent communication and collaboration skills"}
{"text": "requirements for such projectsDevelop applications and reports using Microsoft Excel, Tableau and pertinent databases\n\nMinimum Qualifications\n\nBachelor’s Degree from accredited institutionSAS, SQL and/or Tableau skills with ability to query, compile, and manipulate large datasetsAdvanced skills in Microsoft ExcelA tendency to take full ownership of a situation or deliverable. This means having pride in one’s work, being an expert in the area, and a willingness to do whatever it takes to get to a result.Understand and translate highly complex concepts to a wide range of audience. This means the ability to take a complex program or situation and break it down into simpler, constituent parts.Experience in analyzing membership growth and retention trends and identifying drivers Ability to efficiently validate data and analyses to identify potential errors in final resultsHighly analytical person who can demonstrates problem solving and critical thinking skills.Strong public speaking and oral and written communication skills with the ability to translate data to business insights (in other words, you are an analytic storyteller)Team player who contributes to creating a positive work environment and willing to pull their sleeves up to get things done with a bias towards action and prototyping towards a solution.Demonstrate intellectual curiosity and a desire to continue learning and growing. This means you want to go above and beyond to understand the business context.\n\nPreferred Qualifications\n\nExperience working in a health care delivery system or a health insurance companyKnowledge of Medicare and Medicaid programs, health care, and managed carePython skills with ability to create automated data pulls and manipulations\n\nWE ARE \n\nIf you have a disability under the Americans with Disability Act or a similar law and want a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to careers@Healthfirst.org or calling 212-519-1798 . In your email please include a description of the accommodation you are requesting and a description of the position for which you are applying. Only reasonable accommodation requests related to applying for a position within Healthfirst Management Services will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with Healthfirst Management Services.\n\n\n\nAll hiring and recruitment at Healthfirst is transacted with a valid “@healthfirst.org” email address only or from a recruitment firm representing our Company. Any recruitment firm representing Healthfirst will readily provide you with the name and contact information of the recruiting professional representing the opportunity you are inquiring about. If you receive a communication from a sender whose domain is not @healthfirst.org, or not one of our recruitment partners, please be aware that those communications are not coming from or authorized by Healthfirst. Healthfirst will never ask you for money during the recruitment or onboarding process.\n\nHiring Range*:\n\nGreater New York City Area (NY, NJ, CT residents): $67,200 - $97,155All Other Locations (within approved locations): $59,800 - $88,910\n\nAs a candidate for this position, your salary and related elements of compensation will be contingent upon your work experience, education, licenses and certifications, and any other factors Healthfirst deems pertinent to the hiring decision.\n\nIn addition to your salary, Healthfirst offers employees a full range of benefits such as, medical, dental and vision coverage, incentive and recognition programs, life insurance, and 401k contributions (all benefits are subject to eligibility requirements). Healthfirst believes in providing a competitive compensation and benefits package wherever its employees work and live.\n\nThe hiring range is defined as the lowest and highest salaries that Healthfirst in “good faith” would pay to a new hire, or for a job promotion, or transfer into this role."}
{"text": "skills:M.S. in Computer Science, Software/Computer Engineering, Information Technology, Electronics, Data Management or Applied Math with minimum of 7 years industry experience or B.S. degree with minimum (10) years industry experience.Demonstrated excellent communication skills both written and verbal.Strong data engineering and programming skillsHands on experience with C# and/or PythonProficient with SQL query languageStrong experience working with relational SQL (Microsoft SQL , MySQL, Postgres, Snowflake etc.) and non-relational SQL ( MongoDB, Kafka etc.)Very Strong experience in SnowflakeHands on experience on providing Cloud data solutions on AWS, GCP, Azure.Hands on experience with Data application life cycleSolid knowledge of database modelling and data warehouseevent driven and data streaming architectureApplication ScalabilityApplication security - SAML, OAUTH, Kerberos, JWT Token, SSOAPI DevelopmentExperience working with Windows and Linux OS Strong experience as a must:CI/CD pipeline and Build tools such as GitLab, Jenkins, CircleCI, etc.Modeling and transformation tools – DBT - AirFlowUnderstanding and ability to work with Kubernetes ThanksChinnachinnam@hankersystems.com"}
{"text": "experience, operating performance improvement, and increased safety best practices. Develop and recommend data sampling techniques, data collections, and data cleaning specifications and approaches. Apply missing data treatments as needed. (25%)Analyze data using advanced analytics techniques in support of process improvement efforts using modern analytics frameworks, including � but not limited to � Python, R, Scala, or equivalent; Spark, Hadoop file system and others (15%)Access and analyze data sourced from various Company systems of record. Support the development of strategic business, marketing, and program implementation plans. (15%)Access and enrich data warehouses across multiple Company departments. Build, modify, monitor and maintain high-performance computing systems. (5%)Provide expert data and analytics support to multiple business units (20%)Works with stakeholders and subject matter experts to understand business needs, goals and objectives. Work closely with business, engineering, and technology teams to develop solution to data-intensive business problems and translates them into data science projects. Collaborate with other analytic teams across Exelon on big data analytics techniques and tools to improve analytical capabilities. (20%)\n\nJOB SCOPE\n\nSupport business unit strategic planning while providing a strategic view on machine learning technologies. Advice and counsel key stakeholders on machine learning findings and recommend courses of action that redirect resources to improve operational performance or assist with overall emerging business issues. Provide key stakeholders with machine learning analyses that best positions the company going forward. Educate key stakeholders on the organizations advance analytics capabilities through internal presentations, training workshops, and publications. \n\nQualifications\n\nMINIMUM QUALIFICATIONS\n\nEducation: Bachelor's degree in a Quantitative discipline. Ex: Applied Mathematics, Computer Science, Finance, Operations Research, Physics, Statistics, or related field4-7 years of relevant experience developing hypotheses, applying machine learning algorithms, validating results to analyze multi-terabyte datasets and extracting actionable insights is required. Previous research or professional experience applying advanced analytic techniques to large, complex datasets. Analytical Abilities: Strong knowledge in at least two of the following areas: machine learning, artificial intelligence, statistical modeling, data mining, information retrieval, or data visualization. Technical Knowledge: Proven experience in developing and deploying predictive analytics projects using one or more leading languages (Python, R, Scala, etc.). Experience working within an open source environment and Unix-based OS. Communication Skills: Ability to translate data analysis and findings into coherent conclusions and actionable recommendations to business partners, practice leaders, and executives. Strong oral and written communication skills. \n\nPreferred Qualifications\n\n- Experience with reporting applications (PowerBI, OBIEE, Tableau, etc.) and reporting languages (SQL, XSLT, XML, HTML, Visual Basic Application, etc.) - Understanding of project management philosophies (construction, scheduling, cost, estimating, etc.) Expert level coding skills (Python, R, Scala, SQL, etc) Proficiency in database management and large datasets: create, edit, update, join, append and query data from columnar and big data platforms.  Experience developing key predictive models that lead to delivering a premier customer experience, operating performance improvement, and increased safety best practices."}
{"text": "Experience\n\n10+ Years\n\nTechnical/Functional Skills\n\nData\n\nExperience Required\n\n10\n\nRoles & Responsibilities\n\nCoordinate with business team to understand the gaps and enable the process to make QMS data is one source of truth.\n\nGeneric Managerial Skills \n\nDigital : Python for Data Science"}
{"text": "skills and the ability to connect and communicate across multiple departments.• Adept at report writing and presenting findings.• Ability to work under pressure and meet tight deadlines.• Be able to read and update project and program level resource forecasts.• Identify recurring process issues and work with manager to find solutions and initiate improvements to mitigate future\nBasic Qualifications • Minimum of 5 years of experience with Clarity PPM and 5-8 years in an analyst capacity.• Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, etc.)• Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SAS, etc)• You have a high understanding of PPM disciplines, have worked in a team and covered strategic projects. Experience with Dashboard customization, configuration, user interface personalization and infrastructure management will be helpful.• Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail, accuracy, and actionable insights.• Excellent communicator, adjusting communication styles based on your audience.• Quick learner, adaptable and can thrive in new environments.• Proactive, confident, and engaging; especially when it comes to large stakeholder groups.• Capable of critically evaluating data to derive meaningful, actionable insights.• Demonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background.\nBenefitsAt HTC Global Services our associates have access to a comprehensive benefits package that includes Health, Dental, Vision, Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short- & Long-Term Disability Insurance, and a variety of other offerings.\nDiversity & InclusionOur success as a company is built on practicing inclusion and embracing diversity. HTC Global Services is committed to providing a work environment free from discrimination and harassment, where all employees are treated with respect and dignity. Together we work to create and maintain an environment where everyone feels valued, included, and respected. At HTC Global Services, our differences are embraced and celebrated. HTC is"}
{"text": "Skills include: Proficiency with Python, pyTorch, Linux, Docker, Kubernetes, Jupyter. Expertise in Deep Learning, Transformers, Natural Language Processing, Large Language Models\nPreferred Skills include:Experience with genomics dataMolecular genetics.Distributed computing tools like Ray, Dask, Spark.Masters degree or PhD in related fieldExperience in the farming industry or biotech industry and AI knowledge\nOutstanding opportunity! If you qualify, apply now."}
{"text": "experience as increase conversion rates. Work with marketing analysts and marketing operations to refine, evolve and build out new analytics strategies, models, reports, and executive dashboards/scorecards to provide transparency into the business performance. Build on your pre-existing analytics technical skills, becoming a hands-on expert with tools such as MSIGHTS, Marketo, Adobe Analytics, SAP Analytics Cloud and Excel. Leverage analytics to form recommendations that help provide best-in-class digital experiences and increase conversion rates. Embrace a data-driven approach to turn data into insights to drive results. Develop expertise in delivering performance insights across all seniority levels within Corporate Marketing. \n\nYOUR PROFILE \n\nData-driven results-oriented marketer. Relevant work experience in marketing and analytics. Experience in presenting insights from diverse data sources Ability to work cross-functionally with Marketers, Operations, Tech, Analytics, and other key teams beyond one's reporting line. Experience with the following analytics tools: SAP Analytics Cloud, Microsoft Excel, and Adobe Analytics Experience with HANA and basic SQL knowledge is preferred. Demonstrate an understanding of data visualization best practices, knowing which visualization to apply based on the dataset, the objective, the required analysis, and the audience it relates to. Understand the SAP Marketing strategy, including the end-to-end funnel process. A good baseline understanding of Interactions, MAQLs, MQLs, and SALs. Demonstrate proactive behavior by undertaking training opportunities and being curious about learning often new concepts and skills. You may be asked to share examples of relevant analytics training that you have consumed in the past 12 months \n\nAbout The Team\n\n \n\nThe A&O Team in MarTech and Analytics Services focuses on creating a data-driven culture within corporate Marketing. This team is responsible for various analytics, insights, and optimization requirements across corporate marketing\n\nWe build breakthroughs together\n\nSAP innovations help more than 400,000 customers worldwide work together more efficiently and use business insight more effectively. Originally known for leadership in enterprise resource planning (ERP) software, SAP has evolved to become a market leader in end-to-end business application software and related services for database, analytics, intelligent technologies, and experience management. As a cloud company with 200 million users and more than 100,000 employees worldwide, we are purpose-driven and future-focused, with a highly collaborative team ethic and commitment to personal development. Whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. At SAP, we build breakthroughs, together.\n\nWe win with inclusion\n\nSAP’s culture of inclusion, focus on health and well-being, and flexible working models help ensure that everyone – regardless of background – feels included and can run at their best. At SAP, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. We ultimately believe in unleashing all talent and creating a better and more equitable world.\n\nSAP is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to the values of \n\nFor SAP employees: Only permanent roles are eligible for the SAP Employee Referral Program, according to the eligibility rules set in the SAP Referral Policy. Specific conditions may apply for roles in Vocational Training.\n\n\n\nQualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability.\n\nCompensation Range Transparency: SAP believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating SAP’s commitment to pay equity. SAP provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. The targeted combined range for this position is 45,300 - 99,700 USD. The actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. Any SAP variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance. Please reference this link for a summary of SAP benefits and eligibility requirements: SAP North America Benefits.\n\nRequisition ID: 387715"}
{"text": "experience for our TikTok users. \n\nE-commerce - Alliance \nThe E-commerce Alliance team aims to serve merchants and creators in the e-commerce platform to meet merchants' business indicators and improve creators' creative efficiency. By cooperating with merchants and creators, we aim to provide high-quality content and a personalized shopping experience for TikTok users, create efficient shopping tools at seller centers, and promote cooperation between merchants and creators.\n\nE-commerce - Search\nThe Search E-Commerce team is responsible for the search algorithm for TikTok's rapidly growing global e-commerce business. We use state-of-the-art large-scale machine learning technology, the cutting-edge NLP, CV and multi-modal technology to build the industry's top-class search engine to provide the best e-commerce search experience, for more than 1 billion monthly active TikTok users around the world. Our mission is to build a world where \"there is no hard-to-sell good-priced product in the world\".\n\nE-commerce - Search Growth \nThe Search Growth E-commerce team is at the forefront of developing the search recommendation algorithm for TikTok's rapidly expanding global e-commerce enterprise. Utilizing cutting-edge machine learning technology, advanced NLP, CV, recommendation, and multi-modal technology, we're shaping a pioneering engine within the industry. Our objective is to deliver the ultimate e-commerce search experience to over 1 billion active TikTok users worldwide. Qualifications\n\n Qualifications\n- Bachelor above degree in computer science or relevant areas.\n- 3+ years of experience with a solid foundation in data structure and algorithm design, and be proficient in using one of the programming languages such as Python, Java, C++, R, etc.;\n- Familiar with common machine/deep learning, causal inference, and operational optimization algorithms, including classification, regression, clustering methods, as well as mathematical programming and heuristic algorithms;\n- Familiar with at least one framework of TensorFlow / PyTorch / MXNet and its training and deployment details,as well as the training acceleration methods such as mixed precision training and distributed training;\n- Familiar with big data related frameworks and application, those who are familiar with MR or Spark are preferred\n\nPreferred Qualifications:\n- Experience in recommendation systems, online advertising, ranking, search, information retrieval, natural language processing, machine learning, large-scale data mining, or related fields.\n- Publications at KDD, NeurlPS, WWW, SIGIR, WSDM, ICML, IJCAI, AAAI, RECSYS and related conferences/journals, or experience in data mining/machine learning competitions such as Kaggle/KDD-cup etc.\n\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $145000 - $355000 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "Contract Duration 6+ monthsPay rate up to $51.07/hr\n\nJob Description:\n\nData Analyst is responsible for pulling data to support the trending of product complaints and medical device reports utilizing data that resides in the complaint handling database for all product lines. This will include detailed data reports (e.g. graphs, charts, tables) prepared for routine trending, senior management reviews, ad-hoc requests, and cross-functional requests as needed (e.g. Regulatory, Quality Engineering, R&D). The Data Analyst will establish and maintain complex reporting formulas and templates using reporting tools such as Excel and other databases (e.g. Business Objects).\n\nBenefits:\n\nMedical, Vision, and Dental Insurance Plans401k Retirement Fund"}
{"text": "experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.\n\nJoin Team Amex and let's lead the way together.\n\nAs part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. American Express offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology of #TeamAmex.\n\nHow will you make an impact in this role?\n\n The Data Engineer in the Cyber Analytics team is responsible for building data pipelines to pull together information from different source systems; integrating, consolidating and cleansing data; and structuring it for use in individual analytics applications Should have a good understanding of extract, transform and load (ETL) tools and REST-oriented APIs for creating and managing data integration jobs, and providing data analysts and business users with simplified access to prepared data sets Experience in planning, documenting scope and estimating work effort, producing quality deliverables on time and budget using agile engineering practices Develop and debug complex code, conduct code reviews, and mentor other developers on coding in alignment with best practices Must be capable of building solutions from a framework perspective thus ensuring reusability vs. building siloed solutions which have low utility value Provide technical or analytical guidance as needed for issue management, project assessments, and reporting Engineer and develop standard data analytic service offerings to drive risk reduction opportunities for the enterprise.\n\nMinimum Qualifications\n\n Must have experience in identity and access management, infrastructure technology, big data requirement definition, security event monitoring or some combination thereof. Bachelor's Degree in Software Engineering, Computer Science, Mathematics, Information Systems, or 5+ years of experience Proficient in such programming languages as Scala, Python and SQL Proficient with big data technologies such as Spark, Iceberg, Hbase, Kafka, Hadoop, HDFS, AVRO, Trino, StarBurst Experience with Spark Streaming or other stream processing technologies Proficient with No SQL technologies such as Solr, Elastic and MPP stores such as Greenplum.\n\nPreferred Qualifications\n\n Drives Innovation and Change: Critical thinker, ability to convince and persuade based on logical presentation of well-researched facts. Highly organized, detail-orientated with the ability to synthesize large amounts of data. Builds and Leverages Relationships: Utilizes collaborative working practices. Communicates Effectively: Strong verbal and written communications skills including, strong presentation skills, and ability to elicit and translate information from business and technology SMEs in to written requirements.\n\nSalary Range: $85,000.00 to $150,000.00 annually + bonus + benefits\n\nThe above represents the expected salary range for this job requisition. Ultimately, in determining your pay, we'll consider your location, experience, and other job-related factors.\n\nWe back our colleagues and their loved ones with benefits and programs that support their holistic well-being. That means we prioritize their physical, financial, and mental health through each stage of life. Benefits include:\n\nCompetitive base salaries Bonus incentives 6% Company Match on retirement savings plan Free financial coaching and financial well-being support Comprehensive medical, dental, vision, life insurance, and disability benefits Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need 20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy Free access to global on-site wellness centers staffed with nurses and doctors (depending on location) Free and confidential counseling support through our Healthy Minds program Career development and training opportunities\n\nFor a full list of Team Amex benefits, visit our Colleague Benefits Site.\n\nAmerican Express is \n\nWe back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.\n\nUS Job Seekers/Employees - Click here to view the “Know Your Rights” poster and the Pay Transparency Policy Statement.\n\nIf the links do not work, please copy and paste the following URLs in a new browser window: https://www.dol.gov/agencies/ofccp/posters to access the three posters."}
{"text": "experience in data science, focus on generative model training, NLP and Azure ML and AI tools.• Azure DevOps experience for managing project backlog + release scope.• Agile methodology experience.• Python/R experience.• Cloud experience (Azure/GCP/AWS).• SQL proficiency. Overview:Insight Global is looking for a Senior Data Scientist to join one of our real estate asset management clients in either Orlando FL. This is a direct-hire position on a hybrid schedule (4 days/week onsite).\nThis Sr. Data Scientist will be responsible for leading the development and implementation of advanced analytical models and algorithms, to support decision-making processes and strategic planning. This resource must have a strong background in generative model training, data sampling techniques, and NLP."}
{"text": "Skills/Attributes Data analysis, verbal communication mastery, written communication mastery, Excel, project analyst skills\nEducation Requirement High School Diploma or Equivalent\nNote: The Company is committed to complying with the California Privacy Rights Act (“CPRA”) effective January 1, 2023; and all data privacy laws in the jurisdictions in which it recruits and hires employees. A Notice to California Job Applicants Regarding the Collection of Personal Information can be located on our website. Applicants with disabilities may access this notice in an alternative format by contacting NAhr@spectraforce.com. \nAbout Us: Established in 2004, SPECTRAFORCE® is one of the largest and fastest-growing diversity-owned staffing firms in the US. The growth of our company is a direct result of our global client service delivery model that is powered by our state-of-the-art A.I. proprietary talent acquisition platform, robust ISO 9001:2015/ISO 27001 certified processes, and strong and passionate client engaged teams. We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 140 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation. SPECTRAFORCE is built on a concept of “human connection,” defined by our branding attitude of NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people and clients can reach their highest potential. Learn more at: http://www.spectraforce.com Benefits: SPECTRAFORCE offers ACA compliant health benefits as well as dental, vision, accident, critical illness, voluntary life, and hospital indemnity insurances to eligible employees. Additional benefits offered to eligible employees include commuter benefits, 401K plan with matching, and a referral bonus program. SPECTRAFORCE provides unpaid leave as well as paid sick leave when required by law.\nEqual Opportunity Employer: SPECTRAFORCE is"}
{"text": "experience.Strong SQL, Python or R skills, ability to perform effective querying involving multiple tables and subqueries. - -Experience with SQL and BI tooling is strongly preferred.Examine, interpret and report results of analytical initiatives to stakeholders.Build, develop and maintain health data models, reporting systems, dashboards and performance metrics support that support key business decisions.Experience and knowledge of statistical modeling techniques. A/B Testing experience in a product analytics capacity is a plus.Excellent oral and written communication skills; ability to present complex information in an understandable and compelling manner Must have:4+ years of strong SQL experience (Core SQL competencies)Good experience working with BI tools (Power Bi, Tableau, etc)Experience working with stakeholders and communicating results of analytical initiativesExperience with A/B testing in a product analytics environmentVery strong interpersonal and communication skills with a lot of confidencePlusses:Python expComing from any logistical background"}
{"text": "experience in a data analytics roleBS in Business or related fieldAdvanced Excel skills requiredRobust analytical skills: ability to use and analyze information to glean insights and shape business strategyStrong communication skills to articulate complex data analysisExcellent problem solving skills Ability to work across teams to bring data quality and product enhancements to lifeBalance the analytics of a project while maintaining command of the larger strategic goalsManage multiple internal stakeholders and partner with other business units\n\nWhat’s In It For You\n\nCompetitive compensation, benefits and generous time-off policies4-Day summer work weeks and a winter holiday break401(k) / DCPP matchingAnnual bonus programCasual, dog-friendly, and innovative office spaces\n\nDon’t Just Take Our Word For It\n\n10X Virginia Business Best Places to Work9X Washingtonian Great Places to Work9X Washington Post Top WorkplaceSt. Louis Post-Dispatch Best Places to Work\n\nAbout CARFAX\n\nCARFAX, part of S&P Global Mobility, helps millions of people every day confidently shop, buy, service and sell used cars with innovative solutions powered by CARFAX vehicle history information. The expert in vehicle history since 1984, CARFAX provides exclusive services like CARFAX Used Car Listings, CARFAX Car Care, CARFAX History-Based Value and the flagship CARFAX® Vehicle History Report™ to consumers and the automotive industry. CARFAX owns the world’s largest vehicle history database and is nationally recognized as a top workplace by The Washington Post and Glassdoor.com. Shop, Buy, Service, Sell – Show me the CARFAX™. S&P Global Mobility is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets.\n\nCARFAX is an Affirmative Action/Equal Opportunity Employer. It is the policy of CARFAX to provide"}
{"text": "experiences, beliefs, backgrounds, expertise, talent, and individuality of everyone. We purposefully and meaningfully weave DEI into every aspect of our business. We seek to promote love, inclusion, and happiness in all that we do. #LoveMatters\n\nWe are committed to working and succeeding by incorporating our Guiding Principles into everything we do:\n\nWe can all win together\n\nWe do as we say\n\nDo less and do best\n\nWe are borrowing this earth from our children\n\nLove Matters\n\nThe Role\n\nAs our Senior Data Engineer, you are responsible for optimizing the data ingestion infrastructure that underpins our analytics and operations platforms. Your expertise is critical in architecting robust and scalable systems that can handle the influx of data from various sources, including but not limited to databases, APIs, and streaming platforms. By leveraging cutting-edge technologies and best practices in data engineering, you enable our organization to harness the full potential of its data assets. As our Senior Data Engineer, you are the backbone of our data ecosystem, empowering our organization to derive actionable insights and drive informed decision-making through your adept management and optimization of data ingestion processes.\n\nThis position is remote and will report into Lovesac Corporate HUB based in Stamford, CT. Candidates must fully reside in the United States at all times during employment and should have the ability to travel as needed.\n\nSummary Of Key Job Responsibilities\n\nDesign and architect event-driven data infrastructure on Azure.Build data pipelines for ingesting, processing, and routing events using technologies such as Kafka, Azure Data Factory, Spark streaming, and others.Design and build a data Lakehouse architecture for seamless data management.Create cooperative frameworks for stream, batch, and real-time processing.Develop data models, schemas, and standards for event data.Optimize data replication, loading, storage, and access patterns for fast querying.Improve data reliability, discoverability, and observability.Enhance planning, development, and deployment processes for efficiency.Drive cross-pillar collaboration with Domain Architecture, product managers, and data scientists.Support the data requirements of new and existing solutions by developing scalable and extensible physical data models.Drive efficiency and resilience by mapping data flows, ensuring standardization, and supporting real-time event-based streaming data pipelines.Own end-to-end data and data applications, defining, monitoring, and handling incidents for overall system health.Ensure compliance with data-related requirements and accuracy through standardization and automation.Continuously evolve your craft by staying up-to-date with the latest developments in data engineering and promoting their application within the community.Responsible to meet or exceed all goals and key performance indicators (KPIs).Perform any other duties as requested by management. \n\nRequirements & Qualifications\n\nA bachelor's degree in computer science, MIS, or a related field is preferred.Minimum of 5 years of experience in data engineering or related fields using server-side programming languages like Scala and Python.5+ years of experience building data pipelines and transformations at scale, utilizing technologies such as Kafka, Spark, MySQL, and Azure Data Factory.5+ years of experience in data modeling and handling data streaming.Experience with Lakehouse architecture on cloud storage, storage layers like Delta Lake, SQL, Python, or R.Exemplify each of our Lovesac values, at all times, be results driven and utilize knowledge to meet or exceed key performance indicators (KPIs), goals and deadlines.Must be able to travel using various forms of transportation, as required by the Company in its sole discretion, for meetings and conferences held either at our offices or offsite (i.e. quarterly team connection weeks, companywide meetings).Must comply with all policies and procedures outlined in the Lovesac Employee Handbook and work collaboratively with fellow employees, treating all clients, both internal and external with dignity and respect at all times.Our customers have the opportunity to shop with us seven days a week and select positions may require availability outside of normal weekday hours.\n\nFull Time Benefits*\n\nFinancial Benefits: Annual Bonus Program, Annual and Inaugural Grant Equity Awards, 401K Matching Contribution, Financial Wellness Tools, Sales Incentive Program.Health and Wellness Benefits: Medical, Dental, Vision, Health Savings and Flexible Spending Accounts, Paid Parental Leave, Life/AD&D, Short Term and Long-Term Disability, Critical Illness and Accident Insurance, Employee Assistance Program.Paid Time Off: Up to 160 hours of paid time off within our fiscal calendar year, prorated from date of hire, 8 paid company recognized holidays, Summer Flex Time.Pet Insurance and generous Associate Discounts.Eligibility and terms for all benefits listed are as outlined in Lovesac’s policy and plan documents.\n\nAssociate pay will vary based on factors such as qualifications, experience, skill level and competencies.\n\nLovesac is \n\nLovesac participates in E-Verify as required by law. Immigration sponsorship is not available for this role.\n\nLovesac is committed to the principles of"}
{"text": "requirements and explore requirement development in an iterative, agile process. Additionally, they will perform research of cutting-edge data science innovation to adapt for a government environment. The incumbent will recommend and develop proposals to test data science hypotheses, prioritize research and other projects and establish project goals and perform data management activities and tasks.\nIf this describes you, Apply Now! Share with a colleague! This position can close at any time!\nSalary range: $105,985.00 – $137,784.00\nRelocation incentives may be available based on qualifications\nU.S. Citizenship\nMust be able to obtain and maintain a Top Secret security clearance\nThis is a designated drug testing position\nDegree Requirements*Degree: Mathematics, statistics, computer science, data science or field directly related to the position. The degree must be in a major field of study (at least at the baccalaureate level) that is appropriate for the position.\nor\nCombination of education and experience: Courses equivalent to a major field of study (30 semester hours) as shown in paragraph A above, plus additional education or appropriate experience.\nSee link for more education and experience details: https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/1500/data-science-series-1560/#:~:text=Basic%20Requirements%3A%201%20Degree%3A%20Mathematics%2C%20statistics%2C%20computer%20science%2C,A%20above%2C%20plus%20additional%20education%20or%20appropriate%20experience.\nKnowledge of data science techniques and methods used to conceptualize, develop, adapt, modify, and apply new models to resolve difficult problems and to achieve support for alternative solutions within the commercial and government technical community; skill in performing data management activities.\nKnowledge and skill in a broad range of DOD or private sector C4 systems, data, big data analytics, machine learning, AI, cognitive science, programs, operations, strategies, tactics, resource and information processes, force structure, and weapons systems and analytical capabilities.\nAdvanced skills in articulating and defending complex and sensitive issues with officials of diverse perspectives and often with different or opposing views. Expert ability to negotiate and defend findings and gain executive support for new program concepts. Communicates effectively both orally and in writing; negotiate complex and/or sensitive issues; and maintain good working relations.\nSkills to lead teams composed of industry, government, and academic experts; originate new ideas, projects, and methodologies; and execute projects and/or studies within established financial and/or time constraints; analyze, plan, and adjust work priorities to meet program requirements and objectives within available resources.\nSkills to perform data collection and analytic techniques in support of command requirements.\nAbility to expertly perform command staff officer tasks to include but not limited to plan, organize, and lead required staff and team meetings; generate meeting minutes, briefs, talking papers, white papers, background papers; and develop Course of Action (COA) recommendation briefs.\nDesired skills and previous roles held: Autonomous Systems, Big Data Analytics, Artificial Intelligence (AI), Machine Learning (ML), Data Visualization, Statistics, Data Science; previous roles held such as Data Engineer, Data Analyst, Data Scientist\nYou will be evaluated for this job based on how well you meet the qualifications above.\nYour application package (resume, supporting documents) will be used to determine your eligibility, and qualifications.\nClick the apply button to complete an application and upload resume (PDF or Word Doc) must include name and contact information and/or additional documents (Transcripts, certifications, Vet Docs (DD214), SF-50).\nFor tips on how to write a Federal Resume, checkout the following link(s): https://afciviliancareers.com/PDF/FederalResumeWriting.pdf\nTo receive additional information about current and future job openings with AFCS via email notification, please subscribe at https://afciviliancareers.com/find-a-job/ and sign up to “Get Career Updates.”\nU.S. citizenship required. AFCS is Equal Opportunity Employer. Must be of legal working age."}
{"text": "requirements.\n\nLead the integration of new data management technologies and software engineering tools into existing structures.\n\nQualifications\n\nBachelor’s or Master’s degree in Computer Science, Engineering, or a related technical discipline.\n\nAt least 3 years of hands-on experience in a data engineering role.\n\nStrong command over SQL, Python, and other relevant data manipulation languages.\n\nExperience with data modeling, ETL development, and data warehousing solutions, especially with platforms like Snowflake.\n\nDemonstrated ability to work with large, complex data sets.\n\nExcellent problem-solving skills and attention to detail.\n\nSuperior communication abilities that let you convey intricate concepts to a non-technical audience with clarity.\n\nProven track record of working in cross-functional teams to deliver stellar project outcomes.\n\nOther Requirements\n\nExcellent oral and written communication skills in English/Fluent in English\n\nAble to travel domestically and internationally as required\n\nAble to work in the US without sponsorship now or any time in the future\n\nAbout CAI\n\nCAI is a 100% employee-owned company established in 1996 that has grown to more than 800 people worldwide. We provide commissioning, qualification, validation, start-up, project management and other consulting services associated with operational readiness to FDA regulated and other mission-critical industries.\n\nMeeting a Higher Standard\n\nOur approach is simple; we put the client’s interests first, we do not stop until it is right, and we will do whatever it takes to get there.\n\nAs owners of CAI, we are committed to living our Foundational Principles, both professionally and personally:\n\nWe act with integrity.\n\nWe serve each other.\n\nWe serve society.\n\nWe work for our future.\n\nWith employee ownership, one person’s success is everyone’s success; we work diligently to accomplish team goals. We place Team Before Self, demonstrate Respect for Others, and possess a Can-Do Attitude (our core values). That is how we have grown exponentially.\n\nBenefits\n\nOur full-time positions offer competitive compensation and benefits which include up to 15% retirement contribution, 24 days PTO and 5 sick days per year, health insurance at extremely low cost to employee, financial support for both internal and external professional education as well as 70% long term disability paid for by the company.\n\n$122,000 - $155,000 a year\n\nAverage base salary range - not including benefits.\n\nWe are \n\nThis job description is not all inclusive and you may be asked to do other duties. CAI will also consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Chance Act (FCA) / Fair Chance Ordinance (FCO)."}
{"text": "requirements into problem statements. Analyzes and evaluates solutions both internally generated as well as third party supplied. Develops novel ways to solve problems and discover new products. Provide guidance and leadership to more junior researchers. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\nJob Description\n\nCore Responsibilities\n\nGuides the successful completion of programs that identify patterns and make decisions with minimal human intervention. Determines the technical objectives of an assignment. Leads the design of prototypes, partnering with the product team and other stakeholders through development. Conducts studies to support product or application development.Designs and implements end-to-end solutions using optimization and other advanced computer science technologies and owns live deployments. Aggregates huge amounts of data and information from large numbers of sources to discover patterns and features necessary to automate analytical models.Researches, writes and edits documentation and technical requirements, including evaluation plans, confluence pages, white papers, presentations, test results, technical manuals, formal recommendations and reports. Contributes to the company by creating patents, Application Programming Interfaces (APIs) and other intellectual property.Presents papers and/or attends conferences, as well as displaying leadership in these areas.Tests and evaluates solutions presented to the Company by various internal and external partners and vendors. Completes case studies, testing and reporting.Collaborates with teams outside of immediate work group. Represents the work team in providing solutions to technical issues associated with assigned projects.Mentor a diverse team of junior engineers in machine learning techniques, tools and concepts. Provides guidance and leadership to more junior engineers.Consistent exercise of independent judgment and discretion in matters of significance.Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary.Other duties and responsibilities as assigned.\n\nEmployees At All Levels Are Expected To\n\nUnderstand our Operating Principles; make them the guidelines for how you do your job.Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences.Win as a team - make big things happen by working together and being open to new ideas.Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.Drive results and growth.Respect and promote inclusion & diversity.Do what's right for each other, our customers, investors and our communities.\n\nDisclaimer\n\nThis information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.\n\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.\n\nEducation\n\nBachelor's Degree\n\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n\nRelevant Work Experience\n\n5-7 Years"}
{"text": "Qualifications\n\nYour Experience\n\nM.S. or Ph.D degree in Computer Science, Mathematics, Electrical Engineering or related field or equivalent military experience required8+ years industry experience in Machine Learning techniques and data analytics8+ experience in design, algorithms and data structures - Expertise with one or more of the following languages is must - Java, C++, Python, RustExperience with NLP, Recommender Systems, and LLM is strongly preferredExperience with Formal Methods toolchain (z3, cvc5, TLA+) will be a plusExcellent communication skills with the ability to influence at all levels of the organizationA self driven individual contributor and an excellent team player\n\nAdditional Information\n\nThe Team\n\nDrawing on the near real-time data collected through PAN-OS device telemetry, our industry-leading next generation insights product (AIOps for NGFW) gives large cybersecurity operators a force multiplier that provides visibility into the health of their next-generation-firewall (NGFW) devices. It enables early detection of issues at various levels of the stack via advanced time-series forecasting and anomaly detection using novel deep learning techniques. Our goal is to be able to prevent service-impacting issues in critical security infrastructure that operates 24/7/365 with zero false positives and zero false negatives.You will be working on the best large language model in the cyber security industry.\n\nOur Commitment\n\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.\n\nWe are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\n\nPalo Alto Networks is \n\nAll your information will be kept confidential according to \n\nThe compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $140,100/yr to $220,600/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here.\n\nIs role eligible for Immigration Sponsorship?: Yes"}
{"text": "experience with Python, Cloud, NoSQL, and Databricks or Snowflake. This is a remote position with 5% travel to the Nashville area. \n \n Responsibilities/skills: \n - Lead a team of data engineers to design, develop, and maintain data pipelines, ETL processes, and data storage solutions. \n -Manage data infrastructure on cloud platforms optimizing performance, scalability, and cost-efficiency \n -Collaborate with team and leadership to define strategy, roadmap, and priorities. \n -Foster a collaborative and growth-oriented work environments and provide mentorship and support to team members. \n \n Qualifications: \n -Demonstrated proficiency with Python for Data Engineering \n -5+ years in Data Engineering and experience designing or owning a data solution. \n -Experience with cloud technologies, preferably AWS. \n -Experience with data warehousing and setting up data lake houses using Databricks or Snowflake \n -Experience with NoSQL or MongoDB preferred. \n\n We can not provide sponsorship or do C2C for this position."}
{"text": "requirements and issues.\n\nWe Require\n\nDesired Skills & Experience\n\nBachelor’s Degree in Computer Science, Information Systems, or a related field (or foreign equivalent)Demonstrated ability in SQL, relational and analytical database management, Java software development, JDBC, XML, Web Services APIs, and with version control systems.\n\nDenodo is \n\nWe do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee."}
{"text": "requirements for training and evolving deep learning models and algorithms.Articulate a vision and roadmap for leveraging data as a valued corporate asset.Influence product teams through data-based recommendations.Evangelize best practices to analytics and product teams.Own the entire model development process, from identifying business requirements to presenting results and production scoring.Perform other duties as assigned.Benefits and PerksThis is a great opportunity to work for a mid-sized financial institution that is striving to be the bank of choice; one that focuses on its customers, not its competition. An organization that provides a dynamic, fulfilling work environment that is productive, collaborative and innovative.Highly visible team with a regional financial services company where your work matters and your accomplishments are recognized!Amazing opportunity for growth, healthy work/life balance and a community focused environmentWorking for an organization that focuses on company culture, inclusion and diversityOn a team whose Core values that include: Can-Do Attitude, Service at Heart and Forward Thinking50% medical coverage for you and your entire family, short/long term disability and life insurance options401(k) Life InsuranceDisability coverageThe Ideal CandidateQualifications:Master's Degree in computer science, statistics, economics, or related fields.3+ years of work and/or educational experience in machine learning or cloud computing.Experience using statistics and machine learning to solve complex business problems.Experience conducting statistical analysis with advanced statistical software.Experience with scripting languages and packages.Experience building and deploying predictive models.Experience with web scraping and scalable data pipelines.Experience with big data analysis tools and techniques.Preferred Qualifications:Up-to-date knowledge of machine learning and data analytics tools and techniques.Strong knowledge in predictive modeling methodology.Experience leveraging both structured and unstructured data sources.Willingness and ability to learn new technologies on the job.Demonstrated ability to communicate complex results to technical and non-technical audiences.Ability to work effectively in teams as well as independently across multiple tasks while meeting aggressive timelines.Strategic, intellectually curious thinker with a focus on outcomes.Professional image with the ability to form relationships across functions.Strong experience with R/RStudio, Python, SAS, SQL, NoSQL.Strong experience with Cloud Machine Learning technologies (e.g., AWS Sagemaker)."}
{"text": "experienced data engineer to join our Maps Places Data Engineering team. The successful candidate will be responsible for building scalable pipelines to create and update various feature sets to power the downstream ML models, collaborating with cross-functional teams to understand the requirements and translate them into technical solutions. Key Qualifications• Exceptional skills in Scala and Spark• Hands on experience with data processing technologies, ETL processes and feature engineering• A track record of developing scalable pipelines and delivering data promptly in a collaborative team environment  Additional Nice to Haves:• Experience in commonly used cloud services• Expertise in columnar storage such as Parquet, Iceberg• Knowledge in deep learning models Competencies: Digital : Machine Learning Experience (Years): 10 & Above\nKeywords: Machine Learning, Spark, Scala, ETL\nThanks & Regards, Bhavani Poreddy\nUS IT Recruiter 628-204-4975E-mail: Bhavanip@brillius.com"}
{"text": "experience; staff management experience required; background in R or Python essential. (PR12682)"}
{"text": "experience : 5 to 8\nWe are currently seeking an experienced and motivated Senior Data Engineer to join our client in the manufacturing sector. In this fully remote role, you will be instrumental in designing, implementing, and maintaining robust data solutions within their technology ecosystem. If you're passionate about data and enjoy working in a fast-paced, flexible environment, we want to hear from you.About RevatureRevature is one of the largest and fastest-growing employers of technology talent across the U.S., partnering with Fortune 500 companies and the biggest System Integrators to identify experienced leaders who will truly make an impact on their business.Responsibilities include:\nData Architecture Designa. Design and implement data structures for use with Microsoft Azure services, including but not limited to Azure Data Factory, Azure Synapse Analytics, Data Lake, SQL Server Integration Services (SSIS)b. Working with data architects to develop and manage data models to ensure scalability and optimal performance\nData Integrationa. Design and coordinate the implementation of ETL processes, leveraging Azure Data Factory, SSIS, and other relevant toolsb. Ensure seamless integration with diverse data sources and existing systemsc. Recommend and implement optimized data integration dataflows and pipelines, focusing on efficiency and reliability\nData Governance and Securitya. Collaborate to establish and enforce data governance policies and standards within the client environmentb. Collaborate with security teams to ensure data protection and compliance with regulatory requirementsc. Monitor, identify, and address data security and privacy issues effectively\nPerformance Optimizationa. Identify and address performance bottlenecks within data processing pipelinesb. Implement optimizations to enhance overall system performance\nCollaboration and Documentationa. Work collaboratively with data scientists, data architects, data analysts, and stakeholders to comprehend and address diverse data requirementsb. Thoroughly document data engineering processes, data flows, and architecture for knowledge sharing and referencec. Collaborate seamlessly with the broader IT and cross functional teams to align data solutions with overarching technology strategy\n\nWhat We’re Looking For:· BS/MS in Computer Science, Information Technology, or a related field· Minimum of 5-6 years of experience in designing and implementing scalable enterprise data solutions· Proven experience as a Data Engineer with a strong focus on Microsoft Azure services, specifically Azure Data Factory, Azure Synapse Analytics, Data Lake, SQL Server, SQL Server Integration Services (SSIS), and Visual Studio· Expertise in data integration modeling, optimizing, and designing scalable data structures and scalable ETL processes· Experience developing data extraction and transformation of Enterprise Resource Planning Systems Data (JDE, SAP, etc.), flat file integration solutions, and relational, non-relational, and unstructured DBMS systems· Experience with Agile methodology is required· Strong programming skills within the .NET Framework· Strong project management skills, including the ability to develop and manage project plans, establish priorities, work to deadlines, and control multiple projects at various stages· Strong verbal and written communication skills across all organizational levels· Independent, motivated, critical thinker, and strong self-learner· Familiarity with data governance, security, and compliance best practices· Knowledge of a large variety of business specific data structures: warehouse management, logistics, engineering, finance, sales data, human resources· Other Beneficial Experienceo Azure Databricks platformo Architecting and implementing Enterprise Big Data solutionsEqual Opportunity EmployerRevature (“Company”) is \nWe seek to comply with all applicable federal, state, and local laws related to discrimination and will not tolerate interference with the ability of any of the Company's employees to perform their job duties. Our policy reflects and affirms the Company's commitment to the principles of fair employment and the elimination of all discriminatory practices."}
{"text": "Qualifications\n\n3-5 year's in SQL1 year dashboarding experience using tools like PowerBI and Looker3-5 year's experience deriving insights from data, metricsDemonstrated ability to work collaboratively in a versatile teamExcellent communicator, both in-person and virtually, team playerYou will need to be comfortable with multiple priorities at a time and drive to successful completionAbility to generalize requirements across multiple partnersDashboarding experience using tools like PowerBI and LookerWillingness to learn the ins and outs of our software and data products to be an data partnerExcellent data visualization and data-story-telling skillsBachelor's degree with 3+ years as a data analyst or related data fieldWork in multiple time zones\n\nPreferred Qualifications\n\nFamiliarity with data warehouse ELT concepts (Airflow, DBT, Snowflake)Analytics experience in a cloud platform environmentPredictive modeling experienceExperience working in an Agile development environmentProficient programming for Analytics, PythonFamiliar with Machine Learning and Natural Language Processing concepts\n\nLearn More\n\nAbout Autodesk\n\nWelcome to Autodesk! Amazing things are created every day with our software – from the greenest buildings and cleanest cars to the smartest factories and biggest hit movies. We help innovators turn their ideas into reality, transforming not only how things are made, but what can be made.\n\nWe take great pride in our culture here at Autodesk – our Culture Code is at the core of everything we do. Our values and ways of working help our people thrive and realize their potential, which leads to even better outcomes for our customers.\n\nWhen you’re an Autodesker, you can be your whole, authentic self and do meaningful work that helps build a better future for all. Ready to shape the world and your future? Join us!\n\nBenefits\n\nFrom health and financial benefits to time away and everyday wellness, we give Autodeskers the best, so they can do their best work. Learn more about our benefits in the U.S. by visiting https://benefits.autodesk.com/\n\nSalary transparency\n\nSalary is one part of Autodesk’s competitive compensation package. For U.S.-based roles, we expect a starting base salary between $87,400 and $150,700. Offers are based on the candidate’s experience and geographic location, and may exceed this range. In addition to base salaries, we also have a significant emphasis on annual cash bonuses, commissions for sales roles, stock grants, and a comprehensive benefits package.\n\n\n\nAt Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be \n\nDiversity & Belonging\n\nWe take pride in cultivating a culture of belonging and an equitable workplace where everyone can thrive. Learn more here: https://www.autodesk.com/company/diversity-and-belonging\n\nAre you an existing contractor or consultant with Autodesk? \n\nPlease search for open jobs and apply internally (not on this external site)."}
{"text": "requirements of our clients. You’ll Rapidly prototype containerized multimodal deep learning solutions and associated data pipelines to enable GeoAI capabilities for improving analytic workflows and addressing key intelligence questions. You will be at the cutting edge of implementing State-of-the-Art (SOTA) Computer Vision (CV) and Vision Language Models (VLM) for conducting image retrieval, segmentation tasks, AI-assisted labeling, object detection, and visual question answering using geospatial datasets such as satellite and aerial imagery, full-motion video (FMV), ground photos, and OpenStreetMap.\n\n\n\nWhy join us?\n\n\n Competitive Base Salary Hybrid Role  Bonus Incentives Solid Benefits Plan\n\nJob Details\n\n Requires an active TS/SCI Clearance Bachelor or Master' Degree in Computer Science, Artificial Intelligence, Machine Learning, Data Science, or equivalent experience in lieu of degree. 8+ years of relevant experience. Role will be a hybrid remote role with up to the 3 days a week on a government site. Demonstrated experience applying transfer learning and knowledge distillation methodologies to fine-tune pre-trained foundation and computer vision models to quickly perform segmentation and object detection tasks with limited training data using satellite imagery. Demonstrated professional or academic experience building secure containerized Python applications to include hardening, scanning, automating builds using CI/CD pipelines. Demonstrated professional or academic experience using Python to queryy and retrieve imagery from S3 compliant API's perform common image preprocessing such as chipping, augment, or conversion using common libraries like Boto3 and NumPy. Demonstrated professional or academic experience with deep learning frameworks such as PyTorch or Tensorflow to optimize convolutional neural networks (CNN) such as ResNet or U-Net for object detection or segmentation tasks using satellite imagery. Demonstrated professional or academic experience with version control systems such as Gitlab.\n\nInterested in hearing more? Easy Apply now by clicking the \"Easy Apply\" button.\n\nWant to learn more about this role and Jobot?\n\nClick our Jobot logo and follow our LinkedIn page!"}
{"text": "skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers. \n Responsibilities\nAnalyze raw data: assessing quality, cleansing, structuring for downstream processing Design accurate and scalable prediction algorithms Collaborate with engineering team to bring analytical prototypes to production Generate actionable insights for business improvements\n\nQualifications\nBachelor's degree or equivalent experience in quantative field (Statistics, Mathematics, Computer Science, Engineering, etc.) At least 1 - 2 years' of experience in quantitative analytics or data modeling Deep understanding of predictive modeling, machine-learning, clustering and classification techniques, and algorithms Fluency in a programming language (Python, C,C++, Java, SQL) Familiarity with Big Data frameworks and visualization tools (Cassandra, Hadoop, Spark, Tableau)"}
{"text": "Experience : 8 to 10 Years  Job Description:Mandatry Skill: AWS ,python\nknowledge To ensure successful initiation, planning, execution, control and completion of the project by guiding team members on technical aspects, conducting reviews of technical documents and artefacts.Lead project development, production support and maintenance activities.Fill and ensure timesheets are completed, as is the invoicing process, on or before the deadline. Lead the customer interface for the project on an everyday basis, proactively addressing any issues before they are escalated. Create functional and technical specification documents. Track open tickets/ incidents in queue and allocate tickets to resources and ensure that the tickets are closed within the deadlines.Ensure analysts adhere to SLA/KPI/OLA. Ensure that all in the delivery team, including self, are constantly thinking of ways to do things faster, better or in a more economic manner. Lead and ensure project is in compliance with Software Quality Processes and within timelines. Review functional and technical specification documents. Serve as the single point of contact for the team to the project stakeholders.Promote team work, motivate, mentor and develop subordinates. Provide application production support as per process/RACI (Responsible, Accountable, Consulted and Informed) Matrix."}
{"text": "Qualifications: \n\nBachelor's degree or equivalent combination of education and experience required; degree within Finance, Business Administration, Analytics, Economics or related preferred 5+ years of work experience within finance and/or analytics required \n\nSkills and Abilities:\n\n3+ years SAS, SQL or PowerBI experience requiredAdvanced Microsoft Excel skills required Healthcare, Health Plan experience preferredData-driven, analytic, reporting and financial background strongly preferred\n\nThis position is a Remote role. The employee must be located in any state in which Medica is an employer and will work remotely 100% of the time.\n\nThe full salary range for this position is $67,100 - $115,100. Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to compensation, Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees.\n\nThe compensation and benefits information is provided as of the date of this posting. Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law.\n\nMedica's commitment to diversity, equity and inclusion (DEI) includes unifying our workforce through learning and development, recruitment and retention. We consistently communicate the importance of DEI, celebrate achievements, and seek out community partnerships and diverse suppliers that are representative of everyone in our community. We are developing sustainable programs and investing time, talent and resources to ensure that we are living our values. We are an Equal Opportunity/Affirmative Action employer, where all qualified candidates receive consideration for employment indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic."}
{"text": "QualificationsData Engineering and Data Modeling skillsExperience with ETL (Extract Transform Load) processesData Warehousing knowledgeData Analytics and data visualization skillsStrong problem-solving and troubleshooting abilitiesExperience with cloud platforms and technologiesProficiency in SQL and programming languages like Python and JavaExcellent communication and collaboration skillsA degree in Computer Science, Data Science, or a related field"}
{"text": "skills as we harness the power of technology to help our clients improve the health and well-being of the members they serve — a community’s most vulnerable. Connect your passion with purpose, teaming with people who thrive on finding innovative solutions to some of healthcare’s biggest challenges. Here are the details on this position.\nYour role in our mission\nDesign your career for growth, new possibilities and refining your valuable skills:\nDevelop queries, Visual Basic for Applications (VBA) Modules, and procedures to extract and analyze data pertinent to Managed Care Organizations (MCO), utilizing tools such as MS Access and MS Excel.Design and implement audit tools to seamlessly integrate with existing data sources, leveraging applications like MS Excel, Access, or SQL database functions.Conduct thorough data audits based on established business requirements.Monitor, evaluate, and analyze MCO provider networks to ensure optimal access to healthcare services.Assess submitted reports and data from MCO, ensuring timeliness, accuracy, and network adequacy across various parameters.Prepare comprehensive provider network analysis reports accurately and within specified timelines.Track and manage provider network terminations, conducting impact analyses as needed.Adhere to unit processes for data integrity checks, issuing action plans, and other documentation within defined time frames.Deliver insightful presentations based on conducted analyses.Provide data entry and review support to meet unit needs.Utilize State-approved Geographic Information Systems (GIS) as required for assigned tasks.\nWhat we're looking for \nProficiency in developing queries, Visual Basic for Applications (VBA) Modules, and procedures for data analysis using MS Access and MS Excel.Experience designing and implementing audit tools with applications like MS Excel, Access, or SQL database functions.Strong analytical skills with the ability to perform detailed data audits based on business requirements.Excellent monitoring and analytical abilities to ensure optimal access to care within Managed Care Organization (MCO) provider networks.Attention to detail in reviewing and analyzing MCO submitted reports and data and effective communication skills to deliver presentations on analysis findings.Familiarity with Geographic Information Systems (GIS) preferred.\nWhat you should expect in this role \nThis is a full-time on-site position in Hamilton, New Jersey.Opportunities to travel through your work (0-10%)Video cameras must be used during all interviews, as well as during the initial week of orientation."}
{"text": "Qualifications3+ years of experience in analyzing and interpreting data, and managing data pipelines Proficient in data visualization through platforms like Tableau, PowerBI, or comparable tools. Proficient in Python and the development of ETL pipelines. Experience in writing intricate SQL queries. Exhibit robust oral and written communication abilities.  Preferred QualificationsExperience building applications in Python (or other scripting language) Finance and accounting reconciliation experience Logistics experience"}
{"text": "skills.Excellent communication and organizational skills.\nThe Data Analyst is responsible for researching, developing, and implementing analytics solutions for our healthcare clients. Solutions may be in specific areas such as contractual reimbursement, coordination of benefits, behavioral health, or third-party liability. Solutions may also be general in nature or focused on a system solution. The Data Analyst also provides ETL support to our Operations team by in-taking and loading data.\nMore about the role:\nWork with operations to identify areas of focus for data analysisResearch, develop, and test queries and data solutionsAnalyze, map, and load data to SQL, PostgreSQL, or Mongo databases as part of client implementationAnalyze, interpret, and summarize large data setsIdentify new areas of focus for payer cost containmentWork with C# consoles to make edits for ETL processesCreate complex SQL statements to find claims identified for a refund based on specsWork with the team to brainstorm new ideasLearn medical billing terminology\nWhy choose ClarisHealth?\nClarisHealth unites data and people through innovative technology.Remote-first flexible working environment.OUR AMAZING BENEFITS including, but not limited to:Health insurance, including $0 premium HDHP and $0 deductible PPO options. FIRST-DAY COVERAGE!Generous Paid Time Off including PTO, Wellness, and Volunteer Time Off.401(k) with matching, immediate vesting.“Got Your Back” environment.Professional development reimbursement.We work hard, but we know how to take time and enjoy life.\nClarisHealth embraces a supportive working culture of creativity and innovation internally termed “Got Your Back.” We live out this #GYB spirit every day by leading with Compassion, Communicating with transparency, and committing to improving our Community. For more information about ClarisHealth, our culture, and employment opportunities, please visit us at https://www.clarishealth.com/careers/. #GYB\nHelp develop the industry-leading solution for the healthcare payer market. Join our team at ClarisHealth in challenging the traditional paper-based, time- and labor-intensive audit and data mining methods and impact the evolution of claims payment recovery.\nClarisHealth is the answer to the health plan industry’s siloed solutions and traditional models for identification and overpayment recovery services. Founded in 2013, we provide health plans and payers with total visibility into payment integrity operations through our advanced cost containment technology Pareo®. Pareo enables health plans to maximize avoidance and recoveries at the most optimized cost for a 10x return on their software investment. Currently, nearly 33 million lives are served by our total payment integrity platform.\nApplicants must be currently authorized to work in the United States on a full-time basis.ClarisHealth is not able to sponsor applicants for work visas.\nClarisHealth is"}
{"text": "skills for hosting web applications.Collaboration: Work closely with peers and cross-functional teams within Operations to understand and address business-related challenges and opportunities.Communication: Develop concise and valuable reports and present findings and model insights to both technical and non-technical stakeholders.Continuous-Learner: Actively engage in learning opportunities to stay updated on the latest developments within data science and related domains. Pursue professional development opportunities to expand your expertise.\n\nQualifications\n\nExperiences that make you a strong fit for this role:\n\nRequired:\n\nA Bachelor’s degree in a STEM field. Can include Math, Physics, Engineering, Computer Science, Chemistry, or Biology.0 – 2 years of experience in a STEM or related field.Demonstrated mathematics and statistics skills.Creative-thinker able to apply first principles reasoning to solve complex problems.Able to showcase a detail-oriented approach to work.\n\nBeneficial:\n\nA degree in Chemical Engineering, Chemistry or BiologyExperience in a clinical settingExperience in scripting languages (e.g. Python or R) or data querying languages (e.g. SQL).Basic experience with dashboard tools, including Python Dash, RShiny, and custom HTML and CSS frameworks.Domain knowledge within Pharmaceuticals, Manufacturing Operations, or a translational field from prior internship or career experience.\n\nIf you believe you’re a great fit for this job but don’t have all of the experiences listed above, we encourage you to apply anyway!\n\nWhy Business Technology Solutions\n\nFor anyone who wants to use technology and data to make a difference in people’s lives, shape the digital transformation of a leading biopharmaceutical company, and secure sustainable career growth within a diverse, global team: we’re ready for you.\n\nAdditional Information\n\nApplicable only to applicants applying to a position in any location with pay disclosure requirements under state or local law:\n\nThe compensation range described below is the range of possible base pay compensation that the Company believes in good faith it will pay for this role at the time of this posting based on the job grade for this position. Individual compensation paid within this range will depend on many factors including geographic location, and we may ultimately pay more or less than the posted range. This range may be modified in the future. We offer a comprehensive package of benefits including paid time off (vacation, holidays, sick), medical/dental/vision insurance and 401(k) to eligible employees. This job is eligible to participate in our short-term incentive programs. \n\nNote: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, incentive, benefits, or any other form of compensation and benefits that are allocable to a particular employee remains in the Company's sole and absolute discretion unless and until paid and may be modified at the Company’s sole and absolute discretion, consistent with applicable law.\n\nAbbVie is committed to operating with integrity, driving innovation, transforming lives, serving our community and embracing diversity and inclusion. It is AbbVie��s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a protected veteran, or any other legally protected group status."}
{"text": "SkillsIntermediate to Advanced user with Excel (vlookup/pivot tables)Microsoft Office programs/OutlookSAP experienceAnalytical skills/attention to detail"}
{"text": "Requirements\n\n Proficiency in PC-based Windows software, including Power BI and Microsoft Office applications. SharePoint experience is preferred.  Strong organizational skills and attention to detail for accurate compliance data.  Excellent verbal and written communication skills for effective collaboration with stakeholders.  Education: Associate's Degree with four years of experience, or High School Diploma/GED with six years of experience in an analytical or technical field. Bachelor's Degree preferred with two years of experience.  Preferred Location is New Albany, OH, but other areas within the service territory may be considered based on availability of workspace. Candidates must reside near the work site for in-office work days as needed."}
{"text": "experience and our ability to be compelling to our clients. You’ll find an environment that inspires and empowers you to thrive both personally and professionally. There’s no one like you and that’s why there’s nowhere like RSM.\n\nRSM is looking to hire a Data Analyst on our National Finance and Operations team.\n\nThe Data Analyst is responsible for the development, automation, and compilation of data. This position will prepare reports through Excel, Access, SharePoint, and Power BI using compiled data from several internal systems.\n\nEssential Duties\n\nAnalytical duties may include but are not limited to:\n\nDeveloping data and storage requirements, reporting, analytics, and delivery methods in data standards and metadata.Develop, analyze, and evaluate data to create and maintain business intelligence frameworks.Integrate and mine large data sets, connecting data from disparate sources to identify critical insights and pragmatic data solutions.Maintain/update/create technical specifications (ex. data mapping, data flows, and dashboard content).Develop real-time reporting for analyzing KPIs.Design and create analysis and reports that include summaries, graphs, diagrams, and other visual representations in BI or excel.Develop and maintain forecasting databases/tools and provide analysis to leaders on outcomes.Discover and define new processes for improvement and opportunities.\n\nMinimum Qualifications\n\nBachelor’s degree or equivalent experience. Accounting or Finance major preferred.Strong Microsoft Office skills.Ability to communicate effectively both verbally and written.Ability to work autonomously and part of a team.Effective organization and time management skills.Solid understanding of developing data requirements.Knowledge of Excel, Power BI, SQL, and understanding of concepts of database structures and data querying.Advanced Microsoft Office skills with exposure to Visual Basic.Previous experience with SQL preferred.Previous experience with a Business Intelligence (BI) tool preferred.Previous experience analyzing data and predicting future outcomes preferred.Prepare and review monthly work papers; perform account reconciliations, investigate general ledger balance discrepancies.Assist with month-end closing process.\n\nAt RSM, we offer a competitive benefits and compensation package for all our people. We support and inspire you to prioritize your wellbeing by delivering personalized, holistic programming for your physical, emotional, financial and community wellbeing. RSM has a generous time off policy with at least 14 paid holidays, wellbeing days and associate and above access to self-managed time off. We offer flexibility in your schedule, empowering you to balance life’s demands, while also maintaining your ability to serve clients. Learn more about our total rewards at https://rsmus.com/careers/working-at-rsm/benefits.\n\nRSM is proud to be an Affirmative Action and \n\nAccommodation for applicants with disabilities is available upon request in connection with the recruitment process and/or employment/partnership. RSM is committed to providing equal opportunity and reasonable accommodation for people with disabilities. If you require a reasonable accommodation to complete an application, interview, or otherwise participate in the recruiting process, please call us at 800-274-3978 or send us an email at careers@rsmus.com.\n\nRSM does not intend to hire entry level candidates who will require sponsorship now OR in the future (i.e. F-1 visa holders). If you are a recent U.S. college / university graduate possessing 1-2 years of progressive and relevant work experience in a same or similar role to the one for which you are applying, excluding internships, you may be eligible for hire as an experienced associate.\n\nCompensation Range: $61,200 - $109,900"}
{"text": "Qualifications\n\n3+ years of relevant work experience with a Bachelor's Degree or an Advanced Degree.Ability to run complex analytical projects from data gathering through analysis.A strong understanding of how to gather data across diverse data sources.Demonstrate an appetite and knowledge to solve our challenges.Intermediate to advanced SQL scripting.Demonstrated ability to provide data insights via visualization tools (Tableau preferred) and presentations.Excellent written, verbal,. Comfortable with speaking to internal and external partners at all levels.\n\nIn this flex office/home role, you will be expected to work a minimum of 10 days per month from one of the following office locations: Madison, WI 53783; Boston, MA 02110; Chicago, IL 60601*; Denver, CO 80112; Eden Prairie, MN 55343; Keene, NH 03431; St. Joseph, MO 64507; Phoenix, AZ 85034; Nashville, TN\n\nWe encourage you to apply even if you do not meet all of the requirements listed above. Skills can be used in many different ways, and your life and professional experience may be relevant beyond what a list of requirements will capture. We encourage those who are passionate about what we do to apply!\n\nWe provide benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, 9 paid holidays and a paid time off program (23 days accrued annually for full-time employees). In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns and contingent workers are not eligible for American Family Insurance Group benefits.\n\nWe are"}
{"text": "requirements.You will receive technical guidance and enjoy professional growth opportunities in a supportive environment.Develop creative, technical, and professional skills, with the help of a highly collaborative team.Participate in supervised practical training in a professional field.Under general supervision, working on routine projects with general instruction and non-routine projects with detailed instructions.Participates in the day to day activities of assigned functional area assisting in the completion of routine and non-routine tasks.Assists in applying knowledge to analyze, investigate and resolve problems.Assists in developing electronic and hard copy reports, records, diagrams and charts.Responsible for observing all laws, regulations and other applicable obligations wherever and whenever business is conducted on behalf of the Company.Expected to work in a safe manner in accordance with established operating procedures and practices.\n\n\nWe recognize and appreciate the value and contributions of individuals with diverse backgrounds and experiences and welcome all qualified individuals to apply.\n\nBase Salary: $44,590 - 66,295\n\nJob Type: Part-Time Hourly\n\nThe General Atomics(GA) group of companies is a world renowned leader in developing high - technology systems ranging from the nuclear fuel cycle to electromagnetic systems; remotely operated surveillance aircraft; airborne sensors; advanced electronic, wireless, and laser technologies; and biofuels. We offer an excellent opportunity to realize your full potential and fulfill your career aspirations.Join the GA Team where you can make a difference! Equal Opportunity / Affirmative Action Employer"}
{"text": "experience for yourself, and a better working world for all.\n\nCloud Big Data Engineer, Senior - Tech Consulting \n\nEY delivers unparalleled service in big data, business intelligence, and digital analytics built on a blend of custom-developed methods related to customer analytics, data visualization, and optimization. We leverage best practices and a high degree of business acumen that has been compiled over years of experience to ensure the highest level of execution and satisfaction for our clients. At EY, our methods are not tied to any specific platforms but rather arrived at by analyzing business needs and making sure that the solutions delivered meet all client goals.\n\nThe opportunity \n\nYou will help our clients navigate the complex world of modern data analytics. We’ll look to you to provide our clients with a unique business perspective on how Big Data analytics can transform and improve their entire organization – starting with key business issues they face. This is a high growth, high visibility area with plenty of opportunities to enhance your skillset and build your career.\n\nYour Key Responsibilities\n\nYou’ll spend most of your time working with a wide variety of clients to deliver the latest big data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyse data from multiple sources.\n\nSkills And Attributes For Success\n\nDesigning, Architecting, and Developing solutions leveraging cloud big data technology to ingest, process and analyze large, disparate data sets to exceed business requirements Unifying, enriching, and analyzing customer data to derive insights and opportunities Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches Seeking out information to learn about emerging methodologies and technologies Clarifying problems by driving to understand the true issue Looking for opportunities for improving methods and outcomes Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes Collaborating, influencing and building consensus through constructive relationships and effective listening Solving problems by incorporating data into decision making  \n\nTo qualify for the role you must have \n\nA bachelor's degree and approximately three years of related work experience; or a master's degree and approximately two years of related work experience At least three years hands-on experience with various Cloud and Big Data technologies At least two years experience in implementing, automating and integrating Big Data infrastructure resources like S3, Redshift, Aurora, Kinesis, Kafka, EMR, Lambda, SNS, Azure Blob Storage Account ,SQL Data Warehouse, Microsoft Event Hubs , HDInsights, Azure Databricks, Azure Functions,Event Grid, Data Lake Analytics in an ephemeral/transient and in an elastic manner IaC & Config Management: Tools like Chef, puppet, CloudFormation ,terraform, ansible, boto3 and/or Azure/GCP equivalent Hands on experience of core Operating systems like Linux RHEL, Ubuntu, System administration tasks including shell scripting Network Engineering/Admin (vpc, subnet, security groups, VPC-Endpoints, nat/route tables, etc) Experience with container technology like docker, kubernetes etc. Security tools/concepts like At Rest and in transit Encryption, IAM, key and certificate management etc. CI/CD pipeline management like git/bitbucket, and code deployment tools like Jenkins, sonar cube Communication is essential, must be able to listen and understand the question and develop and deliver clear insights. Outstanding team player. Independent and able to manage and prioritize workload. Ability to quickly and positively adapt to change. A valid driver’s license in the US; willingness and ability to travel to meet client needs. \n\nIdeally, you’ll also have \n\nBachelor’s Degree or above in mathematics, information systems, statistics, computer science, or related disciplines \n\nWhat We Look For\n\nWe’re interested in passionate leaders with strong vision and a desire to stay on top of trends in the Big Data industry. If you have a genuine passion for helping businesses achieve the full potential of their data, this role is for you.\n\nWhat We Offer\n\nWe offer a comprehensive compensation and benefits package where you’ll be rewarded based on your performance and recognized for the value you bring to the business. The salary range for this job in most geographic locations in the US is $96,200 to $158,900. The salary range for New York City Metro Area, Washington State and California (excluding Sacramento) is $115,500 to $180,500. Individual salaries within those ranges are determined through a wide variety of factors including but not limited to education, experience, knowledge, skills and geography. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options. Under our flexible vacation policy, you’ll decide how much vacation time you need based on your own personal circumstances. You’ll also be granted time off for designated EY Paid Holidays, Winter/Summer breaks, Personal/Family Care, and other leaves of absence when needed to support your physical, financial, and emotional well-being.\n\nContinuous learning: You’ll develop the mindset and skills to navigate whatever comes next.Success as defined by you: We’ll provide the tools and flexibility, so you can make a meaningful impact, your way.Transformative leadership: We’ll give you the insights, coaching and confidence to be the leader the world needs.Diverse and inclusive culture: You’ll be embraced for who you are and empowered to use your voice to help others find theirs.\n\nIf you can demonstrate that you meet the criteria above, please contact us as soon as possible.\n\nThe Exceptional EY Experience. It’s Yours To Build.\n\nEY | Building a better working world\n\nEY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.\n\nEnabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.\n\nWorking across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.\n\nEY is an equal opportunity, affirmative action employer providing equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law.\n\nEY is committed to providing reasonable accommodation to qualified individuals with disabilities including veterans with disabilities. If you have a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please call 1-800-EY-HELP3, type Option 2 (HR-related inquiries) and then type Option 1 (HR Shared Services Center), which will route you to EY’s Talent Shared Services Team or email SSC Customer Support at ssc.customersupport@ey.com"}
{"text": "Actively participates in projects in assigned area of responsibility; develops and manages project specific scopes, schedules, and budgets.Coordinates communications with other agency departments, divisions, and outside agencies on behalf of the Chief Engineer and the department.Documents, analyzes, and develops recommendations on how to improve business processes. Participates in the development of readiness and change management activities.Coordinates division budget development and quarterly financial reporting."}
{"text": "skills including but not limited to Pivot Tables, VLookup and XLookup etcManage data and reports to convert raw data into reports.Create dashboards.Generates reports and provides analyses.Prepares data utilizing standardization and normalization techniques ensuring consistency and reliability in all reports generated.Creates new reporting processes, automates manual processes, maintains and executes scheduled reporting processes, and most importantly validates results first, and then provides insight on metric trends, changes, and cause and effect relationships.Individual is a subject matter expert within their reporting domain frequently required to provide information.Proficient knowledge of advanced functionality in MS Office products. Most importantly Excel.Acute attention to the details and validation steps in each reporting process.Advanced project management skills, including time management / prioritization, managing and measuring work, and communicating results and status.Experience analyzing data and developing management reports and tools.Experience leading and independently representing Retail on project teams.Experience developing and delivering professional presentations.\nOpTech is"}
{"text": "Skills / Experience:Required: Proficiency with Python, pyTorch, Linux, Docker, Kubernetes, Jupyter. Expertise in Deep Learning, Transformers, Natural Language Processing, Large Language Models\nPreferred: Experience with genomics data, molecular genetics. Distributed computing tools like Ray, Dask, Spark.\n﻿Thanks & RegardsBharat Priyadarshan GuntiHead of Recruitment & OperationsStellite Works LLC4841 W Stonegate Circle Lake Orion MI - 48359Contact: 313 221 9365bharat@stelliteworks.com ||www.stelliteworks.comNote: This is not an unsolicited mail. If you are not interested in receiving our e-mails, then please reply with subject line Remove.Oh! Lord, you are my Righteousness and My Pride"}
{"text": "Requirements\n\nCurrently enrolled in a Computer Science, Data Science, or related STEM programFamiliarity with SQL and at least one Object-Oriented Programming languageParticipation in ML research, or other extracurriculars - such as Hackathon, Coding or Math Competition, Prior Internship experience, or demonstrated project completion\n\nAbout Us \n\nGetinge is on an exciting transformation journey constantly looking for new ways to innovate together with our customers to meet the healthcare challenges of the future. We are committed to diversity, equity and inclusion and to sustainability with a goal to be CO2 neutral by 2025. We believe in giving our employees the flexibility they need and make every effort to foster a learning culture that supports their personal development and creativity. Our passionate people hold our brand promise ‘Passion for Life’ close to heart. \n\nIf you share our passion and believe that saving lives is the greatest job in the world, then we look forward to receiving your application and resume. We hope you will join us on our journey to become the world’s most respected and trusted medtech company."}
{"text": "experienced and self-reliant professionals with exceptional analytical abilities, communication and customer service skills.\n\nHelp\n\nRequirements\n\nConditions of Employment\n\nYou must be a U.S. citizen & meet specialized experience to qualifySubmit application and resume online by 11:59 PM EST on the closing dateRequired documents must be submitted by the closing date.Direct Hire Authority will be used to fill this position\n\n\nCONDITIONS OF EMPLOYMENT:\n\nSELECTIVE SERVICE: Males born after 12/31/1959 must be registered for the Selective Service.GOVERNMENT TRAVEL CARD: This position involves travel. A government contractor-issued travel card will be issued and must be retained for official business only.PCS/RELOCATION/RECRUITMENT:  Permanent Change of Station (PCS), Relocation, and Recruitment Incentive authorization varies by position/selection and a service agreement may be required.PROBATIONARY PERIOD: Applicants may be required to successfully complete a one-year probationary period (unless already completed).TELEWORK ELIGIBILITY: This position may be eligible for occasional and/or regularly scheduled telework. The number of telework days approved will be determined by your immediate supervisor or manager following the agency’s telework policy and guidance. If participating in the telework program, you will be required to sign a telework agreement detailing agency telework policy requirements, working conditions, and expectations for the position.REMOTE WORK: This position may be eligible for remote work, which does not require reporting to the agency worksite location on a regular basis. If selected for a remote work location, you will be required to sign a remote work agreement detailing agency remote work policy requirements, working conditions, and expectations for the position.\n\n\nQualifications\n\nTo meet the minimum qualifications for this position, you must (1) meet the Education Requirement for the series, (2) provide a copy of transcripts for verification, AND (3) meet either the education or experience qualifications for the grade at which you are requesting consideration.\n\nTo qualify for the GS-11 on Experience, you must have at least one year of experience equal or equivalent to the GS-09 it must include:\n\nExperience analyzing and evaluating transportation related programs.\n\n\nTo qualify for the GS-11 on Education alone, you must have:\n\n3 years of progressively higher-level graduate education leading to a Ph.D. degree or Ph.D. or equivalent doctoral degree.\n\n\nYou can also qualify based on a combination of higher-level graduate education and experience. This must be fully supported by your resume and transcripts, provided with your application.\n\nTo qualify for the GS-12, you must have at least one year of experience equal or equivalent to the GS-11, it must include:\n\nExperience analyzing and evaluating transportation related programs and developing innovative solutions.\n\n\nYou cannot qualify on education at this level, you must have experience.\n\nTo qualify for the GS-13, you must have at least one year of experience equal or equivalent to the GS-12, it must include:\n\nExperience analyzing and evaluating transportation related programs and developing innovative solutions and strategies for solving problems identified in such analyses.\n\n\nYou cannot qualify on education at this level, you must have experience.\n\nTo qualify for the GS-14, you must have at least one year of experience equal or equivalent to the GS-13, it must include:\n\nExperience analyzing and evaluating transportation related programs and developing innovative solutions and strategies for solving problems identified in such analyses.Experience monitoring indicators and mechanisms for changes in Federal, State and local goals, policies, priorities and budgets, and skill interpreting the impacts of these developments on Federal.Transit assistance programs, legislation, policies, priorities and funding allocations.\n\n\nYou cannot qualify on education at this level, you must have experience.\n\n KNOWLEDGE, SKILLS AND ABILITIES (KSAs): Your qualifications will be evaluated on the basis of your level of knowledge, skills, abilities and/or competencies in the following areas:\n\nPlanning and EvaluatingProject managementProblem SolvingOral and Written communication\n\n\nExperience refers to paid and unpaid experience, including volunteer work done through National Service programs (e.g., Peace Corps, AmeriCorps) and other organizations (e.g., professional; philanthropic; religious; spiritual; community, student, social). Volunteer work helps build critical competencies, knowledge, and skills and can provide valuable training and experience that translates directly to paid employment. You will receive credit for all qualifying experience, including volunteer experience.\n\n For additional information about applying to Federal positions, please click on the following link: https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-policies/#url=e4\n\nAll applicants must meet all qualification requirements by the closing date of this announcement.\n\nAdditional information\n\nThe agency has the option of extending a term appointment up to the 4-year limit.\n\nThis announcement may be used to fill additional positions if similar vacancies occur within 90 days of the issue date of the referral certificate.\n\nQualified CTAP/ICTAP candidates receive priority and will be referred to the hiring manager.\n\nWRITING SAMPLE: Varies by position and supervisory determination. You may be asked to provide a writing sample.”\n\nThis position may be eligible for Remote Work.” Varies by position and supervisory determination.\n\nRead more\n\n Help A career with the U.S. government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Opens in a new windowLearn more about federal benefits.\n\n\nReview our benefits\n\nEligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time or intermittent. Contact the hiring agency for more information on the specific benefits offered.\n\nDOT, FEDERAL TRANSIT ADMINISTRATION\n\nSubmit Application and Documents Online\n\n1200 New Jersey Ave SE / HAHR - 50\n\nWashington, District of Columbia 20590\n\nUnited States"}
{"text": "experiences and goals, Charlie Health fosters sustainable healing and achieves industry-leading clinical outcomes, with over 90% of our clients seeing improvement in their most severe mental health symptoms.\n\nEvery member of the Charlie Health team is fueled by an unwavering passion for our mission. If you share this commitment, we invite you to join us in making a tangible impact on the mental health landscape.\n\nAbout This Role\n\nWe are seeking a talented and experienced Data Analyst to join our team. The ideal candidate will have a strong analytical mindset, excellent communication skills, and the ability to translate complex data into actionable insights. The Data Analyst will be responsible for collecting, analyzing, and interpreting large datasets to identify trends, patterns, and opportunities that drive business decisions and strategy.\n\nResponsibilities\n\nCollect and clean data from various sources, ensuring its accuracy and completeness.Analyze large datasets using statistical methods and data visualization techniques.Identify trends, patterns, and correlations in data to provide valuable insights and recommendations.Develop and maintain dashboards, reports, and visualizations to communicate findings to stakeholders.Collaborate with cross-functional teams to understand business requirements and provide data-driven solutions.Perform ad-hoc analysis as required to support business needs.Stay updated on industry trends and best practices in data analytics.\n\nRequirements\n\nProven experience as a Data Analyst or similar role.Proficiency in Tableau and SQL, and experience working with relational databases.Strong analytical skills with the ability to manipulate and interpret complex datasets.Experience with data visualization tools such as Tableau, Power BI, or matplotlib.Knowledge of statistical analysis techniques and tools such as R, Python, or SAS.Excellent communication and presentation skills, with the ability to convey technical concepts to non-technical stakeholders.Attention to detail and ability to work independently as well as part of a team.Experience in industries such as finance, healthcare, or e-commerce is a plus.\n\nBenefits\n\nCharlie Health is pleased to offer comprehensive benefits to all full-time, exempt employees. Read more about our benefits here.\n\nNote: We are not currently considering applicants in CA, CO, NY, and WA for this position. \n\nOur Values\n\nConnectionCare deeplyWe care personally about every single person in the Charlie Health ecosystem: our clients, providers, and team members alike.Inspire hopeWe inspire hope with every interaction, reminding our clients that we truly and unconditionally believe in them.CongruenceStay curiousWe ask “why” five times before we’re satisfied with the answer. We don’t stick to the status quo; we challenge our assumptions and remain humble.Heed the evidenceAbove all, we’re results-oriented. When we find data that calls our original plan into question, we modify or pivot.CommitmentAct with urgencyWe work as swiftly as possible. The mental health crisis is relentless, and so are we.Don’t give upOur clients don’t give up and neither do we. Persistence is our superpower.\nPlease do not call our public clinical admissions line in regard to this or any other job posting.\n\nPlease be cautious of potential recruitment fraud. If you are interested in exploring opportunities at Charlie Health, please go directly to our Careers Page: https://www.charliehealth.com/careers/current-openings. Charlie Health will never ask you to pay a fee or download software as part of the interview process with our company. In addition, Charlie Health will not ask for your personal banking information until you have signed an offer of employment and completed onboarding paperwork that is provided by our People Operations team. All communications with Charlie Health Talent and People Operations professionals will only be sent from @charliehealth.com email addresses. Legitimate emails will never originate from gmail.com, yahoo.com, or other commercial email services.\n\nRecruiting agencies, please do not submit unsolicited referrals for this or any open role. We have a roster of agencies with whom we partner, and we will not pay any fee associated with unsolicited referrals.\n\nAt Charlie Health, we value being \n\nCharlie Health applicants are assessed solely on their qualifications for the role, without regard to disability or need for accommodation."}
{"text": "experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.\n\nOur new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.\n\nWe are looking for a highly motivated and experienced Machine Learning Engineer to join our team and help us shape the future of machine learning at Pinterest. In this role, you will tackle new challenges in machine learning that will have a real impact on the way people discover and interact with the world around them. You will collaborate with a world-class team of research scientists and engineers to develop new machine learning algorithms, systems, and applications that will bring step-function impact to the business metrics (recent publications 1, 2, 3). You will also have the opportunity to work on a variety of exciting projects in the following areas:\n\nrepresentation learningrecommender systemsgraph neural networknatural language processing (NLP)inclusive AIreinforcement learninguser modeling\n\nYou will also have the opportunity to mentor junior researchers and collaborate with external researchers on cutting-edge projects.\n\nWhat you'll do: \n\nLead cutting-edge research in machine learning and collaborate with other engineering teams to adopt the innovations into Pinterest problemsCollect, analyze, and synthesize findings from data and build intelligent data-driven modelScope and independently solve moderately complex problems; write clean, efficient, and sustainable codeUse machine learning, natural language processing, and graph analysis to solve modeling and ranking problems across growth, discovery, ads and search\n\nWhat we're looking for:\n\nMastery of at least one systems languages (Java, C++, Python) or one ML framework (Pytorch, Tensorflow, MLFlow)Experience in research and in solving analytical problemsStrong communicator and team player. Being able to find solutions for open-ended problems8+ years working experience in the r&d or engineering teams that build large-scale ML-driven projects3+ years experience leading cross-team engineering efforts that improves user experience in productsMS/PhD in Computer Science, ML, NLP, Statistics, Information Sciences or related field\n\nDesired skills:\n\nStrong publication track record and industry experience in shipping machine learning solutions for large-scale challenges Cross-functional collaborator and strong communicatorComfortable solving ambiguous problems and adapting to a dynamic environment\n\nThis position is not eligible for relocation assistance.\n\nAt Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.\n\nInformation regarding the culture at Pinterest and benefits available for this position can be found here.\n\nUS based applicants only\n\n$158,950-$327,000 USD\n\nOur Commitment to Diversity:\n\nPinterest is"}
{"text": "experience2+ years Data Analysis experienceProficient in SQLProficient in SnowflakeExperience using Visualization toolsExperience using BI toolsFinancial services background experience is a plus"}
{"text": "Experience with genomics data, and molecular genetics.Distributed computing tools like Ray, Dask, and Spark."}
{"text": "requirements, identify opportunities for improvement, and drive innovation.Provide technical guidance and mentorship to team members, fostering their professional growth and development.Champion technical excellence within the engineering team and ensure adherence to best practices and coding standards.Constantly seek to increase Pareto’s competitive edge by applying world-class technology to solve healthcare data and analytics problems.\nREQUIRED SKILLS:Relevant years of experience in software development, with proficiency in Java, Scala, and Python.Expertise in AWS services coupled with a history of deploying data solutions on cloud platforms.Experience in engineering project and process management in an agile environment.Experience leading engineering teams with a track record of increased quality and productivity.Proficiency in Go language is a plus.Have a background in back-end technologies, including API Rest Interfaces & SQL.Experience working with healthcare data, including but not limited to eligibility, claims, payments, and risk adjustment datasets.Strong programming/debugging skills, with a hands-on approachRelevant certifications in AWS or software engineering would be a plus.\nOTHER DUTIES AND"}
{"text": "experience in DevOps, ML, MLOps, Big Data, Python, integration, and deployment methodologies.\nExpertise in provisioning AI resources on the Azure platform (or GCP). The ideal candidate will have extensive experience in deploying and managing AI/ML solutions while adhering to enterprise security standards.\nGenAI architecture and other cloud providers' AI/ML offerings is highly desirable.\n\nKey requirements, provide updates, and address technical concerns.Provide basic samples to the teams on using the provisioned services.\nRequirements:Bachelor’s degree in computer science, Engineering, or related field.Proven experience as a Cloud Engineer or similar role, with a focus on AI/ML solutions.Strong proficiency in provisioning and automating cloud infrastructure/platforms especially AI resources on the Azure platform.Experience with IAAS, PAAS, and SAAS enablement on the Azure platform.Experience with GenAI architecture principles, including RAG, LLM's, and data pipelines.Solid understanding of data safety and security standards implementation.Familiarity with other cloud providers' AI/ML offerings is an added advantage.Excellent communication skills and ability to collaborate effectively in a team environment.Analytical mindset with a strong attention to detail.\n\n\nThanks & RegardsUtsavManagerChabezTech LLC4 Lemoyne Dr #102, Lemoyne, PA 17043, USADirect : +1-717-441-5440Email: utsav@chabeztech.com | www.chabeztech.com"}
{"text": "Skills: Your Expertise:\n5+ years in industry experience and a degree (Masters or PhD is a plus) in a quantitative field (e.g., Statistics, Econometrics, Computer Science, Engineering, Mathematics, Data Science, Operations Research).Expert communication and collaboration skills with the ability to work effectively with internal teams in a cross-cultural and cross-functional environment. Ability to conduct rigorous analysis and communicate conclusions to both technical and non-technical audiencesExperience partnering with internal teams to drive action and providing expertise and direction on analytics, data science, experimental design, and measurementExperience in analysis of A|B experiments and statistical data analysisExperience designing and building metrics, from conception to building prototypes with data pipelinesStrong knowledge in at least one programming language (Python or R) and in SQLAbility to drive data strategies, with a central source of truth to impact business decisionsKnowledge and experience in insurance industry - a plusKnowledge and experience in customer experience measurement - a plus\nKeywords:Education: Minimum: BS/BA in CS or related field (or self-taught/ equivalent work experience) Preferred: MS/MA in CS or related field"}
{"text": "Skills, & Abilities\n\nExperience working on a diverse team.Experience working with different communication styles.Mastery of statistical analysis packages (R, Stata, SAS, etc.).Problem-Solving skills, including organizing and investigating possible solutions and presenting them to the team for discussion.Excellent organizational, written and verbal communication skills in the preparation and presentation of results.Excellent interpersonal skills in dealing with investigators and a “team-oriented” approach with other staff members. Microsoft Office programs (Word, Excel, PowerPoint, Outlook) proficiency.Technical qualifications or specialized certifications: Mastery of statistical analysis packages (R, Stata, SAS, etc.).\n\n\nMinimum Qualifications\n\nBachelor's Degree in related discipline.Three years related experience.Additional education may substitute for required experience to the extent permitted by the JHU equivalency formula.\n\n\nClassified Title: Research Data Analyst\n\nRole/Level/Range: ACRP/04/MC\n\nStarting Salary Range: $47,500 - $83,300 Annually ($39,000 targeted; Commensurate with experience)\n\nEmployee group: Part-time\n\nSchedule: Monday to Friday: 9 am – 5 pm\n\nExempt Status: Exempt\n\nLocation: School of Public Health\n\nDepartment name: HBS-Research Projects\n\nPersonnel area: School of Public Health\n\nTotal Rewards\n\nThe referenced salary range is based on Johns Hopkins University’s good faith belief at the time of posting. Actual compensation may vary based on factors such as geographic location, work experience, market conditions, education/training and skill level. Johns Hopkins offers a total rewards package that supports our employees' health, life, career and retirement. More information can be found here: https://hr.jhu.edu/benefits-worklife/.\n\nPlease refer to the job description above to see which forms of equivalency are permitted for this position. If permitted, equivalencies will follow these guidelines: JHU Equivalency Formula: 30 undergraduate degree credits (semester hours) or 18 graduate degree credits may substitute for one year of experience. Additional related experience may substitute for required education on the same basis. For jobs where equivalency is permitted, up to two years of non-related college course work may be applied towards the total minimum education/experience required for the respective job.\n\nApplicants who do not meet the posted requirements but are completing their final academic semester/quarter will be considered eligible for employment and may be asked to provide additional information confirming their academic completion date.\n\n\nThe successful candidate(s) for this position will be subject to a pre-employment background check. Johns Hopkins is committed to hiring individuals with a justice-involved background, consistent with applicable policies and current practice. A prior criminal history does not automatically preclude candidates from employment at Johns Hopkins University. In accordance with applicable law, the university will review, on an individual basis, the date of a candidate's conviction, the nature of the conviction and how the conviction relates to an essential job-related qualification or function.\n\nThe Johns Hopkins University values diversity, equity and inclusion and advances these through our key strategic framework, the JHU Roadmap on Diversity and Inclusion.\n\nEqual Opportunity Employer\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\n\n\n\nhttps://www.\n\nAccommodation Information\n\nIf you are interested in applying for employment with The Johns Hopkins University and require special assistance or accommodation during any part of the pre-employment process, please contact the Talent Acquisition Office at jhurecruitment@jhu.edu. For TTY users, call via Maryland Relay or dial 711. For more information about workplace accommodations or accessibility at Johns Hopkins University, please visit https://accessibility.jhu.edu/.\n\nJohns Hopkins has mandated COVID-19 and influenza vaccines, as applicable. The COVID-19 vaccine does not apply to positions located in the State of Florida. Exceptions to the COVID and flu vaccine requirements may be provided to individuals for religious beliefs or medical reasons. Requests for an exception must be submitted to the JHU vaccination registry. For additional information, applicants for SOM positions should visit https://www.hopkinsmedicine.org/coronavirus/covid-19-vaccine/ and all other JHU applicants should visit https://covidinfo.jhu.edu/health-safety/covid-vaccination-information/.\n\nThe following additional provisions may apply, depending upon campus. Your recruiter will advise accordingly.\n\nThe pre-employment physical for positions in clinical areas, laboratories, working with research subjects, or involving community contact requires documentation of immune status against Rubella (German measles), Rubeola (Measles), Mumps, Varicella (chickenpox), Hepatitis B and documentation of having received the Tdap (Tetanus, diphtheria, pertussis) vaccination. This may include documentation of having two (2) MMR vaccines; two (2) Varicella vaccines; or antibody status to these diseases from laboratory testing. Blood tests for immunities to these diseases are ordinarily included in the pre-employment physical exam except for those employees who provide results of blood tests or immunization documentation from their own health care providers. Any vaccinations required for these diseases will be given at no cost in our Occupational Health office.\n\nSchool of Public Health - East Baltimore Campus"}
{"text": "requirements from business stakeholders, identifying opportunities to apply advanced analytic approachesExplore and understand the inner workings and market context of an aircraft OEMPreprocess data using feature selection and/or dimensionality reductions algorithms within a pure code environment.Solve regression and classification problems using established machine learning techniquesEvaluate and optimize the performance of machine learning models through results-based training and testing\n\nEDUCATION/ EXPERIENCE:\n\nBachelor’s degree required in Applied Mathematics, Statistics, Data Science, Computer Science plus 2 years relevant technical experience or bachelor’s degree in other related field plus 4 years relevant technical experienceAviation experience preferred\n\nQUALIFICATIONS:\n\n\n\nStrong written and verbal communication skills\n\nExperience with Microsoft Office including Excel and PowerPointPractical application experience with one or more analytics packages such as SAS, R, SQL, Python (& associated libraries), or similarExperience working with relational databases, APIs, and ML production environmentsAdvanced analytics skills, including statistical characterization of large sets, regression modeling, probability distribution fitting, stochastic simulation, and multivariate sensitivity analysisAbility to identify relevant metrics and explain technical information to a broad audienceDesire and ability to learn and leverage new software, tools, and processes in a self-learning environmentDemonstrated aptitude to clean and prepare data using techniques such as compression, binning, normalization/scaling, and 1-hot encoding within a pure code environment (Notebook or IDE)Advanced matrix, multidimensional array, and table operations in pure coding environmentDemonstrated ability to preprocess data using feature selection and/or dimensionality reductions algorithms within a pure code environment. This may include decision tree analysis, boosting, or Principal Component AnalysisTechnical ability to solve regression and classification problems using established machine learning techniques including SVM, logistic regression, and clustering.Basic understand of time-series data analysis and prediction modeling (ARIMA, SARIMA, exponential smoothing) or otherwiseTechnical ability to evaluate (i.e., F1 Score, Confusion Matrices, RMSE, etc.) and optimize the performance (i.e., grid search, hyperparameter tuning) of machine learning models through results-based training and testingBasic understanding of Neural Networks with the ability to train a basic multi-layer perceptron in a pure code environment utilizing activation functions like ReLU, Sigmoid, etc.\n\nThe above statements are intended to describe the general nature and level of work being performed by employees assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified.\n\nTextron Aviation has been inspiring the journey of flight for nine decades through the iconic and beloved Cessna and Beechcraft brands. We are passionate advocates of aviation, empowering people with the freedom of flight. As you join our legacy as a global leader in private aviation, you’ll have opportunities to try new fields, expand your skills and knowledge, stretch your abilities, and build your career. We provide a competitive and extensive total rewards package that includes pay and innovative benefits to support you and your family members – now and in the future, beginning day one. Your success is our success.\n\nJoin Textron Aviation’s Kansas team and you may be eligible for a $5,000 state of Kansas Aviation tax credit for up to five years. Visit https://www.aircapitaloftheworld.com/taxcredits for more information on the tax credit.\n\nTextron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.\n\nRecruiting Company\n\nTextron Aviation\n\nPrimary Location\n\nUS-Kansas-Wichita\n\nJob Field\n\nBusiness Development\n\nSchedule\n\nFull-time\n\nJob Level\n\nIndividual Contributor\n\nJob Type\n\nStandard\n\nShift\n\nFirst Shift\n\nRelocation\n\nAvailable\n\nJob Posting\n\n04/17/2024, 4:29:47 PM"}
{"text": "experience for our clients. The Data Analyst, Marketing supports and enhances ongoing business processes and has the responsibility to continuously improve the data quality and integrity.\n\nWhat You Will Do\n\nAct as a subject matter expert for Marketing reporting and analysisDevelop and track key performance indicators to evaluate our marketing successProcessing, cleansing, and verifying the integrity of data used for analysisArchitect, manage, and provide data-driven reportsCompiling ad-hoc analysis and presenting results in a clear mannerIdentifies and documents opportunities for change and provides recommendationsWork closely with the marketing team to determine upfront reporting needs of each projectCollect requirements, determine technical issues, and design reports to meet data analysis needsDeveloping and maintaining web-based dashboards for real-time reporting of key performance indicators for Marketing. Dashboards must be simple to use, easy to understand, and accurate.Maintenance of current managerial reports and development of new reportsDevelop and maintain reporting playbook and change logOther duties in the PUA department as assigned\n\nWhat YOU Will Bring To C&F\n\nSolid analytical and problem solving skillsIntuitive, data-oriented with a creative, solutions-based approachAbility to manage time, multi-task and prioritizes multiple assignments effectivelyAbility to work independently and as part of a teamAble to recognize and analyze business and data issues with minimal supervision, ability to escalate when necessaryAble to identify cause and effect relationships in data and work process flows\n\nRequirements\n\n3 years in an Analyst role is requiredA Bachelor’s degree in associated field of study; data science, computer science, mathematics, economics, statistics, marketing etc. is requiredExperience using SQL is requiredExperience with common data science toolkits is requiredPrior experience compiling marketing analysis requiredExperience with Google Analytics is required\n\nWhat C&F Will Bring To You\n\nCompetitive compensation packageGenerous 401K employer match Employee Stock Purchase plan with employer matchingGenerous Paid Time OffExcellent benefits that go beyond health, dental & vision. Our programs are focused on your whole family’s wellness including your physical, mental and financial wellbeingA core C&F tenant is owning your career development so we provide a wealth of ways for you to keep learning, including tuition reimbursement, industry related certifications and professional training to keep you progressing on your chosen pathA dynamic, ambitious, fun and exciting work environmentWe believe you do well by doing good and want to encourage a spirit of social and community responsibility, matching donation program, volunteer opportunities, and an employee driven corporate giving program that lets you participate and support your community\n\nAt C&F you will BELONG\n\nWe value inclusivity and diversity. We are committed to \n\nCrum & Forster is committed to ensuring a workplace free from discriminatory pay disparities and complying with applicable pay equity laws. Salary ranges are available for all positions at this location, taking into account roles with a comparable level of responsibility and impact in the relevant labor market and these salary ranges are regularly reviewed and adjusted in accordance with prevailing market conditions. The annualized base pay for the advertised position, located in the specified area, ranges from a minimum of $68,000 to a maximum of $113,300. The actual compensation is determined by various factors, including but not limited to the market pay for the jobs at each level, the responsibilities and skills required for each job, and the employee’s contribution (performance) in that role. To be considered within market range, a salary is at or above the minimum of the range. You may also have the opportunity to participate in discretionary equity (stock) based compensation and/or performance-based variable pay programs."}
{"text": "Qualifications: 3-5 years of data engineering experience. Proven experience and expertise using Python, SQL, Docker, Snowflake, or PostgresSQL. High Proficiency in SQL codingExperience managing and deploying code using GitLab/GitHub? Experience leveraging containerization technologies such as Docker or KubernetesExperience leveraging job scheduling software like Apache Airflow. Experience with Agile project management (i.e. Scrum)Strong understanding of relational and dimensional database design Knowledgeable on cloud architecture and product offerings, preferably AWSBachelor Degree (CS, Math, Eng, or related field)Preferred:3-5 years of Python programing experience – High proficiency Hands-on experience with SnowSQL in SnowflakeExperience or background in media planning, ad sales, and research is a plus"}
{"text": "qualifications and experience.\nRESPONSIBILITIESData Analysis and Insights: Utilize advanced data analysis techniques to extract insights from large datasets, identify trends, patterns, and correlations, and translate findings into actionable recommendations for business stakeholders. Develop predictive models, algorithms, and data visualization tools to support decision-making processes, optimize business performance, and drive strategic initiatives.Strategy Development: Collaborate with senior leadership and key stakeholders to develop data-driven strategies and roadmaps that align with business objectives and drive innovation across the organization. Conduct market research, competitive analysis, and industry benchmarking to identify opportunities for growth, differentiation, and competitive advantage.Technology Engineering: Design, develop, and implement technology solutions and platforms to support data analytics, reporting, and automation initiatives, leveraging tools and technologies such as SQL, Python, R, Tableau, Power BI, and cloud-based platforms. Architect and maintain data infrastructure, databases, and systems to ensure scalability, reliability, and security of data assets.Cross-Functional Collaboration: Partner with cross-functional teams, including IT, Marketing, Operations, and Finance, to gather requirements, define solution specifications, and ensure successful implementation and adoption of data-driven initiatives. Provide technical guidance, training, and support to stakeholders to enable self-service analytics and empower data-driven decision-making throughout the organization.Performance Monitoring and Optimization: Monitor and analyze the performance of data analytics solutions and technology platforms, identifying opportunities for optimization, scalability, and continuous improvement. Implement best practices, standards, and governance frameworks to ensure data integrity, privacy, and compliance with regulatory requirements.\nREQUIREMENTSOccasionally lift and/or move up to 25 lbs. Ability to understand and follow instructions in English.Ability to sit for extended periods of time, twist, bend, sit, walk use hands to twist, handle or feel objects, tools or controls, such as computer mouse, computer keyboard, calculator, stapler, telephone, staple puller, etc., reach with hands and arms, balance, stoop, kneel, talk or hear.Specific vision abilities required by the job include close vision, distance vision, peripheral vision, depth perception and the ability to adjust focus.\nQUALIFICATIONSBachelor's degree in Computer Science, Data Science, Information Systems, or related field; Master's degree or relevant certification preferred.X years of experience in data analysis, strategy development, and technology engineering roles, preferably in the financial services or banking industry.Strong proficiency in data analysis tools and programming languages, such as SQL, Python, R, and experience with data visualization tools such as Tableau or Power BI.Solid understanding of data modeling, database design, and data warehousing principles, with experience working with relational databases and cloud-based platforms.Proven track record of developing and implementing data-driven strategies and technology solutions that drive business value and operational efficiency.Excellent communication, problem-solving, and stakeholder management skills.Ability to work independently as well as collaboratively in a fast-paced, dynamic environment. Strong analytical mindset, attention to detail, and a passion for leveraging data and technology to solve complex business challenges.\nABOUT STEARNS BANKStearns Bank is a leading financial institution dedicated to leveraging cutting-edge technology and data analytics to provide innovative banking solutions. With a commitment to excellence and continuous improvement, Stearns Bank offers a dynamic and collaborative work environment for professionals seeking to make a significant impact in the finance and technology sectors.\nWHY JOIN STEARNS BANK?Opportunity to work at the intersection of finance, technology, and data analytics, driving innovation and shaping the future of banking. Collaborative and inclusive work culture that values diversity, creativity, and continuous learning. Competitive compensation package with comprehensive benefits and opportunities for professional development and advancement. Make a meaningful impact by leveraging your expertise to drive data-driven decision-making and technology innovation, contributing to the success and growth of Stearns Bank.Note: The above job description is intended to outline the general nature and level of work being performed by individuals assigned to this position. It is not intended to be construed as an exhaustive list of responsibilities, duties, and skills required. Management reserves the right to modify, add, or remove duties as necessary to meet business needs.\nEQUAL OPPORTUNITY EMPLOYER /AFFIRMATIVE ACTION PLANWe are"}
{"text": "Experience in Machine Learning and Deep Learning, including regression, classification, neural network, and Natural Language Processing (NLP).2. Extensive experience on Natural Language Processing (NLP) libraries such as Spacy, NLTK, flair, and sklearn-crfsuite. 3. Strong background in DNN, CNN, RNN(LSTM), GAN, and libraries to deploy these models, such as Sklearn, Keras, Pandas, and TensorFlow. 4. Experience in Text Analytics, developing different Statistical Machine Learning, Data Mining solutions to various business problems, and generating data visualizations using R, Python. 5. Experience with common data science toolkits and libraries, such as Pandas, NumPy, SciPy, Scikit-learn. 6. Experience with data exploration to find actionable insights and make Product Recommendations through Funnel Analyses, A/B testing, Churn analysis, User Segmentation, Retention Rate, and business KPIs"}
{"text": "Qualifications)\n\n 5+ years of data analytic, data validation, data manipulation experience Six Sigma yellow or green belt certification Strong Power BI skills Strong Excel skills\n\nHow To Stand Out (Preferred Qualifications)\n\n Six Sigma Black Belt certification\n\n#DataAnalysis #RemoteWork #CareerGrowth #CompetitivePay #Benefits\n\nAt Talentify, we prioritize candidate privacy and champion equal-opportunity employment. Central to our mission is our partnership with companies that share this commitment. We aim to foster a fair, transparent, and secure hiring environment for all. If you encounter any employer not adhering to these principles, please bring it to our attention immediately. Talentify is not the EOR (Employer of Record) for this position. Our role in this specific opportunity is to connect outstanding candidates with a top-tier employer.\n\nTalentify helps candidates around the world to discover and stay focused on the jobs they want until they can complete a full application in the hiring company career page/ATS."}
{"text": "experienced and passionate professional to join our talented team as a Senior Data Engineer, DataBricks. At Self Esteem Brands, we offer a fun, fast growing, inspirational culture that incorporates a flexible, hybrid work schedule.\n\nNOTE: This position is a Hybrid position, coming into the Woodbury, MN office every week on Tuesday and Thursdays. Our first priority candidates will be current local residents, able to do the hybrid work week immediately. \n\nJob Summary\n\nSelf Esteem Brands is at the forefront of a digital transformation. We're not just evolving; we're revolutionizing our data infrastructure. Our goal? A modernized data platform that is more than \"the data warehouse\" and embodies principles of a data mesh culture to better serve our internal and global customers. We are looking for a Senior Data Engineer, someone well-versed in data platforms such as Databricks or Snowflake, cloud environments (preferably azure) and and keen to lead this transformative journey as we look to enhance our capabilities to support our multi-brand, global organization that incorporates many different sources, velocities and volumes of data\n\nPurpose/Impact: (Duties & Essential Functions)\n\nArchitect and refine our Databricks-centric data platform, emphasizing scalable integrations and advanced entity resolution strategies.Lead the charge towards a data mesh infrastructure, promoting domain-centric design and decentralized data management.Enhance our DataOps capabilities, emphasizing data observability, discovery, and lineage to maintain data integrity and accuracy.Pioneer the adoption of Databricks data lakehouse architectures, focused on simplifying data management and enhancing data processing capabilities.Serve as a technical liaison among analytics engineers, BI developers, and analysts within the Databricks environment, integrating solutions like Segment.io.Mentor junior data engineers, instilling best practices in Databricks and techniques for sophisticated data processing and entity resolution.Develop and optimize SQL and Python/Scala scripts within Databricks for complex data transformation and integration tasks.Work closely with cross-functional teams to align data strategies with operational needs and objectives, incorporating advanced data resolution methodologies.Stay at the forefront of industry trends and technological advancements to ensure our Databricks platform remains innovative and effective.\n\nStrengths And Background\n\nBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.Strong leadership skills and the ability to advocate for best practices in data management and processing.5+ years of experience in a data engineering role, with history of implementing data platform modernizationMust have 2+ years of hands on databricks experience.In-depth understanding of Databricks, along with experience in other cloud data warehouses like Snowflake, Redshift, Big Query.Building data integration and orchestration within the databricks environment, with hands-on experience in technologies such as Delta Live Tables, CDC, dbt, airflow, Segment.io.Advanced skills in SQL, Python, and Scala, with an emphasis on their application within Databricks for complex data tasks.Knowledge of best practices in version control and CI/CD with Git and GitHub Actions; Agile project management with Jira and Confluence.Proficiency with the Azure data suite (Azure SQL, Data Factory, Synapse Analytics, Power BI).Solid understanding of data observability, discovery, and lineage, and their application in maintaining high data quality standards.Additional experience with AI and ML capabilities in the context of Databricks is a plus.\n\nWhat’s In It For You\n\nMedical, Dental and Vision CoverageUnlimited Time off & Fridays off Memorial Day to Labor Day401(K) Savings PlanPaid Parental LeaveCoaching & Therapy SessionsBrand Discounts & ReimbursementsProfessional Development Opportunities"}
{"text": "skills to collect, analyze and interpret large datasets to help develop data and value-driven solutions to solve challenges for our Supply Chain end to end. You will join a newly formed team transforming our analytical and digital culture. Daily responsibilities include partnering with cross-functional teams across Conagra to hypothesize, formulate, develop, deliver and improve data science products to help improve and advance data-driven insights, decisions, simulation, actions and automation\n\nWhat You’ll Do\n\nDevelop and deploy data mining and advanced analytics to monitor, benchmark, and optimize business needs, identifying areas for improvement and deeper, root-cause analysisDevelop and deploy models, simulation models, and other advanced analytics solutions to enable data-driven decision-making to meet Supply Chain objectivesApply business acumen to continuously develop new features to improve analytical modelsPartner with cross-functional business stakeholders on assumptions, opportunities, and solutionsCollaborate to guide standards, best practices, solution innovation, future solution needs and keep current with industry trendsIdentify relationships and trends, perform statistical analysis and implement machine learning algorithms for prediction, forecasting and classificationAdvance our analytics maturity and data-driven culture\n\n\nYou’ll Have\n\nBachelor's Degree3+ years of experience developing and applying operational research models, data mining applications, and advanced analyticsStrong problem solving skills with an emphasis on product developmentExperience using statistical computer languages (R, Python, SQL) to manipulate data and draw insights from large datasetsKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks) and their real-world advantages/drawbacksKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage) and experience with applicationsHands-on experience with Databricks, Snowflake and other Cloud platformsA drive to learn and master new technologies and techniquesExcellent written and verbal communication skills for coordinating across teamsWe ask that you travel up to 20-30% to OmahaNumber of days in the office: 3\n\n\nRelocation assistance is available for this position. Preference will be given to local candidates\n\nAt this time, we require applicants for this role to be legally authorized to work in the United States without requiring employer sponsorship either now or in the future.\n\nOur Benefits\n\nWe care about your total well-being and will support you with the following, subject to your location and role:\n\nHealth: Medical, dental and vision insurance, company-paid life, accident and disability insuranceWealth: great pay, incentive opportunity, matching 401(k) and stock purchase planGrowth: online courses, virtual and classroom development experiencesBalance: paid-time off, parental leave, flexible work-schedules (subject to your location and role)\n\n\nOur Company\n\nConagra Brands is one of North America's leading branded food companies. We have a rich heritage of making great food, and a team that’s passionate about innovation and growth. Conagra offers choices for every occasion through iconic brands, such as Birds Eye®, Marie Callender's®, Banquet®, Healthy Choice®, Slim Jim®, Reddi-wip®, and Vlasic®, and emerging brands, including Angie's® BOOMCHICKAPOP®, Duke's®, Earth Balance®, Gardein®, and Frontera®.\n\nWe pride ourselves on having the most impactful, energized and inclusive culture in the food industry. For more information, visit www.conagrabrands.com.\n\nConagra Brands is"}
{"text": "Role - Azure Data Engineer + Hadoop and SQL Exp.Location - Bellevue, WA (Day 1 Onsite)\nJob description – Exp 10 + Years Azure Data Engineer Hadoop Exp Strong SQL"}
{"text": "experience in data engineering, data analysis, or a related role, preferably in a fast-paced, technology driven environment.Proficiency in programming languages such as Python, SQL, or Java, and experience with data manipulation and transformation frameworks (e.g., Pandas, Spark).Strong understanding of database technologies (e.g., SQL, NoSQL, BigQuery), data warehousing concepts, and cloud platforms (e.g., AWS, Azure, GCP). Experience with data integration tools (e.g., Apache NiFi, Talend, Informatica) and workflow management systems (e.g., Apache Airflow, Luigi).Familiarity with data visualization and BI tools (e.g., Tableau, Power BI) is a plus. Prior experience with Smartsheet, Air Table, Power Query, and SharePoint for data organization, tracking, and collaboration is highly desirable.Collaborate and communicate with data and scorecard stakeholders and other non-technical teammates to work through technical requirements.Detail-oriented mindset with a focus on data quality, accuracy, and consistency.Manage competing priorities aligned to desired outcomes in a dynamic environment.Respond to ad hoc requests.\nQualifications:Bachelor's or Master's degree in Computer Science, Information Systems, Data Engineering, or related field.Proven experience in data engineering, data analysis, or a related role, preferably in a fast-paced, technology driven environment.Proficiency in programming languages such as Python, SQL, or Java, and experience with data manipulation and transformation frameworks (e.g., Pandas, Spark).Strong understanding of database technologies (e.g., SQL, NoSQL, BigQuery), data warehousing concepts, and cloud platforms (e.g., AWS, Azure, GCP).Experience with data integration tools (e.g., Apache NiFi, Talend, Informatica) and workflow management systems (e.g., Apache Airflow, Luigi).Familiarity with data visualization and BI tools (e.g., Tableau, Power BI) is a plus.Excellent analytical, problem-solving, and communication skills, with the ability to collaborate effectively across teams and communicate technical concepts to non-technical stakeholders.Detail-oriented mindset with a focus on data quality, accuracy, and consistency.Strong organizational skills and the ability to manage multiple tasks and priorities in a dynamic environment.Prior experience with Smartsheet, Air Table, Power Query, and SharePoint is highly desirable due to the specific data organization, tracking and collaboration requirements of the role.\nIf you are interested in this job or other job opportunities available through Modis, please apply online with professional references at www.akkodisgroup.com or e-mail marie.badger@akkodisgroup.com. Referrals are greatly appreciated.\nEqual Opportunity Employer/Veterans/DisabledBenefit offerings could include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits and 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs which are direct hire to a clientTo read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.modis.com/en-us/candidate-privacy/The Company will consider qualified applicants with arrest and conviction records."}
{"text": "experienced Data Analyst, who is proactive, independent, and comfortable with identifying and resolving blockers.\nRole includes creating and maintaining centralized SharePoint site and associated content for the overall Data Remediation Transformation Program.\nDevelop and maintain automated workflow tools to facilitate regulatory remediation efforts.\nSupport BAU and analytics processes.\n\n \nQualifications:\n\n10+ years of experience in finance/ project management\nExperience and proficiency building data pipelines and performing analytics using KNIME (or similar software)\nExperience creating team SharePoint sites and maintaining content to make information and documents easily accessible\nProficiency with Visual Basic for Applications (VBA) for MS Office\nProficiency with SQL and relational database management systems\nStrong proficiency with MS Excel\nSignificant experience building end-user tools with MS Access\n\n \nNice to have:\n\nExperience in using Lynx UI, Optima Cognos Reporting Tool, Bank's Risk (Facility Management, Collateral) and extracting data from Data Globe (especially data schemas: DGSTREAM, DGFU, DGREF & DGLOBE)\nGood understanding on Loan data hierarchy (Request/Credit Agreement/Facility/GFRN) in Lynx.\n\n \nEducation:\n\nBachelor's/University degree\n\n \nAll interested applicants can apply directly by sending your resume to sumit.parihar@collabera.com"}
{"text": "requirements.Supports a database optimization project supporting trade dress cut-over requirements.Prepare and present reports for and to key leaders throughout the organization. Perform and assist with additional duties as directed by the Project Lead/Team Leader.\n\n\nThe minimum qualifications for this role are:\n\nThe successful candidate will have preferably completed a minimum of their sophomore year and be currently enrolled in an accredited college or university in a Data Analytics, Industrial Engineering, Supply Chain, or a related field of study.A minimum 3.0 GPA is strongly preferred, however, a combination of experience and/or education will be taken into consideration.Must possess advanced computer skills, MS Office, Power BI/Tableau, Data Analytics platforms.Knowledge of Data Analytics, Industrial Engineering or Supply Chain competencies.Strong verbal and written communication including presentation skills.Able to work in diverse multi-level, cross-divisional and multi-cultural working environment.Excellent organization skills, takes initiative and is proactive and persistent.\n\n\nThe salary range for this position is: If pursuing Bachelors degree = $20/hour, if pursuing Masters degree = $25/hour, if pursuing Doctorate degree = $30/hour.\n\nAt Viatris, we offer competitive salaries, benefits and an inclusive environment where you can use your experiences, perspectives and skills to help make an impact on the lives of others.\n\nViatris is"}
{"text": "skills and the ability to connect and communicate across multiple departments.Adept at report writing and presenting findings.Ability to work under pressure and meet tight deadlines.Be able to read and update project and program level resource forecasts.Identify recurring process issues and work with managers to find solutions and initiate improvements to mitigate future recurrence. \nSkills and Qualifications:5+ years in a Data Analyst and/or Data Scientist capacity.5 years of experience with Clarity PPM reporting, developing data dashboards, charts and datasets in Clarity.Strong knowledge of and experience with reporting packages (Business Objects, Tableau, Power BI, etc.), databases (SQL), programming (XML, JavaScript, etc.).Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SAS, R, SPSS, etc.)High understanding of PPM disciplines has worked in a team and covered strategic projects. Experience with Dashboard customization, configuration, user interface personalization and infrastructure management will be helpful.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail, accuracy, and actionable insights.Excellent communicator, adjusting communication styles based on your audience.Quick learner, adaptable and can thrive in new environments.Proactive, confident, and engaging; especially when it comes to large stakeholder groups.Capable of critically evaluating data to derive meaningful, actionable insights.Demonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background."}
{"text": "skills of the future by making high-quality education accessible and affordable to individuals, companies, and governments around the world. It does this by collaborating with more than 50 top-tier universities across the United States, Europe, Latin America, Southeast Asia, India and China. Emeritus’ short courses, degree programs, professional certificates, and senior executive programs help individuals learn new skills and transform their lives, companies and organizations. Its unique model of state-of-the-art technology, curriculum innovation, and hands-on instruction from senior faculty, mentors and coaches has educated more than 250,000 individuals across 80+ countries.\nFounded in 2015, Emeritus, part of Eruditus Group, has more than 1,800 employees globally and offices in Mumbai, New Delhi, Shanghai, Singapore, Palo Alto, Mexico City, New York, Boston, London, and Dubai. Following its $650 million Series E funding round in August 2021, the Company is valued at $3.2 billion, and is backed by Accel, SoftBank Vision Fund 2, the Chan Zuckerberg Initiative, Leeds Illuminate, Prosus Ventures, Sequoia Capital India, and Bertelsmann.\nJob Description:Emeritus is seeking a Data Analyst to join our client operations team on the Enterprise (B2B) team. This role will report into the Director of Client Operations. As part of the client operations team, which functions as the “back of ho/use” for our delivery function, this role will be responsible for working closely with the Director of Client Operations to establish and execute processes/best practices around measuring, tracking, and reporting on learner interest, participation, progress, and engagement. This person will also work closely with the account-owning members of our organization to analyze and present compelling data-backed success stories that support growing client accounts in high-caliber and intuitive visualizations.\nKey ResponsibilitiesOn an account-by-account basis, support data analysis and interpretation so our engagement team can tell better impact stories to our clients, which may include activities such as:Report and Dashboard Development: Create persuasive and compelling reports that capture key insights around learning impact for our clients; create effective and intuitive dashboards to track learner participation, progress, and engagement.Data Visualization and Storytelling: Utilize advanced visualization techniques to present complex data in an easily understandable format.Learner and Client Insights Analysis: Analyze behaviors, patterns, trends, and interests of our learners and clients to provide insights to our account management and sales teams about potential upselling opportunities.Cross-Functional Collaboration: Work closely with account management, sales, and technical operations teams to align data-driven insights with business objectives.Enablement of learner measurement and account storytelling. Inform the client engagement and instructional design teams on the creation of measurement tactics to assess the participation, engagement, and learning outcomes of our programs.\nRequired experience4+ years of experience in data analytics focused on insights and data storytelling2+ years of experience in spreadsheeting software (like Excel or Google Sheets), SQL, and dashboarding software (like Tableau, Looker, or Power BI)2+ years of experience in the creation of effective measurement tactics (surveys, assessments, interviews, polls, etc)Strong analytical thinking and problem-solving skills.Experience in a creative, highly collaborative, agile, ambiguous, and fast-changing environment.Results-oriented. Organized & methodical. Ability to meet deadlines and multi-task under pressure.Passion for Customer centricity, enablement, operations, innovation, and customer delightExcellent verbal and written communication skills\nPreferred experiencePrevious experience as a Learning & Development Analyst, Marketing Analyst, or Business AnalystA background in the professional learning/education space.A quantitative background or degree.Machine learning and artificial intelligence (ML/AI) technical skills will not be used in this position.\nSalary Range Description:Applicants must be currently authorized to work in the United States on a full-time basis. The compensation for this position ranges from $80,000 - $100,000 annually. Pay will be determined on several factors including but not limited to location, skills, experience and may vary from the range listed above. Additional compensation includes industry leading benefits Emeritus provides such as: a comprehensive benefits package, 401K savings plan, company holidays, flexible PTO, and parental leave, just to name a few.\nEmeritus provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws."}
{"text": "experience for GSK’s scientists, engineers, and decision-makers, increasing productivity and reducingtime spent on “data mechanics”Providing best-in-class AI/ML and data analysis environments to accelerate our predictive capabilities and attract top-tier talentAggressively engineering our data at scale to unlock the value of our combined data assets and predictions in real-time\n\n\nData Engineering is responsible for the design, delivery, support, and maintenance of industrialized automated end-to-end data services and pipelines. They apply standardized data models and mapping to ensure data is accessible for end users in end-to-end user tools through the use of APIs. They define and embed best practices and ensure compliance with Quality Management practices and alignment to automated data governance. They also acquire and process internal and external, structured and unstructured data in line with Product requirements.\n\nA Data Engineer II is a technical contributor who can take a well-defined specification for a function, pipeline, service, or other sort of component, devise a technical solution, and deliver it at a high level. They have a strong focus on the operability of their tools and services, and develop, measure, and monitor key metrics for their work to seek opportunities to improve those metrics. They are aware of, and adhere to, best practices for software development in general (and data engineering in particular), including code quality, documentation, DevOps practices, and testing. They ensure the robustness of our services and serve as an escalation point in the operation of existing services, pipelines, and workflows.\n\nA Data Engineer II should be deeply familiar with the most common tools (languages, libraries, etc) in the data space, such as Spark, Kafka, Storm, etc., and aware of the open-source communities that revolve around these tools. They should be constantly seeking feedback and guidance to further develop their technical skills and expertise and should take feedback well from all sources in the name of development.\n\nKey responsibilities for the Senior Data Engineer include:\n\n\nBuilds modular code / libraries / services / etc using modern data engineering tools (Python/Spark, Kafka, Storm, …) and orchestration tools (e.g. Google Workflow, Airflow Composer)Produces well-engineered software, including appropriate automated test suites and technical documentationDevelop, measure, and monitor key metrics for all tools and services and consistently seek to iterate on and improve themEnsure consistent application of platform abstractions to ensure quality and consistency with respect to logging and lineageFully versed in coding best practices and ways of working, and participates in code reviews and partnering to improve the team’s standardsAdhere to QMS framework and CI/CD best practicesProvide L3 support to existing tools / pipelines / services\n\n\nWhy you?\n\nBasic Qualifications:\n\nWe are looking for professionals with these required skills to achieve our goals:\n\n\n4+ years of data engineering experience with a Bachelors degree.2+ years of data engineering experience with a PhD or a Masters degree.Cloud experience (e.g., AWS, Google Cloud, Azure, Kubernetes)Experience in automated testing and design Experience with DevOps-forward ways of working \n\n\nPreferred Qualifications:\n\nIf you have the following characteristics, it would be a plus:\n\n\nSoftware engineering experienceDemonstratable experience overcoming high volume, high compute challengesFamiliarity with orchestrating toolingKnowledge and use of at least one common programming language: e.g., Python (preferred), Scala, Java, including toolchains for documentation, testing, and operations / observabilityStrong experience with modern software development tools / ways of working (e.g. git/GitHub, DevOps tools, metrics / monitoring, …)Cloud experience (e.g., AWS, Google Cloud, Azure, Kubernetes)Application experience of CI/CD implementations using git and a common CI/CD stack (e.g. Jenkins, CircleCI, GitLab, Azure DevOps)Experience with agile software development environments using Jira and ConfluenceDemonstrated experience with common tools and techniques for data engineering (e.g. Spark, Kafka, Storm, …)Knowledge of data modeling, database concepts, and SQL\n\n\n#GSKOnyx\n\nThe annual base salary for new hires in this position ranges from $143,055 to $193,545 taking into account a number of factors including work location, the candidate’s skills, experience, education level and the market rate for the role. In addition, this position offers an annual bonus and eligibility to participate in our share based long term incentive program which is dependent on the level of the role. Available benefits include health care and other insurance benefits (for employee and family), retirement benefits, paid holidays, vacation, and paid caregiver/parental and medical leave.\n\nPlease visit GSK US Benefits Summary to learn more about the comprehensive benefits program GSK offers US employees.\n\nWhy GSK? \n\nUniting science, technology and talent to get ahead of disease together. \n\nGSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. We prevent and treat disease with vaccines, specialty and general medicines. We focus on the science of the immune system and the use of new platform and data technologies, investing in four core therapeutic areas (infectious diseases, HIV, respiratory/ immunology and oncology).\n\nOur success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves – feeling welcome, valued, and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.\n\nIf you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).\n\nGSK is \n\nImportant notice to Employment businesses/ Agencies\n\nGSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.\n\nPlease note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site."}
{"text": "QUALIFICATIONS:\n\nBachelor’s degree in computer science (or STEM or related field) and three (3) or more years of proven data-centric work experience; OR graduate degree in data science (or a related field with applicable work experience). \n\n\nPREFERRED QUALIFICATIONS:\n\nUnderstand the general concepts of statistics, data mining, machine learning, data visualization, information retrieval, artificial intelligence, and computer vision. Strong proficiency with Python (Anaconda distribution) and SQL. Experience with other languages (Bash, R, Java, C++, Scala, etc.). Experience with big data tools and architectures, such as Cloudera/Apache Hadoop, HDFS, Hive, Kudu, Impala, and Spark. Working knowledge of telematics interfaces and streaming solutions (MQTT, NiFi, Kafka, HBASE, etc.). Prior experience in industrial and/or manufacturing environments desired, and any experience with automotive or heavy duty on or off-road vehicles and controller area network (CAN) signals is a plus. Highly organized and detail-oriented, with strong critical thinking, analytical, and problem solving skills. Ability to handle multiple tasks in a fast-paced environment, both independently and as part of a team. Display excellent interpersonal skills as well as the ability to effectively present information and respond to questions from leadership and peers. Strongly motivated in learning new technologies and skills in data fields. \n\n\nWORKING CONDITIONS: \n\nPhysical Demands: Frequent: Standing, Walking/Running, Sitting, Hearing, Talking, Visual, Typing; Occasional: Driving, Bending/Kneeling, Hearing, Talking, Visual, Typing, Fine Dexterity, Manual Dexterity. \n\n\nOshkosh is committed to working with and offering reasonable accommodations to job applicants with disabilities. If you need assistance or an accommodation due to a disability for any part of the recruitment process, please contact our reception desk by phone at +1 (920) 502.3009 or our talent acquisition team by email corporatetalentacquisition@oshkoshcorp.com.\n\nOshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application.\n\nOshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information.\n\nCertain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be \"U.S. Persons,\" as defined in these regulations. Generally, a \"U.S. Person\" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum."}
{"text": "requirements and offer expert technical guidance, particularly in insurance-specific contexts, will be instrumental in driving our projects forward. We value your input and believe that the best solutions are born out of teamwork. \nWe're looking for candidates with a relevant degree in Computer Science or a related field and a solid understanding of the insurance industry. Your hands-on experience of at least five years with cloud providers is crucial, as is proficiency in SQL, PostgreSQL, Elasticsearch, Redis, Snowflake and ETL methodologies. Experience with public cloud platforms such as Azure, AWS, or Google Cloud is highly valued, and a comprehensive grasp of data warehousing, big data, and data lake concepts is explicitly tailored to insurance challenges and opportunities. If you're passionate about driving innovation in the insurance sector through cutting-edge data solutions, we want to hear from you! \n\n\nDesired Skills and Experience\n\nSQL, PostgreSQL, Elasticsearch, Redis, Snowflake, ETL, AWS, Azure, Google Cloud"}
{"text": "Skills:Design and Develop data ingestion, data pipelines• Unit test• Documentation• Experience in datawarehouse / data lake • ADF• Azure Databricks• Medallion architecture knowledge / work experience• Data Vault knowledge• SQL• DBT (One resource should have DBT prior experience)• Python or Java"}
{"text": "skills who has superior written and verbal communications skills and excellent time management. The Data Scientist/Statistician will:\n\nReport to the Study PIs and Program Managers of the LIINC and SCOPE research program and work closely with other analysts. The incumbent will work collaboratively with other investigators, trainees, staff, and partner organizations.Determine data sources for gathering available data – including but not limited to study-specific databases for SCOPE, LIINC, and the relevant substudies and clinical trials, as well as data from the national RECOVER study database, local and national biorepository databases, and public health data if applicable. Plan and design data management and data analysis of multiple studies.In consultation with program managers and principal investigators, develop multivariable data analysis plans, using appropriate statistical methods. The Data Scientist/Statistician should be able to carry out the analysis plan independently.Provide statistical input into development of data collection tools, including questionnaires.Document processes to ensure accuracy and outcomes of research.Evaluate inconsistencies and trends in the data and present hypotheses related to the implications in the analysis.Present findings to PI and incorporate the feedback into additional iterations of the report.Provide statistical expertise to PI.Design processes to guarantee strict privacy of data.Provide documentation, including drafting statistical sections of grant proposals, manuscripts and reports.Develop systems to allow data to be used adhering to data use agreement.Contribute to study design, ensure the data necessary to evaluate study hypotheses are collected, and the resulting datasets are of sufficient quality to provide reliable results.Collaborate with investigators on research papers. (Preferred) Create presentations of data using data visualization software.(Preferred) Experience (e.g., content knowledge) with the nuances of infectious disease datasets, in particular HIV datasets and phenotypes, and Long Covid datasets and phenotypes.(Preferred) Familiarity with NIH databases such as the Seven Bridges platform.\n\nThis position requires extensive experience conducting statistical analysis of observational studies, program evaluations, administrative datasets, and longitudinal cohorts.\n\nThe final salary and offer components are subject to additional approvals based on UC policy.\n\nTo see the salary range for this position (we recommend that you make a note of the job code and use that to look up): TCS Non-Academic Titles Search (https://tcs.ucop.edu/non-academic-titles)\n\nPlease note: An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role.\n\nFor roles covered by a bargaining unit agreement, there will be specific rules about where a new hire would be placed on the range.\n\nTo learn more about the benefits of working at UCSF, including total compensation, please visit: https://ucnet.universityofcalifornia.edu/compensation-and-benefits/index.html\n\nDepartment Description\n\nABOUT DIVISION OF HIV, INFECTIOUS DISEASES AND GLOBAL MEDICINE\n\nThe Division of HIV, Infectious Diseases and Global Medicine at the University of California San Francisco (UCSF) is an internationally recognized global leader in clinical care, research, and education. With its home-base located at Zuckerberg San Francisco General Hospital and Trauma Center (ZSFG), the Division has been ranked as the top facility in the country for AIDS care since 1983. The Division’s work is featured frequently in the media, and it is often visited by numerous international, national, state, and local dignitaries and delegations. The Division consists of approximately 36 faculty members, 11 fellows/postdoctoral scholars and 170 academic and staff employees. It is the largest Division of the Department of Medicine (DOM) at ZSFG, with a total expenditure budget of approximately $52 million dollars.\n\nAbout Ucsf\n\nThe University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative disease, aging, and stem cells.\n\nRequired Qualifications\n\nBachelor's degree in statistics or related area (e.g., biomedical sciences) and / or equivalent experience / training.Minimum 3+ years of related experience3 or more years of experience in a related field / with relevant research experience.Thorough skills in programming, statistical analysis and data management in STATA, SAS or R.Substantive knowledge of quantitative research methods.Research skills at a level to evaluate alternate solutions and develop recommendations.Extensive experience conducting statistical analysis of observational studies, program evaluations, administrative data or longitudinal cohorts.Strong skills in project management.Ability to work discreetly with sensitive and confidential data.Ability to multi-task with demanding timeframes.Ability to work independently and as part of a team.Skills to communicate complex information in a clear and concise manner both verbally and in writing.\n\nPreferred Qualifications\n\nGraduate degree in statistics, epidemiology or related area and / or equivalent experience / training.Familiarity with NIH databases such as the Seven Bridges platform.Experience (e.g., content knowledge) with the nuances of infectious disease datasets, in particular HIV datasets and phenotypes, and Long Covid datasets and phenotypes.Knowledge of data visualization software.\n\nAbout UCSF\n\nThe University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative diseases, aging and stem cells.\n\nPride Values\n\nUCSF is a diverse community made of people with many skills and talents. We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism, respect, integrity, diversity and excellence – also known as our PRIDE values.\n\nIn addition to our PRIDE values, UCSF is committed to equity – both in how we deliver care as well as our workforce. We are committed to building a broadly diverse community, nurturing a culture that is welcoming and supportive, and engaging diverse ideas for the provision of culturally competent education, discovery, and patient care. Additional information about UCSF is available at diversity.ucsf.edu\n\nJoin us to find a rewarding career contributing to improving healthcare worldwide.\n\n\n\nThe University of California San Francisco is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.\n\nOrganization\n\nCampus\n\nJob Code and Payroll Title\n\n006257 RSCH DATA ANL 3\n\nJob Category\n\nProfessional (Non-Clinical), Research and Scientific\n\nBargaining Unit\n\n99 - Policy-Covered (No Bargaining Unit)\n\nEmployee Class\n\nCareer\n\nPercentage\n\n100%\n\nLocation\n\nZuckerberg San Francisco General (ZSFG)\n\nShift\n\nDays\n\nShift Length\n\n8 Hours\n\nAdditional Shift Details\n\nM-F 8am-5pm"}
{"text": "Experience with genomics data, and molecular genetics. Distributed computing tools like Ray, Dask, and Spark.\nNote:\nWe need a Data Scientist with demonstrated expertise in training and evaluating transformers such as BERT and its derivatives."}
{"text": "requirements, objectives, and challenges. Translate business needs into actionable insights and data-driven solutions Stay abreast of industry trends, emerging technologies, and best practices in data analytics. Continuously enhance skills and knowledge through self-learning, training, and participation in professional development activities Maintain documentation related to data analysis processes, methodologies, and findings. Share knowledge and insights with team members to foster a culture of collaboration and knowledge exchange Conduct thorough quality assurance checks on data analysis outputs to ensure accuracy, completeness, and consistency. Implement validation protocols and procedures to validate data accuracy and integrity Provide ad hoc data analysis support to address urgent business needs or inquiries. Quickly analyze data, identify trends, and generate actionable insights to support decision-making in real-time scenarios Collaborate with cross-functional teams such as IT, finance, marketing, and operations to integrate data analytics into various business processes and functions. Identify opportunities for synergy and collaboration to maximize the impact of data insights \n\nQualifications\n\nBachelor’s degree in business, a quantitative field, or equivalent 0-2 years of relevant experience Demonstrate proficiency in using a variety of business intelligence (BI) analytics and reporting tools such as Tableau, Power BI, or QlikView. Ability to leverage these tools to extract, analyze, and visualize data effectively Possess hands-on experience in designing and configuring interactive and user-friendly dashboards. Ability to customize dashboards to meet specific business requirements and present data in a visually compelling manner Strong understanding of data mining techniques and algorithms. Ability to apply data mining methods to discover patterns, trends, and insights from large datasets, driving actionable business recommendations Experience in implementing data quality assurance processes and methodologies. Ability to assess data quality issues, develop strategies for data cleansing and validation, and ensure data accuracy and consistency Familiarity with a wide range of data visualization techniques such as charts, graphs, heatmaps, and infographics. Ability to select the most appropriate visualization method to effectively communicate insights and findings Proficiency in performing forecasting and predictive analytics using statistical methods and machine learning algorithms. Ability to build predictive models, analyze trends, and generate accurate forecasts to support decision-making Strong programming skills in scripting languages such as Python, R, and SQL. Ability to write efficient scripts and queries to manipulate, analyze, and extract insights from complex data sets Solid understanding of statistical analysis concepts such as hypothesis testing, regression analysis, and probability distributions. Ability to apply statistical techniques to analyze data and derive meaningful insights Experience in conducting trend analysis to identify patterns, anomalies, and correlations in historical data. Ability to extrapolate trends and make informed predictions about future outcomes Familiarity with industry-standard data standards and protocols. Proficiency in using statistical software packages such as SPSS, SAS, or Stata for advanced data analysis and modeling Proficiency in using general data analysis software such as Microsoft Excel, Google Sheets, or MATLAB. Ability to perform basic data manipulation, analysis, and visualization tasks using these tools \n\nPrimePay Offers:\n\nA competitive salary in the $60 - $80K range based on experience Access to personal, group training and career advancement Leadership development through individualized support and career mentoring Medical, dental and vision insurance, 401(k) with match, paid time off, paid holidays, flexible spending account, life insurance and STD/LTD"}
{"text": "Skillset – - JAVA (BATCH PROCESSING), PYTHON, , SPARK (OR PYSPARK),TERRAFORM - AWS + BIG DATA Locations: Houston TX and Plano TX Hybrid 3 days onsite and 2 days remote. Contract : 3-month Contract and then full time.  Interview process. ONSITE INTERVIEWS PREFERRED AT AN OFFICE - CONCEPTUAL LIVE CODING IN THE INTERVIEW1st interview heavily focussed on Java, Terraform, SPARK + AWS\nTOP SKILLS - JAVA (BATCH PROCESSING), PYTHON, , SPARK (OR PYSPARK),TERRAFORM - AWS + BIG DATAJava/Python developer on Hadoop/Spark/Big Data platform with AWS experience preferably on EMR, EKS, Glue, Lake Formation. (6+ years of experience) – (Plano/Houston)JAVA (BATCH PROCESSING EXPERIENCE NEEDED) - NOT HEAVY API Security - financial services - migrating application to cloud.most of the applications are using - java (Primary language application are written in) Python, spark, EC2, EMR, EKS, would consider former java (batch processing) - NOT FRONT-END JAVA - developer that moved into big data and python, spark - java experience can be a few years dated."}
{"text": "experience in data engineeringStrong understanding of Datawarehousing conceptsProficient in Python for building UDFs and pre-processing scriptsProficient in sourcing data from APIs and cloud storage systemsProficient in SQL with analytical thought processExperience working on Airflow orchestrationMust have experience working on any of the cloud platforms - AWS would be preferredExperience with CI/CD tools in a python tech stackExperience working on Snowflake Datawarehouse would be nice to haveCompetent working in secured internal network environmentsExperience working in story and task-tracking tools for agile workflowsMotivated and Self-Starting: able to think critically about problems, decipher user preferences versus hard requirements, and effectively use online and onsite resources to find an appropriate solution with little interventionPassionate about writing clear, maintainable code that will be used and modified by others, and able to use and modify other developers’ work rather than recreate itBachelor’s Degree in related field"}
{"text": "experience is a MUSTScala - Highly desiredSoftware Engineering - Highly desired"}
{"text": "experience working with Amazon Aurora and/or PostgreSQL in a production environment.Strong SQL skills and experience with SQL tuning techniques.Proficiency in AWS services such as EC2, Route 53, VPC, IAM, and CloudFormation.Hands-on experience with scripting languages (e.g., Python, Bash) for automation.Familiarity with database security concepts and best practices.Excellent problem-solving skills and attention to detail.Strong communication and collaboration skills, with the ability to work effectively in a team environment.Preferred Qualifications:AWS Certification -Not mandatoryExperience with other AWS database services such as RDS..Knowledge of containerization technologies (e.g., Docker, Kubernetes).Experience with DevOps practices and tools (e.g., CI/CD pipelines, Git).\nQualificationsAnalytical Skills, Data Analytics, and StatisticsExcellent written and verbal communication skillsData modeling and visualization skillsExperience with statistical analysis tools and softwareAbility to work independently and remotelyExperience in the finance, e-commerce, healthcare, or marketing industries is a plusBachelor's or Master's degree in Data Science, Statistics, Mathematics, or a related field"}
{"text": "requirements, activities and design. The ACH Data Analyst will develop and interpret analysis and reporting capabilities. They will also monitor performance and quality control plans to identify improvements.\n\nJob Description\n\n Works closely with ACH Product Manager, Business Analyst, and Support teams  Interpret data, analyze results using statistical techniques and provide ongoing reports  Research outgoing ACH batches and files and their response files to troubleshoot discrepancies  Acquire data from primary or secondary data sources and maintain databases/data systems  Identify, analyze, and interpret trends or patterns in complex data sets  Work with management to prioritize business and information needs  Locate and define new process improvement opportunities  Using automated tools to extract data from primary and secondary sources  Work with developers to address merchant and or partner impacting issues  Assigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.  Preparing reports for the management stating trends, patterns, and predictions using relevant data  Working with programmers, engineers, and management heads to identify process improvement opportunities, propose system modifications, and devise data governance strategies.  Works with Merchants and Support to research and resolve escalations regarding reconciliation and reporting issues. I.e. funding discrepancies  Works with internal departments to research funding and reconciliation issues  Works with internal business units to research and resolve reporting, support and processing issues;  Meet with Merchants and Partner as needed to discuss research findings and get feedback on open items  Work with Pricing or Sales Comp on any discrepancies or issues that arise  Maintain and manage the ACH Debit Exceptions items page  Manages and responds to all ACH dispute items received from Wells.  Assist with annual internal audits material gathering  Assists with documenting and reviewing new functionality within the ACH gateway for training \n\nKnowledge And Experience\n\n Minimum of 4+ years of experience in building and delivering successful software services and/or payments solutions.  Proven working experience as a Data Analyst or Business Data Analyst  Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy  Adept at queries, report writing and presenting findings  Bachelor’s degree strongly preferred or equivalent experience  Knowledge and experience of agile/scrum product development  Must excel working in team-oriented roles that rely on your ability to collaborate and positively influence others.  Strong attention to detail and organizational skills.  Excellent communication, collaboration, team dynamics, and conflict resolution skills.  Exceptional interpersonal and organizational savvy with the ability to work within a complex matrix organization.  Self-starter with strong problem identification and resolution skills.  Ability to thrive in a fast-paced, sales-intensive environment.  Exceptional communication skills with the ability to communicate to all stakeholders, including customers, executive leadership, and functional business leaders.  Experience with Azure, Aha! or similar software"}
{"text": "experienced Data Scientist who has Data Engineering leanings and has built out multiple data solutions before in the bio space.\n\nEssential Duties and Responsibilities\nCreate Data Science solutions for internal stakeholder requirements and self-identified problems through advanced analytics techniques like multivariate testing, optimization, and machine learning.Assemble and munge data sets across multiple sources and systems.Support the understanding and management of data including cleaning, mapping, mastering, deduping, and QA, and a canonical view.Present solutions and findings to stakeholders and members at all levels within BioMADE.Tackle complex problems in the biological world requiring a creative mindset to find innovative and elegant solutions.Manage stakeholder relations and requirements with staff and members of BioMADE.Present solutions and findings to stakeholders and members at all levels within BioMADE.Mentoring and leading junior members of the team.Roadmapping, drafting technical specs, and overall project scoping.Creating and updating internal system documentation.Reviewing and implementing data systems and infrastructure.Other projects as assigned or as they become apparent.\nRequirements\n4+ years of building out Data Science systems and solutions.Experience working with relational SQL and/or NoSQL databases (i.e. you can pull and munge your own data).Experience operationalizing Data Science Solutions.Experience working with big data and cloud platforms.Fluency with one or more major programing language (Python, Java, Scala, etc.).Good understanding of CS fundamentals.Expertise with Machine Learning techniques (classification, clustering, regularization, optimization, dimension reduction, etc.). Good communication skills and ability to explain complex topics to a non-technical audience.Bachelor’s Degree in computer science, engineering, physical sciences, or related discipline.\nDesired Characteristics 7+ years of building out Data Science systems and solutions in the bio space.Experience working with one of the large public cloud providers: AWS, GCC, or Azure.Experience with Data Science packages and toolkits like: TensorFlow, SageMaker, Vertex AI, etc.Familiarity with statistics concepts and analysis, e.g. hypothesis testing, regression, etc.Experience building dashboards in platform: Power BI, Tableau, etc.History of running data centric and/or data management projects.Ability to manage stakeholder discussions.Masters or PhD in computer science, engineering, physical sciences, or related discipline."}
{"text": "Skills: AWS, Spark, Adobe Analytics/AEP(Adobe Experience Platform) platform experience, Glue, Lamda, Python, Scala, EMR, Talend, PostgreSQL, Redshift\n\n Configure AEP to get the data set needed and then use spark (AWS glue ) to load data in the data lake Evaluate new use cases and design ETL technical solutions to meet requirements Develop ETL solutions to meet complex use cases\n\nAdobe Data Engineer || Remote"}
{"text": "Qualifications)\n\n Bachelor's degree in a relevant field such as mathematics, statistics, or computer science Minimum of 5 years of experience as a data analyst or similar role Proficiency in SQL, Python, and data visualization tools Strong analytical and problem-solving skills Excellent written and verbal communication skills\n\nHow To Stand Out (Preferred Qualifications)\n\n Master's degree in a relevant field Experience with machine learning and predictive modeling Knowledge of cloud-based data platforms such as AWS or Google Cloud Familiarity with Agile methodologies and project management tools Strong attention to detail and ability to work independently\n\n#RecruitingSoftware #DataAnalysis #RemoteWork #CareerOpportunity #CompetitivePay\n\nAt Talentify, we prioritize candidate privacy and champion equal-opportunity employment. Central to our mission is our partnership with companies that share this commitment. We aim to foster a fair, transparent, and secure hiring environment for all. If you encounter any employer not adhering to these principles, please bring it to our attention immediately.\n\nTalentify is not the EOR (Employer of Record) for this position. Our role in this specific opportunity is to connect outstanding candidates with a top-tier employer.\n\nTalentify helps candidates around the world to discover and stay focused on the jobs they want until they can complete a full application in the hiring company career page/ATS."}
{"text": "skills to work as you empower business partners and team members improve healthcare delivery. You will research cutting edge big data tools, and design innovative solutions to solve business problems that only a Data Engineer can. Youll be in the drivers seat on vital projects that have strategic importance to our mission of helping people live healthier lives. Yes, we share a mission that inspires. And we need your organizational talents and business discipline to help fuel that mission.\n\nYou will be part of the team who is focused on building a cutting-edge data analytics platform to support reporting requirements for the business. As a Senior Data Engineer, you will be responsible for the development of complex data sources and pipelines into our data platform (i.e. Snowflake) along with other data applications (i.e. Azure, Airflow, etc.) and automation.\n\nThis is a fully remote role based in the United States. Your counterpart team is located in Dublin, Ireland office. While there is no requirement to work in shift hours, there might be an occasional call with Dublin team which can require flexible working.\n\nPrimary Qualifications:\n\n Computer Science bachelors degree or similar Min 3-6 years of industry experience as a Hands-on Data engineer Excellent communication skills Excellent knowledge of SQL, Python Excellent knowledge of Azure Services such as - Blobs, Functions, Azure Data Factory, Service Principal, Containers, Key Vault etc. Excellent knowledge of Snowflake - Architecture, best practices Excellent knowledge of Data warehousing & BI Solutions Excellent Knowledge of change data capture (CDC), ETL, ELT, SCD etc. Knowledge of CI CD Pipelines using GIT & GIT Actions Knowledge of different data modelling techniques such as Star Schema, Dimensional models, Data vault Hands on experience on the following technologies:Developing data pipelines in Azure & snowflakeWriting complex SQL queriesBuilding ETL/ELT/data pipelines using SCD logicExposure to Kubernetes and Linux containers (i.e. Docker)Related/complementary open-source software platforms and languages (e.g. Scala, Python, Java, Linux) Previous experience with Relational Databases (RDBMS) & Non- Relational Database Analytical and problem-solving experience applied to a Big Data datasets Good understanding of Access control and Data masking Experience working in projects with agile/scrum methodologies and high performing team(s) Exposure to DevOps methodology Data warehousing principles, architecture and its implementation in large environments Very good understanding of integration with Tableau\n\nPreferred Qualifications:\n\n Design and build data pipelines (in Spark) to process terabytes of data Very good understanding of Snowflake integration with data visualization tool such as Tableau Orchestrate in Airflow the data tasks to run on Kubernetes/Hadoop for the ingestion, processing and cleaning of data Terraform knowledge and automation Create real-time analytics pipelines using Kafka / Spark Streaming Work on Proof of Concepts for Big Data and Data Science Understanding of United States Healthcare data\n\n\n\nApex Systems is \n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\n4400 Cox Road\n\nSuite 200\n\nGlen Allen, Virginia 23060\n\nApex Systems is"}
{"text": "Requirements\n\n6+ years of experience in an analytics role or equivalent experience/trainingShare and work in accordance with our valuesPositive and solution-oriented mindsetClearly and concisely communicate complex business logic, technical requirements, and design recommendations through iterative solutionsExpert in SQL within analytical data warehouses such as Snowflake (preferred), Databricks, or Redshift and in business intelligence tools such as Power BI (preferred), or TableauAble to write complex SQL including multi-table joins, grouping and aggregation, common table expressions, and conditional filtersDemonstrable understanding of Gremlin/Cypher and graph databases such as Neo4j (preferred) or AWS NeptunePassionate about data, analytics and automation. Experience cleaning and modeling large quantities of raw, disorganized dataExperience working with a variety of complex data sources. Our data includes Salesforce, Marketo, NetSuite, and many othersEffective communication and collaboration skills, including clear status updatesComfort working in a highly agile, intensely iterative environmentSelf-motivated and self-managing, with strong organizational skillsHands on experience working with Python, API calls, and JSON, to generate business insights and drive better organizational decision makingDemonstrated analytical experience with one or more of the following business subject areas: marketing, finance, sales, product, customer success, customer support, engineering, or peopleDemonstrated ability to define, design, and develop key performance indicators related to one or more business subject areas\n\nWorking Conditions And Physical Requirements\n\nAbility to work for long periods at a computer/deskStandard office environment\n\nAbout The Organization\n\nFullsight is an integrated brand of our three primary affiliate companies – SAE Industry Technologies Consortia, SAE International and Performance Review Institute – and their subsidiaries. As a collective, Fullsight enables a robust resource of innovative programs, products and services for industries, their engineers and technical experts to work together on traditional and emergent complex issues that drive their future progress.\n\nSAE Industry Technologies Consortia® (SAE ITC) enables organizations to define and pilot best practices. SAE ITC industry stakeholders are able to work together to effectively solve common problems, achieve mutual benefit for industry, and create business value.\n\nThe Performance Review Institute® (PRI) is the world leader in facilitating collaborative supply chain oversight programs, quality management systems approvals, and professional development in industries where safety and quality are shared values.\n\nSAE International® (SAEI) is a global organization serving the mobility sector, predominantly in the aerospace, automotive and commercial-vehicle industries, fostering innovation, and enabling engineering professionals. Since 1905, SAE has harnessed the collective wisdom of engineers around the world to create industry-enabling standards. Likewise, SAE members have advanced their knowledge and understanding of mobility engineering through our information resources, professional development, and networking."}
{"text": "experienced crew who love to collaborate and think failing is just another form of learning. Transparency into decisions: We’re wildly transparent—you'll see the 'why' behind the decision we make. Recognition and Ownership: See the tangible impacts of your contributions on our users and the broader community.\nWhat You Should Bring With You A Passion for Data: Demonstrated love for solving data problems and enabling impactful decisions. Technical Expertise: Proficiency in Python, SQL, deep understanding of data transformation/transportation technologies and cloud technologies, with a knack for building scalable data architectures. Collaborative Spirit: The ability to work cross-functionally and uplift the team with your positive energy. People Focus: A genuine interest in understanding user needs and translating them into technical solutions. Collaborative Spirit: You don’t just contribute to your codebase; you contribute to team morale. Curious Nature: Your inquisitiveness should be like our code—never-ending and always improving."}
{"text": "experience. Strong SQL Skills Strong Python Skills \n\n\nWhat are the top three PREFERRED skill sets (technical)?\n\nAWS technologies like redshift, S3, AWS Glue, EMR, etc. BI report development experience. \n\n\nSoft Skill requirements (team fit/personality requirements)\n\nEffective communication skills Strong MS Excel skills Data analysis skills"}
{"text": "skills to translate the complexity of your work into tangible business goals \n\nThe Ideal Candidate is:\n\n Customer first. You love the process of analyzing and creating, but also share our passion to do the right thing. You know at the end of the day it’s about making the right decision for our customers.  A leader. You challenge conventional thinking and work with stakeholders to identify and improve the status quo. You're passionate about talent development for your own team and beyond.  Technical. You’re comfortable with open-source languages and are passionate about developing further. You have hands-on experience developing data science solutions using open-source tools and cloud computing platforms.  Statistically-minded. You’ve built models, validated them, and backtested them. You know how to interpret a confusion matrix or a ROC curve. You have experience with clustering, classification, sentiment analysis, time series, and deep learning.  A data guru. “Big data” doesn’t faze you. You have the skills to retrieve, combine, and analyze data from a variety of sources and structures. You know understanding the data is often the key to great data science. \n\nBasic Qualifications:\n\n Currently has, or is in the process of obtaining a Bachelor’s Degree plus 2 years of experience in data analytics, or currently has, or is in the process of obtaining Master’s Degree, or currently has, or is in the process of obtaining PhD, with an expectation that required degree will be obtained on or before the scheduled start date  At least 1 year of experience in open source programming languages for large scale data analysis  At least 1 year of experience with machine learning  At least 1 year of experience with relational databases \n\nPreferred Qualifications:\n\n Master’s Degree in “STEM” field (Science, Technology, Engineering, or Mathematics), or PhD in “STEM” field (Science, Technology, Engineering, or Mathematics)  Experience working with AWS  At least 2 years’ experience in Python, Scala, or R  At least 2 years’ experience with machine learning  At least 2 years’ experience with SQL \n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nNew York City (Hybrid On-Site): $138,500 - $158,100 for Data Science Masters\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan\n\n Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "experience. 2+ years of industry work experience in SQL, Python to implement statistical models, machine learning, and analysis (Recommenders, Prediction, Classification, Clustering, etc.) preferably in a big data environment 2+ years of industry experience in building production-grade software applications 2+ years of industry experience with AI/ML frameworks, with all aspects of model training, tuning, deploying, serving, and monitoring. PyTorch, Keras, Tensorflow are a plus. 2+ years of industry experience with database query, visualization, and analysis tools such as Power BI and Jupyter notebooks Exceptional written and verbal communication to educate and work with cross functional teams Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data Be motivated, self-driven in a fast-paced environment with a proven track record demonstrating impact across several teams and/or organizations. Strong background in machine learning technology, especially in the field of Deep Learning / Deep Neural Networks, NLP, OCR, NER, ML frameworks Experience in both SQL and NoSQL databases such as MySQL, SQL Server, Cosmos Experience in big data systems such as Hadoop, MapReduce, Spark Experience in software engineering roles and development experience in Java, C#, or similar programming languages is a plus. Solid understanding in software engineering best practices, with Agile methodologies, DevOps practices, CI/CD pipelines, GitLab/GitHub, Shell scripting etc. Experience with Azure cloud services, AKS, Cognitive services, MLOps, Azure Data Framework is a big plus. \n\nWe’re obsessed with making this the best job you’ve ever had!  \n\nWe want our teams to love working here, so we’ve created some incredible perks for you to enjoy: \n\nJoin our employee-led groups to maximize your experience at work such as our Diversity, Equity and Inclusion committee, employee resource groups such as Women and Allies, and our Pride Event GroupEnjoy peace of mind over yours and your family’s health with our medical coverage options and HSA benefitInvest in our competitive 401k plan and help set you up for your futureBig on family? So are we! We understand family is important and being able to spend quality time with your family is a wonderful experience. Our Parental Leave Program is designed to give you the opportunity to spend even more time with your new arrival(s)Enjoy a fantastic work-life balance with 20 days PTO plus observed Holidays, plus 15 hours of ‘Flexi’ time a yearFurther your professional development and growth with our generous Tuition Reimbursement offerings Enjoy the flexibility of working from anywhere in the world for two weeks out of the year\n\nAt MRI, our company culture is more than a talking point – it’s what makes us shine! We value your hard work and encourage you to be your whole self while you do it. Passion, integrity, and inclusion mixed with a healthy dose of fun is what makes us the best fit for your next career move!\n\nMRI continues to strive to amaze as a global industry leader in real estate software. Whether you are joining as a new pride member or bringing your expertise back, your talent is important to maintaining MRI’s high client experience standard and continuing our growth in the PropTech space.\n\nAmazing growth takes amazing employees. Are you up to the challenge?\n\nApplicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.\n\nWe are not accepting unsolicited resumes from agencies and/or search firms for this job posting.\n\nMRI Software is"}
{"text": "experience in Data Engineering , architecture, design and development.• Minimum 5 years of experience with ETL/ELT design and development using tools like IICS, AWS Glue, Talend, Databricks, Oracle Data Integrator (ODI) or equivalent.• Minimum 5 years of experience in database architecture using relational SQL (e.g. Snowflake, Oracle, PostgreSQL, MySQL), and NoSQL (DynamoDB, MongoDB, Elasticsearch).• Minimum 3 years of experience in working as Engineering Lead/Architect on cloud platforms (AWS is highly desirable).• Minimum 3 years of experience with programming languages such as Node.js, Python, Java or Scala.• Minimum 2 years of experience with building applicating with serverless architecture on AWS platforms using Athena, Glue, Kafka, EC2, Lamda, Kenesis.• Minimum 2 years of experience with large-scale data processing platforms such as Spark, EMR, and/or HPC computing experience with e.g. Apache Aurora, Slurm.• Experience working with RESTful API and general service-oriented architectures.• Experience with DevOps, Continuous Integration and Continuous Delivery technologies is desirable.• AWS Solution Architect certification (associate & above) is highly desirable.• Healthcare domain knowledge is a huge plus.• Excellent verbal and written communication skill\ns.\nWhat You Bring:• Bachelors degree or higher in Computer Science or related fie\nld."}
{"text": "skills in Scala and Spark Hands on experience with data processing technologies, ETL processes and feature engineering A track record of developing scalable pipelines and delivering data promptly in a collaborative team environment\n\nSkills:\n\nSpark, scala, Etl, Aws\n\nTop Skills Details:\n\nSpark,scala,Etl\n\nAdditional Skills & Qualifications:\n\nExperience in commonly used cloud services (AWS) Expertise in columnar storage such as Parquet, Iceberg Knowledge in deep learning models\n\nExperience Level:\n\nIntermediate Level\n\nAbout TEKsystems:\n\nWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.\n\nThe company is"}
{"text": "requirements in a manner well understood by the developers and provide acceptance criteria.Writes epics, user stories, acceptance criteria in automation scripts and participate in all agile events of the product group as a team member.Collaborates with other product teams, technical architects, developers, and tech leads to understand the technology roadmap for modernization.Verifies the results of testing and raise defects/issues against the features owned.Supports the end-to-end testing of the feature and provides sign-off for the code release into production.Validates the production release by working with business users utilizing test accounts in production environment.Builds and maintains strong working relationships with multiple diverse stakeholders across the company.Understands customer issues, converting them into problem statements and provides out of box solutions thru design thinking\n\nREQUIRED EXPERIENCE FOR PRODUCT ANALYST:\nExcellent written and verbal communication skills: able to present facts, thoughts, and ideas in a clear, concise, and manner to senior stakeholders.Knowledgeable about software development / programming due to the technical nature of the role.Skills with Office, Microsoft Excel, stored procedures or other data toolingDemonstrated capabilities working in Jira and Confluence.Capability working with customers, presentations, and participating in requirements sessions.Understanding of the complete software implementation lifecycle - including data requirements gathering, dataset migration, data reconciliation, and move to production.Ability to collaborate with cross-organizational partners, both corporate and in the market, to define requirements and resolve data issues.Capable of experience mapping, transforming, and loading data from source to target software systems.\n\nSKILLS AND QUALIFICATIONS FOR PRODUCT ANALYST:\n3+ years professional experience with SQL or similar RDBMS architecture.Has some experience in change and design initiatives, covering areas such as operations, IT, and product development.Experience designing (Power Bi/Tableau) reports with either relational data or big data.Experience with GitHub, PythonExperience in user centric designs to drive the right customer journey outcomes.5+ years of product execution / technical business analyst experience with a Bachelor’s degree or 3+ years relevant experience and a Master’s degree.•Experience with data migration, integration development, report building, or software development.Experience defining user experience needs, writing epics, stories, and acceptance criteria in any automation scripting language such as Gherkin etc.Direct experience working in an Agile team.\n\nWhy should you choose Epitec?We started Epitec with a single focus, “Placing People First.” Knowing every good endeavor begins with listening and understanding, we’ve set about challenging every part of the employment process. Bringing the proper connections together for the perfect fit.\nHow is Epitec different?Epitec gets to know our prospective employees, using these insights to locate the perfect placement for you. We are there, every step of the way. Providing a best-in-class compensation package combined with the opportunity to grow financially and personally through your work.\nWhat is the result?Epitec represents the world’s top companies and works to fill their open jobs with the world’s best talent. That’s led to Epitec servicing an impressive list of Fortune 100 companies. We've also won many awards, including one of Crain’s Detroit Business “Cool Places to Work,” and 101 Best & Brightest – local, national and elite winner. And that’s just the beginning, as we work to innovate the way the world thinks about employment."}
{"text": "experience with Transformers\nNeed to be 8+ year's of work experience. \nWe need a Data Scientist with demonstrated expertise in training and evaluating transformers such as BERT and its derivatives.\nRequired: Proficiency with Python, pyTorch, Linux, Docker, Kubernetes, Jupyter. Expertise in Deep Learning, Transformers, Natural Language Processing, Large Language Models\nPreferred: Experience with genomics data, molecular genetics. Distributed computing tools like Ray, Dask, Spark"}
{"text": "requirements and future client-focused solutions and services.  Establish robust data governance and quality control frameworks to guarantee data security, data accuracy and accessibility.  Promote the organization’s capabilities in leveraging advanced technologies, including AI/ML and immersive technologies in client-facing and internal initiatives.  Work across diverse scientific and technical disciplines to translate technical architectures into proposal solutions.  Ensure compliance with all relevant government contracting regulations, emphasizing data security and integrity.  Maintain awareness of industry trends and technological advancements, leveraging insights to drive organizational improvement and competitive advantage.  Guide, mentor, and provide leadership with data engineers, cultivating an environment of innovation, high performance, diverse thought and continuous learning . This position is located in Reston, VA with the ability to work in a hybrid work environment.\n\nRequired Qualifications\n\n Master’s degree in computer science, Data Science, Information Systems, Engineering, or a related field.  High profile technical experience as a Chief Engineer or related technology-forward executive position with demonstrated experience interfacing frequently with clients, partners, and employees.  Must have 15+ years of experience in data engineering or a related field, with at least 10 years in a hands-on leadership role. Exceptional leadership skills and experience managing and/or collaborating across multiple technical teams in high-stakes, fast-paced environments is required.  Expert background in software engineering, database management, data architecture, networking, infrastructure design, and deployment.  Proven expertise in commercial software pricing to define structures based on volume, capacity, and usage patterns such as database as a service (DBaaS), platform as a service (PaaS), infrastructure as a service (IaaS), and software as a service (SaaS).  Proven expertise in data modeling, data lakehouse architectures, data warehousing, ETL processes, and big data technologies to include integrating data from multiple sources into a common information pool for use by data scientists and ML engineers across multiple disciplines.  Expertise in containerization and data orchestration (e.g., Docker, Kubernetes, etc.)  Expert proficiency working in both Linux and Windows operating environments with DevSecOps, automated software deployment and full-lifecycle CI/CD experience.  Proficiency in designing architectures for relational database management systems (RDBMS) including PostGres, Oracle, MS SQL Server, and noSQL.  Expert proficiency in programming languages such as Shell Scripting, C, C++, Python, SQL and/or PL/pgSQL, and Java, along with experience in designing and implementing scalable data infrastructure.  Experience with Infrastructure as Code solutions and familiarity with data processing services across one or more commercial cloud providers.  Proven track record of developing and implementing data and data security strategies in compliance with stringent government regulatory and security requirements in the context of government contracting.  Ability to translate complex technical concepts and opportunities into clear, strategic plans aligned with business objectives.  Strong written and verbal communication skills to include effective engagement with C-level executives, clients, and technical teams.  Ability to travel up to 30%. \n\nDesired Qualifications\n\n Experience with cloud services (AWS, Azure, Google Cloud) geared towards government and defense contracting.  In-depth knowledge of data storage and infrastructure solutions to support a wide range of platforms requiring scaling and high performance such as AI/ML algorithms, spatial computing, high fidelity graphics, and immersive technology/XR platforms.  Basic understanding of machine learning algorithms and analytics to better support data scientists and analytical workflows.  Industry-recognized code committer and/or technical publisher/speaker in deep tech areas.  Data center management experience with proven ability to forecast and scale infrastructure and computing according to organizational needs.  Ability to obtain a security clearance. \n\nOverview\n\nNoblis and our wholly owned subsidiaries, Noblis ESI , and Noblis MSD tackle the nation's toughest problems and apply advanced solutions to our clients' most critical missions. We bring the best of scientific thought, management, and engineering expertise together in an environment of independence and objectivity to deliver enduring impact on federal missions. Noblis works with a wide range of government clients in the defense, intelligence and federal civil sectors. Learn more at Noblis -About Us\n\n Why work at a Noblis company? \n\nOur employees find greater meaning in their work and balance the other things in life that matter to them. Our people are our greatest asset. They are exceptionally skilled, knowledgeable, team-oriented, and mission-driven individuals who want to do work that matters and benefits the public. Noblis has won numerous workplace awards . Noblis maintains a drug-free workplace.\n\n Client Engagement \n\n Lead proposals by utilizing capabilities across the company Lead strategic and business development initiatives, including account strategy development, capture efforts, and branding Identify opportunities that are aligned with Noblis’ strategic priorities, and cultivate relationships with clients expanding the footprint within an account or porting the work to another account/client\n\n Salary Range Explanation \n\n At Noblis we recognize and reward your contributions, provide you with growth opportunities, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, and work-life programs. Our award programs acknowledge employees for exceptional performance and superior demonstration of our service standards. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in our benefit programs. Other offerings may be provided for employees not within this category. We encourage you to learn more about our total benefits by visiting the Benefits page on our Careers site. \n\n Salary at Noblis is determined by various factors, including but not limited to, the combination of education, certifications, knowledge, skills, competencies, and experience, internal and external equity, location, and clearance level, as well as contract-specific affordability and organizational requirements and applicable employment laws. The projected compensation range for this position  is provided within the posting and  are based on full time status. Part time staff receive a prorated salary based on regularly scheduled hours. The estimated minimum and maximum displayed represents the broadest range for this position (inclusive of high geographic and high clearance requirements), and is just one component of Noblis’ total compensation package for employees. \n\n Posted Salary Range \n\nUSD $145,100.00 - USD $253,900.00 /Yr.\n\n \n\nNoblis is \n\nNoblis is committed to the full inclusion of all qualified individuals. As part of this commitment, Noblis will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact  employee-relations@noblis.org ."}
{"text": "experience to a 3M career.\nThe Impact You’ll Make in this Role3M is looking for a skilled Unstructured Data Engineering Lead to join our team. As a key member of our organization, you will be responsible for leading the development of pipelines, preprocessing unstructured data, eliminating duplicate data and text noise, chunking data, and generating vector embeddings. In addition to these key capabilities, the candidate should possess strong Python programming skills, expertise in cloud engineering, and experience with open source software to drive innovation and efficiency in handling unstructured data. The ideal candidate will have a strong background in data engineering, particularly in handling unstructured data, and possess the capabilities to drive innovation and efficiency in data preprocessing tasks.\nAs an Unstructured Data Engineering Lead, you will have the opportunity to tap into your curiosity and collaborate with some of the most innovative and diverse people around the world. Here, you will make an impact by:\nLeading the development of pipelines for preprocessing unstructured data, eliminating duplicate data and text noise, chunking data, and generating vector embeddings.Implementing efficient and scalable solutions using Python programming skills and cloud engineering expertise to handle unstructured data effectively.Determining the best approaches and techniques for data preprocessing tasks, driving innovation and efficiency in handling unstructured data.Supporting the team by providing guidance, mentorship, and technical expertise in data engineering, particularly in the context of unstructured data.\nBy taking on this role, you will play a crucial part in driving the success of our organization's unstructured data initiatives and contribute to the advancement of data engineering practices.\nKey requirements and objectives.Optimize data preprocessing and embedding generation pipelines for scalability and performance.Leverage strong Python programming skills to develop efficient and reliable data engineering solutions.Utilize cloud engineering expertise to design and implement scalable and cost-effective data processing architectures.Explore and leverage open source software and tools to drive innovation and efficiency in handling unstructured data.Stay up-to-date with the latest advancements in data engineering and unstructured data processing techniques.Mentor and guide junior engineers, fostering a collaborative and innovative team environment.\nYour Skills and Expertise To set you up for success in this role from day one, 3M requires (at a minimum) the following qualifications:\nBachelor's degree or higher (completed and verified prior to start) in Computer Science or EngineeringThree (3) years of experience in unstructured data engineering at a large manufacturing company in a private, public, government or military environment Three (3) years of experience as a data engineer, with expertise in handling unstructured data.\nAdditional qualifications that could help you succeed even further in this role include:\nMaster’s degree in Computer Science, Engineering, or related field from an accredited institutionStrong understanding of data engineering concepts and best practices.Proficiency in Python programming, with the ability to develop efficient and reliable data engineering solutions.Expertise in cloud engineering, with experience in designing and implementing scalable and cost-effective data processing architectures.Familiarity with open source software and tools for data engineering and unstructured data processing.Experience with data preprocessing techniques, including duplicate elimination, noise removal, and chunking.Knowledge of algorithms and methods for generating vector embeddings from unstructured data.Knowledge of distributed computing frameworks, such as Apache Spark or Hadoop.Strong analytical and problem-solving skills, with the ability to optimize data processing pipelines.Excellent communication and collaboration abilities, with the capacity to work effectively in cross-functional teams.Ability to adapt to a fast-paced and dynamic environment\nWork location:Hybrid Eligible (Job Duties allow for some remote work but require travel to Maplewood, MN at least 2 days per week)\n#LI-hybrid\nTravel: May include up to 10% InternationalRelocation Assistance: May be authorized\nMust be legally authorized to work in country of employment without sponsorship for employment visa status (e.g., H1B status).\nSupporting Your Well-being 3M offers many programs to help you live your best life – both physically and financially. To ensure competitive pay and benefits, 3M regularly benchmarks with other companies that are comparable in size and scope. \nChat with MaxFor assistance with searching through our current job openings or for more information about all things 3M, visit Max, our virtual recruiting assistant on 3M.com/careers.\nApplicable to US Applicants Only:The expected compensation range for this position is $177,961 - $217,508, which includes base pay plus variable incentive pay, if eligible. This range represents a good faith estimate for this position. The specific compensation offered to a candidate may vary based on factors including, but not limited to, the candidate’s relevant knowledge, training, skills, work location, and/or experience. In addition, this position may be eligible for a range of benefits (e.g., Medical, Dental & Vision, Health Savings Accounts, Health Care & Dependent Care Flexible Spending Accounts, Disability Benefits, Life Insurance, Voluntary Benefits, Paid Absences and Retirement Benefits, etc.). Additional information is available at: https://www.3m.com/3M/en_US/careers-us/working-at-3m/benefits/.\nLearn more about 3M’s creative solutions to the world’s problems at www.3M.com or on Twitter @3M.\nResponsibilities of this position include that corporate policies, procedures and security standards are complied with while performing assigned duties.\nOur approach to flexibility is called Work Your Way, which puts employees first and drives well-being in ways that enable 3M’s business and performance goals. You have flexibility in where and when work gets done. It all depends on where and when you can do your best work.\nPay & Benefits Overview: https://www.3m.com/3M/en_US/careers-us/working-at-3m/benefits/\n3M is \nPlease note: your application may not be considered if you do not provide your education and work history, either by: 1) uploading a resume, or 2) entering the information into the application fields directly.\n3M Global Terms of Use and Privacy Statement\nCarefully read these Terms of Use before using this website. Your access to and use of this website and application for a job at 3M are conditioned on your acceptance and compliance with these terms.\nPlease access the linked document by clicking here, select the country where you are applying for employment, and review. Before submitting your application you will be asked to confirm your agreement with the terms."}
{"text": "skills through exceptional training as well as frequent coaching and mentoring from colleaguesEstablish best practices and statistical rigor around data-driven decision-making\n\nWhat we're looking for:\n\nYour academic background is in a quantitative field such as Computer Science, Statistics, Engineering, Economics or Physics. Advanced degree preferred.You have 4+ years of experience working in an analytical role.You have proven experience with at least one programming language (Python preferred) and are comfortable developing code in a team environment (e.g. git, notebooks, testing).You have a working knowledge of relational databases, (e.g. SQL)You think about data in terms of statistical distributions and have a big enough analytics toolbox to know how to find patterns in data and identify targets for performanceYou have a high tolerance for ambiguity. You find a way through. You anticipate. You connect and synthesize.You are delivery-oriented, able to lead and execute modeling efforts from start to finishYou have excellent verbal and written communication skills and experience in influencing decisions with information\n\nLife at TRM Labs\n\nOur Principles\n\nTRM's culture is shaped by how we make decisions, how we execute, and how we treat each other\n\nImpact-Oriented Trailblazer: We put customers first, driving for speed, focus, and adaptability.Master Craftsperson: We prioritize speed, high standards, and distributed ownership.Inspiring Colleague: We value humility, candor, and a one-team mindset.\n\nBuild a Career\n\nJoining TRM means being part of a mission-driven team comprised of industry leaders.\n\nAt TRM, you'll experience:\n\nPurpose: Have a real-world impact, from disrupting terrorist networks to returning stolen funds.Growth: Professional advancement opportunities through clearly defined career pathways and biannual performance reviews.Ownership: Take pride in your work. Have a large breadth and scope of contributions and impact.\n\nWork Environment\n\nRemote First: Our headquarters is online but we highly value in-person interactions, organizing regular meetups and offsites for team building.Async Communication: Clear communication is key in our decentralized setup. We use tools such as Slack, Notion, and Loom, to promote transparency and efficiency.High Ownership: Small teams drive big goals at TRM with ownership, responsibility, and a direct impact. There's no strict clocking in or out. Team members are trusted to balance personal schedules with team needs.TRM Speed: We move “surprisingly fast” while maintaining a high bar in service of our customers and mission. This can feel both intense and rewarding. Our unique approach emphasizes quick wins, rapid iterations, and constant feedback.Diversity and Inclusion: Diversity at TRM encompasses backgrounds, experiences, and perspectives. Every day is an opportunity to learn from a colleague, whether they're a law enforcement expert or a tech pioneer.\n\nBenefits And Perks\n\nAt TRM, we know that supporting our team members can take many forms. Our goal is to enable you to care for yourself, your family, and your community with a diverse and curated benefits package for full-time employees.\n\nRemote-first work environment Regular team offsites and retreatsCompetitive salaries and stock optionsPremium health, dental, and vision insuranceFSA, HSA, and 401K programsLife & disability coverageGenerous paid time for vacation, holidays, and parental leaveFertility, professional coaching, and mental health resources\n\nJoin Us\n\nWe are looking for team members who love building from the ground up and want to work hard in a fast-paced and ambitious environment. We are remote-first, with exceptionally talented team members located around the world. If you like solving tough problems quickly and seeing your work improve the lives of billions of people, we want you at TRM.\n\nWe encourage you to reach out even if your experience doesn't precisely match the job description. Don't worry about picking exactly the right job; we can always explore other options after starting the conversation. Your passion and interests will set you apart, especially if your background or career is unconventional."}
{"text": "experience. You are comfortable with a range of statistical and ML techniques with the ability to apply them to deliver measurable business impact at Turo.\n\nYou’re someone who constantly thinks about how data can support Turo’s work across domains, actively utilizing it to work-through challenges and unlock new opportunities. You’re proficient in translating unstructured problems into tangible mathematical frameworks, and are able to bring others with you on that journey. You’re someone who enjoys working with business stakeholders to drive experimentation and foster a data-centric culture. You’re able to recognize the right tools for each problem and design solutions that scale the impact of your work. You have a passion for contributing to a best in class product and take ownership of your work from inception to implementation and beyond.\n\nWhat You Will Do\n\nTuro’s marketplace has enjoyed continued growth as a business, which has in part been achieved through significant Marketing investments to ensure we’re continuing to bring in new hosts and guests into the Turo ecosystem. As we expand into new international markets and continue to pursue growth in existing markets, building a best-in-class Marketing organization has continued to rise as a key priority and has manifested in a significant up-scaling of the organization and supporting teams.\n\nAs the second Data Scientist hire in support of a Global Marketing Organization with no shortage of high impact projects to work on, you’ll work alongside our existing Staff Data Scientist to identify focus areas that can support your own career growth and significantly contribute to Turo’s company goals and the financial livelihoods of over 150,000 people who host with Turo across the globe. This role will focus on supporting testing and measurement (market-based testing, synthetic controls, audience testing) agile/lightweight solutions for new markets, improving/expanding Customer LTV Models models, pioneering new tactics through audience segmentation and content personalization, providing thought partnership to analytics, and\n\nPrior Marketing-specific Data Science experience is preferred but not a requirement for this role so long as you’ve had experience working directly with business teams.\n\nYour Primary Responsibilities Will Include The Following\n\nTesting/Measurement\n\nWork with Marketing partners and external Advertising partners to design, implement, and execute structured experiments to evaluate marketing impact.Utilize a variety of frameworks (e.g. Frequentist Hypothesis Testing, Counterfactual Analysis, CATE) to support in-house evaluation of tests as well as development of internal toolsPartner with Analytics to improve measurement frameworks (e.g. identity resolution, multi-touch attribution, incrementality assumptions)\n\nLTV Modeling\n\nImprove our existing customer LTV framework to inform global acquisition efforts, audience segmentation, and estimating the long-term impacts of marketing actions (e.g. re-engagement campaigns)Support extension of our customer LTV solution to other markets to help scale operations\n\nCRM MarketingUtilize ML techniques (e.g. clustering, propensity scoring, multi-target optimization) to support our current tactic targeting strategies and ensure our Guests receive relevant communications\n\nBuild and deploy personalization solutions both built from existing and net new\n\nSupport campaign operationalization as needed (e.g. identifying audiences, performing randomized assignments)Analytics/International Support\n\nProvide thought partnership to analytics team on problem approachesSupport on infrastructure validation and data investigationsContribute to the growth of a global Data Service in support of the recently established Global Marketing Operations team \n\nYour profile\n\n2+ years of Data Science experience.Experience working in cross functional teams with PMs, Marketing teams, Analytics and Engineers.Deep understanding of mathematical methods in statistics, optimization, test measurement, and ML. Demonstrable experience applying these methods to real world prediction problems (classification, regression, time series forecasting, clustering).Value Autonomy; Ability to think critically, understand and communicate complex problems with a focus on business impact, and identify the most efficient/effective path to successExperience and appetite to work directly with business stakeholders on a frequent basis as a key partnerA demonstrated ability to manage and deliver on multiple projects with high attention to detail.Fluent in a production-ready programming language such as Python. Ability to deploy and scale models into a production environment collaborating with engineers.Fluent in SQL, ability to efficiently query large datasets.\n\nThe SF base salary target range for this full-time position is $131,000-$164,000 + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your work location during the hiring process. Please note that the salary range listed in this posting reflects the base salary only, and does not include bonus (if applicable), equity, or benefits.\n\nBenefits\n\nCompetitive salary, equity, benefits, and perks for all full-time employeesEmployer-paid medical, dental, and vision insurance (Country specific)Retirement employer match$2,000 Learning & Development stipend to invest in your professional development$1,000 USD Turo host matching and $1,500 USD vehicle reimbursement program$100 USD Monthly Turo travel creditCell phone, internet and Fringe benefit stipendPaid time off to relax and rechargePaid holidays, volunteer time off, and parental leaveFor those who are in the office full-time or hybrid we have weekly in-office lunch, office snacks, and fun activitiesAnnual Turbo Week (week-long, company-wide conference)\n\nWe are committed to building a diverse team. If you are from a background that's underrepresented in tech, we'd love to meet you.\n\nAside from an award winning work environment and the opportunity to be part of the world’s largest car sharing marketplace, we are also growing the team quickly - join us! Even if you don't meet every qualification, we are looking for people with enthusiasm for what we do and we will consider you for this and other possibilities.\n\nTuro is \n\nTuro will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance, as applicable.\n\nWe welcome candidates with physical, mental, and/or neurological disabilities. If you require assistance applying for an open position, or need accommodation during the recruiting process due to a disability, please submit a request to People Operations by emailing PeopleOps@turo.com.\n\nAbout Turo\n\nTuro is the world’s largest car sharing marketplace where you can book the perfect car for wherever you’re going from a vibrant community of trusted hosts across the US, UK, Canada, Australia, and France. Whether you're flying in from afar or looking for a car down the street, searching for a rugged truck or something smooth and swanky, Turo puts you in the driver's seat of an extraordinary selection of cars shared by local hosts.\n\nDiscover Turo at https://turo.com, the App Store, and Google Play, and check out our blog, Field Notes.\n\nRead more about the Turo culture according to Turo CEO, Andre Haddad."}
{"text": "experience. Insurance industry claims system experience preferred.Claims Systems Modernization project.\nRequirements and Skill sets needed:10+ years of software development experience10 + years in MS SQL Server and Cloud based data warehouses. 3+ Experience in AWS services including AWS Step function, Lambda (using node.js), SNS, SQS, Eventbridge, API Gateway etc3+ years of Python/Spark using AWS Glue.Advanced Python skills. Experience in RESTful API servicesExperience with Terraform, experience with setting up CI/CD pipelines is a plusExperience with NoSQL Databases such as MongoDB/DynamoDBExperience with containers such as Open Shift/ Kubernetes, DockerExperience with KafkaExperience in SDLC, including testing and deploymentWorking knowledge of scripting languagesBe passionate about resolving user pain points through great designBe open to receiving feedback and constructive criticismExperience in the Agile methodology"}
{"text": "Requirements:Masters degree in Data Analytics, Data Science, Computer Science or related technical subject areaDemonstrated experience developing hockey or sports betting models at production scale Expertise in Probability Theory, Machine Learning, Inferential Statistics, Bayesian Statistics, Markov Chain Monte Carlo methods4+ years of demonstrated experience developing and delivering effective machine learning and/or statistical models to serve business needsExperience with relational SQL & PythonExperience with source control tools such as GitHub and related CI/CD processesExperience working in AWS environments etcProven track record of strong leadership skills. Has shown ability to partner with teams in solving complex problems by taking a broad perspective to identify innovative solutionsExcellent communication skills to both technical and non-technical audiences\nBase salary: $107,000-175,000\nSwish Analytics is"}
{"text": "experienced Databricks professional to join our client, a growing Data Consultancy, as a Lead Data Engineer.\nAs a Lead Data Engineer, you will play a pivotal role in driving the success of our client's Databricks practice through your expertise in presales, strong technical acumen, consulting skills, and effective team management.\nKey Qualifications:Proven experience in presales activities, with a focus on Databricks solutions.Strong technical background in Databricks and related technologies.Extensive experience in consulting, advising clients on data and analytics solutions.Demonstrated success in team management, fostering a positive and collaborative work environment.Excellent communication and interpersonal skills, with the ability to build strong client relationships.Education and Certifications:Bachelor's or higher degree in a relevant field.Relevant certifications in Databricks and related technologies are highly desirable.If you are a motivated and strategic leader with a passion for driving innovation in the realm of Databricks, we invite you to apply and be a key contributor to our client's dynamic and growing team."}
{"text": "skills, modeling, energy data analysis, and critical thinking are required for a successful candidate. Knowledge of energy systems and distributed solar is required.\n\nReporting to the Senior Manager of Government Affairs, you will work across different teams to model data to inform policy advocacy. The ability to obtain data from multiple sources, including regulatory or legislative hearings, academic articles, and reports, are fundamental to the role.\n\nA willingness to perform under deadlines and collaborate within an organization is required. Honesty, accountability, and integrity are a must.\n\nEnergy Policy & Data Analyst Responsibilities\n\nSupport Government Affairs team members with energy policy recommendations based on data modelingEvaluate relevant regulatory or legislative filings and model the impacts to Sunnova’s customers and businessAnalyze program proposals (grid services, incentives, net energy metering, fixed charges) and develop recommendations that align with Sunnova’s objectivesCollaborate with interdisciplinary teams to model impacts to our customers by using a variety of software and data management tools Python, R, SQL, and ExcelDevelop clear and concise descriptions of data, methodology and results for inclusion in public filings, testimony and reportsPerform modeling to evaluate the impacts and effectiveness of policies and regulationsQuantify the benefits to ratepayers from solar and storage programsWork with customer management software such as Tableau\n\n\nMinimum Requirements\n\nBachelor’s Degree in data science, economics or applicable area of focus; advanced degree preferred3-5 years of applicable experience including conducting data science projects in renewable energy, grid services, or distributed energy resource managementExceptional quantitative, modeling, analytical and communication skillsAbility to work independently and as part of a teamAnalytical thinker capable of defining value for customersSkill in managing multiple activities, delivering on commitments, and operating with speed and accuracy\n\n\nPreferred Qualifications\n\nStrong understanding of the solar energy market and regulatory environmentKnowledge and experience with energy systems analysisPrior work with Public Utility Commissions and energy market regulatorsThrives in a challenging and fast-paced workplaceExperience producing documents for regulators and legislators\n\n\nAdditional Knowledge, Skills And Abilities\n\nSelf-starter, diligent and detail orientedHonesty, integrity, and accountability\n\n\nWorking Conditions\n\nRemote work from home office environment15% Travel\n\n\nBenefits\n\nSunnova offers a generous employee reward package that includes:\n\nComprehensive benefits, including medical, dental, vision, life insurance, healthcare flexible spending account, and 401(k) with employer match.Competitive compensation & annual bonusPaid time off, including 10 holidays and paid parental LeaveCell phone allowance for many rolesFree access to onsite fitness center in Houston and/or discounted fitness memberships through health providerComplimentary garage parking in Houston\n\n\n$95,000 - $105,000 a year\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.\n\nWe appreciate the capabilities of artificial intelligence (AI), and we are looking to hire real people. If you use AI for any part of the job application process, we highly encourage that you review any AI generated content to ensure your personality and unique capabilities are highlighted. We reserve the right to disqualify candidates that we reasonably believe solely relied on AI generated content in the application process.\n\nIf you are selected for a position, your employment will be contingent upon submission to and successful completion of a post-offer/pre-placement drug test (and medical examination if required by the role) as well as pre-placement verification of the information and qualifications provided during the selection process."}
{"text": "experiences that leverage the latest technologies in open source and the Cloud. Digital Information Management (DIM) is a team of engineers committed to championing a data-driven decision-making culture and meets the business demand for timely insight-focused analytics and information delivery.\n\nYou will be working with all levels of technology from backend data processing technologies (Databricks/Apache Spark) to other Cloud computing technologies / Azure Data Platform. You should be a strong analytical thinker, detail-oriented and love working with data with a strong background in data engineering and application development. Must be a hand-on technologist passionate about learning new technologies and help improve the ways we can better leverage Advanced Analytics and Machine Learning.\n\nResponsibilities\n\nBuild end-to-end direct capabilities.Create and maintain optimal data pipeline architecture.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.Use analytics for capitalizing on the data for making decisions and achieving better outcomes for the business.Derive insights to differentiate member and team member experiences. Collaborate with cross-functional teams.Analyze and define with product teams the data migration and data integration strategies.Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems.Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights. Integrate source systems with information management solutions and target systems for automated migration processes.Create proof-of-concepts to demonstrate viability of solutions under consideration.\n\n\nQualifications\n\nBachelor’s degree in computer science, information systems, or other technology-related field or equivalent number of years of experience.Advanced hands-on experience implementing and supporting large scale data processing pipelines and migrations using technologies (eg. Azure Services, Python programming).Significant hands-on experience with Azure services such as Azure Data Factory (ADF), Azure Databricks, Azure Data Lake Storage (ADLS Gen2), Azure SQL, and other data sources. Significant hands-on experience designing and implementing reusable frameworks using Apache Spark (PySpark preferred or Java/Scala).Solid foundation in data structures, algorithms, design patterns and strong analytical and problem-solving skills.Strong hands-on experience leading design thinking as well as the ability to translate ideas to clearly articulate technical solutions. Experience with any of the following Analytics and Information Management competencies: Data Management and Architecture, Performance Management, Information Delivery and Advanced Analytics.\n\n\nDesired Qualifications\n\nProficiency in collaborative coding practices, such as pair programming, and ability to thrive in a team-oriented environment.The following certifications:Microsoft Certified Azure Data EngineerMicrosoft Certified Azure Solutions ArchitectDatabricks Certified Associate Developer for Apache 2.4/3.0\nHours: Monday - Friday, 8:00AM - 4:30PM\n\nLocation: 820 Follin Lane, Vienna, VA 22180 | 5510 Heritage Oaks Drive Pensacola, FL 32526 | 141 Security Drive Winchester, VA 22602\n\nAbout Us\n\nYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:\n\n Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services\n\n\n\n\nDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market position\n\nBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."}
{"text": "experience in Spark and Scala, SQL and AWS Cloud Desired Skills: Strong in spark with good knowledge of HadoopJob Description• Create Scala/Spark jobs for data transformation and aggregation.• Produce unit tests for Spark transformations and helper methods• Write Scala doc-style documentation with all code• Design data processing pipelines \nRegards,Guru Prasath M US IT RecruiterPSRTEK Inc.Princeton, NJ 08540guru@psrtek.com"}
{"text": "Skills:· Proven experience as a Data Scientist / Cloud Engineer or similar role, with a focus on AI/ML solutions.· Strong proficiency in provisioning and automating cloud infrastructure/platforms especially AI resources on the Azure platform.· Experience with IAAS, PAAS, and SAAS enablement on the Azure platform.· Experience with Gen-AI architecture principles, including RAG, LLM's, and data pipelines.· Solid understanding of data safety and security standards implementation.· Familiarity with other cloud providers' AI/ML offerings is an added advantage.· Excellent communication skills and ability to collaborate effectively in a team environment.· Analytical mindset with a strong attention to detail.· Fast learner with a passion for staying updated on emerging technologies and industry trends.· Good to have some python skills to provide samples to the teams on how to use the API’s / security / services etc.."}
{"text": "experience in data engineering, software engineering, data analytics, or machine learning.Strong expertise working with one or more cloud data platforms (Snowflake, Sagemaker, Databricks, etc.)Experience managing Snowflake infrastructure with terraform.Experience building batch, near real-time, and real-time data integrations with multiple sources including event streams, APIs, relational databases, noSQL databases, graph databases, document stores, and cloud object stores.Strong ability to debug, write, and optimize SQL queries in dbt. Experience with dbt is a must.Strong programming experience in one or more modern programming languages (Python, Clojure, Scala, Java, etc.)Experience working with both structured and semi-structured data.Experience with the full software development lifecycle including requirements gathering, design, implementation, testing, deployment, and iteration.Strong understanding of CI/CD principles.Strong ability to document, diagram, and deliver detailed presentations on solutions.\nPreferred Experience:Expertise managing and integrating with cloud data streaming platforms (Kinesis Data Streams, Kafka, AWS SNS/SQS, Azure Event Hubs, StreamSets, NiFi, Databricks, etc.)Expertise in working with cloud data integration platforms (Airflow / AWS MWAA, Snowflake Snowpipe, Kinesis Data Firehose, AWS Glue / Glue schema registry, Azure Data Factory, AWS DMS, Fivetran, Databricks, Dell Boomi, etc.)Experience building data infrastructure in a cloud environment using one or more infrastructure as code tools (Terraform, AWS CloudFormation, Ansible, etc.)Production experience with one or more cloud machine learning platforms (AWS Sagemaker, Databricks ML, Dataiku, etc.)Understanding of machine learning libraries (MLlib, Scikit-learn, Numpy, Pandas, etc.)Experience managing data governance and security enablement (role-based access, authentication, network isolation, data quality, data transparency, etc.) on a cloud data warehouse, especially Snowflake.Experience building and optimizing data models with tools like dbt and Spark.Experience integrating with data visualization tools (Sisense, Tableau, PowerBI, Looker, etc.)Our data engineering and analytics stack includes Snowflake, dbt, Fivetran, Airflow, AWS, Sagemaker, and Python programming for custom data engineering. We use Sisense and Sigma for BI capability. Experience with this or similar tool would be preferred. Data team owns the provisioning and administration of all the tools we work with.\nBENEFITS:Comprehensive and affordable insurance benefitsUnlimited paid time off policy401(k) enrollment9 paid company holidaysPaid parental leave\nEmployment at Splash is based on individual merit. Opportunities are open to all, without regard to race, color, religion, sex, creed, age, handicap, national origin, ancestry, military status, veteran status, medical condition, marital status, sexual orientation, affectional preference, or other irrelevant factors. Splash is"}
{"text": "Experiences (DX) is a world-class entertainment and technology leader. Walt’s passion was to innovate continuously and push the boundaries of what is possible, which remains central in our business today. Uniting each endeavor is a commitment to creating and delivering unforgettable experiences, and we’re constantly looking for new ways to enhance these exciting experiences for our guests. A Sr. Data Scientist in this group will help guide the creation, installation, and support the data analysis and development of machine learning models for our attractions. We work closely with internal partners to deliver world-class guest experiences with interactive & data systems across DPEP. You will be reporting to the Attractions Operational Technology organization.\n\nThis is a full time role.\n\nWhat You Will Do\n\n\nConducting advanced data analysis, modeling, and experimentation to derive actionable insights and drive strategic decision-making.Staying ahead of the latest advancements in data science techniques, tools, and technologies and applying them to solve complex problems optimally.Developing and deploying predictive models to forecast future trends, behavior, or outcomes based on historical data in the attraction space.Evaluating model performance and refining algorithms to improve accuracy, reliability, and scalability.Designing and implementing A/B tests or other experiments to validate model predictions and optimize business processes.Collaborating with business partners to translate predictive insights into actionable recommendations and strategic initiatives.Regularly supervising and updating predictive models ensure they remain relevant and effective in dynamic business environments.\n\n\nRequired Qualifications & Skills\n\n\n5+ year experience in Python.3+ year experience with GCP or other cloud provider.3+ years of predictive modeling experience.Knowledge of Software Development Life cycle and QA processes.Experience with revision control software, such as GitHub and GitLab.Experience with Model Ops and setting up CI/CD pipelines.Experience with Docker.Experience with both structured and unstructured data.Experience with computer vision.Experience leading a complex projectAbility to explain technical solutions in simple terms\n\n\nPreferred Qualifications\n\n\nExperience in Airflow or another data workflow management toolExperience with SQL and PostgreSQL databasesExperience with KubernetesExperience with cloud deployments in GCP or another cloud platformExperience with Atlassian Tools such as Jira & ConfluenceExperience with Agile development methodsExperience with automated testing, continuous integration, and continuous developmentExperience with data visualization and data monitoring tools (e.g. Splunk, Plotly, Tableau)Knowledge of Object-Oriented Software Design Patterns\n\n\nEducation\n\n\nBachelor of Science in Data Science, Statistics, Software Engineering, Computer Engineering, Computer Science, or equivalent technical backgroundPREFERRED: Master of Science in Data Science, Statistics, Software Engineering, Computer Engineering, Computer Science, or equivalent technical background\n\n\nAdditional Information\n\nDisney offers a rewards package to help you live your best life. This includes health and savings benefits, educational opportunities, and special extras that only Disney can provide. Learn more about our benefits and perks at https://jobs.disneycareers.com/benefits.\n\n#DXFOS #DXMedia"}
{"text": "requirements for systems in production, to ensure maximum usability\nQualifications[Some qualifications you may want to include are Skills, Education, Experience, or Certifications.]Example: Excellent verbal and written communication skills"}
{"text": "experience with Cloud Engineering / Services.3+ years of work experience as a backend software engineer in Python with exceptional software engineering knowledge. Experience with ML workflow orchestration tools: Airflow, Kubeflow etc. Advanced working knowledge of object-oriented/object function programming languages: Python, C/C++, JuliaExperience in DevOps: Jenkins/Tekton etc. Experience with cloud services, preferably GCP Services like Vertex AI, Cloud Function, BigQuery etc. Experience in container management solution: Kubernetes, Docker.Experience in scripting language: Bash, PowerShell etc. Experience with Infrastructure as code: Terraform etc.\nSkills Preferred:Master focused on Computer Science / Machine Learning or related field. Experience working with Google Cloud platform (GCP) - specifically Google Kubernetes engine, Terraform, and infrastructure.Experience in delivering cloud engineering products.Experience in programming concepts such as Paired Programming, Test Driven Development, etc. Understanding of MLOPs/Machine Learning Life Cycle and common machine learning frameworks: sklearn, TensorFlow, pytorch etc. is a big plus.Must be a quick learner and open to learning new technology. Experience applying agile practices to solution delivery. Experience in all phases of the development lifecycle. Must be team-oriented and have excellent oral and written communication skills. Good organizational and time-management skills. Must be a self-starter to understand existing bottlenecks and come up with innovative solutions. Knowledge of coding and software craftsmanship practices.Experience and good understanding of GCP processing /DevOPs/ Machine Learning"}
{"text": "experience and drive enterprise performance. Riverbed is the only company with the collective richness of telemetry from network to app to end user, that illuminates and then accelerates every digital interaction. Riverbed offers two industry-leading portfolios: Riverbed Observability, a full-stack Observability portfolio that unifies data, insights, and actions across IT, so customers can deliver seamless, secure digital experiences; and Riverbed Acceleration, providing fast, agile, secure acceleration of any app, over any network, to users anywhere.\n\nTogether with our thousands of partners, and market-leading customers globally – including 95% of the FORTUNE 100 –, we empower every click, every digital experience.\n\n Position \n\n Job Title:  Senior AI Engineer\n\n Location Preference:  Greater Boston MA; Greater Raleigh/Durham, NC; Greater Dallas, TX\n\nDo you want to be at the forefront of Big Data and AI innovation? Do you thrive on tackling complex problems with intelligent solutions at 100-Petabyte Enterprise scale? Do you have a passion for delivering those solutions to production services? If so, then this AI Engineer role is for you!\n\nWe are looking for a talented and passionate AI Engineer to join our team and play a key role in developing and deploying cutting-edge AI solutions. You will be responsible for the AI lifecycle, from working with the Data Science team designing and building models to implementing production services including testing, deployment, and monitoring of AI solutions in a production SaaS environment.\n\nWhat You Will Do\n\n AI service Design and Develop:  Design, develop, and implement AI services, algorithms, and machine learning models.  Train, test, and evaluate those services and models to ensure accuracy, efficiency, and scalability.  Deployment and Integration:  Develop and maintain service(s) and infrastructure that provide interfaces for integrating with AI solutions.  Monitor and maintain AI solutions in production, identifying and resolving any issues.  Data Pipeline Management:  Work as part of the engineering team to create robust data pipelines that feed curated data into AI models. Ensure data quality, reliability, and security.  Improvement & Growth:  Stay up-to-date on the latest advancements in AI and machine learning research.  Collaboration:  Collaborate with cross-functional teams (data science, engineers, product managers, solutions engineers) to ensure successful integration of AI solutions.  Communicate complex technical concepts clearly and concisely to both technical and non-technical audiences. \n\nWhat Makes You An Ideal Candidate\n\n Bachelors or Master's degree in Computer Science, Artificial Intelligence, Engineering, or a related field (or equivalent experience).  Strong understanding of machine learning algorithms (deep learning, reinforcement learning, etc.).  Strong understanding of Natural Language Processing (NLP) and use of Generative AI  Strong programming skills in a microservices structure. (C# .NET preferred)  Familiarity with SaaS architecture, microservices and RESTful APIs.  Experience working in Cloud platforms (eg. Azure, AWS, GCP).  Proven experience in developing and deploying AI models (ideally with experience in Python).  Experience working with big data frameworks (Spark, DataBricks, etc.) is a plus.  Excellent analytical and problem-solving skills.  Strong communication and collaboration skills.  Ability to work independently and as part of a team. \n\nWhat We Offer\n\nOur employee benefits including flexible workplace policies, employee resource groups, learning and development resources, career progression pathways, and community engagement initiatives are some of the reasons why we have had great success in bringing in new talent. In addition, our global employee wellness programs are crafted to support the physical, emotional, and financial well-being of our employees.\n\nBenefits & Perks vary by Country.\n\nAbout Riverbed\n\nWith a 20-year history of innovation, Riverbed is agile, yet proven, and we are disrupting the market with differentiated solutions that enable our customers to deliver secure, seamless digital experiences and accelerate enterprise performance While we are a ‘customer-first’ company, we are all about our people with a shared commitment to creating a global impact. We bring our best selves to work and pursue excellence, trust, and respect for one another. We welcome diversity and value and encourage transparency and open communication throughout the company. We strive to be an inclusive, fair, and enjoyable workplace for our people globally and care about their wellbeing. We are committed to our people, partners, and customers while supporting the communities where we work and live. It’s the Power of WE that binds us together.\n\nWe want people of all backgrounds to see themselves represented and included in our work, so we actively seek to diversify our team and bring more voices to the table. We understand that when people can be themselves, more is possible. We would love to have more people on board to join us on our journey to be better every day! So, come speak with us about career possibilities at Riverbed.\n\nRiverbed is an \n\nRiverbed encourages all of its U.S. employees to be fully vaccinated for COVID-19, but does not presently require its U.S. employees to be fully vaccinated unless such vaccination is required by applicable law or the duties of the specific position. If a specific position requires an employee to be fully vaccinated because of either applicable law or the duties of the position, then the offer of employment will be expressly conditioned on the individual being fully vaccinated for COVID-19 on or prior to their start date and providing proof of such vaccination unless the individual is legally exempt.\n\nCheck us out on:\n\nwww.riverbed.com\n\n@LifeAtRiverbed\n\n Tags \n\n#-LI-Remote"}
{"text": "requirements. Can work with large scale computing frameworks, data analysis systems and modeling environments. 5-7 years of experience working in AI Accelerators and doing performance analysis, experience working on Multimedia applications and Image/Video generation models."}
{"text": "requirements, developing reporting, and enabling efficiencies. You will also encourage analytics independence as a subject matter expert and champion of business intelligence software (e.g. Power BI, Tableau, etc.). The group also leads the Accounting Department’s Robotic Process Automation efforts.\n\nKiewit is known as an organization that encourages high performers to challenge themselves by operating in roles they may not be classically trained for. This position embodies this spirit as the experiences will lend themselves nicely into several potential paths including accounting roles / leadership, operations management, data analysis roles and technology group positions.\n\nDistrict Overview\n\nAt Kiewit, the scale of our operations is huge. Our construction and engineering projects span across the United States, Canada and Mexico, improving and connecting communities with every initiative. We depend on our high-performing operations support professionals — they’re the glue that holds multiple areas of the business together and the catalysts that make things happen. We hire only the most driven people and we offer them diverse career opportunities, strong leadership and mentorship, rewarding, collaborative work, and responsibility they won’t find just anywhere. We’re always pushing new limits. You can too.\n\nLocation\n\nThis position will be located in our Omaha Nebraska headquarters. We do not offer relocation for this position.\n\nResponsibilities\n\nUnderstand Management’s accounting needs and collaborate with other Accountants to design world class reporting as well as automated solutions to eliminate manual activities. Build tools independently and with the assistance of Kiewit’s technology and analytics resources including data visualizations in Power BI, Tableau. Design and maintain BI solutions that provide accurate data, enabling users to make well informed decisions. Provide internal consulting services to the accounting department to improve their self-service analytics capabilities. Prepare month, quarter and year-end financial reporting packages for executive leadership. Data Acquisition – Extract data from enterprise sources, transform, clean, and prepare for use by accounting, using SQL, and Teradata. Analyze Data – Conduct data analysis using MS Power BI and Tableau.Present Data – Explain results of analysis to team members, train business how to use decision support tools, develop training materials. Use data visualization to aid communication.Provice internal consulting services to various departments to improve their self-service capabilities. Respond to ad hoc requests made by Management and assist in special projects as needed.\n\n\nQualifications\n\nThis position suits a self-starter with initiative, curiosity, and a desire to learn. You must be proactive in seeking developmental opportunities and additional responsibilities as they present themselves. A successful candidate will be highly motivated and can build professional relationships quickly and naturally. These relationships will bring this candidate the credibility needed to operate as a successful business partner within Kiewit and our partners.\n\nBachelor’s degree in Accounting, Finance, Business Intelligence & Analytics, MIS, or equivalentMust have two or more years of accounting, finance or business analytics experienceGeneral knowledge of U.S. GAAP accounting principles and practicesProficiency with Microsoft Office Suite. Advanced Excel abilities.Excellent customer service and collaboration skills; must be comfortable with proactively engaging department members.Strong communicator with the ability to translate technical terms and concepts into visualizations and business terms.Can work on multiple projects simultaneously and translate business data into digestible information that improves decision making. Passion for learning new data analysis methods and tools. Must have strong problem solving skills, and creativity to develop automated solutions for financial/accounting teams.Experience with MS Power BI or Tableau reporting tools preferred.Experience writing SQL queries (SQL Server, Teradata) a plus.\n\n\nOther Requirements:\n\nRegular, reliable attendance Work productively and meet deadlines timelyCommunicate and interact effectively and professionally with supervisors, employees, and others individually or in a team environment.Perform work safely and effectively. Understand and follow oral and written instructions, including warning signs, equipment use, and other policies.Work during normal operating hours to organize and complete work within given deadlines. Work overtime and weekends as required.May work at various different locations and conditions may vary.\n\n\nWe offer our fulltime staff employees a comprehensive benefits package that’s among the best in our industry, including top-tier medical, dental and vision plans covering eligible employees and dependents, voluntary wellness and employee assistance programs, life insurance, disability, retirement plans with matching, and generous paid time off.\n\nWe are"}
{"text": "requirements.Reporting and Dashboard Development: Design, develop, and maintain reports for the HRSA HCCN Grant and other assignments. Create and maintain complex dashboards using Microsoft Power BI.Infrastructure Oversight: Monitor and enhance the data warehouse, ensuring efficient data pipelines and timely completion of tasks.Process Improvements: Identify and implement internal process improvements, including automating manual processes and optimizing data delivery.Troubleshooting and Maintenance: Address data inconsistencies using knowledge of various database structures and workflow best practices, including NextGen EHR system.Collaboration and Mentorship: Collaborate with grant PHCs and analytic teams, mentor less senior analysts, and act as a project lead for specific deliverables.\nExperience:Highly proficient in SQL and experienced with reporting packages.Enterprise ETL experience is a major plus!data visualization tools (e.g., Tableau, Power BI, Qualtrics).Azure, Azure Data Factory, SQL management instances in AzureNextGen electronic health record software experience.Bachelor's degree (BA/BS) in mathematics, computer science, statistics, engineering, or a related field"}
{"text": "experienced analyst for its Coffee Manufacturing operation. If you are looking for a dynamic and challenging work environment with the opportunity to expand your influence across the organization, grow personally and professionally with long-term goals, this position has much to offer. This position requires a process leader who will partner with the financial management team to provide decision support, drive savings, and increase productivity.\n\nAs a Financial Data Analyst, your primary responsibilities will be driven by interpretation and reporting requests. By applying transformations and analysis to SQL/Excel-based datasets, you will create clear and concise reporting via SQL, Power BI, and Excel. Previously developed models will require consistent maintenance, modification, and summarization.\n\nThis position can be based in one of the following locations: Frisco, TX, Knoxville, TN or Sumner, WA\n\nKey experience, skills, and other factors Benefits, subject to election and eligibility: Medical, Dental, Vision, Disability, Paid Time Off (including paid parental leave, vacation, and sick time), 401k with company match, Tuition Reimbursement, and Mileage Reimbursement Annual bonus based on performance and eligibility\n\n\nRequirements\n\nBS/BA degree in Finance, Analytics, or equivalent experience.2+ years of experience with problem-solving and analytical thinking.Required advanced technical skillset in Excel.Familiarity with Power BI/Power Query.Strong understanding of table relationship management, including joins between multiple fact and dimension tables.VBA, Tableau, SQL experience a plus.Manufacturing Finance/Accounting experience a plus.Ability to work both independently and collaboratively on parallel process streams.\n\n\nCompany Overview\n\nKeurig Dr Pepper (NASDAQ: KDP) is a modern beverage company with a bold vision built to deliver growth and opportunity. We operate with a differentiated business model and world-class brand portfolio, powered by a talented and engaged team that is anchored in our values. We work with big, exciting beverage brands and the #1 single-serve coffee brewing system in North America at KDP, and we have fun doing it!\n\nTogether, we have built a leading beverage company in North America offering hot and cold beverages together at scale. Whatever your area of expertise, at KDP you can be a part of a team that’s proud of its brands, partnerships, innovation, and growth. Will you join us?\n\nWe strive to be an employer of choice, providing a culture and opportunities that empower our team of ~28,000 employees to grow and develop. We offer robust benefits to support your health and wellness as well as your personal and financial well-being. We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work.\n\nKeurig Dr Pepper is"}
{"text": "experience in deploying real-time AI/ML models using Google Cloud Platform.Strong programming skills in Python and PySpark.Proficiency with SQL and relational databases, data warehouses, and Big Query.Experience in scaling marketing-related AI/ML solutions such as cross/upsell, recommended systems, and category propensity.Experience in deploying and managing Large scale Machine Learning Models is a plusExpertise with classical ML algorithm like K-NN, LSH, logistic regression, linear regression, SVM, Random forest and clustering.Good understanding of ML & DL algorithms and frameworks (Scikit-learn,Spacy, Tensor flow/Keras/ PyTorch)Experience in deep learning Algorithms like MLP, CNN, RNN, LSTMs and GANs, Transformers and LLMs.Excellent programming skills in PythonExpertise in Google Cloud and operationalization of models using MLOPs.Experience in scheduling jobs for automated training and inference of AI/ML models using airflow or any other workflow orchestration platform.Proficiency in collecting data from different data sources, data cleaning, preprocessing, and feature engineering.Understanding of regression, classification, and unsupervised ML algorithms.Experience in mentoring junior associates in scaling AI/ML models.Excellent problem-solving and analytical skills.Strong written and verbal communication skills, with the ability to present and explain complex concepts to both technical and non-technical audiences."}
{"text": "experience preferred but not required.\nMust-Have Skills:10+ years of total IT experience required.of 4 years of proven and relevant experience in a similar Data Engineer role and/or Python Dev role.Strong proficiency in Python programming is essential for data manipulation, pipeline development, and integration tasks.In-depth knowledge of SQL for database querying, data manipulation, and performance optimization.Experience working with RESTful APIs and integrating data from external sources using API calls.Azure: Proficiency in working with Microsoft Azure cloud platform, including services like Azure Data Factory, Azure Databricks, and Azure Storage."}
{"text": "QUALIFICATIONS Required Certifications DoD IAT Level III Certification (Must obtain within 180 days of hire). Education, Background, and Years of Experience 3-5 years of Data Analyst experience. ADDITIONAL SKILLS & QUALIFICATIONS Required Skills At least 3 years of hands-on experience with query languages, such as SQL and Kusto to facilitate robust reporting capabilities. Preferred Skills Understanding of Microsoft Power Platform. Power BI authoring, in combination with designing and integrating with data sources. Tier III, Senior Level Experience with Kusto Query Language (KQL). Tier III, Senior Level Experience with Structured Query Language (SQL). WORKING CONDITIONS Environmental Conditions Contractor site with 0%-10% travel possible. Possible off-hours work to support releases and outages. General office environment. Work is generally sedentary in nature but may require standing and walking for up to 10% of the time. The working environment is generally favorable. Lighting and temperature are adequate, and there are not hazardous or unpleasant conditions caused by noise, dust, etc. Work is generally performed within an office environment, with standard office equipment available. Strength Demands Sedentary - 10 lbs. Maximum lifting, occasional lift/carry of small articles. Some occasional walking or standing may be required. Jobs are sedentary if walking and standing are required only occasionally, and all other sedentary criteria are met. Physical Requirements Stand or Sit; Walk; Repetitive Motion; Use Hands / Fingers to Handle or Feel; Stoop, Kneel, Crouch, or Crawl; See; Push or Pull; Climb (stairs, ladders) or Balance (ascend / descend, work atop, traverse). Employees of Agile Defense are our number one priority, and the importance we place on our culture here is fundamental. Our culture is alive and evolving, but it always stays true to its roots. Here, you are valued as a family member, and we believe that we can accomplish great things together. Agile Defense has been highly successful in the past few years due to our employees and the culture we create together. We believe several attributes are the root of our very best employees and extraordinary culture. We have named these attributes \"The 6 H's\" - Happy, Helpful, Honest, Humble, Hungry, and Hustle. Happy : We exhibit a positive outlook in order to create a positive environment. Helpful : We assist each other and pull together as teammates to deliver. Honest : We conduct our business with integrity. Humble : We recognize that success is not achieved alone, that there is always more to learn, and that no task is below us. Hungry : We desire to consistently improve. Hustle : We work hard and get after it. These Core Values are present in all our employees and our organization's aspects. Learn more about us and our culture by visiting us here. COVID-19 Vaccination Requirements Agile Defense is subject to federal vaccine mandates or other customer/facility vaccination requirements as a federal contractor. As such, to protect its employees' health and safety and comply with customer requirements, Agile Defense may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR 60-1.35(c)"}
{"text": "experience in ETL/DW Strong experience in setting up enterprise data platforms with modern cloud technologies·Hands on experience in DW modeling, ETL / ELT design and development, Data Lake and Lake house modeling·Experience in different ingestion patterns including batch loads (Full and Incremental), CDC, replication etc.·Exposure to real time replication tools like Qlik Replicate / Stitch / Matilion / DBMotto etc.·Exposure to ETL tools like SSIS / ADF / Synapse pipelines / Snow pipe / Snow park /PySpark / Informatica / Talend etc.·Hands on experience in setting up data governance using tools like Azure Purview, Collibra etc.·Hands on experience in Azure Data tool stack including Azure SQL, Synapse, Fabric·Exposure to big data technologies like DataBricks, Snowflake etc.· Exposure to analytics design and development especially with the tools like Power BI, SSRS etc.·Excellent communication skills·Technical team management skills"}
{"text": "requirements. · Strong SQL skills and proficiency in data visualization tools are essential for this role."}
{"text": "skills and business mindset to make a difference every day. We are looking for people who can operate at a company that grows as fast as ours by dealing with multiple moving pieces while still holding up quality, long-term thinking, and delivering value to our customers. We take great pride in our diversity of backgrounds, cultures, and perspectives and we strongly believe this is helping us to grow as a successful and impactful team.\n\nWhat You'll Do\n\nIdentify trends and opportunities for growth through analysis of complex data setsWork alongside stakeholders (from ICs/Engineers to Directors/E-Staff) to understand requests and provide solutions using data analyticsProvide data solutions end-to-end (working with raw data, developing SQL models, building dashboards and presenting insights to stakeholders)Support and strengthen our fast paced fintech product team in all functional areas and special requests as neededCreate and deliver documentation and training on reporting and BI tools for business users to enable self-service on company dataConsolidate data across business units and acquisitions with different definitions and measurement. Ongoing commitment to identify and implement process improvements for operational efficiencies and enhanced analytics as well as maintain existing documentation of control procedures performed by the financial planning and analysis teamDevelop SQL models using data warehousing principles (we use dbt).\n\nWhat We're Looking For\n\nOver 5 years of professional experience in the field of Data AnalysisProficiency with Data Visualization Tools for at least 2 yearsStrong command of SQLPrior experience in fintech is a big plusAbility to effectively interpret business requirements and translate them into analytical solutions, insights, and reportsProficient in standard statistical techniques such as significance testing and regression modelingDemonstrated eagerness to acquire new technological skills and problem-solving abilitiesFamiliarity with Snowflake and dbt is beneficialPreferred qualifications include an advanced degree in mathematics, statistics, computer science, information science, or a related technical fieldExcellent communication skills and adept at building relationships with stakeholders; must also be a collaborative team player\n\nThe posted pay range represents the anticipated low and high end of the compensation for this position and is subject to change based on business need. To determine a successful candidate’s starting pay, we carefully consider a variety of factors, including primary work location, an evaluation of the candidate’s skills and experience, market demands, and internal parity.\n\nFor roles with on-target-earnings (OTE), the pay range includes both base salary and target incentive compensation. Target incentive compensation for some roles may include a ramping draw period. Compensation is higher for those who exceed targets. Candidates may receive more information from the recruiter.\n\nPay Range\n\n$120,000—$209,000 USD"}
{"text": "Qualifications:experience as a Data Analyst or related role.expertise regarding data models, database design, data mining, and segmentation techniques.knowledge of and experience with reporting packages and databases (SQL, etc.).in programming languages (Python, R, etc.) is a plus.analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.at queries, report writing, and presenting findings.knowledge of Microsoft Office with an emphasis on Microsoft Excel.and Experience: Bachelor's degree in a relevant field (e.g., Data Science, Statistics, Computer Science)."}
{"text": "skills, strong business acumen, and the ability to communicate complex ideas to non-technical stakeholders. This role will be responsible for gathering and analyzing data from various sources, identifying trends, patterns, and opportunities, and presenting actionable recommendations to support informed decision making at the highest level of the organization.\n\nThe primary focus of this role will be to support the ARR Project. This will include partnering with Data Engineers to translate and design business logic, design and create data products that help drive business decisions, partner with other analysts on Data Analytics team with ARR related projects, and support senior level (Director to C-Suite) stakeholders.\n\n\n4 Month Contract Role** \n\nQualifications \n\n\nBachelor’s Degree required (Business Administration, Finance, MIS, or related field, Advanced degree highly preferred) 4+ years of experience in field of Data Analytics, Business Intelligence & Reporting 4+ years of experience in finance, accounting, operations, or similar domains Proven experience in a senior data analyst or similar role, preferably supporting C Suite executives. Advanced in data manipulation, analysis, and visualization tools (Tableau, and SQL required) (Python and/or R are a plus) Tableau Experience: Including Building Dashboards, Publishing Dashboards, and Creating Extracts using writing SQLs and Prep Strong business acumen and the ability to understand and interpret complex organizational challenges. Excellent communication skills, with the ability to explain complex data concepts to non-technical stakeholders. Experience considered a plus but not required:Work in a SaaS business model Work in a Finance Department Experience with Annual Recurring Revenue (ARR) This is not an excel heavy role, but experience with complex excel formulas may help \nBenefits\n\n\nUnlimited PTOMedical, dental, and vision benefits that start on day oneFlexible spending accountsLife insurance and short-term and long-term disability coverageFamily planning support benefits, along with 100% paid maternity and parental leave401k matchVeeam Care Days – additional 24 hours for your volunteering activitiesProfessional training and education, including courses and workshops, internal meetups, and unlimited access to our online learning platforms (Percipio, Athena, O’Reilly) and mentoring through our MentorLab program.\n\nThe salary range posted is On Target Earnings (OTE), which is inclusive of base and variable pay. When making an offer of employment, Veeam will take into consideration the candidate’s expectations, experience, education, scope of responsibility for the role, and the current market demands.\n\nUnited States of America Pay Range\n\n$101,200—$144,500 USD\n\nVeeam Software is \n\nPlease note that any personal data collected from you during the recruitment process will be processed in accordance with our Recruiting Privacy Notice.\n\nThe Privacy Notice sets out the basis on which the personal data collected from you, or that you provide to us, will be processed by us in connection with our recruitment processes.\n\nBy applying for this position, you consent to the processing of your personal data in accordance with our Recruiting Privacy Notice."}
{"text": "Qualifications\n\nTHE EXPERIENCE YOU BRING TO THE TEAM\n\nMinimum Required Skills\n\n2 years of relevant Data Engineering and insight experiencePython with very strong AWS experience in delivering Python based solutionsSkilled in SQL and experience in the process of analyzing data to identify trends or relationships to inform conclusions about the dataExperience with building and deploying applications in AWS using services like (S3,Glue,Redshift,RDS,AWS EMR, Cloudwatch, Lambda, State Machine, SNS, SQS, ECS Fargate, AppFlow, etc.)2 years of experience in APIs, RESTful services.Skilled in cloud technologies and cloud computingStrong experience building CI/CD pipelines on AWS (CloudFormation and Gitlab)Good communication skills and ability to work in a team environment.Ability to work independently as well as part of an agile team (Scrum / Kanban)Programming including coding, debugging, and using relevant programming languages\n\nDesired Skills\n\nKnowledge on Informatica Power center ETL tool or any other ETL tools and spark SQL\n\nAdditional Information\n\nThe future is what you make it to be. Discover compelling opportunities at careers.fanniemae.com.\n\nFannie Mae is \n\nThe hiring range for this role is set forth on each of our job postings located on Fannie Mae's Career Site. Final salaries will generally vary within that range based on factors that include but are not limited to, skill set, depth of experience, certifications, and other relevant qualifications. This position is eligible to participate in a Fannie Mae incentive program (subject to the terms of the program). As part of our comprehensive benefits package, Fannie Mae offers a broad range of Health, Life, Voluntary Lifestyle, and other benefits and perks that enhance an employee’s physical, mental, emotional, and financial well-being. See more here."}
{"text": "experience in autogen, langchain, Python programming, and prompt engineering. As an Agentic AI Engineer, you will be responsible for designing, developing, and implementing advanced AI systems that exhibit goal-oriented behavior and decision-making capabilities. \nKey Qualifications:1. Master's degree or higher in Computer Science, Artificial Intelligence, or a related field2. Minimum of 5 years of experience in AI development, with a focus on agentic AI systems3. Strong proficiency in Python programming language4. Hands-on experience with autogen and langchain frameworks5. Expertise in prompt engineering and developing effective prompts for AI agents6. Familiarity with machine learning frameworks such as TensorFlow or PyTorch7. Excellent problem-solving and analytical skills8. Strong communication and collaboration abilities9. Ability to work independently and in a team environment\nPreferred Qualifications:1. PhD in Computer Science, Artificial Intelligence, or a related field2. Experience with natural language processing (NLP) and conversational AI3. Knowledge of reinforcement learning and decision-making algorithms4. Familiarity with cloud computing platforms such as AWS or Google Cloud5. Contributions to open-source AI projects or research publications in the field of agentic AI\nWe offer a competitive salary, comprehensive benefits package, and the opportunity to work on groundbreaking AI projects. If you are passionate about agentic AI and have the required skills and experience, we encourage you to apply for this exciting opportunity.\nDesign Engineering (IdeaXDesign) is"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\nJob Description\nThis position requires in-depth knowledge and expertise in GCP services, architecture, and best practices. Will work closely with clients to understand their business objectives and develop strategies to leverage GCP to meet their needs. They will collaborate with cross-functional teams to design, implement, and manage scalable and reliable cloud solutions. They will also be responsible for driving innovation and staying up-to-date with the latest GCP technologies and trends to provide industry-leading solutions.\nYour Impact:\nCollaborate with clients to understand their business requirements and design GCP architecture to meet their needs.Develop and implement cloud strategies, best practices, and standards to ensure efficient and effective cloud utilization.Work with cross-functional teams to design, implement, and manage scalable and reliable cloud solutions on GCP.Provide technical guidance and mentorship to the team to develop their skills and expertise in GCP.Stay up-to-date with the latest GCP technologies, trends, and best practices and assess their applicability to client solutions.Drive innovation and continuous improvement in GCP offerings and services to provide industry-leading solutions.Collaborate with sales and business development teams to identify and pursue new business opportunities related to GCP.Ensure compliance with security, compliance, and governance requirements in GCP solutions.Develop and maintain strong relationships with clients, vendors, and internal stakeholders to promote the adoption and success of GCP solutions.\nQualifications\nMust have good implementationexperience onvariousGCP’s Data Storage and Processing services such as BigQuery, Dataflow, Bigtable, Dataform, Data fusion, cloud spanner, Cloud SQLMust have programmatic experience with tools like Javascript, Python, Apache Spark.Experience in building advance Bigquery SQL and Bigquery modelling is requiredExperience in orchestrating end-end data pipelines with tools like cloud composer, Dataform is highly desired.Experience in managing complex and reusable dataflow pipelines is highly desired.\nWhat sets you apart:\nExperience in complex migrations from legacy data warehousing solutions or on-prem datalakes to GCPExperience in maneuvering resources in delivering tight projectsExperience in building real-time ingestion and processing frameworks on GCP.Adaptability to learn new technologies and products as the job demands.Experience in implementing Data-governance solutionsKnowledge in AI, ML and GEN-AI use casesMulti-cloud & hybrid cloud experienceAny cloud certification\nAdditional Information\nFlexible vacation policy; Time is not limited, allocated, or accrued16 paid holidays throughout the yearGenerous parental leave and new parent transition programTuition reimbursementCorporate gift matching program\nCareer Level: Senior Associate\nBase Salary Range for the Role: 115,000-150,000 (varies depending on experience) The range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. Actual range for this position may differ, depending on location and specific skillset required for the work itself."}
{"text": "experience in Python Experience in creating AI/ML models from scratch"}
{"text": "experience would not survive. But in healthcare, patients lack market power. Which means they are expected to accept the unacceptable.\n\nZocdoc’s mission is to give power to the patient. To do that, we’ve built the leading healthcare marketplace that makes it easy to find and book in-person or virtual care in all 50 states, across +200 specialties and +12k insurance plans. By giving patients the ability to see and choose, we give them power. In doing so, we can make healthcare work like every other consumer sector, where businesses compete for customers, not the other way around. In time, this will drive quality up and prices down.\n\nWe’re 15 years old and the leader in our space, but we are still just getting started. If you like solving important, complex problems alongside deeply thoughtful, driven, and collaborative teammates, read on.\n\nYour Impact on our Mission\n\nWe are looking for a Principal Data Scientist to join our Search team at Zocdoc to work on our core Search product offerings such as our patient facing Provider Recommendation System. Using a variety of machine learning algorithms, you will build and implement models to create algorithms, run simulations and test your results. We are looking for a statistically-minded individual who has the coding skills to independently work on data and interpret research outcomes to help shape the data science strategy. A close collaboration with business partners (including product, engineering, marketing and sales) will enable you to implement data-driven initiatives.\n\nYou’ll enjoy this role if you are…\n\nPassionate about leveraging data science to solve real world problems Passionate about communicating important data insights to business stakeholders stories that tell cohesive, logical stories about the value and uses of Data ScienceA product-driven individual who loves working in a highly collaborative and supportive environmentMotivated by building products that make healthcare easierAn individual who enjoys leading and mentoring data scientists \n\nYour day to day is…\n\nWorking closely with our product team to build and iterate on user-facing features using data analytics and machine learning to optimize the results to drive conversion.Applying advanced statistical techniques to measure efficacy of various products, suggesting improvements to the products and our processes as you see themLeading and mentoring a team of Data Scientists within the Search team, sharing your experience and expertise with others who are eager to learn\n\nYou’ll be successful in this role if you have… \n\n10+ years of experience performing data analysis and a Master’s degree/PhD in statistics, math, physical sciences, computer science or other STEM related degreesProven experience on leading and implementing data science initiatives on a product using strong domain knowledge combined with data intuition to understand the most impactful opportunities Ability to mentor other data scientists, increasing both technical data ability and business acumenExpertise working with large, complex SQL and NoSQL database infrastructureSolid understanding of statistics and common machine learning techniquesA strong perspective regarding data engineering and the most appropriate infrastructure to use (including trade-offs)An understanding of the nuances and tradeoffs of different types of experiment designBonus if you have a strong understanding of learning to rank recommendation systems.\n\nZocdoc is committed to fair and equitable compensation practices. Salary ranges are determined through alignment with market data. Base salary offered is determined by a number of factors including the candidate’s experience, qualifications, and skills. Certain positions are also eligible for variable pay and/or equity; your recruiter will discuss the full compensation package details.\n\nNYC Base Salary Range\n\n$177,000—$239,000 USD\n\nAbout Us\n\nZocdoc is the country’s leading digital health marketplace that helps patients easily find and book the care they need. Each month, millions of patients use our free service to find nearby, in-network providers, compare choices based on verified patient reviews, and instantly book in-person or video visits online. Providers participate in Zocdoc’s Marketplace to reach new patients to grow their practice, fill their last-minute openings, and deliver a better healthcare experience. Founded in 2007 with a mission to give power to the patient, our work each day in pursuit of that mission is guided by our six core values. Zocdoc is a private company backed by some of the world’s leading investors, and we believe we’re still only scratching the surface of what we plan to accomplish.\n\nZocdoc is a mission-driven organization dedicated to building teams as diverse as the patients and providers we aim to serve. In the spirit of one of our core values - Together, Not Alone, we are a company that prides itself on being highly collaborative, and we believe that diverse perspectives, experiences and contributors make our community and our platform better. We’re \n\nJob Applicant Privacy Notice"}
{"text": "Machine Learning / AI Engineers / Developers\nCustomer: Method360Location: San Antonio, Texas or Finlay, Ohio (5 days a week in office, may get some Fridays Remote)Start Date: 5/2Duration – 1 year contract to start with good chance for multiple year assignmentExpenses- None\n\nDescription:Direct partner has a need for Machine Learning and AI Engineers / Developers:Will heavily utilize Synapse and Azure Data Lake Storage using Azure MI and AI Services. Also use Python and Python Libraries."}
{"text": "skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers.  ResponsibilitiesAnalyze raw data: assessing quality, cleansing, structuring for downstream processingDesign accurate and scalable prediction algorithmsCollaborate with engineering team to bring analytical prototypes to productionGenerate actionable insights for business improvements\nQualifications\nBachelor's degree or equivalent experience in quantative field (Statistics, Mathematics, Computer Science, Engineering, etc.)Experience in Data Visualization (Tableau, Python required; Splunk a plus)At least 1 - 2 years' of experience in quantitative analytics or data modelingDeep understanding of predictive modeling, machine-learning, clustering and classification techniques, and algorithmsFluency in a programming language (Python, C,C++, Java, SQL)Familiarity with Big Data frameworks and visualization tools (Cassandra, Hadoop, Spark, Tableau)"}
{"text": "Experience required.\nKey requirements and translate them into innovative machine learning solutions.- Conduct ongoing research to stay abreast of the latest developments in machine learning, deep learning, and data science, and apply this knowledge to enhance project outcomes. Required Qualifications:- Bachelor’s or Master’s degree in Computer Science, Applied Mathematics, Engineering, or a related field.- Minimum of 12 years of experience in machine learning or data science, with a proven track record of developing custom, complex solutions.- Extensive experience with machine learning frameworks like PyTorch and TensorFlow.- Demonstrated ability in designing algorithms from the ground up, as indicated by experience with types of algorithms like Transformers, FCNN, RNN, GRU, Sentence Embedders, and Auto-Encoders, rather than plug-and-play approaches.- Strong coding skills in Python and familiarity with software engineering best practices.Preferred Skills:- Previous experience as a software engineer, applied mathematician, or in roles involving DevOps, MLOps, Databricks, and Apache Spark is highly regarded.- Ability to communicate complex technical details effectively to stakeholders with varying levels of technical knowledge.- Creative problem-solving skills and a strong analytical mindset.\n\n\nIf I missed your call ! Please drop me a mail.\nThank you,HarishAccounts Manager/Talent Acquisition Astir IT Solutions, Inc - An E-Verified CompanyEmail:harishj@astirit.comDirect : 7326946000*78850 Cragwood Rd. Suite # 219, South Plainfield, NJ 07080www.astirit.com"}
{"text": "experience? Do you want to create a next-generation data storage product for large language models? If so then Customer Service’s Customer Engagement Technology Team is for you!\n\nWe are seeking a Sr Data Engineer to join the Customer Engagement Technology Team (CET) focused on Self-Service Automation. As a Senior Data Engineer you will be responsible for designing and building our data storage platforms for LLMs.\n\nThe ideal candidate relishes working with large volumes of data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling with unstructured data, ingestion pipelines and ETL design and business intelligence tools with the business to identify strategic opportunities where improvements in data infrastructure creates out-sized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail), mentors other engineers in team on high quality tech and enjoys working in a fast-paced team. We're excited to talk to those up to the challenge!\n\nThe mission of the CET team within Customer Service is to create earth’s best customer service solutions through empowering our customers to utilize automation to resolve their issues quickly and efficiently. You will be a key component of our leadership team and will influence our science and engineering roadmap to harness the power of LLMs to solve customer problems.\n\nKey job responsibilities\n\n Design, develop and maintain product data pipelines, meeting data and privacy standards. Develop robust and scalable data and insight platforms using SQL and Spark Support analytical research and provide recommendations to business challenges Continually improve the data quality and operations, via automation and building full CI/CD data pipelines Develop and influence the teams’ data strategy and data storage roadmap through working closely with business stakeholders.\n\nA day in the life\n\nSummary\n\n“If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply!”Benefits Summary:\n\n“Amazon offers a full range of benefits that support you and eligible family members, including domestic partners and their children. Benefits can vary by location, the number of regularly scheduled hours you work, length of employment, and job status such as seasonal or temporary employment. The benefits that generally apply to regular, full-time employees include:\n\n Medical, Dental, and Vision Coverage Maternity and Parental Leave Options Paid Time Off (PTO) 401(k) Plan”\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nAustin, TX, USA | Dallas, TX, USA | Seattle, WA, USA\n\nBasic Qualifications\n\n Bachelor's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent 7+ years of data engineering experience Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Experience with SQL Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Experience mentoring team members on best practices Knowledge of distributed systems as it pertains to data storage and computing Bachelor's degree\n\nPreferred Qualifications\n\n Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience operating large data warehouses Master's degree Experience communicating with users, other technical teams, and management to collect requirements, describe data modeling decisions and data engineering strategy\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $123,700/year in our lowest geographic market up to $240,500/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2617753"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Data Architect -AWS Cloud to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating them to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and subsystems on production servers\n\n• Mentor, support and manage team members\n\nYour Skills & Experience:\n\n• 8+ years of demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platform- AWS (Amazon Web Services)\n\n• Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension concepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAnnual base pay range: $128,000 - $193,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "experience needed to grow and develop your career.An open mind for new ideas and creative methods.A strong compensation and benefits package, including health, vision, dental, 401k with a strong match and much more!\n\nGeneral Summary….\n\nThe Analyst, Data II is part of a team that shares the responsibility for success and profitability by providing services to our customers which may include: data warehousing, post audits, reporting, carrier bids management, dashboard creation, project management, transportation analysis, application mastery, consulting support, and data analysis. The Data Analyst works with customers, carriers, and internal employees to analyze and identify cost saving opportunities for customers.\n\nThis position will be responsible for…..\n\nManage data gathering for customers’ benchmark key performance metrics.Create a strategic approach to carrier bids through lane, mode, and service balancing (Bid team) by performing the following tasks: Scorecard and performance tracking, transportation dashboard, on-going analysis of data. Determine the best mode, carrier, and service for the customer, resulting in customer savings by providing the analysis and metrics for transportation bids.Use professional judgment to assess the impact of decisions/actions on the customer and the Company which would be approved by both the customer and the person’s leader.Act as an internal technical resource for role specific applications.Analyze large amounts of data and then recommend broad based innovative improvement initiatives for customer(s).Reporting and analyzing on an ad hoc basis for the customer. Develop customer presentations showing data trends and possible solutions to the customer. Collaborate with the objective of agreeing to the most effective and profitable solution for the customer, carrier, and the Company.Developing standard operating procedures based on the direction from manager.\n\nYou might be a great fit if….\n\nEducation/Experience\n\nBachelor’s Degree in Statistics, Engineering, Accounting/Finance or related field preferred and 5+ years of relevant experience.In lieu of degree, high school diploma or GED and 4-6 years of relevant experience.Proficient with technology, specifically Microsoft applications such as Access and Excel.Experience with SQL is preferred.Ability to work in a fast paced environment with multiple deadlines.Strong organizational skills and the ability to handle multiple tasks simultaneously.Strong interpersonal skills with the ability to work with internal and external customers.Experience or knowledge in transportation, logistics, parcel shipping or freight pay is preferred.Excellent written and verbal communication skills.\n\nPhysical/Cognitive Requirements\n\nWith or without accommodation:\n\nAbility to follow policies and procedures.Ability to read, write and interpret information.Ability to add, subtract, multiply and divide. Ability to use hands to finger, handle, or feel.Ability to sit/walk/stand for up to 8 hours per day. Must possess visual acuity, i.e., close, distance, and color vision, depth perception and the ability to adjust focus.\n\nFedEx Supply Chain, Inc., as well as its subsidiary companies, is \n\nThe FedEx Logistics companies are committed to providing access, equal opportunity, and reasonable accommodation for qualified individuals with disabilities in its application procedures, hiring, and other terms and conditions of employment. To request a reasonable accommodation, please contact Fxl.talentacquisition@fedex.com.\n\nJob ID: 52021\n\nSchedule: Full-time"}
{"text": "skills, education, experience, and other qualifications. \nFeatured Benefits:\nMedical Insurance in compliance with the ACA401(k)Sick leave in compliance with applicable state, federal, and local laws\nDescription: Responsible for performing routine and ad-hoc analysis to identify actionable business insights, performance gaps and perform root cause analysis. The Data Analyst will perform in-depth research across a variety of data sources to determine current performance and identify trends and improvement opportunities. Collaborate with leadership and functional business owners as well as other personnel to understand friction points in data that cause unnecessary effort, and recommend gap closure initiatives to policy, process, and system.  Qualification: Minimum of three (3) years of experience in data analytics, or working in a data analyst environment.Bachelor’s degree in Data Science, Statistics, Applied Math, Computer Science, Business, or related field of study from an accredited college or university Ability to:  Strong attention to detail; Ability to apply data quality assurance and troubleshooting to data profiling, analysis, and reporting; Ability to apply appropriate data cleansing and transformation techniques to prepare data for reporting and analysis; Demonstrate strong analytical ability to identify appropriate analysis, data anomalies, trends, etc.; Advanced presentation skills leveraging appropriate software, adapting to audience, and excellent written and grammatical skills; Work with minimal supervision; self-directed; seeks assistance when needed; o Excellent written and verbal communications skills; Use advanced Microsoft Office Suite (Excel, PowerPoint, Word, Outlook, etc.) and standard office equipment (telephone, computer, copier, etc.); Make arithmetic computations using whole numbers, fractions and decimals, rates, ratios, and percentages; MS Access - advanced skills including relational table joins, data transformation through joins, filtering, updates, and summarization, reporting (preferred); Reporting (Cognos, OBIEE, Crystal) - advanced skills in standard columnar reporting, requirements gathering, data preparation requirements, report creation, testing, scheduling, and deployment. (preferred) Primary Functions:  Participate in the creation, validation, and implementation of statistical models. Participate in the improvement of performance of these models over time to ensure accuracy, statistical confidence, and business goal alignment; Identify trends and actionable insights to inform and enable qualitative and quantitative data-driven decisions across the organization; Participate in the recommendation, promotion, and auditing of best practices related to data usage, reporting standards, dashboard formats, visualization style, and analysis methods; Participate in communicating the significance of statistical findings using business acumen and vernacular common to the utilities industry including use of safety terminology and metrics; Participate in discussions with stakeholders regarding data, analyses, visualizations, conclusions and recommendations in a manner that influences decisions and outcomes; Participate in deep data analysis, research, and studies relative to business discovery use cases; Collaborate with I.T. and external consultants in decisions related to data modeling, dimensionality, data granularity, fit-for-use architecture, and overall data governance; Participate in performing data mining for new business insights; interpret data; draw conclusions; communicate findings to relevant stakeholders; Develop strong understanding of data sources, relationships, and best practice usage; • Lead / participate in troubleshooting and debugging efforts; Prepare and present visualizations, dashboards, and reporting; Update data visualizations and dashboards; Identify and escalate data anomalies that might affect accuracy; Verify information integrity of reports, dashboards, and analysis; Generate scheduled and ad hoc reports; Generate documentation related to \"reporting and analysis\" development, implementation, and support; Participation in deep data profiling efforts to gain an understanding of the raw data available for analysis. Participation in data mining efforts as part of a data science or machine learning exercise to identify themes and trends for further analysis; Participation in identifying trends, drawing conclusions, and summarizing results derived from data analysis to produce business-relevant and actionable conclusions; Participation in transforming information into actionable insights; Perform routine research and analysis to identify data trends, anomalies, and actionable insights that are applicable to Coordinate assigned departmental programs, projects, and activities; respond to program customers to identify needs and resolve issues; act as a resource for other departments; Identify and research gaps in departmental programs; identify opportunities and recommend and/or implement improvements; Research and recommend new technologies, programs, and procedures; Support operational software and technical applications related to the department; Perform other duties as assigned. Working knowledge of all, but not limited to, the following:  Processes for leveraging data from data warehousing / data mart / data lake environments; Visualization Development - Generate analysis through data visualizations from multiple data sets using standard best-in-class analytics software; Query complex data structures and derive information for reporting, visualizations, and statistical analysis; Requirements gathering and analysis; Basic Analytics - Perform basic data analysis to include data profiling, data quality, joining of data tables, graphing, basic trend analysis, data segmentation; Ad Hoc Query Development - Quickly develop, test, and provide ad hoc (one-time) information based on a business request leveraging internal or external data and using standard querying toolsets; Report Development - Create reports from multiple data sets using standard best-in-class reporting software; SQL - basic query and data manipulation skills including selects, inserts, updates, table joins, and grouping; Visualization (Qlik, PowerBI, Cognos, Tableau) - advanced skills in a best-in-class data visualization tool to include data preparation, rationalization of visualization type, standard charting (time series, Pareto, bar, area, multi-axis, geospatial, scatter plots, etc.), filtering, drill-downs, drill-throughs, navigation, dashboard creation, deep understanding of user interface and effective presentation; Excel - advanced skills including graphing, Pivot Tables, VLOOKUP, and multi-sheet references; Experience working with a best-in-class DBMS (Oracle, SQL Server, etc.) to extract and transform data for reporting, analysis, or data science. Familiarity with all, but not limited to, the following:  Enterprise resource planning (ERP) software (JD Edwards EnterpriseOne) and specialty software programs used to assemble business operations data in the functional area of assignment (billing, budget, accounting, workforce management, etc.); Familiar with a data warehouse / data mart OLAP environment leveraging data in star schemas, snowflake schemas, and similar data structures; Familiar with data modeling in the context of transforming data from an OLTP system to an OLAP or other data warehouse related structure. Familiar with the importance of how data is modeled to support the needs of a data reporting and analysis environment; Familiarity with generally accepted data and information privacy standards (GDPR, PCI, PII, HIPAA, etc.); o Familiarity with leveraging large data sets for data science, machine learning and related analysis; Dashboard Development - Gather requirements, identify metrics and goals, leverage data sources, select appropriate dashboard objects, and implement a dashboard using a best-in-class tool; Project Management - Facilitate, create, implement, and manage a project or projects using MS Project or a similar project tracking tool; ability to define, document, and communicate a project charter, resource assignments, risks, issues, and status over the course of a project; Query Optimization – ability create / modify SQL or other query code to ensure request has minimal impact on the target database and executes in the most efficient manner possible; Knowledge / application of related industry, organizational, and departmental policies, practices and procedures, legal guidelines, ordinances and laws; Predictive Model Development - Leverage historic internal and external data to generate predictive business models forecasting trends and providing insights with relevant statistical confidence measures and using appropriate statistical methods; Process flow documentation; Related industry, organizational and departmental policies, practices and procedures; legal guidelines, ordinances and laws."}
{"text": "experience of business analytics, CRM reporting, writing and revising reportsFirm understanding of the structure and operations of relational databases and the ability to aggregate data through ExcelAbility to independently make decisions and deliver work quicklyAbility to analyze data and identify trends to inform strategic decisionsStrong strategy, planning, and organizational skillsInterpersonal skills to work effectively across functional teamsMust be self-managed, responsive and able to work in a virtual team environment\n\nA few nice to haves\n\nPrevious experience using Jira \n\nAt Adaptavist, we are committed to promoting a diverse and inclusive community, and believe this positively impacts both the creation of our innovative products and our delivery of bespoke solutions to our global customers and our own unique culture. We encourage all qualified applicants, regardless of age, disability, race, sexual orientation, religion or belief, sex, gender identity, pregnancy and maternity, marriage, and civil partnership status. From our family-friendly policies to our flexible work environment we offer a range of benefits and policies in order to support staff from all different backgrounds. If you have any questions, please do ask us.\n\nCheck out our WORK180 page\n\nCheck out our Flexa page\n\nWe look forward to your application!"}
{"text": "requirements and deliver tailored solutions that meet business objectives.Troubleshoot and resolve data-related issues promptly, ensuring data integrity and availability.Stay updated with industry trends and best practices in data engineering, continuously enhancing the organization's data capabilities.Requirements:Bachelor's degree in Computer Science, Engineering, or a related field.3-5 years of experience in data engineering, with a focus on data modeling, data warehousing, and ETL pipeline development.Proficiency in scripting languages such as Python, Scala, and Java, with a strong emphasis on writing clean, efficient code.Hands-on experience with cloud platforms, particularly AWS, and familiarity with serverless computing using AWS Lambda.Strong understanding of data engineering concepts and methodologies, including data governance, data quality, and data security.Experience working with reporting tools such as Tableau and Qlik Sense, connecting them to backend data sources for visualization and analysis.Excellent communication skills and the ability to collaborate effectively with cross-functional teams.A self-starter mentality with the ability to thrive in a fast-paced, dynamic environment.Previous experience working in a startup environment is highly desirable."}
{"text": "experience at Amazon, driving productivity and retention, and resulting in a motivated workforce of over 1.5 million associates and corporate employees. These are the questions we ask — Are we facilitating the right conversations to build an engaged workforce? What trends are we seeing in our employee data and what should managers do about it? How do we solve customer problems in the most efficient way possible? If these challenges sound interesting to you, you want to be a part of building ‘first of their kind’ products, and you are passionate about putting employee experience first, consider the PeopleInsight team. PI helps Amazon drive improvements in employee talent outcomes (e.g., job satisfaction and retention), and strive to be Earth’s Best Employer through scalable technology.\n\nPI is looking for a customer-obsessed Data Scientist for Employee Engagement Services, a suite of internal employee engagement and recognition products supporting Amazonians WW, with a strong track record of delivering results and proven research experience. This role will own and execute strategic cross-functional employee engagement experiments, analysis and research initiatives across Operations and Corporate audiences for high CSAT products. The Data Scientist must love extracting, cleaning and transforming high volume of data into actionable business information and be able to drive actionable insights. The data scientist will partner with Product, UX and Dev teams to own end-to-end business problems and metrics with a direct impact on employee experience. Success in this role will include influencing within your team and mentoring peers. The problems you will consider will be difficult to solve and often require a range of data science methodologies combined with subject matter expertise. You will need to be capable of gathering and using complex data set across domains. You will deliver artifacts on medium size projects, define the methodology, and own the analysis. Your findings will affect important business decisions. Solutions are testable and reproducible. You will create documents and share findings in line with scientific best practices for both technical and nontechnical audiences.\n\nKey job responsibilities\n\n Implement statistical methods to solve specific business problems utilizing code (Python, R, Scala, etc.). Drive design and development of user classification models and other predictive models to enable a personalized experience for a user. Improve upon existing methodologies by developing new data sources, testing model enhancements, and fine-tuning model parameters. Collaborate with product management, software developers, data engineering, and business leaders to define product requirements, provide analytical support, and communicate feedback; develop, test and deploy a wide range of statistical, econometric, and machine learning models. Build customer-facing reporting tools to provide insights and metrics which track model performance and explain variance. Communicate verbally and in writing to business customers with various levels of technical knowledge, educating them about our solutions, as well as sharing insights and recommendations. Earn the trust of your customers by continuing to constantly obsess over their needs and helping them solve their problems by leveraging technology\n\nAbout The Team\n\nThe PeopleInsight team is a collaborative group of Business Intelligence Engineers, Data Scientists, Data Engineers, Research Scientists, Product Managers, Software Development Engineers, Designers and Researchers that studies a workforce numbering in the hundreds of thousands. Our work is dedicated to empowering leaders and enabling action through data and science to improve the workplace experience of associates and ensure Amazon is Earth's Best Employer.\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nSeattle, WA, USA\n\nBasic Qualifications\n\n 2+ years of data scientist experience 3+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience 3+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience Experience applying theoretical models in an applied environment\n\nPreferred Qualifications\n\n Experience in Python, Perl, or another scripting language Experience in a ML or data scientist role with a large technology company\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $111,600/year in our lowest geographic market up to $212,800/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2605420"}
{"text": "experienced and passionate full-stack data scientist in our Data organization. This role will partner directly with product managers, engineers, marketing, and other business partners across the business to research, develop, deploy, and continuously improve the machine learning solutions to drive growth at Dave and improve user experience for our customers.\n\nWhat You'll Do:\n\nBe constantly curious and continue to pursue in-depth explorations of our datasets.Collaborate with key stakeholders to understand the business problems, ideate ML solutions, and effectively communicate with stakeholders. Research, develop, and apply ML/AI solutions to solve business problems, including prediction, optimization, segmentation, and more. Perform in-depth analysis and optimization of state-of-the-art ML models to ensure the best performance on current and next-generation GPU, network, and compute architectures.Partner with Engineering and Product teams to develop and deploy ML solutions into product features.Continuous monitoring and evaluation of model performance and communication to the relevant stakeholders. Conduct model related analyses to provide comprehensive insights about ML solutions.\n\n\nWhat You'll Need:\n\nBS/MS/PhD or equivalent experience in Computer Science, Electrical/Computer Engineering, Mathematics, Statistics, or other STEM fields.4+ years of modeling and machine learning experience to build ML-driven products for solving business problems eg. prediction, optimization, segmentation, etc. 4+ years experience as an ML scientist/engineer with proven ability in developing ML models in Python/R.Experience with ML/DL algorithms with frameworks such as TensorFlow, PyTorch, Spark, and cloud platforms.Ability to communicate your ideas/code clearly to business stakeholders.Enjoy working with multiple levels and teams across organizations (engineering, product, and marketing teams etc.)Effective verbal/written communication and technical presentation skills.Self-starter with a passion for growth, enthusiasm for continuous learning, and sharing findings across the team.Experience in Financial Services or FinTechML model deployment in production experience is a plus.Experience in underwriting and/or settlement is a plus. \n\n\n$138,000 - $222,000 a year\n\nDave has established a nationally applicable pay program that recognizes that regardless of where Daves choose to live, their time and effort is of equal value to the company. As such we utilize base salary ranges that are intended to be competitive in all markets and do not differentiate pay based on employee work location.\n\nThe application window will be open until at least April 17, 2024. This opportunity will remain online based on business needs, which may be \"met\" before or after the specified date. \n\nDon’t let imposter syndrome get in your way of an incredible opportunity. We’re looking for people who can help us achieve our mission and vision, not just check off the boxes. If you’re excited about this role, we encourage you to apply. You may just be the right candidate for this or other roles.\n\nWhy you’ll love working here: \n\nAt Dave, our people are just as important as our product. Our culture is a reflection of our values that guide who we are, how we work, and what we aspire to be. Daves are member centric, helpful, transparent, persistent, and better together. We strive to create an environment where all Daves feel valued, heard, and empowered to do their best work. As a virtual first company, team members can live and work anywhere in the United States, with the exception of Hawaii.\n\nA few of our benefits & perks :\n\n Opportunity to tackle tough challenges, learn and grow from fellow top talent, and help millions of people reach their personal financial goals Flexible hours and virtual first work culture with a home office stipend Premium Medical, Dental, and Vision Insurance plans Generous paid parental and caregiver leave 401(k) savings plan with matching contributions Financial advisor and financial wellness support Flexible PTO and generous company holidays, including Juneteenth and Winter Break All-company in-person events once or twice a year and virtual events throughout to connect with your team members and leadership team\n\n\nDave Operating LLC is proud to be an"}
{"text": "QualificationsRequiredAdvanced degree in statistics, mathematics, engineering, computer science, data science, economics, or other quantitative field2+ years of prior experienceExcellent skills in R, Python, and/or SQLKnowledge of and experience with crypto, decentralized finance, and smart contractsAttention to detailIntellectual curiosity and interest in fraud analysisHighest ethical standardsStrong teamwork abilities\nApplicants must be authorized to work for any employer in the U.S. We cannot currently sponsor or take over sponsorship of an employment visa."}
{"text": "requirements for data integration and business intelligence applications. Review project details for upcoming tests and determine if additional preliminary analytical support is needed up front. Use standardized dashboards and build customized analytics dashboards. Monitor test analytics dashboards, primarily in Adobe Analytics. Monitor test analytics dashboards regularly and advise on test duration estimates. Troubleshoot analytics issues that may arise. Conduct analyses on assigned test projects to identify additional insights. Stay up to date with the latest trends and best practices in web analytics and be proactive in identifying new opportunities for data-driven insights. Evaluate and improve analytics processes and tools to improve the quality of data and insights generated. Utilize Adobe Target test platform. Apply knowledge of AB testing and eCommerce optimization. Utilize web analytics tools including Google Analytics and Adobe Analytics including Firehose. Utilize software including Workfront, Power BI, Tableau, Snowflake, and EDM. Utilize Python and other computer programming languages. Apply analytical and problem-solving skills with the ability to uncover insights and make data-driven recommendations.\n\nREQUIREMENTS: Requires a Master’s degree, or foreign equivalent degree, in Computer Engineering, Information Systems, Information Technology Management, Math, Sciences, or Data Analytics and 3 years of experience in the job offered or 3 years of experience in a related occupation utilizing Adobe Target test platform; applying knowledge of AB testing and eCommerce optimization; utilizing web analytics tools including Google Analytics and Adobe Analytics including Firehose; utilizing software including Workfront, Power BI, Tableau, Snowflake, and EDM; utilizing Python and other computer programming languages; and applying analytical and problem-solving skills with the ability to uncover insights and make data-driven recommendations.\n\nOur Senior-Data Analysts earn between $140,000 - $190,900 yearly. Not to mention all the other amazing rewards that working at AT&T offers.\n\nJoining our team comes with amazing perks and benefits:\n\n Medical/Dental/Vision coverage 401(k) plan Tuition reimbursement program Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays) Paid Parental Leave Paid Caregiver Leave Additional sick leave beyond what state and local law require may be available but is unprotected Adoption Reimbursement Disability Benefits (short term and long term) Life and Accidental Death Insurance Supplemental benefit programs: critical illness/accident hospital indemnity/group legal Employee Assistance Programs (EAP) Extensive employee wellness programs Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone\n\n\nWeekly Hours:\n\n40\n\nTime Type:\n\nRegular\n\nLocation:\n\nDallas, Texas\n\nIt is the policy of AT&T to provide"}
{"text": "Skills You BringBachelor’s or Master’s Degree in a technology related field (e.g. Engineering, Computer Science, etc.) required with 6+ years of experienceInformatica Power CenterGood experience with ETL technologiesSnaplogicStrong SQLProven data analysis skillsStrong data modeling skills doing either Dimensional or Data Vault modelsBasic AWS ExperienceProven ability to deal with ambiguity and work in fast paced environmentExcellent interpersonal and communication skillsExcellent collaboration skills to work with multiple teams in the organization\nNEW ROLE: Data Engineer (Informatica / Snaplogic / SQL) . Smithfield RI, or merrimack NH.. MUST HAVE: Informatica Power Center / ETL (not IICS), Exposure to Snaplogic, Oracle (PL/SQL Stored procedures), Snowflake, Very strong SQL skills."}
{"text": "requirements and building relationships.Drive risk-based data and integration decisions to minimize ERP implementation risks.Lead data extraction, transformation, and loading from legacy sources into Dynamics 365.Design, develop, and troubleshoot integrations with Dynamics 365 and other systems.Develop and maintain documentation for data processes and integration architecture.Enhance the enterprise data strategy in collaboration with leadership.Build and deploy scalable data pipelines and APIs to support evolving data needs.Drive data integrations for future acquisitions and ensure data integrity and governance.Collaborate with stakeholders to design and implement data models, dashboards, and reports.\n\nQualifications for the Enterprise Data Engineer include: \n\nProficiency in ETL processes and tools, preferably with experience in Microsoft Dynamics 365.Knowledge of Azure data platforms and tools like Power Automate, Azure Synapse, SQL database, Power BI, and more.Experience with REST-based APIs, HTTP, SFTP, SSH, SOAP Protocols.Proficiency in programming languages such as Python, SQL, Scala for data manipulation.Familiarity with relational databases, unstructured data, data models, and SQL functions.Strong analytical and problem-solving skills, ability to work in cross-functional teams.Knowledge of Microsoft Visual Studio and Azure DevOps is a plus.Bachelor’s Degree in Computer Science or related field, with 3 years of experience as a data engineer.\n\nCompensation for the Enterprise Data Engineer include:\n\nSalary Range: $130,000 - $160,000 Full benefits: PTO, Paid Holidays, Cigna Healthcare, Dental, Vision\n\nKeywords:\n\nEnterprise Data Engineer, Microsoft Dynamics 365, Data Conversion, System Integrations, ERP Implementation, Data Strategy, Data Models, Data Governance, Data Pipelines, Azure Platform, ETL Processes, Power BI, API Integration, SQL, Python, Data Manipulation, Azure Synapse, Business Systems, Data Analysis, Data Engineering, Data Solutions, Data Integration, Data Migration, Data Modeling, Data Governance."}
{"text": "Requirements\n\nWe are seeking 3+ years of related experience and a bachelor's or advanced degree in STEM from an accredited institution.Active in scope DoD TS/SCI security clearance. Ability to conduct analysis and import / ingest test data sets into the ArcGIS platform. Support testing events and ensure the data is collected and brought back for ingestion. Must possess the ability to work independently with minimal oversight while maintaining focus on research objectives defined by the client.\n\nWhat We Can Offer You\n\n We’ve been named a Best Place to Work by the Washington Post. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive benefits and learning and development opportunities. We are mission-oriented and ever vigilant in aligning our solutions with the nation’s highest priorities. For over 55 years, the principles of CACI’s unique, character-based culture have been the driving force behind our success.\n\nCompany Overview\n\nCACI is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other protected characteristic.\n\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits. Learn more here\n\nThe Proposed Salary Range For This Position Is\n\n$74,600-$156,700"}
{"text": "requirements for assigned clients and design dashboards to meet their needs and help manage inventory to optimize results.Work with clinical operations to understand and develop key indicator reporting.Support development and integration testing for algorithm logic enhancements, new products/services, and new technology applications.Review exception cases to determine status of maternity episodes and address client inquiries.\n\nQualifications\n\nAdvanced SQL knowledge and experience working with relational databases and data warehouses such as SQL Server, Oracle, Postgres or similarExperience analyzing and working with healthcare data preferably claims dataProficiency with Power BI, Power Query, and DAXExperience with Databricks a plus but not requiredExperience with CI/CD principles a plus but not requiredExperience with Python a plus but not requiredExperience with AWS and Azure cloud environment experience a plus but not requiredStrong communication skills with ability to work across internal and external teams"}
{"text": "experience, education, geographic location, and other factors. \nThe Role:As a Data Engineer, you will play a pivotal role in enabling the TSA solution to combine and collate data necessary to generate insights that support the human capital mission. Your primary focus will be on prioritizing standardization through integration to handle disparate data types and architectures using common data models and AI tools that support built-in data governance. Responsibilities include designing and implementing the data architecture, data cleaning and manipulation, statistical modeling and machine learning for insights and action, reporting and visualization, and data integration. You will work across multiple technologies in an agile team setup and collaborate closely with functional analysts and client users.\nRole Experience:RequiredMinimum 7 years of experience + Bachelor's Degree or equivalent5+ years of experience in large and complex IT projects, preferably in the Human Capital space5+ years of experience with supporting Data Integration, Interoperability, and Data Migrations5+ years of experience using common data models and AI tools that support built-in data governanceExperience applying data quality standardsProven ability to learn and adopt new technologiesExperience designing and implementing the data architecture and other data-related activitiesExperience leading data strategy to support the creation and improvement of data architecture, data usage, and data governanceMust be able to work the hours of 8 am-5 pm Eastern Time regardless of your home location\nRequired CertificationsRelevant certifications in supported toolsets or equivalent experience\nPreferred SkillsSystem administration and/or other hands-on technical experienceExperience with human capital systems, especially in support of Federal customersExperience with security incident/problem/change management and reportingExperience creating reports and analytics using TSA business intelligence tools, including PowerBI, using agile principles and methodologies\nIf you are interested in this position, then please click APPLY NOW. For other opportunities available at Akkodis go to www.akkodis.com. If you have questions about the position, please contact Narendra Pratap at narendra.pratap@akkodis.com\nEqual Opportunity Employer/Veterans/Disabled\nBenefit offerings include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits, and a 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs that are direct hires to a client.\nTo read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy\nThe Company will consider qualified applicants with arrest and conviction records."}
{"text": "Skills Required:* Configures and maintains database management systems. * Provides technical expertise to less experienced database administrators develops and monitors procedures for maintaining and updating organizational metadata. * Provides technical assistance and consultation to applications developers who create and maintain applications using RDBMS`s. * Assist technical infrastructure staff in resolving problems between the operating system hardware integration points and the database management system. * Participates in and influences the direction of the overall automation architecture Expert SQL, SSMS, SSRM, and Excel knowledge and skills.\nExperience Required:9 years of progressively responsible programming experience or an equivalent combination of training and experience. Some positions may require a valid driver`s license.A minimum of 7 years of experience in QA, data and reporting optimization\nEducation Required:Bachelor`s degree in Information Technology or Computer Science"}
{"text": "experience with architecting and implementing solutions using Azure, including Azure Open AI. They must also possess knowledge of Python, and machine learning frameworks. \nResponsibilities Develop solutions that leverage Azure services and OpenAI technologies to address complex business problems and deliver innovative AI-powered applications. Evaluate and recommend appropriate Azure services and OpenAI technologies based on project objectives, scalability, performance, and cost considerations Design and implement end-to-end AI solutions, including data acquisition, data preprocessing, model training, deployment, and monitoring Develop and optimize scalable and reliable cloud-based infrastructure on Azure, ensuring high availability, fault tolerance, and security Create design documents, solution artifacts, and technical specifications to guide the development and implementation process Collaborate with stakeholders, including product managers, data scientists, and others, to understand business requirements and translate them into technical solutions \nRequirements Bachelor's or Master's degree in Computer Science, Engineering, or a related field. Minimum of 5 years' experience with large-scale data and analytics solutions Knowledge of Azure Data & Analytics PaaS Services: Azure Data Factory, Azure Data Lake, Azure Synapse Analytics, Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics, and Azure SQL DB Experience with Azure ARM templates, PowerShell, and CI/CD using Azure DevOps Experience with preparing data for Data Science and Machine Learning purposes Experience with Azure data, Machine Learning, and Artificial Intelligence solutions Ability to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows Experience with different database technologies including relational and non-relational. Experience with exposing data to end-users via Power BI, Azure API Apps. Experience in turning business use cases and requirements into technical solutions \nPreferred Qualifications: Bachelor's or master's degree in computer science, Data Science, Statistics, Math, Physics, or other Science related discipline with course work in data and AI/ML. Experience with Azure Cognitive Services and OpenAI technologies, such as GPT-3 and GPT-4, prompt engineering techniques Knowledge of data privacy and security regulations and best practices related to Azure and AI solutions Ability to work collaboratively in a team environment and communicate complex technical concepts to non-technical stakeholders. Azure Solutions Architect certification preferred"}
{"text": "experience better.\n\nThe ideal candidate sees beyond the numbers to find opportunities to strengthen the employee experience and organizational performance. The candidate is relentless problem-solver, innovator and change agent who can translate complex sets of information into actionable talent and culture intelligence. They are responsible for partnership and communication with business clients on workplace insights: diagnosis, hypothesis testing, analysis, visualization, and presentation. They will guide business leaders with data-driven recommendations that improve performance, retention, and organizational agility. The candidate possesses strong quantitative skills, is capable of managing multiple projects simultaneously, and can communicate effectively to a wide-ranging audience. In addition, the candidate is comfortable working with technology partners as the voice of business to collect business and functional requirements to enhance relevant programs. An “outside in” perspective, high intellectual curiosity, and a consultative posture are essential skills to possess in this role.\n\nKey Responsibilities\n\nThe HR Data Science Consultant at Citizens Bank will:\n\nRapidly perform exploratory data analysis, generate and test working hypotheses, and uncover trends and relationships to support workforce planning efforts; provide descriptive and advanced analyses on workforce and workplace priorities.Synthesize business goals and strategy with HR, productivity, and market data to build integrated, tangible recommendations on talent and culture topics.Communicate results of analysis to stakeholders; employ storytelling techniques to make recommendations, moving from the “so what” and “now what,” to drive impact; develop executive presentations and dashboards for recurring and ad hoc workforce measurement needs.Build technology and analytical solutions that anticipate business needs across workforce and workplace dimensions; drive digital solutions for self-service.Project manage key initiatives that drive efficiency and value.Assist with change management efforts to scale HR analytics solutions across the enterprise.Assist in development, execution, and evaluation of client OKRs, human capital KPIs and other measurement strategies to enable business results.Partner and train clients and HR stakeholders on impactful use of data and data literacy techniques to drive business outcomes. Provide guidance on business priorities and opportunities for replication of solutions and self-service to HRA partners.Advise on enterprise-level and business unit level analytics work with analytics partners through the organization; contribute to enterprise data governance methods and priorities.\n\nCritical Skills\n\nCreative and strategic thinker - ability to assess needs, collaborate, and deliver data-driven, practical solutions that drive results Advanced analytical and problem-solving skills - approaches problems with rigorous logic and a curious mind – looks beyond the obvious and searches for deeper answersAbility to communicate effectively with technical and non-technical audiences – tells stories with data that resonate with stakeholders and maximize impact; excellent written and verbal communication skillsExperience designing analytics solutions with HR technology to accelerate the employee experience Ability to manage multiple client groups simultaneously; strong project management backgroundHighly proficient in Excel and PowerPointProficient in data querying languages (e.g., SQL), database management systems, and employee listening platforms (e.g., Perceptyx, Glint)Proficient in scripting languages (e.g., Python) and/or mathematical/statistical software (e.g., R), and other advanced analytical tools (e.g., Sagemaker, Tableau, PowerBI, Quicksight, Visier, Alteryx)Applied statistics or experimentation (A/B testing)Proficiency in change management approaches (e.g., PROSCI, etc.)Strong decision-making skills; stellar interpersonal, relationship building, and collaboration skills Highly-organized with strong attention to detail and documentation; results orientation\n\nAdditional Knowledge & Skills\n\nProven ability to handle multiple projects while meeting deadlines and documenting progress towards those deadlinesProficiency in agile and design-thinking methodsSome experience with LLM/text analysis or interest in learningPossesses a learning orientation, active exploratory mind, and interest to learn from othersExperience with AWS Cloud suite, Snowflake, Oracle HCM, Service Now, Saba a plus\n\nEducation And Experience\n\nMaster’s or PhD in a quantitative social science field (e.g., psychology, behavioral economics), Business Analytics, or other relevant technical field5+ years of client-facing People Analytics, Workforce Strategy, Business Intelligence, or Human Capital Consulting experience, working in/with complex organizations, or combination of education and experience Experience providing insights on talent management (e.g., talent mobility, DEI, performance, learning and development)\n\nHours & Work Schedule\n\nHours per Week: 40Work Schedule: 8:30-5\n\nSome job boards have started using jobseeker-reported data to estimate salary ranges for roles. If you apply and qualify for this role, a recruiter will discuss accurate pay guidance.\n\n\n\nAt Citizens we value diversity, equity and inclusion, and treat everyone with respect and professionalism. Employment decisions are based solely on experience, performance, and ability. Citizens, its parent, subsidiaries, and related companies (Citizens) provide equal employment and advancement opportunities to all colleagues and applicants for employment without regard to age, ancestry, color, citizenship, physical or mental disability, perceived disability or history or record of a disability, ethnicity, gender, gender identity or expression (including transgender individuals who are transitioning, have transitioned, or are perceived to be transitioning to the gender with which they identify), genetic information, genetic characteristic, marital or domestic partner status, victim of domestic violence, family status/parenthood, medical condition, military or veteran status, national origin, pregnancy/childbirth/lactation, colleague’s or a dependent’s reproductive health decision making, race, religion, sex, sexual orientation, or any other category protected by federal, state and/or local laws.\n\nEqual Employment and Opportunity Employer\n\nCitizens is a brand name of Citizens Bank, N.A. and each of its respective affiliates.\n\nWhy Work for Us\n\nAt Citizens, you'll find a customer-centric culture built around helping our customers and giving back to our local communities. When you join our team, you are part of a supportive and collaborative workforce, with access to training and tools to accelerate your potential and maximize your career growth"}
{"text": "Skills: GCP Data Engineer with 3-4 years of hands-on GCP/BigQuery experience (GCP, BigQuery, DataProc, DataFlow, Composer, etc.).  Candidates can work remote while on contract, but once converted full-time, the individual will need to relocate to Phoenix and work a hybrid schedule with the team (onsite every Tues/Wed/Thurs).  Thanks Aayushi Senior Technical Recruiter/Lead | Empower Professionals"}
{"text": "Skills Looking For:- The project involves creating a unified data structure for Power BI reporting.- Candidate would work on data architecture and unifying data from various sources.- Data engineering expertise, including data modeling and possibly data architecture.- Proficiency in Python, SQL, and DAX.- Work with AWS data, and data storage.- Experience with cloud platforms like AWS is preferred.- Familiarity with Microsoft Power Automate and Microsoft Fabric is a plus.- Collaborating with users to understand reporting requirements for Power BI. Must be good at using Power BI tools (creating dashboards); excellent Excel skills.- Supply chain background preferred.\nEducation and Level of Experience:- Bachelor's degree (quantitative learnings preferred- data analytics, statistics, computer science, math) with 3 to 5 years of experience.- Must have recent and relevant experience.\nTop 3 Skills:- Data engineering, including data modeling and data architecture.- Proficiency in Python, SQL, and DAX.- Experience with cloud platforms, especially AWS."}
{"text": "Skills:\nStrong experience in data science and analytics - 4 years minimumProficiency in PythonAdvanced knowledge of statistical analysis and data visualization toolsAbility to work with large datasets and databasesExcellent communication and presentation skillsKnowledge in AI/MLWorking experience with Databricks, Azure ML, and Azure CloudWroking experience with health claims dataGenerative AI experience is beneficialMedicaid or Managed Care experience is highly beneficial\nThis is a permanent position offering a competitive salary and benefits package."}
{"text": "requirements.Document operational procedures.\nQualifications\nBachelor’s degree in business, management, economics, accounting, finance, or computer information science required; master’s degree preferred.5 years of related experience.\nCompetenciesAdvanced knowledge of mathematics and statistics.Proficient in Microsoft Office Suite or related software.Ability to collect and compile relevant data.Deep understanding of database queries and reporting system solutions.Excellent ability to analyze information and think systematically.Strong business analysis skills.Thorough understanding of the company’s business processes and the industry at large.Data security and privacyData visualization, including tools such as Tableau and QlikProficient in ETL (extract, transform, load)Cloud computing and data storage technologyExcellent communication skills both verbal and written.Desire and drive to be proactive and take ownership of the team and processes.Excellent trouble-shooting skills.Excellent time and project management skills.Able to work as part of a large project team and interact effectively with others at all levels of the organization.A self-starter, versatile and adept at functioning in a small department in either a lead or support role.Thorough and attentive to details; proactive and deadline oriented.Problem analysis and problem resolution at both a strategic and functional level"}
{"text": "Experience : 10 yearsLocation : RemoteDuration: Full TimeJob DetailsData Warehouse, ETL, Advanced SQL,Data Profiling, Source to Target Mapping,Business Requirement Document, FRS, Healthcare.Should be able to navigate the code - developer background\n﻿Thanks & Regard's\nMohd FurquanLead Technical RecruiterE-mail: furqan@msrcosmos.comDirect No: +1 925 313 8949LinkedIn-ID :linkedin.com/in/mohd-furquan-94237816aVisit us: www.msrcosmos.com"}
{"text": "experience is built in a number of ways. Even if your background doesn’t match the exact requirements, we encourage you to apply and share any relevant skills in a cover letter. Sprout welcomes all candidates to apply, including those who identify as BIPOC, women and underrepresented genders in tech, LGBTQIA+, parents, veterans, persons with disabilities and people of all faiths.\n\nSenior Data Scientist\n\nDescription\n\nSprout Social is looking for a Senior Data Scientist to join its AI team.\n\nWhy join Sprout’s AI team? \n\nSprout empowers businesses worldwide to harness the immense power and opportunity of social media in today’s digital-first world. Processing over one billion social messages daily, our platform serves up essential insights and actionable information to over 30,000 brands, informing strategic decisions that drive business growth and innovation, and fostering deeper, authentic connections to their end customers. Our full suite of social media management solutions includes comprehensive publishing and engagement functionality, customer care solutions, influencer marketing, connected workflows, and business intelligence. We're actively working to intuitively weave AI throughout our products, enabling organizations to work smarter, unleash their creativity, and maximize the business impact of social media–that’s where you come in!\n\nWhat You’ll Do\n\nLead the end-to-end development and deployment of AI models, driving projects from concept to customer delivery in production. Work closely with product managers, engineers, and designers on our AI product teams to define opportunities for applying data science to our products. Empower team growth by upholding our standards and fostering a culture of excellence and collaborationStays current with the latest AI and ML research, identifying opportunities to integrate innovative solutions into Sprout's AI capabilities.\n\nWhat You’ll Bring\n\nWe’re looking for an experienced and passionate data scientist who is eager to develop exceptional software and high impact features in collaboration with our Product team. If you excel at crafting sophisticated AI models, have a love for learning and mentoring, all while being motivated by the desire to deliver substantial value to our customers,, we’d love to talk with you!\n\nThe minimum qualifications for this role include:\n\n3+ years working as a data scientist working in cross-functional teams to deploy production-level data products3+ years of experience leveraging Python for model development and experience with several ML frameworks such as, scikitlearn, tensorflow, pytorch, etc..Deep proficiency of the end-to-end ML lifecycle and applying that to a broad range of ML problem spaces\n\nPreferred qualifications for this role include:\n\nExperience working with natural language processing, generative AI and LLMs. Experience with model services, deployment technologies, and ML-Ops practicesFamiliar with cloud services and databases such as GCP, AWS, and Azure\n\nHow You’ll Grow\n\nWithin 1 month, you’ll plant your roots, including:\n\nComplete Sprout’s New Hire training program alongside other new Sprout team members.Learn about our existing model and deployment patterns.Become familiar with our existing services and available data.Begin meeting with product and data science stakeholders to understand existing problem spaces and needs.\n\nWithin 3 months, you’ll start hitting your stride by:\n\nWork with your manager to define the first data product you’ll work on and begin the process of developing itEither extend existing services or develop new tools to help our customers optimize their social contentContinue learning about Sprout’s products and customers to inform a potential new AI product or improvement of existing feature through AI \n\nWithin 6 months, you’ll be making a clear impact through:\n\nDeploy your first data product in concert with product management, design, engineering, and ML EngineeringContribute to our longer-term roadmap of data productsProvide input into our AI practice to elevate our standards and best practice\n\nWithin 12 months, you’ll make this role your own by:\n\nActively monitoring and iterating on the data product you’ve built Continuing to develop new data productsIdentify new opportunities for data science-backed capabilitiesMentor other team membersCollaborate with our AI team to identify technical debt, performance bottlenecks and opportunities to improve the quality of our models and come up with a plan to improve the codeExpand your skills by learning from other engineers and data scientists around Sprout.Surprise us! Use your unique ideas and abilities to change your team in beneficial ways that we haven’t even considered yet.\n\nOf course what is outlined above is the ideal timeline, but things may shift based on business needs\n\nIndividual base pay is based on various factors, including work location, relevant experience and skills, the responsibility of the role, and job duties/requirements. In the United States, we have two geographic pay zones. You can confirm the pay zone for your specific location with your recruiter during your interview process. For this role, our current base pay ranges for new hires in each zone are:\n\nZone 1: $155,000 to $175,000 USD annuallyZone 2: $145,000 to $165,000 USD annually\n\nSprout’s compensation ranges are intentionally broad to allow for our team members' growth within their role. These ranges were determined by a market-based compensation approach; we used data from trusted third-party compensation sources to set equitable, consistent, and competitive ranges. We also evaluate compensation bi-annually, identify any changes in the market and make adjustments to our ranges and existing employee compensation as needed.\n\nBase pay is only one element of an employee's total compensation at Sprout. Every Sprout team member has an opportunity to receive restricted stock units (RSUs) under Sprout’s equity plan. Employees (and their dependents) are covered by medical, dental, vision, basic life, accidental death, and dismemberment insurance, and Modern Health (a wellness benefit). Employees are able to enroll in Sprout’s company’s 401k plan, in which Sprout will match 50% of your contributions up to 6% with a maximum contribution. Sprout offers “Flexible Paid Time Off” and ten paid holidays. We have outlined the various components to an employee’s full compensation package here to help you to understand our total rewards package.\n\nSprout Social is proud to be \n\nIf you need a reasonable accommodation for any part of the employment process, please contact us by email at accommodations@sproutsocial.com and let us know the nature of your request and your contact information. We'll do all we can to ensure you're set up for success during our interview process while upholding your privacy, including requests for accommodation. Please note that only inquiries concerning a request for reasonable accommodation will be responded to from this email address.\n\nFor more information about our commitment to \n\nWhen you apply for employment with Sprout Social, we will process your job applicant data, including your employment and education history, transcript, writing samples, and references as necessary to consider your job application for open positions. Your personal data will be shared with Greenhouse Software, Inc., and Crosschq, Inc., cloud services providers located in the United States of America and engaged by Sprout Social to help manage its recruitment and hiring process on Controller’s behalf. Accordingly, if you are located outside of the United States, by clicking “Submit Application” on this site, you consent to the transfer of your personal data to the United States. For more information about our privacy practices please visit our Privacy Policy. California residents have additional rights and should review the Additional Disclosures for California Residents section in our Privacy Policy.\n\nAdditionally, Sprout Social participates in the E-Verify program in certain locations, as required by law.\n\nApply now\n\nBack to all jobs\n\nAbout Sprout\n\nSprout Social is a global leader in social media management and analytics software. Sprout’s intuitive platform offers comprehensive social media management solutions, including publishing and engagement functionality, customer care, influencer marketing, advocacy, and AI-powered business intelligence to more than 30,000 brands. Founded in 2010, Sprout has a hybrid team located across the globe. Sprout Social has been recognized as a Glassdoor Best Places to Work, PEOPLE Companies that Care, Great Place to Work Best Workplace for Parents and more.\n\nSprout Social powers open communication between individuals, brands and communities through elegant, sophisticated software. We are relentless about solving hard problems for our customers and committed to both customer and team success. Our team’s shared belief in Sprout’s mission promotes a culture of openness, empowerment and fun."}
{"text": "Requirements: Bachelor's degree in Computer Science, Engineering, or a related field.8+ Years of experience with data engineering.6+ years of experience working with python.4+ years working with AWS Big Data services, particularly CloudWatch, EKS, KMS, Lambda, and S3.Strong programming skills in languages such as Python, Java, or Scala.Experience building and maintaining large-scale data pipelines using modern ETL tools and frameworks.Solid understanding of data warehousing concepts and technologies.Excellent problem-solving skills and ability to work effectively in a fast-paced environment.AWS certifications (e.g., AWS Certified Big Data - Specialty) are a plus."}
{"text": "experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Senior Advisory Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.\n\nSome Of What You'll Do\n\nCreate and maintain optimal data pipeline architectureAssemble large, complex data sets that meet functional / non-functional business requirementsProactively identify and lead the design and implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Palantir and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Partner with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needsEnsure our data is separated and secure across national and international boundaries through multiple data centersWork with data and analytics experts to strive for greater functionality in our data systemsBecome an SME in Data Engineering and mentor peers on appropriate technical methodologies and implementations\n\nRequirements\n\nBachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field Minimum of 12+ years of experience in a Data Engineer role or related fieldExperience using the following software/tools: Big data tools: Hadoop, Palantir, Spark, Kafka, etc. Relational SQL: Postgres, Oracle, etc. Data pipeline and workflow management tools: StreamSets, Palantir Foundry, etc. Stream-processing systems: Storm, Spark-Streaming, Apache Flink etc. Object-oriented/object function scripting languages: Python, Perl, etc.Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databasesExperience building and optimizing ‘big data’ data pipelines, architectures, and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvementStrong analytic skills related to working with unstructured datasetsAbility to build processes supporting data transformation, data structures, metadata, dependency, and workload managementA successful history of manipulating, processing, and extracting value from large, disconnected datasetsWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data storesStrong project management and organizational skillsExperience supporting and working with cross-functional teams in a dynamic environment\n\nWhy You Should Join Us\n\nJoin us as we write a new chapter, guided by world-class leadership. Come be a part of an exciting and growing organization where we offer a competitive total compensation, flexible/remote work and with a leadership team committed to fostering an inclusive, collaborative, and transparent organizational culture.\n\nAt Syniverse connectedness is at the core of our business. We believe diversity, equity, and inclusion among our employees is crucial to our success as a global company as we seek to recruit, develop, and retain the most talented people who want to help us connect the world.\n\nKnow someone at Syniverse?\n\nBe sure to have them submit you as a referral prior to applying for this position."}
{"text": "Experience, & Skills \n\nThis position can be part-time for the right candidate. Prefer full-time, but will consider 27+ hours per week.\n\n\n\n\nPosition Description\n\n\n\n\nExperience with data analytics using Tableau; experience with Power BI HIGHLY DESIRED\n\n\nRecommend site design improvements for workflow optimization, improved communication and easy of data collection/discovery for:Business Process ImprovementKnowledge ManagementPermission ManagementAssist data and metrics collection activities in support monthly reporting and enterprise transformational efforts:Collect metrics and applicable data points from varied enterprise systems in support of consolidated reporting requirementsAssist in data formatting and delivery per customer standardsIdentify opportunities for process efficiencies through automated organization informational workflows and dashboardsTroubleshoot SharePoint problems and issues.Provide SharePoint Content Management and Administrator\nPreferred Education, Experience, & Skills Bachelor's Degree Required\n\n\nPay Information\n\nFull-Time Salary Range: $92290 - $156860\n\nPlease note: This range is based on our market pay structures. However, individual salaries are determined by a variety of factors including, but not limited to: business considerations, local market conditions, and internal equity, as well as candidate qualifications, such as skills, education, and experience.\n\nEmployee Benefits: At BAE Systems, we support our employees in all aspects of their life, including their health and financial well-being. Regular employees scheduled to work 20+ hours per week are offered: health, dental, and vision insurance; health savings accounts; a 401(k) savings plan; disability coverage; and life and accident insurance. We also have an employee assistance program, a legal plan, and other perks including discounts on things like home, auto, and pet insurance. Our leave programs include paid time off, paid holidays, as well as other types of leave, including paid parental, military, bereavement, and any applicable federal and state sick leave. Employees may participate in the company recognition program to receive monetary or non-monetary recognition awards. Other incentives may be available based on position level and/or job specifics.\n\n\nAbout BAE Systems Intelligence & Security BAE Systems, Inc. is the U.S. subsidiary of BAE Systems plc, an international defense, aerospace and security company which delivers a full range of products and services for air, land and naval forces, as well as advanced electronics, security, information technology solutions and customer support services. Improving the future and protecting lives is an ambitious mission, but it’s what we do at BAE Systems. Working here means using your passion and ingenuity where it counts – defending national security with breakthrough technology, superior products, and intelligence solutions. As you develop the latest technology and defend national security, you will continually hone your skills on a team—making a big impact on a global scale. At BAE Systems, you’ll find a rewarding career that truly makes a difference.\n\nIntelligence & Security (I&S), based in McLean, Virginia, designs and delivers advanced defense, intelligence, and security solutions that support the important missions of our customers. Our pride and dedication shows in everything we do—from intelligence analysis, cyber operations and IT expertise to systems development, systems integration, and operations and maintenance services. Knowing that our work enables the U.S. military and government to recognize, manage and defeat threats inspires us to push ourselves and our technologies to new levels.\n\n\n\nOur Commitment To Diversity, Equity, And Inclusion\n\n\nAt BAE Systems, we work hard every day to nurture an inclusive culture where employees are valued and feel like they belong. We are conscious of the need for all employees to see themselves reflected at every level of the company and know that in order to unlock the full potential of our workforce, everyone must feel confident being their best, most sincere self and be equipped to thrive. We provide impactful professional development experiences to our employees and invest in social impact partnerships to uplift communities and drive purposeful change. Here you will find significant opportunities to do meaningful work in an environment intentionally designed to be one where you will learn, grow and belong."}
{"text": "experience developing applications and advance SQL and developer skills. The position requires writing and debugging code, so we are looking for applicants with a basic understanding of Java, HTML, and Python. This role also requires experience using SQL to update and retrieve data. This position is currently remote, with the potential to work in an office in the future. If you want to build your current skills, learn new systems and solve problems creatively, this is your opportunity.\n\nResponsibilities\n\nWork as part of the Marketing Operations Development team to build marketing automation. Utilize Python, Smartsheet, Azure, and Microsoft SQL Server to develop solutions based on stakeholder needsFollow the peer review standards in place to reduce errors  Brainstorm and operationalize ideas for automation to streamline current processes  Create branches in GitHub with requirements template for developer consistency  Manage version control through development, quality assurance, user acceptance testing, and production environments  Ensure the development process is auditable at every step  Write and update SQL query templates for mail file creation  Work with large data sets to create reporting dashboards and manage production files \n\nRequirements\n\n Qualifications \n\n2+ years using SQL to update and retrieve data; OR2+ years working in PythonHave a working knowledge of B2C channel outputs including:Email Service/API (B2C enterprise campaigns, on-demand, and personalized notification through a service provider, such as MailChimp, SendGrid, etc.) SMS text service (push notifications, enterprise messaging, on-demand, and personalized two-way interaction).Possess a thorough understanding of API function and setup; creation and execution of APIs between software/systems specific to digital B2C marketing. Knowledge of software testing methodologies Experience or the willingness to learn low-code platforms Flexibility; ability to adapt to new processes and tools. Willingness to learn new software, systems, and platforms.Bachelor's degree preferred or 3+ years of Business Analysis experience or any equivalent combination of experience and training which provided the required knowledge, understanding, and skill sets needed to perform the role.\nBonus Points \n\nComfortable using InDesign, SmartCommunications, Quadient, or similar software\n\nNote: Employment-based non-immigrant visa sponsorship and/or assistance is not offered for this specific job opportunity. This position will remain posted for a minimum of three business days from the date posted or until a sufficient/appropriate candidate slate has been identified.\n\nCompensation And Benefits\n\nBase salary range and benefits information for this position are being included in accordance with requirements of various state/local pay transparency legislation. Please note that salaries may vary for different individuals in the same role based on several factors, including but not limited to location of the role, individual competencies, education/professional certifications, qualifications/experience, performance in the role and potential for revenue generation (Producer roles only)\n\nCompany Benefits\n\nWTW provides a competitive benefit package which includes the following (eligibility requirements apply):\n\nHealth and Welfare Benefits: Medical (including prescription coverage), Dental, Vision, Health Savings Account, Commuter Account, Health Care and Dependent Care Flexible Spending Accounts, Group Accident, Group Critical Illness, Life Insurance, AD&D, Group Legal, Identify Theft Protection, Wellbeing Program and Work/Life Resources (including Employee Assistance Program)Leave Benefits: Paid Holidays, Annual Paid Time Off (includes paid state/local paid leave where required), Short-Term Disability, Long-Term Disability, Other Leaves (e.g., Bereavement, FMLA, ADA, Jury Duty, Military Leave, and Parental and Adoption Leave), Paid Time Off (Washington State only)Retirement Benefits: Contributory Pension Plan and Savings Plan (401k). All Level 38 and more senior roles may also be eligible for non-qualified Deferred Compensation and Deferred Savings Plans. \n\nAt WTW, we trust you to know your work and the people, tools and environment you need to be successful. The majority of our colleagues work in a ”hybrid” style, with a mix of remote, in-person and in-office interactions dependent on the needs of the team, role and clients. Our flexibility is rooted in trust and “hybrid” is not a one-size-fits-all solution.\n\nWe understand flexibility is key to supporting an inclusive and diverse workforce and so we encourage requests for all types of flexible working as well as location-based arrangements. Please speak to your recruiter to discuss more."}
{"text": "skills: 2-5 y of exp with data analysis/ data integrity/ data governance; PowerBI development; Python; SQL, SOQL\n\nLocation: Juno Beach, FL\nPLEASE SEND LOCAL CANDIDATES ONLY\n\nSeniority on the skill/s required on this requirement: Mid.\n\nEarliest Start Date: ASAP\n\nType: Temporary Project\n\nEstimated Duration: 12 months with possible extension(s)\n\nAdditional information: The candidate should be able to provide an ID if the interview is requested. The candidate interviewing must be the same individual who will be assigned to work with our client. \nRequirements:• Availability to work 100% at the Client’s site in Juno Beach, FL (required);• Experience in data analysis/ data integrity/ data governance;• Experience in analytical tools including PowerBI development, Python, coding, Excel, SQL, SOQL, Jira, and others.\n\nResponsibilities include but are not limited to the following:• Analyze data quickly using multiple tools and strategies including creating advanced algorithms;• Serve as a critical member of data integrity team within digital solutions group and supplies detailed analysis on key data elements that flow between systems to help design governance and master data management strategies and ensure data cleanliness."}
{"text": "Requirements\n\nConditions of Employment\n\nMust be at least 16 years old.Must be a U.S. Citizen.Candidate required to obtain the necessary security/investigation level.Requires a probationary period if the requirement has not been met.\n\n\nQualifications\n\nYou must meet the Basic Requirements and the Specialized Experience to qualify for Data Scientist - Interdisciplinary, as described below.\n\nBasic Requirements For 0800 Engineering Series\n\nBasic Requirements for 1515 Operations Research Series\n\nBasic Requirements For 1529 Mathematical Statistics Series\n\nBasic Requirements for 1530 Statistics Series\n\nBasic Requirements For 1550 Computer Science Series\n\nBasic Requirements for 1560 Data Science Series\n\nIN ADDITION TO MEETING THE BASIC REQUIREMENTS LISTED ABOVE, APPLICANTS MUST ALSO MEET MINIMUM QUALIFICATIONS TO BE CONSIDERED.\n\nApplicants must have 52 weeks of specialized experience equivalent to at least the next lower grade level GS-13 in the Federal Service.\n\nSpecialized Experience is the experience that equipped the applicant with the particular knowledge, skills, and abilities (KSA's) to perform the duties of the position successfully, and that is typically in or related to the position to be filled. To be creditable, specialized experience must have been equivalent to at least the next lower grade level.\n\nQualifying specialized experience for GS-14 includes:\n\nApplying technical or policy knowledge of Artificial Intelligence (AI) generative models and Machine Learning (ML) algorithms.Experience using theories, techniques, and methods of mathematical, statistical, computer, and/or data science to serve as a technical advisor.Experience using data analytics, modeling, and data mining to validate data sources, establish testing, and implement projects with new approaches.Experience defining objectives and goals and formulating/tracking measures of success.\n\n\nEducation\n\nAny applicant falsely claiming an academic degree from an accredited school will be subject to actions ranging from disqualification from federal employment to removal from federal service.\n\nIf your education was completed at a foreign college or university, you must show comparability to education received in accredited educational institutions in the United States and comparability to applicable minimum course work requirements for this position.\n\nClick\n\nAdditional information\n\nThis position is being filled through the Direct Hire Authority. Traditional rating and ranking of applications do NOT apply. The Veteran's preference does not apply.\n\nThis position is inside the bargaining unit. If the duty location is within the Washington, D.C. metropolitan area, the position will be included in the Local 12, AFGE bargaining unit. If the duty location is outside the Washington, D.C. metropolitan area, the position will be included in the NCFLL bargaining unit.\n\nDOL seeks to attract and retain a high-performing and diverse workforce in which employee differences are respected and valued to better meet the varying needs of the diverse customers we serve. DOL fosters a diverse and inclusive work environment that promotes collaboration, flexibility, and fairness so that all individuals can participate and contribute to their full potential.\n\nRefer to these links for more information:\n\nAs a condition of employment, all personnel must undergo a background investigation for access to DOL facilities, systems, information and/or classified materials before they can enter on duty:\n\nBased on agency needs, additional positions may be filled using this vacancy.\n\nThe Department of Labor may use certain incentives and hiring flexibilities, currently offered by the Federal government to attract highly qualified candidates.\n\nThe Fair Chance Act (FCA) prohibits Federal agencies from requesting an applicant's criminal history information before the agency makes a conditional offer of employment. If you believe a DOL employee has violated your rights under the FCA, you may file a complaint of the alleged violation following our agency's complaint process\n\nNote: The FCA does not apply to some positions specified under the Act, such as law enforcement or national security positions.\n\nA career with the U.S. government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding.\n\n\nEligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time or intermittent. Contact the hiring agency for more information on the specific benefits offered."}
{"text": "requirements and provide technical support during and after product implementation.Stay updated on best practices surrounding data strategy to support Gen AI products.\nEducation:\nBachelor’s Degree required\nExperience and Skills:3+ years of relevant work experience Understanding of complex data flows, identification of data processing bottlenecks, and designing and implementing solutions.Ability to assess business rules, collaborate with stakeholders, and perform source-to-target data mapping, design, and review.Proficiency in C#, Python, SQL.Experience working with Azure Functions.Experience working with PowerBI and other Microsoft Power Platform products.Experience in software development in a production environment.Experience in cloud computing and data storage.Experience processing large sets of structured, semi-structured, and unstructured data (cleansing, storage, retrieval).Experience supporting Web Applications is preferred.Proven ability to balance and manage multiple, competing priorities.Collaborative interpersonal skills and ability to work within cross-functional team."}
{"text": "Qualifications:Deep expertise in Data Management, Data Governance, and Data Quality activities, leveraging tools and frameworks to handle large datasets and meet deliverables with precision.Proven track record in implementing and utilizing data management tools such as data quality and metadata catalogs, along with mastery in implementing master data management processes.Hands-on experience with master data management projects, particularly in company or person disambiguation.Ability to curate datasets from diverse sources to drive data governance initiatives and enhance processes.Proficiency in data mining techniques on extensive datasets to bolster data governance quality improvement endeavors.Proficient in SQL and Python, with adeptness in both relational and non-relational databases, including structured and unstructured databases, and preferably graph and other NoSQL databases.Solid grasp of data quality frameworks within data lifecycle management, coupled with a demonstrated ability to lead data quality initiatives and resolve issues.Experience in process enhancement, workflow optimization, and benchmarking, with a knack for evaluating business processes and driving improvements.Skilled in crafting various documents such as functional requirements, data quality rules, and policy definitions.\nIf you're ready to unleash your expertise and drive digital innovation forward, reach out to Brett Middleton at bmiddleton@alinestaffing.com or simply apply to this posting. Let's shape the future of data together! 🚀"}
{"text": "Qualifications:\n\n7+ years of experience in data science or analytics roles, with a focus on analytics and machine learning.Expertise in programming languages such as Python, R, or SQL for data extraction, cleaning, and analysis.Expertise in working with machine data / time series data Excellent communication skills to effectively convey complex technical concepts to non-technical stakeholders.Strong analytical and problem-solving skills to derive insights from large datasets.Bachelor's degree in data science, computer science, statistics, or a related field (master’s or PhD preferred)\n\nKey Competencies: \n\nExpertise in statistics, supervised and unsupervised machine learning techniques and their appropriate uses; ability to apply common modeling best practices to build models using high-volume, asynchronous time series dataStrategic Thinking- Ability to develop and implement a strategic framework on how to deploy Artificial Intelligence within HRCustomer focus- The need to design solutions with a customer first perspective. The ability meet customers where they are, understand business needs and co-create solutionsAttention to detail- A natural disposition to distrust all data. The need to quality check every number is critical given the importance of the information we own and the seniority of leaders information flows toDelivering Results- Ability to independently deliver results consistently with a focus on incremental value\n\nBase Compensation Range is $142,800 to $189,200\n\nDisclaimer: This base salary range is based on US national averages. Actual base pay could be a result of seniority, merit, geographic location where the work is performed\n\nWe offer competitive compensation and comprehensive benefits and programs. We are \n\n2403356"}
{"text": "Skills • Expertise and hands-on experience on Spark, and Hadoop echo system components – Must Have • Good and hand-on experience* of any of the Cloud (AWS/GCP) – Must Have • Good knowledge of HiveQL & SparkQL – Must Have Good knowledge of Shell script & Java/Scala/python – Good to Have • Good knowledge of SQL – Good to Have • Good knowledge of migration projects on Hadoop – Good to Have • Good Knowledge of one of the Workflow engines like Oozie, Autosys – Good to Have Good knowledge of Agile Development– Good to Have • Passionate about exploring new technologies – Good to Have • Automation approach – Good to Have \nThanks & RegardsShahrukh KhanEmail: shahrukh@zentekinfosoft.com"}
{"text": "Experience in building robust cloud-based data engineering and curation solutions to create data products useful for numerous applicationsDetailed knowledge of the Microsoft Azure tooling for large-scale data engineering efforts and deployments is highly preferred. Experience with any combination of the following azure tools: Azure Databricks, Azure Data Factory, Azure SQL D, Azure Synapse AnalyticsDeveloping and operationalizing capabilities and solutions including under near real-time high-volume streaming conditions. Hands-on development skills with the ability to work at the code level and help debug hard to resolve issues. A compelling track record of designing and deploying large scale technical solutions, which deliver tangible, ongoing valueDirect experience having built and deployed robust, complex production systems that implement modern, data processing methods at scaleAbility to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle, and to work through problems as they are still being definedDemonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver valueAn ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impactComfort with working with distributed teams on code-based deliverables, using version control systems and code reviewsAbility to conduct data analysis, investigation, and lineage studies to document and enhance data quality and accessUse of agile and devops practices for project and software management including continuous integration and continuous deliveryDemonstrated expertise working with some of the following common languages and tools:Spark (Scala and PySpark), Kafka and other high-volume data toolsSQL and NoSQL storage tools, such as MySQL, Postgres, MongoDB/CosmosDBJava, Python data toolsAzure DevOps experience to track work, develop using git-integrated version control patterns, and build and utilize CI/CD pipelinesWorking knowledge and experience implementing data architecture patterns to support varying business needsExperience with different data types (json, xml, parquet, avro, unstructured) for both batch and streaming ingestionsUse of Azure Kubernetes Services, Eventhubs, or other related technologies to implement streaming ingestionsExperience developing and implementing alerting and monitoring frameworksWorking knowledge of Infrastructure as Code (IaC) through Terraform to create and deploy resourcesImplementation experience across different data stores, messaging systems, and data processing enginesData integration through APIs and/or REST service PowerPlatform (PowerBI, PowerApp, PowerAutomate) development experience a plus\nMinimum Qualifications:\n\nData Engineer I:\n\nBachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or One (1) year equivalent formal training or work experience. Basic knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Basic knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Experience as a junior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.\n\nSponsorship is not available for Data Engineer I role.\n\nData Engineer II:\n\nBachelor's Degree in Computer Science, Information Systems, a related quantitative field such as Engineering or Mathematics or equivalent formal training or work experience. Two (2) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Strong knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Strong knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience as a member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.\n\nSponsorship is not available for Data Engineer II role.\n\nData Engineer III:\n\nBachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Three to Four (3 - 4) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a senior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.\n\nData Engineer Lead:\n\nBachelor’s Degree in Information Systems, Computer Science, or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Five to Seven (5 - 7) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a leader or a senior member of multi-function project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.\n\nDomicile / Relocation Information:\n\nThis position can be domiciled anywhere in the United States.\n\nApplication Criteria:\n\nUpload current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire.\n\nAdditional InformationColorado, Nevada, Connecticut, New York, California, Rhode Island, Washington, Hawaii, Illinois and New Jersey Residents Only - Compensation: Monthly Salary: $6,317.00 - $15,477.00. This compensation range is provided as a reasonable estimate of the current starting salary range for this role. Factors that may be used to determine your actual salary may include but are not limited to your specific skills, your work location, how many years of experience you have, and comparison to other employees already in this role.\n\nBorn out of FedEx, a pioneer that ships nearly 20 million packages a day and manages endless threads of information, FedEx Dataworks is an organization rooted in connecting the physical and digital sides of our network to meet today's needs and address tomorrow's challenges.\n\nWe are creating opportunities for FedEx, our customers, and the world at large by:\n\nExploring and harnessing data to define and solve true problems;Removing barriers between data sets to create new avenues of insight;Building and iterating on solutions that generate value;Acting as a change agent to advance curiosity and performance. \n\nAt FedEx Dataworks, we are making supply chains work smarter for everyone.\n\nEmployee Benefits: medical, dental, and vision insurance; paid Life and AD&D insurance; tuition reimbursement; paid sick leave; paid parental leave, paid vacation, paid military leave, and additional paid time off; geographic pay ranges; 401k with Company match and incentive bonus potential; sales Incentive compensation for selling roles.\n\nDataworks does not discriminate against qualified individuals with disabilities in regard to job application procedures, hiring, and other terms and conditions of employment. Further, Dataworks is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position, to perform the essential functions of the position in question, or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities, unless the accommodation will impose an undue hardship. If a reasonable accommodation is needed, please contact DataworksTalentAcquisition@corp.ds.fedex.com."}
{"text": "experience as a data scientist.Proficient in Python, SQL, Spark, the associated Python and Spark packages commonly used by data scientists.Experience in using data visualization and dashboard tools.Proficient in wrangling and analyzing data with complex relationships and time scale.Strong understanding of and practical experience in a wide range of machine learning algorithms and statistical models.Out-of-the-box thinker and problem solver who can turn ambiguous business problems into clear data-driven solutions that deliver meaningful business impacts.Excellent organizational skills, verbal and written communication skills, and presentation skills.\n\nAbout Us\n\nFanatics is building a leading global digital sports platform. The company ignites the passions of global sports fans and maximizes the presence and reach for hundreds of sports partners globally by offering innovative products and services across Fanatics Commerce, Fanatics Collectibles, and Fanatics Betting & Gaming, allowing sports fans to Buy, Collect and Bet. Through the Fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its Sportsbook and iGaming platform. Fanatics has an established database of over 100 million global sports fans, a global partner network with over 900 sports properties, including major national and international professional sports leagues, teams, players associations, athletes, celebrities, colleges, and college conferences, and over 2,000 retail locations, including its Lids retail business stores. \n\nAs a market leader with more than 18,000 employees, and hundreds of partners, suppliers, and vendors worldwide, we take responsibility for driving toward more ethical and sustainable practices. We are committed to building an inclusive Fanatics community, reflecting and representing society at every level of the business, including our employees, vendors, partners and fans. Fanatics is also dedicated to making a positive impact in the communities where we all live, work, and play through strategic philanthropic initiatives.\n\nOrganization\n\nLaunched in 2021, Fanatics Betting and Gaming is the online and retail sports betting and online gaming business of Fanatics. In August 2023, Fanatics Betting and Gaming closed on the operating businesses of PointsBet USA in eight states to accelerate its plans in the gaming space with additional state closings to continue throughout the year. Using a two-brand approach that features Fanatics Sportsbook and PointsBet, a Fanatics Experience, Fanatics Betting and Gaming now operates in 11 states for legal sports betting, four states for online casino and 42 states for horse racing (ADW). The Fanatics Sportsbook is the most rewarding online sportsbook with up to 5% back in FanCash on every bet and makes being a fan easy with fast signup, easy betting, transparent withdrawals, industry leading search functionality and a curated Discover page with the sports and bets that matter most to a customer. The Fanatics Sportsbook has two retail locations in Ohio and one location in Maryland – all connected to a pro sports team’s stadium.Launched in 2021, Fanatics Betting and Gaming is the online and retail sports betting and online gaming business of Fanatics. In August 2023, Fanatics Betting and Gaming closed on the operating businesses of PointsBet USA in eight states to accelerate its plans in the gaming space with additional state closings to continue throughout the year. Using a two-brand approach that features Fanatics Sportsbook and PointsBet, a Fanatics Experience, Fanatics Betting and Gaming now operates in 11 states for legal sports betting, four states for online casino and 42 states for horse racing (ADW). The Fanatics Sportsbook is the most rewarding online sportsbook with up to 5% back in FanCash on every bet and makes being a fan easy with fast signup, easy betting, transparent withdrawals, industry leading search functionality and a curated Discover page with the sports and bets that matter most to a customer. The Fanatics Sportsbook has two retail locations in Ohio and one location in Maryland – all connected to a pro sports team’s stadium.Launched in 2021, Fanatics Betting and Gaming is the online and retail sports betting and online gaming business of Fanatics. In August 2023, Fanatics Betting and Gaming closed on the operating businesses of PointsBet USA in eight states to accelerate its plans in the gaming space with additional state closings to continue throughout the year. Using a two-brand approach that features Fanatics Sportsbook and PointsBet, a Fanatics Experience, Fanatics Betting and Gaming now operates in 11 states for legal sports betting, four states for online casino and 42 states for horse racing (ADW). The Fanatics Sportsbook is the most rewarding online sportsbook with up to 5% back in FanCash on every bet and makes being a fan easy with fast signup, easy betting, transparent withdrawals, industry leading search functionality and a curated Discover page with the sports and bets that matter most to a customer. The Fanatics Sportsbook has two retail locations in Ohio and one location in Maryland – all connected to a pro sports team’s stadium."}
{"text": "skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers.  ResponsibilitiesCollaborate with subject matter and technical experts to strategize analyses, utilize existing algorithms/tools, and derive actionable insights.Recommend data framework or architecture to optimize data analytics.Use storyboards, wireframes, mockups, and simple prototypes; and develop them into real solutions.Help to develop design specifications and proof-of-concept solutions in response to business insight needs.Ensure that outputs, including data visualization and associated messaging, are of consistent high quality and ready for client presentation.Collect feedback from the application of models, contributing to model refinement and the development of innovative tools/features.Communicate both in writing and verbally with technical and non-technical cross-functional teams and use analytical tools effectively and efficiently.\nQualifications\nPossession of a master’s or Ph.D. degree or equivalent professional experience, with a significant focus on quantitative, computational, descriptive, inferential, and mathematical statistical analytics.Technologies: Node, Typescript, Java, React, JavaScript, PostgreSQL, MongoDB, Github, Git Flow, AWS, Agile, ScrumExperience with machine learning and AI.5+ years experience"}
{"text": "SKILLS – Very Strong, Microsoft Excel (Pivot Tables, Sumifs, Vlookups etc), Data manipulation, Logistics and operations terminology Job SummaryApple AMR Ops Logistics is looking for an experienced Data Analyst to support its Business Analytics team. This position will be responsible for ensuring maintenance and frequent updates to Apple’s internal Shipping Exceptions Management System. The position will work closely with AMR Logistics stakeholders to ensure timely execution of daily jobs by transforming data in Excel into Apple’s internal tools.  Key Responsibilities• Review multiple Excel reports and ensure timely uploads into the Shipping Exceptions Management System• Develop robust data visualizations that will help to answer commonly asked questions quickly and thoroughly about Shipping Exceptions• Identify data anomalies, work to root cause and remediate issues in data collection, storage, transformation, or reporting Key Qualifications1 – 2 years of work experience preferredSkilled in Excel and data manipulation (mandatory)Familiarity with Logistics and Operations terminologyFamiliarity with Business Objects a plusAbility to create cross-platform reportsAbility to turn data into information and insightsHigh-level attention to detail, including the ability to spot data errors and potential issues in Apple’s internal systems Hard Skills:Microsoft Excel (Pivot Tables, Sumifs, Vlookups etc)Good Verbal and Communication skills"}
{"text": "experience, disciplined cloud and data-engineering practices, and cutting-edge artificial intelligence research to achieve quantifiable business impact at unprecedented speed.Some company highlights:Delivered 2.5x growth YoY since its inception in 2013Winner of the \"Machine Learning Partner of the Year\" award from Google for two consecutive years - 2017 and 2018Winner of the \"Social Impact Partner of the Year\" award from Google for 2019Winner of the Data Analytics and Public Sector partner of the year from Google for 2020Headquartered in Boston, with 3000+ Quantiphi professionals across the globeLEADER in The Forrester New Wave Computer Vision Consultancies, Q4 2020Great Places to Work certified in 2021For more details, visit: our Website or our LinkedIn PageRole: Senior Machine Learning EngineerWork location: Remote - USAJob Description:Must have skills:Bachelor&#39;s or Master&#39;s degree in Computer Science, Engineering, Mathematics, or related field.Hands-on technical experience implementing, and developing cloud ML solutions, preferably on Google Cloud Platform (Google Cloud Platform).Hands-on experience with Google Cloud Platform machine learning services, including Vertex AI, BigQuery ML, TensorFlow, and AutoML.Good understanding and experience in developing applications using large language models (LLMs) on Google Cloud, with a preference for Langchain.Experience with GenAI frameworks such as Vertex AI and other open-source platforms suitable for Google Cloud Platform environments.Hands-on experience fine-tuning large language models (LLMs) and Generative AI (GAI) models.Hands-on experience with retrieval augmented generation (RAG) architecture and experience using vector indexing tools on Google Cloud Platform.Strong familiarity with higher-level trends in LLMs and open-source platforms, particularly in the Google Cloud Platform ecosystem.Familiarity with Deep Learning Concepts, including Transformers, BERT, and Attention models, on Google Cloud Platform.Ability to engineer prompts and optimize few-shot techniques to enhance model performance on specific tasks. Proficiency in model evaluation, hyperparameter tuning, and ensuring task generalization and model interpretability on Google Cloud Platform.Response Quality: Collaborate with ML and Integration engineers to leverage LLM&#39;s pre-trained potential, delivering contextually appropriate responses in a user-friendly web app. Thorough understanding of NLP techniques for text representation and modeling on Google Cloud Platform.Ability to effectively design software architecture as required for Google Cloud Platform environments.Experience with workflow orchestration tools such as Google Cloud Composer. Knowledge of a variety of machine learning techniques and their real-world advantages/drawbacks on Google Cloud Platform.Ability to create end-to-end solution architectures for model training, deployment, and retraining using native Google Cloud Platform services such as AI Platform, Cloud Functions, etc.Ability to collaborate effectively with cross-functional teams such as Developers, QA, Project Managers, and other stakeholders to understand their requirements and implement solutions.You have:Ability to develop sophisticated yet simple interpretations and communicate insights to clients that lead to quantifiable business impact.Ability to build deep relationships with clients by understanding their stated but more importantly, latent needs.Hands on experience with Statistics/Machine Learning: Statistical Analysis, Linear/Logistic Regression, Clustering, Natural, Language Processing (NLP), Classification, Cross Validation, Decision Trees, Random Forest, Regularization, Principal, Component Analysis (PCA), Data Mining, Data Visualization, Text, Analytics, Neural Networks, Long Short-Term Memory (LSTM)An understanding of Deep Learning techniques (CNNs, RNNs, GANs, Reinforcement Learning).Ability to think creatively and work well both as part of a team and as an individual contributorStrong communication skills and the ability to simplify the complexity of various algorithmsDemonstrated exceptional abilities in some area of their life and will raise the effectiveness of your team. In short, we are looking for a Difference Maker .It s a bonus if you have:A high-level understanding of automation in a cloud environment- Google Cloud preferred.Experience of working for customers/workloads in the Contact Centers/ Banking domain with use cases.Experience with software development What is in it for you:Be part of the fastest-growing AI-first digital transformation and engineering company in the worldBe a leader of an energetic team of highly dynamic and talented individualsExposure to working with fortune 500 companies and innovative market disruptorsExposure to the latest technologies related to artificial intelligence and machine learning, data and cloud"}
{"text": "experience with bash, python, or equivalent script development, deployment, and execution.  1 + year of Windows experience with remote access and dos shell.  Minimum of 1 year of experience implementing machine learning and NLP models using real-life (“industry”) data  Experience working with deep learning models  Knowledge of statistical techniques and concepts (regression, statistical tests and proper usage, etc.)  Desire and ability to learn and continually expand knowledge in the data science, machine learning, and speech analytics. \n\n\n Desired S kills \n\n Proficiency with one more deep learning libraries (PyTorch, TensorFlow, JAX, etc.)  Experience deploying NLP models in production environments, ensuring scalability and performance  Experience with building and/or fine-tuning large language models  Experience as part of a software organization"}
{"text": "Requirements\n\n Description and Requirements \n\n Role Value Proposition: \n\nThe Platforms & Engineering organization provides modern capabilities and services that are common to the enterprise, that can be used by the various CIO and Business teams. These services range from API platform capabilities, CRM functions, Cloud Data platforms and Developer Productivity tools to name a few. The most recent addition has been with Generative AI capabilities, which has been offered as a platform to drive more reuse and ease the burden for app teams to onboard into using AI capabilities.\n\nAs part of our continued focus on driving efficiencies related to platforms and developer productivity, we are looking for an experienced Principal AI engineer to work as part of a multidisciplinary team with the mission to research, develop, and implement responsible use of AI to provide superior customer service and create operational and market differentiating capabilities to further establish MetLife as a global industry leader in the Insurance and Financial Services industry.\n\nThe Principal AI engineer will utilize deep learning, neuro-linguistic programming, computer vision, chatbots, model fine-tuning, and retrieval augmented generation to help us improve various business outcomes and drive innovation.\n\n Objectives of this role \n\n Manage and direct processes and R&D (research and development) to meet the needs of our AI strategy  Understand company and customer challenges and how integrating AI capabilities can help lead to solutions  Lead cross-functional teams in identifying and prioritizing key areas of business where AI solutions can drive significant business benefit  Analyze and explain AI and machine learning (ML) solutions while setting and maintaining high ethical standards \n\n\nResponsibilities\n\n Advise executives and business leaders on a broad range of technology, strategy, and policy issues associated with AI  Work on functional design, process design (including scenario design, flow mapping), prototyping, testing, training, and defining support procedures, in collaboration with an advanced engineering team, internal partners, and executive leadership  Articulate and document the solutions architecture and lessons learned for each exploration and accelerated incubation  Stay abreast of advancements in the AI and automation market and competitor landscape  Research, develop, and test various language models against internal benchmarks to identify the most optimal model for the given use case \n\n\nPreferred Skills And Qualifications\n\n Bachelor’s Degree in Computer Science, Information Systems, Business Administration, Engineering, or related field.  10+ years of experience in full stack engineering, especially in cloud environments, in a large enterprise  5+ years of Experience with cloud service providers such as Azure (mandatory), AWS & GCP (preferred)  Expertise with the DevSecOps processes and driving improvements to Developer Experience  Two or more years of experience in applying AI to practical and comprehensive technology solutions  Experience with ML, deep learning, TensorFlow, Python, NLP  Knowledge of basic algorithms, object-oriented and functional design principles, and best-practice patterns  Experience in REST API development, NoSQL database design, and RDBMS design and optimization \n\n\nThe wage range for applicants for this position is$140k to $180k. This role is also eligible for annual short-term incentive compensation. MetLife offers a comprehensive benefits program, including healthcare benefits, life insurance, retirement benefits, parental leave, legal plan services and paid time off. All incentives and benefits are subject to the applicable plan terms.\n\n\n\nIf you need an accommodation due to a disability, please email us at accommodations@metlife.com. This information will be held in confidence and used only to determine an appropriate accommodation for the application process.\n\nMetLife maintains a drug-free workplace."}
{"text": "skills in Looker (or other related BI tools), SQL, and programming to rapidly turn data into insights and to develop practical tools and analyses that can help solve complex business problems, transforming volumes of data into actionable information.\n\nYou will be part of a highly skilled and collaborative team that uses human-centered design to implement modern, usable interfaces. The project goal is to achieve better health care, improving population health and lowering cost through enhancements in care. This team is creating a centralized and comprehensive mechanism for tracking and managing the existing and portfolio of systems and to provide a state-of-the-art platform for advanced querying, reporting and analytics.; working on high-impact government projects. We are seeking confident, self-motivated individuals to conduct this valuable work in a fully remote capacity within the USA. This position is open to a mid-level to principal professional depending on experience.\n\nResponsibilities\n\nCreate and maintain datasets from disparate data sources and provide analysis thereof using SQL and LookerEnsuring 508 compliance with all AMS application designs and documentation in accordance with commitments to our SOWUtilizing research and user based approach to propose enhancements to the existing application to increase usability and usage of the application Balance technical data architecture and data flows with end user UX best practices Manage data visualization life cycles. Working in an Agile environment Simplify complex ideas to stakeholders at various levels (e.g. explain technical solutions in a manner that matches the audience) Ability to tell stories with data, educate effectively, and instill confidence, motivating stakeholders to act on recommendationsAbility to influence business decisions and utilizing data at the leadership level Strategic thinker with ability to provide solutions for business problems Excellent problem-solving skills and end-to-end quantitative thinking. Engage with Data Engineering to build out next-level data infrastructure Working with the government stakeholders as well as other stakeholders to capture and document requirements. \n\nBasic Qualifications\n\nBachelor's degree in technological or related field 2+ year of SQL experienceMust be a U.S. citizen (required by federal government for the position)Must have lived in the US 3 full years out of the last 5 years (required by federal government for the position)Candidate must reside in the United States and be able to obtain a Public Trust clearance\n\nProfessional Skills\n\nStrong grasp of statistics, programming, and modeling techniques (machine learning, classification, cluster analysis, data mining, databases, and visualization)Dataset parsing, dataset merging, dataset analysis, dataset synthesisStrong knowledge of databases (relational, OLAP, and NoSQL)Strong technical communication skills; both written and verbal Ability to understand and articulate the “big picture” and simplify complex ideasStrong problem solving and structuring skills\n\nPreferred Qualification\n\nMaster’s degreeExperience in the healthcare industry or in a consulting capacity 3+ years of experience in creating Looker visualizations and dashboards Experience with backend SQL Server management Strong understanding of relational database and data warehousing concepts (e.g. OLAP, dimensional modeling Has extensive experience dashboarding (building out from scratch and adding new features) Any additional Data Science or Statistical Modeling experience is welcome Prior experience working remotely full-time Experience with the following technologies: Jupyter, Spark, Apache Superset or other BI software, AWS cloud computingFederal Government contracting work experience \n\nJob Location: This position requires that the job be performed in the United States. If you accept this position, you should note that ICF does monitor employee work locations and blocks access from foreign locations/foreign IP addresses, and also prohibits personal VPN connections.\n\n#DMX-HES\n\nWorking at ICF\n\nICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.\n\nWe can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are \n\nReasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nRead more about workplace discrimination rights, the Pay Transparency Statement, or our benefit offerings which are included in the Transparency in (Benefits) Coverage Act.\n\nPay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:\n\n$57,737.00 - $98,153.00\n\nNationwide Remote Office (US99)"}
{"text": "requirements and provide data-driven recommendations.Assist in the development and evaluation of business strategies based on data analysis.\n\nData Quality Assurance:\n\nImplement data quality checks and validation processes to ensure accuracy and reliability of data.Proactively identify and resolve data discrepancies or anomalies.\n\nForecasting and Predictive Analysis:\n\nUtilize forecasting models and predictive analytics to support strategic planning and decision-making.Provide insights into potential risks and opportunities based on data analysis.\n\nContinuous Improvement:\n\nStay abreast of industry trends and emerging technologies in data analysis.Recommend and implement improvements to data collection and analysis processes.\n\nRequirements:\n\nRequired Qualifications:\n\n2+ years in consumer products / consumer serviceBachelor’s degree in business or related field.Excellent analytical and problem-solving skills.Strong proficiency in data analysis tools and programming languages (e.g., SQL, Python, R).Familiarity with data visualization tools (e.g., Tableau, Power BI).\n\nPreferred Qualifications:\n\nBi-lingual (English & Spanish)Master’s degree in business administration, finance, or related field\n\nCompetencies: To perform the job successfully, an individual should demonstrate the following competencies in this position; Resourcefulness, Customer Focus, Team Player, Passion, Integrity, Organizational/Planning, Communication, Self-Awareness, Energy, Analytical, Judgement/Decision Making, Detail Oriented\n\nAbout us - The world’s leading water experts, we deliver high-quality water solutions to residential, commercial, and industrial customers. Headquartered in Rosemont, Illinois, Culligan International offers technologically advanced, state-of-the-art water filtration products, including water softeners, water filtration systems, industrial water treatment solutions, drinking water systems, and whole- house filtration systems. We improve water across all aspects of consumers’ lives.\n\nGLOBAL CONSUMER PRODUCTS DIVISION (CPD)\n\nMission Statement: Become the global leader in water transforming systems, by creating world-class, consumer-centric brands that drive penetration of iconic products and deliver recurring revenue through proprietary consumables and omni-channel experiences that empower consumers in choice for their water needs that meet their lifestyle.\n\nWe offer competitive compensation & benefits including: \n\nMedicalDentalVisionLife401(k) & 401k MatchUnlimited PTOAdditional voluntary benefits"}
{"text": "experience to be a key member of our team. This candidate will be working to accelerate SSC’s data-driven investment process by using advanced analytics to help source, evaluate, and monitor potential target acquisitions. This position will be in-office at SSC’s headquarters in Dallas, TX.\nThe ideal candidate is a data practitioner who has experience working with diverse data sets of structured and unstructured data. This individual needs to understand and own all steps of the data aggregation and analysis process and needs to be passionate about using data science to solve real-world problems. In addition to the data analysis portion of this role, this position will also provide an opportunity for involvement in deal sourcing through market mapping and direct outreach to target acquisitions. Familiarity with corporate finance concepts is a plus but is not required for the role, although the ideal candidate must have the desire to learn corporate finance and investing concepts over time.\nQualifications:Bachelor’s degree in a quantitative discipline such as Computer Science, Data Science, Technology, Statistics, Mathematics, Finance, Engineering, or Economics1-2 years of experience in computer programming, data analytics, or data scienceHigh level of proficiency in Python, SQL, API integration and Power BI/Tableau; familiarity with JSON, R and Shiny are a plusDetail-oriented with strong analytical and problem-solving skills, with the ability to translate complex data into actionable insightsEntrepreneurial mindset with ability to independently drive projects and effectively manage parallel workstreamsUnderstand and own all steps of the data aggregation and data analysis process, including extracting structured and unstructured data, transforming data into usable and relevant formats, and performing detailed database-level analyticsHave familiarity with or desire to learn corporate finance and investing concepts Strong communication and collaboration skills, both written and verbal Compensation and Benefits:Competitive and commensurate with experiencePlatinum healthcare benefits401-K"}
{"text": "experience to build machine-learning models. You will collaborate closely with our Field Marketing and Sales stakeholders to solve critical and highly visible business problems with machine learning.You will:You will work with other Data Scientists, Data Engineers, Machine Learning (ML) Engineers, and Business Analysts to support the end-to-end ML lifecycle, from use-case identification through model productionization and business outcome measurement.Play a critical role in growing and maturing our marketing capabilities with machine learning at its coreEngage with business stakeholders to support customer-centric design of solutionsEssential Requirements:U.S. Citizenship is required with a Bachelor’s degree in Data Science, Machine Learning, Statistics, Economics, Marketing Analytics, Finance, other quantitative fields, or equivalent professional experience8+ years of industry experience in machine learning, data science, statistics, or related, including a demonstrated experience in model training, evaluation, validation, implementation, and monitoring5+ years’ experience in statistical programming with at least 2+ years in Python (including packages such as pandas, scikit-learn, or PyTorch) and Jupyter Notebooks; Equivalent experience in R may be acceptableUnderstanding of techniques used for small, biased, and sparse datasets and the ability to gain a rapid understanding of domain data, business processes, and business objectivesAbility to grow strong relationships with Marketing stakeholdersDesired Requirements:Experience with U.S. Federal industry is highly desired; alternatively, experience in highly regulated industries (i.e. finance, insurance, local government, etc.) plusExperience with Cloudera and with model registries (i.e. MLflow) and version control (i.e.GitHub)"}
{"text": "requirements and offer effective solutions. The Senior Procurement Data Analyst provides key analytical support to Category leaders through high-level statistical data analysis and modeling, database development and management, and financial analysis. With a focus on collaboration, this role is responsible for responding to internal requests and using creative and critical thinking skills to identify and implement solutions that will support the data analysis needs of various category teams within the Procurement organization.\n\nPrincipal Responsibilities\n\nWorks with internal stakeholders to identify and analyze data sources from disparate systems to derive insights and turn data into actionable information.Accesses and compiles large amounts of data and applies statistical techniques to analyze the data, forecast, interpret, and quantify trends on various aspects of information.Develops, manages, and maintains key statistical and/or financial databases, using SQL, Excel, Access and/or other tools.Obtains data for cost analysis studies respective to assigned categories and conducts spend analysis as necessary to support sourcing activities.Optimizes category solutions and drives continuous improvement.Analyzes and recommends the use of alternative sources or materials to reduce costs.Analyzes and evaluates suppliers. Prepares related reports and provides analysis to senior procurement leadership and internal stakeholders as appropriate.Compiles and analyzes data to determine the feasibility of buying products and establishes price objectives for contract transactions. Provides team and Procurement leadership with insight into competitive pricing as appropriate.Implements processes to enable optimal category solutions including cost modeling, price/volume analysis, TCO analysis.Aligns with sourcing initiatives and leverages internal and external resources to assist with and perform research to develop the supply base. Develops a comprehensive understanding of the supply base.Identifies, analyzes and creates opportunities to improve/reduce the supply base. This may include, but is not limited to, the identification of processes and controls to ensure data accuracy and consistency.Performs market and industry trend analysis, evaluates supply sources, and assists stakeholders in identifying solutions to their needs.Develops effective management presentation materials based on findings, including professional display of information.Communicates regularly with internal stakeholders to maintain knowledge of the stakeholder’s business and opportunities and anticipate their needs/requests.Uncovers issues, evaluates various courses of action, and promotes the best option for resolution.Conducts complex spend analyses to support sourcing related activities, using intermediate to advanced analytical modeling techniques to understand supplier price and cost.Leads multiple projects simultaneously, including process planning and supplier quality improvement efforts.May coordinate activities of others, including assigning and delegating work.Performs other duties as assigned or apparent.\n\nQualifications\n\n3-5 years of supply chain and/or equivalent experience (i.e. Finance, Engineering, Accounting, Transportation, Operations, etc.).Supply Chain, Continuous Improvement, Six Sigma, or Lean Sigma experience.Advanced proficiency with spreadsheet and database management tools; advanced MS Excel skills including VBA programming, macros, and advanced formulas.Experience with intermediate SQL querying.Experience with MS PowerPoint and Access and eProcurement systems.Experience analyzing and documenting complex business processes or problems using intermediate to advanced statistical and analytical modeling techniques and with a focus on detail, cost drivers, and total cost of ownership.Ability to effectively communicate, both written and verbally, with customers and stakeholders to build strong internal and external relationships while maintaining a high sense of urgency and customer focus.Ability to effectively manage multiple projects or tasks with varying and sometimes competing timelines.Demonstrated experience leading or providing direction to others.\n\nRewarding Compensation And Benefits\n\nEligible employees can elect to participate in:\n\n Comprehensive medical benefits coverage, dental plans and vision coverage. Health care and dependent care spending accounts. Short- and long-term disability. Life insurance and accidental death & dismemberment insurance. Employee and Family Assistance Program (EAP). Employee discount programs. Retirement plan with a generous company match. Employee Stock Purchase Plan (ESPP).\n\nThe statements used herein are intended to describe the general nature and level of the work being performed by an employee in this position, and are not intended to be construed as an exhaustive list of responsibilities, duties and skills required by an incumbent so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of the Company."}
{"text": "experience, this is your chance to make history. The team continues to innovate with delivery speed initiatives for customers with the objective ensuring the client continues to own fast in the minds of our customers.\n\nM-F, 9-5, 3 days a week in office\n\nTHE OPPORTUNITY FOR YOU\n\n As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You will be developing and supporting the analytic technologies that give our customers timely, flexible and structured access to their data. Design, implement and support an analytical infrastructure providing ad-hoc access to large datasets and computing power. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using and technologies. Must possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment. Enjoy working closely with your peers in a group of very smart and talented engineers. Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers Explore and learn the latest technologies to provide new capabilities and increase efficiency\n\nKEY SUCCESS FACTORS\n\n 3-6 years of related experience. Good knowledge of SQL & Python Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Very Strong development experience with notable BI reporting tools (Tableau/Quicksight). A good candidate has strong analytical skills and enjoys working with large complex data sets. A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.\n\nBenefits\n\nCompany-sponsored Health, Dental, and Vision insurance plans.\n\nAdvantis Global is \n\nThis policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\n\n#AG-IT"}
{"text": "requirements and develop solutions that meet those needs.Stay up-to-date with emerging trends and technologies in robotics, machine learning, and UAS technology.\n\nDue to the nature of the work, the selected applicant must be able to work onsite.\n\nQualifications We Require\n\n Bachelor's degree in Computer Engineering, Computer Science, Electrical Engineering, Software Engineering, Mechanical Engineering, Optical Science, Robotics, or related STEM field. A higher-level degree (MS, PhD) in rellevant field may also be considered in lieu of Bachelor's degree. Equivalent experience in lieu of degree must be directly related experience that demonstrate the knowledge, skills, and ability to perform the duties of the job.  Ability to obtain and maintain a DOE Q-level security clearance. \n\nQualifications We Desire\n\n Strong knowledge of computer vision, deep learning, and other machine learning techniques.  Strong written communication skills (e.g., published research in technical journals)  Desire to work on solutions to National Security problems, especially in counter-autonomy and physical security system applications.  Ability to work in a fast-paced environment with multiple priorities and tight deadlines.  Demonstrated ability to perform machine learning related activities such as pipeline development, model explainability, and uncertainty quantification.  Strong teamwork and leadership skills.  Ability to travel domestically and internationally as needed (less than 15% of the time).  Experience in the following:  Python, ROS, and other scripting and scientific computing languages (R, C++, Java, C#)  Simulation software such as Gazebo.  Simulation engines such as Unreal or Unity.  3D modeling software.  Linux/Unix operating systems.  FPGAs.  Familiarity with embedded systems and microcontrollers.  Multi-sensor data fusion and coordination.  Active DOE Q-level or DOD equivalent security clearance. \n\nAbout Our Team\n\nThe Mission of department 6534 is to counter evolving autonomous threats to key national facilities and to improve the performance of physical security systems protecting those sites. We are part of a larger group focused on Autonomy and Unmanned Systems. We address real-world problems through research, development, testing, and evaluation of components and systems to advance the science of physical security. This enables customers to mitigate threats to these facilities by improving the ability to sense, assess, track, and respond to physical incursions. Our work addresses current physical security operational challenges and evolving threats such as unmanned aircraft systems (UAS). We specialize in the testing and evaluation of Counter-UAS (C-UAS) systems, which counter the danger posed by UAS, and we are the C-UAS test agent for DOE, NNSA, and DHS.\n\nPosting Duration\n\nThis posting will be open for application submissions for a minimum of seven (7) calendar days, including the ‘posting date’. Sandia reserves the right to extend the posting date at any time.\n\nSecurity Clearance\n\nSandia is required by DOE to conduct a pre-employment drug test and background review that includes checks of personal references, credit, law enforcement records, and employment/education verifications. Applicants for employment need to be able to obtain and maintain a DOE Q-level security clearance, which requires U.S. citizenship. If you hold more than one citizenship (i.e., of the U.S. and another country), your ability to obtain a security clearance may be impacted.\n\nApplicants offered employment with Sandia are subject to a federal background investigation to meet the requirements for access to classified information or matter if the duties of the position require a DOE security clearance. Substance abuse or illegal drug use, falsification of information, criminal activity, serious misconduct or other indicators of untrustworthiness can cause a clearance to be denied or terminated by DOE, resulting in the inability to perform the duties assigned and subsequent termination of employment.\n\n\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or veteran status and any other protected class under state or federal law.\n\nNNSA Requirements For MedPEDs\n\nIf you have a Medical Portable Electronic Device (MedPED), such as a pacemaker, defibrillator, drug-releasing pump, hearing aids, or diagnostic equipment and other equipment for measuring, monitoring, and recording body functions such as heartbeat and brain waves, if employed by Sandia National Laboratories you may be required to comply with NNSA security requirements for MedPEDs.\n\nIf you have a MedPED and you are selected for an on-site interview at Sandia National Laboratories, there may be additional steps necessary to ensure compliance with NNSA security requirements prior to the interview date.\n\nJob ID: 693235"}
{"text": "Qualifications:Master's degree in Computer Science or Information Systems, Decision Sciences, Statistics, Operations Research, Applied Mathematics, Engineering, or a STEM degr"}
{"text": "requirements to determine feasibility of design within time and cost constraints.Consults with other vehicle development engineers and engineering staff to evaluate interfacing, operational, and performance requirements of overall systems mainly in cockpit.Formulates and designs software systems, using scientific analysis to measure outcomes of designs.\nQualifications:1-2 years previous automotive engineering experience including internship.Bachelor’s degree in mechanical engineering, electrical engineering, computer science, software engineering, or equivalent, or equivalent combination of education and experience.Knowledge and experience of big data analysis or statistical data processing is a plus.Knowledge and experience in Python, C++, or JAVA is a plus.Knowledge and/or certificate around AWS, GCP, or Azure is mandatory.Proof of relevant work via internships or an active Github page in lieu of professional experience is accepted.Communication skills across cultural and language barriers.\nCompensation and Benefit:Individual base salary is determined by factors such as job-related skills, experience, and relevant education or training. In addition to competitive salary, Subaru offers an amazing benefits package that includes:Medical, Dental, Vision Plans Medical, Dental, Vision plans available on your first dayPension and 401K Match Offerings12 Vacation days for the first year. (The amount increases with the length of service.)14 Company Holidays, 3 Floating Holidays, and 5 Sick daysEducation Assistance Program/ Gym Membership AssistanceVehicle Discount Program/ Vehicle Lease Program\nEqual Opportunity:Subaru R&D is"}
{"text": "experienced Meteorological Data Scientist to join our growing data science team. The ideal candidate will have a strong background in ML model development, specifically in the context of renewable power generation and energy demand (load) forecasting.\nKey Qualifications:\n﻿﻿We are looking for either (1) a trained meteorologist that has had significant experience building load/renewable models or (2) a trained data scientist that has worked in the energy/weather space for a considerable amount of time.﻿﻿Proven experience in ML modeling in the context of renewable energy.﻿﻿Strong programming skills in Python, R, or similar languages.﻿﻿Experience with data visualization and analysis tools.﻿﻿Excellent problem-solving abilities and attention to detail.﻿﻿Ability to focus on team rather than individual accolades."}
{"text": "Skills You BringBachelor’s or Master’s Degree in a technology related field (e.g. Engineering, Computer Science, etc.) required with 6+ years of experienceInformatica Power CenterGood experience with ETL technologiesSnaplogicStrong SQLProven data analysis skillsStrong data modeling skills doing either Dimensional or Data Vault modelsBasic AWS Experience Proven ability to deal with ambiguity and work in fast paced environmentExcellent interpersonal and communication skillsExcellent collaboration skills to work with multiple teams in the organization\n\n\nLocation- TX/NC/RIOnly w2 / no C2C / NO OPT/CPT"}
{"text": "experience and drive business outcomes is at the heart of everything FanDuel does, the Director of Data Governance will be responsible for defining the strategy for our data governance vertical in providing well defined, quality, consistent and compliant data available to all stakeholder groups throughout FanDuel. We are looking for a passionate, hands-on Data Governance professional to join our team.\n\nTHE GAME PLAN\n\nEveryone on our team has a part to play\n\nDefine, communicate and execute the data governance strategy to meet the needs of the business as it scales over the next 2-3 yearsEvaluate data quality, data profiling, data lineage and metadata managementEstablish and maintain relationships with stakeholders within the organizationInterpret and enforce data requirements for data governance initiativesMonitor and enforce compliance with legal and security policies and standards for access to dataMonitor and enforce compliance with data governance policies and standardsUse data governance tools to access data quality, integrity and completenessProvide guidance and support to teams on data governance best practicesPut in place the right organizational structure to support the strategy and ensure teams can deliver predictably, at pace and to high qualityIdentify opportunities for new approaches and new technologies that can deliver relevant data, faster to the organizationCollaborate with cross-functional partners in product, engineering, Business units, marketing, finance and legal to define and build data definitions and data stewardshipManage 6-8 team members across multiple office locations and continents. Expectation will be to increase the team in size quickly while not sacrificing quality or pace of output.\n\nTHE STATS\n\nWhat we're looking for in our next teammate\n\nMust be able to lead a team with a diverse set of skill sets including product manager, analysts and engineers.A minimum of 5+ years’ experience of leadership positions in data governance within a technical organizationDemonstrable experience in creating a culture of inclusion, ownership and accountability through role modelling company principlesTrack record in being able to inspire people and teams by creating compelling visions for the futureAccomplished in scaling teams, managing multiple geographic locations and inspiring a team to deliver high quality projects at startup paceDeep technical domain knowledge and have the ability to roll up sleeves to teach and develop team membersExperience with data governance tools such as Alation, Collibra, Databricks Unity catalog, Informatica etc. High familiarity with data platform and applications such as S3, Tableau, Databricks, Redshift and AirflowExperience in supporting your managers to identify, develop and grow talent.Be a thought leader and evangelist of data governance practices to drive adoption and knowledge at all levels of the organization\n\nPlayer Benefits\n\nWe treat our team right\n\nFrom our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:\n\nAn exciting and fun environment committed to driving real growthOpportunities to build really cool products that fans loveMentorship and professional development resources to help you refine your gameBe well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another \n\nFanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to \n\nThe applicable salary range for this position is $194,000 - $255,000 which is dependent on a variety of factors including relevant experience, location, business needs and market demand. This role may offer the following benefits: medical, vision, and dental insurance; life insurance; disability insurance; a 401(k) matching program; among other employee benefits. This role may also be eligible for short-term or long-term incentive compensation, including, but not limited to, cash bonuses and stock program participation. This role includes paid personal time off and 14 paid company holidays. FanDuel offers paid sick time in accordance with all applicable state and federal laws."}
{"text": "experienced team that caters to niche skills demands for customers across various technologies and verticals.\n Role Description\n This is a full-time on-site role for a Data Engineer at Computer Data Concepts, Inc. The Data Engineer will be responsible for day-to-day tasks related to data engineering, data modeling, ETL (Extract Transform Load), data warehousing, and data analytics. The role requires expertise in handling and manipulating large datasets, designing and maintaining databases, and implementing efficient data processing systems.\n Qualifications\n Data Engineering skillsData Modeling skillsETL (Extract Transform Load) skillsData Warehousing skillsData Analytics skillsStrong analytical and problem-solving abilitiesProficiency in programming languages such as Python or SQLExperience with cloud-based data platforms like AWS or AzureKnowledge of data visualization tools like Tableau or PowerBIExcellent communication and teamwork skillsBachelor's degree in Computer Science, Data Science, or a related fieldRelevant certifications in data engineering or related areas"}
{"text": "experience with a minimum of 0+ years of experience in a Computer Science or Data Management related fieldTrack record of implementing software engineering best practices for multiple use cases.Experience of automation of the entire machine learning model lifecycle.Experience with optimization of distributed training of machine learning models.Use of Kubernetes and implementation of machine learning tools in that context.Experience partnering and/or collaborating with teams that have different competences.The role holder will possess a blend of design skills needed for Agile data development projects.Proficiency or passion for learning, in data engineer techniques and testing methodologies and Postgraduate degree in data related field of study will also help. \n\n\nDesirable for the role\n\n\nExperience with DevOps or DataOps concepts, preferably hands-on experience implementing continuous integration or highly automated end-to-end environments.Interest in machine learning will also be advantageous.Experience implementing a microservices architecture.Demonstrate initiative, strong customer orientation, and cross-cultural working.Strong communication and interpersonal skills.Prior significant experience working in Pharmaceutical or Healthcare industry environment.Experience of applying policies, procedures, and guidelines.\n\n\nWhy AstraZeneca?\n\nWe follow all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment.\n\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That’s why we work, on average, a minimum of three days per week from the office. But that doesn't mean we’re not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\n\nCompetitive Salary & Benefits\n\nClose date: 10/05/2024\n\nSo, what’s next! \n\n\nAre you already imagining yourself joining our team? Good, because we can’t wait to hear from you. Don't delay, apply today!\n\n\nWhere can I find out more?\n\nOur Social Media, Follow AstraZeneca on LinkedIn: https://www.linkedin.com/company/1603/\n\nInclusion & Diversity: https://careers.astrazeneca.com/inclusion-diversity\n\nCareer Site: https://careers.astrazeneca.com/"}
{"text": "RequirementsTo perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n• BS in Engineering, Computer Science, Data Science, Information Technology, or equivalent work experience.• Strong data analytic skills and in associated visualization tools like Tableau.• Demonstrated abilities in SQL and relational database interactions.• Skilled in use of MS Office suite - Excel, Word, PowerPoint, Visio.• Excellent oral and written communication skills to facilitate customer support.• Good time management skills to adapt to varying paces of workflow.• Strong attention to detail.• High analytical and problem solving ability.\nPreferred• Basic understanding of power system components and their database models.• Demonstrated working knowledge of Atlassian Confluence and JIRA.• Basic understanding of one-line diagrams and EMS models.• Programming experience geared towards automation and data mining. Python, SAS, and PI experience preferred.• Demonstrated understanding of time series data and linear regression modelling.\nResponsibilities\nGeneration Outage Support:o Maintain daily generation outage report.o Work with System Operators to maintain the reliability of the power grid.o Perform data analysis / validation tasks across various generator operational databases.o Create and maintain various dashboards for visualization of generator operational data.\nLoad Forecasting:o Perform analysis on load and weather impacts related to summer and winter peak periods, or other operational events.o Maintain load forecasting metrics dashboard and support regular operational metrics reports.o Onboard incoming renewable resources (Wind/Solar) into forecasting systems. o Support ongoing improvements to Load Forecast accuracy.\nGas / Electric Coordination:o Coordinate with third-party gas market to understand operational impacts to gas generators in .o Create and maintain various internal gas / electric coordination related informational dashboards and displays.\nGenerator Outreach/Data Collectiono Develop targeted data request questions to generation units to capture relevant data in a widely usable format.o Present and lead discussions around findings of data analysis including data visualization in a reproducible manner and outlier detection.\n\nThank You!"}
{"text": "Qualifications 1 – 2 years of work experience preferredSkilled in Excel and data manipulation (mandatory)Familiarity with Logistics and Operations terminologyFamiliarity with Business Objects a plus Ability to create cross-platform reportsAbility to turn data into information and insightsHigh-level attention to detail, including the ability to spot data errors and potential issues in internal systemsSelf-starter with the ability to work independently Excellent written and verbal communicator, with engaging interpersonal skillsA positive, enthusiastic attitude and a sincere willingness to learn from team members at all levels\nHard Skills:Microsoft Excel (Pivot Tables, Sumifs, Vlookups etc)Good Verbal and Communication skills"}
{"text": "experience integrating Google data into internal systems and connecting Google’s platforms to extract maximum value for both website and digital advertising performance management.\nEssential Functions and ResponsibilitiesManaging the setup and implementation of Plymouth Rock’s Google Marketing PlatformTranslate business needs into technical requirements.Integrating platform data with internal data for enhanced insights into the customer journeyCreate dashboards in Looker Studio and Tableau using data from Google platforms to track website and digital advertising performance.Assist in marketing campaign setup using Google’s best practices for tracking and optimization.Leverage data-driven insights to proactively recommend solutions for optimizing these metrics.Analyze website KPIs, user engagement trends, and conversion data of customer behavior; report recommendations for testing.Collaborate with cross-functional teams and align reporting requirements to ensure data consistency across platforms.Provide leadership and guidance to the team of analysts to accomplish these tasks.\nQualifications and Education10 years of experience in marketing/business analyticsBachelor’s degree (B.A.) in computer science, Information systems, math, or statisticsProven experience leading the establishment of GA4, Google Ads, Google Campaign Manager, Google Tag Manager, Google Search Console, and Looker Studio accounts for mid-size to large organizations.Proven experience managing website and digital advertising performanceFamiliarity with database tools such as SQL, R, Oracle, Tableau, or PythonProficiency in developing custom reports, advanced segments, and business goalsProficient working knowledge of Microsoft Office Suite productsExcellent verbal and written communication skills\n About the CompanyThe Plymouth Rock Company and its affiliated group of companies write and manage over $2 billion in personal and commercial auto and homeowner’s insurance throughout the Northeast and mid-Atlantic, where we have built an unparalleled reputation for service. We continuously invest in technology, our employees thrive in our empowering environment, and our customers are among the most loyal in the industry. The Plymouth Rock group of companies employs more than 1,900 people and is headquartered in Boston, Massachusetts. Plymouth Rock Assurance Corporation holds an A.M. Best rating of “A-/Excellent”."}
{"text": "experience in IT projects supporting governmental agencies. \nMandatory Qualifications:Must have a minimum of two (2) years of experience as a data analyst or in other quantitative analysis or engineering disciplines, such as researcher, data engineer or Business Intelligence analyst.Must possess a bachelor’s degree.\nAdditional Qualifications: minimum of two (2) years of experience with Data quality management tools such as Experian Aperture Studio.A minimum of two (2) years of experience developing data quality goals, metrics, and key performance indicators."}
{"text": "experience3+ years MS SQL Server experienceExceptional T-SQL skills, specifically with the use of sub-queries, aggregate functions and joins.Deep understanding of the principles involved in a relational database (indexing, triggers, execution plans, etc.)Working with the Microsoft .NET Framework and Visual StudioWorking with RESTful applicationsExperience developing and monitoring ETL servicesFamiliarity with SSAS, SSRS, and SSIS is a plusProven background authoring and maintaining ETL jobs from various data sourcesWorking with SQL Server data-tier applications (DAC and DACPAC)Working with SaaS applications and Continuous Integration + Continuous Deployment modelScripting/Programming experience (PowerShell, Perl, Python, etc.)Strong attention to detail is a must as the job will involve lengthy operations against live production data.Accounting industry background or a willingness to procure industry knowledge through independent research."}
{"text": "Skills You Will Bring\n\nBachelor’s degree in statistics, applied mathematics, Computer Science (CS) or Computer Informational Systems (CIS)3+ years’ experience, preferably in manufacturingProficiency with data mining, mathematics, and statistical analysisExperience building and maintaining digital twin, including working with stakeholders to construct model, manage data connectivity and flow, model calibration, and end user interfaceExperience with manufacturing analytical models, using Simca or similar platforms.Familiarity with Power BI, Tableau, Matlab, Minitab, Microsoft Excel, Advanced Pattern Recognition, PI Data Archive Tools, GE Proficy, etcExperience with PI, PI Vision and AF, and Plant ApplicationsComfort working in a dynamic, manufacturing based, results oriented group with several ongoing concurrent projects\n\nThe Benefits You Will Enjoy\n\nPaid time off including Vacation and HolidaysRetirement and 401k Matching ProgramMedical & DentalEducation & Development (including Tuition Reimbursement)Life & Disability Insurance\n\nThe Career You Will Build\n\nLeadership trainingPromotional opportunities\n\nThe Impact You Will Make\n\nWe continue to build a better future for people, the plant, and our company! IP has been a good steward of sustainable practices across communities around the world for more than 120 years. Join our team and you’ll see why our team members say they’re Proud to be IP.\n\nThe Culture You Will Experience\n\nInternational Paper promotes employee well-being by providing safe, caring and inclusive workplaces. You will learn Safety Leadership Principles and have the opportunity to opt into Employee Networking Circles such as IPVets, IPride, Women in IP, and the African American ENC. We invite you to bring your uniqueness, creativity, talents, experiences, and safety mindset to be a part of our increasingly diverse culture.\n\nThe Company You Will Join\n\nInternational Paper (NYSE: IP) is a leading global supplier of renewable fiber-based products. We produce corrugated packaging products that protect and promote goods, and enable worldwide commerce, and pulp for diapers, tissue and other personal care products that promote health and wellness. Headquartered in Memphis, Tenn., we employ approximately 38,000 colleagues globally. We serve customers worldwide, with manufacturing operations in North America, Latin America, North Africa and Europe.\n\nInternational Paper is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law. International Paper complies with federal and state disability laws and makes reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact reasonable.accommodations@ipaper.com or (877) 973-3919.\n\nCedar Rapids IA 52404"}
{"text": "requirementsWork with development teams in building and testing the solutionsMaintain active communication channels with all stakeholders on deliverables and report statusesTrack all outstanding issues and manage them from initiation to production deploymentAbility to multitask and work with multiple teamsQualification:At least 7+ Years working experience in the U.S. Financial Industry as a business or data analystStrong knowledge of Banking and Investment productsStrong communication skills: Both written and oral with technical and non-technical staffFamiliarity with issues workflow management tools such as JIRAAt least 3+ Years working with Databases such as SQL Server & OracleExperience working with a Data management team and monitoring data quality and/or performing data quality issue remediation activities, inclusive of conducting root cause analysisAdvanced Excel skillsDetail oriented, organized, and thoroughAbility to thrive in a team-based environment"}
{"text": "requirements2-6+ years of relevant operational experienceExcellent SQL and Python skills, with additional technical background or education strongly preferredExtremely organized, with an eye for precision and a love for keeping things tidyExperience with SaaS and/or startups highly preferredPrior experience in the restaurant industry is a plus!"}
{"text": "skills, data skills, analytics skills, and chemistry subject matter expertise. Role will engage and have regular discussions with other data scientists, data analysts, chemists, scientists, and internal customers to successfully move a given project forward.\n\nResponsibilities\n\nEssential Functions\n\nProblem Analysis and Project Management\n\n Collaborate with chemists and material scientists on methods and processes to create and manage experimental results using FAIR data principles. Participate in establishing the technical approach for integrating scientific knowledge, formulation science, and machine learning methods to accelerate the development of coatings. Lead project discovery through requirements gathering, analysis, design documentation, and impact analysis for model design. Understand business needs, determine data/model usage goals, and create project plans. Plan and organize tasks, report progress, and coordinate with other team members.  Identify opportunities to create data-driven ML models in R&D.  Identify, lead the implementation of, and validate appropriate statistical/ML models for specific projects in the R&D organization. \n\nData Exploration and Preparation\n\n Apply statistical analysis, machine learning, and visualization techniques to various types of data. Test hypotheses using various quantitative methods.  Display drive and curiosity to understand the business process to its core.  Network with R&D experts to better understand the mechanics that generate data in R&D.  Network with external functional areas to connect and join lab generated data to enterprise data sets.  Perform data discovery and wrangling to run models utilizing experience in data extraction and data pre-processing and manipulation. \n\nMachine Learning\n\n Apply various ML and advanced analytics techniques to perform classification or prediction tasks.  Apply chemical and materials domain knowledge to develop models that accelerate the development of new formulations.  Testing of ML models, such as cross-validation and new data collection.  Keep team appraised of developments in machine learning/AI/statistical research literature that may be of practical use in R&D. \n\nDesign and Deployment\n\nDevelop, debug, refine, deploy, and maintain analytical models using Python (including SimPy, SciPy, SciKit, RDKit, NumPy, and other data science and data visualization libraries in Python), R, and other software development and data science tools, including maintaining and updating existing models. Develop, deploy, and maintain visualizations and interactive reporting/analytics tools for analytical models using Python, Tableau, Visual Components, a [SC1] nd other data visualization tools. Coach peers on advanced statistical and ML techniques. \n\nOther\n\n Train and mentor other R&D staff on data science principles and techniques.  Train peers on specialist data science topics.  Network with internal and external partners.  Upskill yourself (through conferences, publications, courses, local academia, and meetups).  Promote collaboration with other teams within the organization. Encourage reuse of artifacts. \n\nIncidental Functions\n\nEvaluate data services and products: Perform product proof of concept analysis. Assists with various projects as may be required to contribute to the efficiency and effectiveness of the work. Participate in hiring activities and fulfilling affirmative action obligations and ensuring compliance with the \n\nQualifications\n\nFormal Education & Certification\n\n Bachelor’s degree (or foreign equivalent) in a Computer Science, Computer Engineering, or Information Technology field of study (e.g., Information Technology, Electronics and Instrumentation Engineering, Computer Systems Management, Mathematics) or equivalent experience.  Master’s Degree in Data Science, Computer Science, Statistics, Applied Mathematics, or other relevant discipline is preferred.  Significant coursework, training, or experience in Chemistry/Materials Science/Polymer Science or similar discipline preferred. \n\nKnowledge & Experience\n\n 8+ years total Data Science/IT experience.  5+ years of hands-on experience with statistical modeling, machine learning, and artificial intelligence preferably in chemistry, formulation science and/or materials science.  5+ years of hands-on experience with Python language for ML and tasks.  2+ years of hands-on experience with R statistical language.  Database and programming languages experience and data manipulation and integration skills using SQL, Oracle, Hadoop, NoSQL Databases, or similar tools.  Advanced knowledge of data analysis, cleaning, and preparation.  Proven ability in using exploratory analysis and preparing unstructured data to draw conclusions.  Experience designing experiments through statistical approaches such as Design of Experiments or other techniques.  Strong ability to work with both IT and R&D in integrating analytics and data science output into business processes and workflows. \n\nInterpersonal Skills and Characteristics\n\n Excellent verbal and written communications.  Highly responsive and alert to new learning opportunities, growth, and development of technical, interpersonal and business skills.  Motivated to develop objectives and timelines to accomplish goals.  Strong experience supporting and working with cross-functional teams in a dynamic business environment.  Strong collaboration experience with both the business and IT teams to define the business problem, refine the requirements, and design and develop data deliverables accordingly.  Is a confident, energetic self-starter, with strong interpersonal skills.  Has good judgment, a sense of urgency and has demonstrated commitment to high standards of ethics, regulatory compliance, customer service and business integrity.  Flexibility, able to adapt to change and embrace it.  Strong commitment to inclusion and diversity. \n\nThis position is not eligible for sponsorship for work authorization now or in the future, including conversion to H1-B visa.\n\nThis position works in the office three days a week and is eligible to work remotely two days a week.\n\nAbout Us\n\nHere, we believe there’s not one path to success, we believe in careers that grow with you. Whoever you are or wherever you come from in the world, there’s a place for you at Sherwin-Williams. We provide you with the opportunity to explore your curiosity and drive us forward. Sherwin-Williams values the unique talents and abilities from all backgrounds and characteristics. All qualified individuals are encouraged to apply, including individuals with disabilities and Protected Veterans. We’ll give you the space to share your strengths and we want you show us what you can do. You can innovate, grow and discover in a place where you can thrive and Let Your Colors Show!\n\nAt Sherwin-Williams, part of our mission is to help our employees and their families live healthier, save smarter and feel better. This starts with a wide range of world-class benefits designed for you. From retirement to health care, from total well-being to your daily commute—it matters to us. A general description of benefits offered can be found at http://www.myswbenefits.com/ . Click on “Candidates” to view benefit offerings that you may be eligible for if you are hired as a Sherwin-Williams employee.\n\nCompensation decisions are dependent on the facts and circumstances of each case and will impact where actual compensation may fall within the stated wage range. The wage range listed for this role takes into account the wide range of factors considered in making compensation decisions including skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nThe wage range, other compensation, and benefits information listed is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, with or without notice, subject to applicable law.\n\nSherwin-Williams is proud to be an \n\nAs a VEVRAA Federal Contractor, Sherwin-Williams requests state and local employment services delivery systems to provide priority referral of Protected Veterans."}
{"text": "Experience working in AWS environment (S3, Snowflake, EC2, APIs)Skilled in coding languages (Python, SQL, Spark)Ability to thrive in a fast-paced, evolving work environment Experience with BI tools like Tableau, QuicksightPrevious experience building and executing tools to monitor and report on data quality"}
{"text": "Requirements & Day-to-Day:  Design, develop, and support scalable data processing pipelines using Apache Spark and Java/Scala. Lead a talented team and make a significant impact on our data engineering capabilities. Implement and manage workflow orchestration with AirFlow for efficient data processing. Proficiently use SQL for querying and data manipulation tasks. Collaborate with cross-functional teams to gather requirements and ensure alignment with data engineering solutions.  Essential Criteria:  a bachelor’s degree in computer science or another relevant discipline, and a minimum of five years of relevant experience in data engineering. Solid experience with Apache Spark for large-scale data processing. Proficiency in Java or Scala programming languages. Strong knowledge of AirFlow for workflow orchestration. Proficient in SQL for data querying and manipulation."}
{"text": "requirements and data mapping documents into a technical design.Develop, enhance, and maintain code following best practices and standards.Execute unit test plans and support regression/system testing.Debug and troubleshoot issues found during testing or production.Communicate project status, issues, and blockers with the team.Contribute to continuous improvement by identifying and addressing opportunities.\n Qualifications / Skills: Minimum of 5 years of experience in ETL/ELT development within a Data Warehouse.Understanding of enterprise data warehousing best practices and standards.Familiarity with DBT framework.Comfortable with git fundamentals change management.Minimum of 5 years of experience in ETL development.Minimum of 5 years of experience writing SQL queries.Minimum of 2 years of experience with Python.Minimum of 3 years of cloud experience with AWS, Azure or Google.Experience in P&C Insurance or Financial Services Industry preferred.Understanding of data warehousing best practices and standards.Experience in software engineering, including designing and developing systems.\n Education and/or Experience: Required knowledge & skills would typically be acquired through a bachelor’s degree in computer sciences or 5 or more years of related experience in ELT and/or Analytics Engineering."}
{"text": "Experience, & Skills \n\nBS degree or higher in Engineering or a related technical field is required.Each higher-level related degree, i.e., Master’s Degree or Ph.D., may substitute for two years of related experience. Related technical experience may be considered in lieu of education. Degree must be from a university, college, or school which is accredited by an agency recognized by the US Secretary of Education, US Department of Education.Ability to interpret, modify, and create scripts using Python and/or R.Experience data manipulation and cleaning.Coursework in one or more: Algorithms, Artificial Intelligence, Data Science, Machine Learning.Ability to manage multiple priorities, meet deadlines, and follow through on work assignments.Good communication and writing skills.Ability to interface with government customers.Solid organizational skills and attention to detail.Ability to problem-solve.\n\nA security clearance or access with Polygraph is not required to be eligible for this position. However, the applicant must be willing and eligible for submission, depending on program requirements, after an offer is accepted and must be able to maintain the applicable clearance/access.\nPreferred Education, Experience, & Skills \n\nA current, active security clearance.BS degree in Computer Science, Data Science, Mathematics, Statistics, or related field.\n\n\nPay Information\n\nFull-Time Salary Range: $75500 - $93750\n\nPlease note: This range is based on our market pay structures. However, individual salaries are determined by a variety of factors including, but not limited to: business considerations, local market conditions, and internal equity, as well as candidate qualifications, such as skills, education, and experience.\n\nEmployee Benefits: At BAE Systems, we support our employees in all aspects of their life, including their health and financial well-being. Regular employees scheduled to work 20+ hours per week are offered: health, dental, and vision insurance; health savings accounts; a 401(k) savings plan; disability coverage; and life and accident insurance. We also have an employee assistance program, a legal plan, and other perks including discounts on things like home, auto, and pet insurance. Our leave programs include paid time off, paid holidays, as well as other types of leave, including paid parental, military, bereavement, and any applicable federal and state sick leave. Employees may participate in the company recognition program to receive monetary or non-monetary recognition awards. Other incentives may be available based on position level and/or job specifics.\n\n\nAbout BAE Systems Space & Mission Systems BAE Systems, Inc. is the U.S. subsidiary of BAE Systems plc, an international defense, aerospace and security company which delivers a full range of products and services for air, land and naval forces, as well as advanced electronics, security, information technology solutions and customer support services. Improving the future and protecting lives is an ambitious mission, but it’s what we do at BAE Systems. Working here means using your passion and ingenuity where it counts – defending national security with breakthrough technology, superior products, and intelligence solutions. As you develop the latest technology and defend national security, you will continually hone your skills on a team—making a big impact on a global scale. At BAE Systems, you’ll find a rewarding career that truly makes a difference.\n\nHeadquartered in Boulder, Colorado, Space & Mission Systems is a leading provider of national defense and civil space applications, advanced remote sensing, scientific and tactical systems for the U.S. Intelligence, Department of Defense and scientific communities. We continually pioneer ways to innovate spacecraft, mission payloads, optical systems, and other defense and civil capabilities. Powered by endlessly curious people with an unwavering mission focus, we continually discover ways to enable our customers to perform beyond expectation and protect what matters most."}
{"text": "experience where customer success continues to motivate what is next.\n\nNetradyne is committed to building a world-class team of technologists and industry experts to deliver products that improve safety, increase productivity, and optimize collaboration within organizations. With growth exceeding 4x year over year, our solution is quickly being recognized as a significant disruptive technology – that has put ‘legacy’ providers in a “spin” cycle trying to catch up. Our team is growing, and we need forward-thinking, uncompromising, competitive team members to continue to facilitate our growth.\n\nAI Engineer - Deep Learning\n\nWe are looking for a highly independent and self-driven Senior Research Engineer who is passionate about pushing the boundaries of deep learning research, to join our fast-growing technology team. This person should be able to work autonomously, think creatively, and explore new ideas and approaches to tackle complex problems in the field. You will have an opportunity to work with very large-scale real-world driving data. Netradyne analyzes over 100 million miles of driving data every month, covering over 1.25 million miles of US roads. This role provides a unique opportunity to work with cutting-edge technology and tackle complex problems in the field of deep learning using vast real-world datasets. The Deep Learning Research Engineer will have the chance to make a significant impact on road safety and advance the field of deep learning research. If you are driven by curiosity and have a passion for innovation, we encourage you to apply.\n\nResponsibilities\n\nDevelop and implement deep learning algorithms to extract valuable insights from large-scale real-world vision data.Design and commercialize algorithms characterizing driving behavior.Innovate and develop proof-of-concept solutions showcasing novel capabilities.\n\n\nRequirements\n\nPh.D. in Computer Science, Electrical Engineering, or a related field with publications in top conferences (CVPR/NeurIPs/ICML/ICLR).Strong background in deep learning, machine learning, and computer vision.Excellent programming skills – Python.Proficiency in PyTorch or TensorFlow.Experience with training large models with huge datasets.Ability to take abstract product concepts and turn them into reality.Location: San Diego, CA - Hybrid\n\n\nDesired Skills\n\nExperience with image, video, and time-series data.Experience with road scene understanding (objects, lanes, interactions, signs, etc.).Experience with person/driver scene understanding (pose, distracted, eye status etc.).Experience with Predictive analytics.\n\n\nOther Essential Abilities and Skills: \n\nStrong analytical and problem-solving skills.Excellent verbal and written communication skills.Energetic or passionate about AI.Ability to work independently and as part of a team.\n\n\nEconomic Package Includes:\n\nSalary $145,000- $180,000Company Paid Health Care, Dental, and Vision CoverageIncluding Coverage for your partner and dependentsThree Health Care Plan OptionsFSA and HSA OptionsGenerous PTO and Sick Leave401(K) Disability and Life Insurance Benefits$50 phone stipend per pay period\n\nSan Diego Pay Range\n\n$145,000—$180,000 USD\n\nWe are committed to an inclusive and diverse team. Netradyne is an equal-opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status, or any legally protected status.\n\nIf there is a match between your experiences/skills and the Company's needs, we will contact you directly.\n\nNetradyne is an equal-opportunity employer.\n\nApplicants only - Recruiting agencies do not contact.\n\nCalifornia Consumer Privacy Act Notice\n\nThis notice applies if you are a resident of California (“California Consumer”) and have provided Personal Information to Netradyne that is subject to the California Consumer Privacy Act (“CCPA”). We typically collect Personal Information in the capacity of a service provider to our clients, who are responsible for providing notice to their employees and contractors and complying with CCPA requirements.\n\nDuring the past 12 months, we have collected the following categories of Personal Information: (a) identifiers; (b) biometric information (see our Biometric Data Privacy Policy for more information); (c) Internet or other electronic network activity information; (d) geolocation data; (e) Audio, electronic, visual, thermal, olfactory, or similar information; (f) professional or employment-related information (from job applicants and from clients regarding their employees and contractors); and (g) education information (from job applicants). We will not discriminate against any person that exercises any rights under the CCPA.\n\nWe have collected this Personal Information for the business purposes and commercial purposes described in this Policy, including to provide the Services to our clients, process job applications, and for marketing and promotion.\n\nThe sources of such Personal Information are you, our clients and our service providers. We have shared this information this only with our clients (if you are an employee or contractor of them) or our service providers.\n\nIf you are a California Consumer, you have the following rights under the CCPA:\n\nYou have the right to request:The categories and specific pieces of your Personal Information that we’ve collected;The categories of sources from which we collected your Personal Information;The business or commercial purposes for which we collected or sold your Personal Information; andThe categories of third parties with which we shared your Personal Information.You can submit a request to us for the following additional information:The categories of third parties to whom we’ve sold Personal Information, and the category or categories of Personal Information sold to each; andThe categories of third parties to whom we’ve disclosed Personal Information, and the category or categories of Personal Information disclosed to each.You can request that we delete the Personal Information we have collected about you, except for situations when that information is necessary for us to: provide you with a product or service that you requested; perform a contract we entered into with you; maintain the functionality or security of our systems; comply with or exercise rights provided by the law; or use the information internally in ways that are compatible with the context in which you provided the information to us, or that are reasonably aligned with your expectations based on your relationship with us.You have the right to request that we not sell your Personal Information. However, we do not offer this opt-out as we do not sell your Personal Information as that term is defined under the CCPA.\n\nYou can make a request under the CCPA by e-mailing us at privacy@netradyne.com We may request additional information from you to verify your identify. You may also designate an authorized agent to submit a request on your behalf. To do so, we will require either (1) a valid power of attorney, or (2) signed written permission from you. In the event your authorized agent is relying on signed written permission, we may also need to verify your identity and/or contact you directly to confirm permission to proceed with the request.\n\nAs noted above, if your request concerns Personal Information collected in our capacity as a service provider to a client, we are not responsible for responding to the request and may send the request to the client for a response.\n\nGoverning law\n\nThis Services are provided in the United States, and are located and targeted to persons in the United States and our policies are directed at compliance with those laws. If you are uncertain whether this Policy conflicts with the applicable local privacy laws where you are located, you should not submit your Personal Information to Netradyne."}
{"text": "Experience\n\nBachelor’s Degree in Statistics, Engineering, Accounting/Finance or related field preferred and 5+ years of relevant experience.In lieu of degree, high school diploma or GED and 4-6 years of relevant experience.Proficient with technology, specifically Microsoft applications such as Access and Excel.Experience with SQL is preferred.Ability to work in a fast paced environment with multiple deadlines.Strong organizational skills and the ability to handle multiple tasks simultaneously.Strong interpersonal skills with the ability to work with internal and external customers.Experience or knowledge in transportation, logistics, parcel shipping or freight pay is preferred.Excellent written and verbal communication skills.\n\nPhysical/Cognitive Requirements\n\nWith or without accommodation:\n\nAbility to follow policies and procedures.Ability to read, write and interpret information.Ability to add, subtract, multiply and divide. Ability to use hands to manipulate, handle, or feel.Ability to sit/walk/stand for up to 8 hours per day. Must possess visual acuity, i.e., close, distance, and color vision, depth perception and the ability to adjust focus.\n\nWorking Conditions\n\nGeneral office environment that is generally favorable. Lighting and temperature are adequate, and there are no hazardous or unpleasant conditions caused by noise, dust, etc. Work is generally performed within an office environment with standard office equipment available.\n\nADA\n\nThe Company is committed to making reasonable accommodations for qualified individuals with disabilities in accordance with the ADA and any other applicable federal, state, or local laws. If you require an accommodation to perform the job, now or in the future, please contact your Human Resources Representative. Upon request, Human Resources will engage in an interactive process with you to determine whether or not a reasonable accommodation is available.\n\nDisclaimer\n\nThe above information is only an illustration of the general nature and level of work performed by the employee within this classification. The omission of specific statements of duties does not exclude them from the position if the work is similar, related or a logical assignment to the position.\n\nThe job description does not constitute an employment agreement between the Company and employee and is subject to change by the Company as the needs of the Company and requirements of the job change.\n\nJob ID: 52079\n\nSchedule: Full-time"}
{"text": "skills for this position are:Natural Language Processing (NLP)Python (Programming Language)Statistical ModelingHigh-Performance Liquid Chromatography (HPLC)Java Job Description:We are seeking a highly skilled NLP Scientist to develop our innovative and cutting-edge NLP/AI solutions to empower life science. This involves working directly with our clients, as well as cross-functional Biomedical Science, Engineering, and Business leaders, to identify, prioritize, and develop NLP/AI and Advanced analytics products from inception to delivery.Key requirements and design innovative NLP/AI solutions.Develop and validate cutting-edge NLP algorithms, including large language models tailored for healthcare and biopharma use cases.Translate complex technical insights into accessible language for non-technical stakeholders.Mentor junior team members, fostering a culture of continuous learning and growth.Publish findings in peer-reviewed journals and conferences.Engage with the broader scientific community by attending conferences, workshops, and collaborating on research projects. Qualifications:Ph.D. or master's degree in biomedical NLP, Computer Science, Biomedical Informatics, Computational Linguistics, Mathematics, or other related fieldsPublication records in leading computer science or biomedical informatics journals and conferences are highly desirable\n\nRegards,Guru Prasath M US IT RecruiterPSRTEK Inc.Princeton, NJ 08540guru@psrtek.comNo: 609-917-9967 Ext:114"}
{"text": "requirements and provide technical guidance.Key Qualifications:Bachelor's degree in computer science or similar degree preferred.3+ years of hands-on experience with cloud providers required.Proficient in SQL, PostGres, ElasticSearch, Redis and ETL.Expert at building performant data pipelines and optimizing existing workflows for new features.Experience with public cloud providers such as Azure, AWS, or GPC.Strong understanding of data warehousing, big data, and data lake concepts.Excellent coding skills in Python, Scala, Java.Have a strong sense of passion and pride for your capabilities and what you create."}
{"text": "requirements necessary for successful member integration into the dialer system. This role involves synthesizing complex datasets to inform strategic decisions and optimize outreach efforts. \nNeeds:4+ years of hands-on experience in data analysis and reporting development, with a focus on deciphering actionable insights from diverse datasets.Advanced proficiency in crafting SQL queries and stored procedures within relational databases, enabling comprehensive data exploration.Intermediate-level competency in MS packages, facilitating clear communication of analytical findings.Strong problem-solving skills to plan, implement, and troubleshoot intricate data analysis processes.Familiarity with statistical tools for robust interpretation of data trends and predictive modeling.Previous exposure to SSIS or similar ETL tools is advantageous for seamless data integration."}
{"text": "Skills: SQL, Python, Databricks, Airflow, Azure/AWS/GCP, Data Asset Management, Data Engineering\n\nDuration: 6+ Months (Possible Extension)\n\nContract Type: W2 Only\n\nLocation: 100% Remote\n\nPay Range:$53.00 - $55.00 per hour on W2\n\n#LP\n\nTALK to a recruiter NOW: Contact Amruta 408-512-2368\n\n“Work where you’re valued and paid what you’re worth”\n\nJob Responsibilities\n\nConduct a thorough inventory of existing data assets, including tables, dashboards, and pipelines, and assess their current use and efficiency.Implement and maintain a centralized metadata management system for improved documentation and access to data asset information.Clean up, restructure, and consolidate data pipelines and tables, adhering to consistent standards and eliminating redundancies.Establish monitoring and alerting systems for critical workflow operations to enhance timely issue detection and resolution.Develop performant data models to support analytics use cases within the Stock Business, ensuring scalability and efficiency.\n\nJOB REQUIREMENTS:\n\nProficient in SQL and Python, with a strong understanding of data processing languages and tools.Extensive experience in data modeling and ETL processes, with familiarity in workflow orchestration tools like Airflow or Databricks.Excellent analytical, problem-solving, and communication skills, with a commitment to high-quality documentation and knowledge transfer.This position requires a proactive and detail-oriented individual with 4-7 years of experience in data engineering and asset management. The role offers the opportunity to significantly contribute to the optimization of our data infrastructure, improving efficiency and accessibility for all stakeholders. If you are a passionate data engineer looking to make a meaningful impact in a dynamic environment, we would love to hear from you.\n\nCALL NOW: Amruta 408-512-2368\n\nAbout Akraya\n\nAkraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. Akraya was recently voted as a 2021 Best Staffing Firm to Temp for by Staffing Industry Analysts and voted by our employees and consultants as a 2022 Glassdoor Best Places to Work."}
{"text": "requirements and translate them into analytical solutions. Provide analytical support for strategic decision-making and problem-solving. Data Quality and Governance: Ensure data quality standards are met and maintained throughout the data lifecycle. Implement data governance practices to ensure data integrity, security, and compliance. Continuous Improvement: Identify opportunities to automate data processes and improve efficiency. Stay updated on industry trends and best practices in data analytics and technology. \nQualifications and Skills: Bachelor’s degree in statistics, mathematics, computer science, economics, or related field. Proficiency in data analysis tools and programming languages (e.g., SQL, Python, Excel). Experience with data visualization tools (e.g., Tableau) to create compelling visualizations and reports. Strong analytical and problem-solving skills with the ability to interpret complex datasets. Knowledge of statistical methods and techniques for data analysis. Database and Data Manipulation Skills: Experience working with relational databases and proficiency in SQL for data retrieval and manipulation. Familiarity with data warehousing concepts Business Acumen: Understanding of business operations and key performance indicators (KPIs). Ability to translate business requirements into data-driven analytical solutions. Communication and Collaboration: Excellent communication skills with the ability to present technical information in a clear and concise manner. Strong interpersonal skills for collaborating with stakeholders across different levels and functions. Attention to Detail and Accuracy: Detail-oriented mindset with a focus on data accuracy, quality, and integrity. Ability to handle multiple tasks and prioritize workload effectively. Problem-Solving Orientation: Proactive approach to identifying issues, analyzing root causes, and proposing solutions.  Genpact is"}
{"text": "QualificationsAbility to gather business requirements and translate them into technical solutionsProven experience in developing interactive dashboards and reports using Power BI (3 years minimum)Strong proficiency in SQL and PythonStrong knowledge of DAX (Data Analysis Expressions)Experience working with APIs inside of Power BIExperience with data modeling and data visualization best practicesKnowledge of data warehousing concepts and methodologiesExperience in data analysis and problem-solvingExcellent communication and collaboration skillsBachelor's degree in Computer Science, Information Systems, or a related fieldExperience with cloud platforms such as Azure or AWS is a plus\nHoursApproximately 15 - 20 hours per week for 3 months with the opportunity to extend the contract further"}
{"text": "experience.\nRequirements:Proven AI research in finance industry. Ideally published with multiple citations. Ph.D./Masters/Bachelor's degree in computer science, mathematics, statistics, engineering, or relevant field from a top 10 university in the US or equivalent. Proficiency in key data science tools and methodologies, including Python, PyTorch, TensorFlow, Jax, Numpy, Scikit-learn, time-series forecasting, classification, regression, large-language models, and experiment design.A commitment to staying abreast of the latest advancements in AI research and a drive to continuously push boundaries.Extensive relevant work experience, encompassing a solid grasp of statistical data analysis, machine learning algorithms, and deep learning frameworks.\nJoin my client on this thrilling journey and contribute to shaping the future of data science and AI in the investment sector."}
{"text": "Qualifications:2-5 YOE in programming with SQL and PythonExperience and/or passion for working in financial servicesFamiliarity with enterprise applicationsBachelor’s Degree in Computer ScienceValue Added (but not required):Experience with Azure data platforms (Synapse, SQL Database, Data Factory and Data Lake)Experience working with data (files, rest APIs, databases) in PythonNo C2C at this time!"}
{"text": "REQUIREMENTS AND SKILLS:• Experience analyzing data sets to find ways to solve problems relating to a business's customers• Familiarity with the eCommerce industry is a plus• Experience using data analyst tools• Experience presenting to peers, management, and other stakeholders• Excellent communication and problem-solving skills• Outstanding organizational skills• Multi-tasking abilities• Detailed oriented• Self-starter• Critical Thinking• Exceptional teamwork and leadership skills to help other technical support workers• Ability to learn new technologies and implement them• Personable and attentive – excellent customer service skills• Strong interpersonal skills• Performs incidental project management, including planning, research, analysis and implementation of deliverables and action items• Manage daily tasks"}
{"text": "skills to provide innovative data solutions and drive business outcomes!\n\nOwn technical aspects of data management functions including creating, loading, transforming, cleansing, processing, analyzing, and visualizing data.Work directly or in support of data science/analytics to design, develop, test and integrate data from various source systems into large-scale, easily consumable data platforms used for providing insights that promotes business growth and efficiency. Build data solution designs, models and infrastructure by applying architectural patterns and principles, data governance and security, researching new technologies and approaches, understanding requirements, all phases of testing, debugging, documenting, quality assurance practices, implementation and maintenance. Demonstrate knowledge of industry trends, create optimized data components and systems that use appropriate development environment. Employ technology (i.e., scripting languages, data movement tools) for efficient system integration.Recommend ways to improve data reliability, efficiency and quality.\n\n\nOperating at the intersection of financial services and technology, Principal builds financial tools that help our customers live better lives. We take pride in being a purpose-led firm, motivated by our mission to make financial security accessible to all. Our mission, integrity, and customer focus have made us a trusted leader for more than 140 years.\n\nAs Principal continues to modernize its systems, this role will offer you an exciting opportunity to build solutions that will directly impact our long-term strategy and tech stack, all while ensuring that our products are robust, scalable, and secure!\n\nWho You Are\n\nBachelor's degree plus 2 + years related work experience or a Master's in a related fieldYou have experience with ETL (extract/transform/load) concepts and tools.You have worked with relational database concepts and table structures and are proficient in writing queries using SQL.You are experienced in development of systems for data extraction, ingestion and processing of large volumes of data.\n\n\nSkills That Will Help You Stand Out\n\nCloud technologies (i.e, AWS, CD/CI pipelines) Python scriptingSnowflakeData warehouse experienceExperience supporting HR Systems such as ICIMS, Oracle human Capital Management, Peoplesoft or other similar environmentsData SecurityTest automation\n\n\nSalary Range Information\n\nSalary ranges below reflect targeted base salaries. Non-sales positions have the opportunity to participate in a bonus program. Sales positions are eligible for sales incentives, and in some instances a bonus plan, whereby total compensation may far exceed base salary depending on individual performance. Actual compensation for all roles will be based upon geographic location, work experience, education, licensure requirements and/or skill level and will be finalized at the time of offer.\n\nSalary Range (Non-Exempt expressed as hourly; Exempt expressed as yearly)\n\n$75000 - $123000 / year\n\nTime Off Program\n\nFlexible Time Off (FTO) is provided to salaried (exempt) employees and provides the opportunity to take time away from the office with pay for vacation, personal or short-term illness. Employees don’t accrue a bank of time off under FTO and there is no set number of days provided.\n\nPension Eligible\n\nYes\n\nAdditional Information\n\nOur Engineering Culture\n\nThrough our product-driven Agile/Lean DevOps environment, we’ve fostered a culture of innovation and experimentation across our development teams. As a customer-focused organization, we work closely with our end users and product owners to understand and rapidly respond to emerging business needs.\n\nCollaboration is embedded into everything we do – from the products we develop to the quality service we provide. We’re driven by the belief that diversity of thought, background, and perspective is critical to creating the best products and experiences for our customers.\n\nWork Environments\n\nThis role offers in-office, hybrid (blending at least three office days in a typical workweek).\n\nJob Level\n\nWe’ll consider talent at the next level with the right experience, background and skill level.\n\nWork Authorization/Sponsorship\n\nAt this time, we're not considering candidates that need any type of immigration sponsorship (additional work authorization or permanent work authorization) now or in the future to work in the United States? This includes, but IS NOT LIMITED TO: F1-OPT, F1-CPT, H-1B, TN, L-1, J-1, etc. For additional information around work authorization needs please use the following links.\n\nNonimmigrant Workers and Green Card for Employment-Based Immigrants\n\nInvestment Code of Ethics\n\nFor Principal Asset Management positions, you’ll need to follow an Investment Code of Ethics related to personal and business conduct as well as personal trading activities for you and members of your household. These same requirements may also apply to other positions across the organization.\n\nExperience Principal\n\nWhile our expertise spans the globe, we're bound by one common purpose: to foster a world where financial security is accessible to all. And our success depends on the unique experiences, backgrounds, and talents of our employees – individually and all of us together. Explore our core values, benefits and why we’re an exceptional place to grow your career.\n\nPrincipal is \n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.\n\nPosting Window\n\nWe will accept applications for 3 full days following the Original Posting Date, after which the posting may remain open or be removed based upon applications received. If we choose to post the job again, we will accept additional applications for at least 1 full day following the Most Recently Posted Date. Please submit applications in a timely manner as there is no guarantee the posting will be available beyond the applicable deadline.\n\nOriginal Posting Date\n\n4/17/2024\n\nMost Recently Posted Date\n\n4/18/2024\n\nLinkedIn Hashtag"}
{"text": "experience as a data engineer or in a similar role using Snowflake rigorouslyAdvanced working PL/SQL or SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing data pipelines, architectures and data sets.Experience with Snowflake.Experience with building data pipeline using Snowflake, AWS, Python.Experience with AWS cloud services,EC2, ECS, S3, Lambda, GlueExperience with building data pipeline on API datasets"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Manager/ ArchitectData Engineering- AWS Cloud to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating them to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and subsystems on production servers\n\n• Mentor, support and manage team members\n\nYour Skills & Experience:\n\n• 8+ years of demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platform- AWS (Amazon Web Services)\n\n• Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension concepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAnnual base pay range: $117,000 - $175,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "requirements, determine technical issues, and design reports to meet data analysis needsDeveloping and maintaining web-based dashboards for real-time reporting of key performance indicators for Operations. Dashboards must be simple to use, easy to understand, and accurate.Maintenance of current managerial reports and development of new reportsDevelop and maintain reporting playbook and change logOther duties in the PUA department as assigned\n\nWhat YOU Will Bring To C&F\n\nSolid analytical and problem solving skillsIntuitive, data-oriented with a creative, solutions-based approachAbility to manage time, multi-task and prioritizes multiple assignments effectivelyAbility to work independently and as part of a teamAble to recognize and analyze business and data issues with minimal supervision, ability to escalate when necessaryAble to identify cause and effect relationships in data and work process flows\n\nRequirements\n\n3 years in an Analyst role is requiredA Bachelor’s degree in associated field of study; data science, computer science, mathematics, economics, statistics, etc. is requiredExperience using SQL is requiredExperience with common data science toolkits is requiredPrior experience creating operations analysis\n\nWhat C&F Will Bring To You\n\nCompetitive compensation packageGenerous 401K employer match Employee Stock Purchase plan with employer matchingGenerous Paid Time OffExcellent benefits that go beyond health, dental & vision. Our programs are focused on your whole family’s wellness including your physical, mental and financial wellbeingA core C&F tenant is owning your career development so we provide a wealth of ways for you to keep learning, including tuition reimbursement, industry related certifications and professional training to keep you progressing on your chosen pathA dynamic, ambitious, fun and exciting work environmentWe believe you do well by doing good and want to encourage a spirit of social and community responsibility, matching donation program, volunteer opportunities, and an employee driven corporate giving program that lets you participate and support your community\n\nAt C&F you will BELONG\n\nWe value inclusivity and diversity. We are committed to \n\nCrum & Forster is committed to ensuring a workplace free from discriminatory pay disparities and complying with applicable pay equity laws. Salary ranges are available for all positions at this location, taking into account roles with a comparable level of responsibility and impact in the relevant labor market and these salary ranges are regularly reviewed and adjusted in accordance with prevailing market conditions. The annualized base pay for the advertised position, located in the specified area, ranges from a minimum of $68,000 to a maximum of $113,300. The actual compensation is determined by various factors, including but not limited to the market pay for the jobs at each level, the responsibilities and skills required for each job, and the employee’s contribution (performance) in that role. To be considered within market range, a salary is at or above the minimum of the range. You may also have the opportunity to participate in discretionary equity (stock) based compensation and/or performance-based variable pay programs."}
{"text": "Qualifications:\n\n Bachelor’s degree  At least 4 years of experience programming with Python, Scala, or Java (Internship experience does not apply)  At least 3 years of experience designing and building data-intensive solutions using distributed computing  At least 2 years of on-the-job experience with an industry recognized ML frameworks (scikit-learn, PyTorch, Dask, Spark, or TensorFlow)  At least 1 year of experience productionizing, monitoring, and maintaining models \n\nPreferred Qualifications:\n\n 1+ years of experience building, scaling, and optimizing ML systems  1+ years of experience with data gathering and preparation for ML models  2+ years of experience developing performant, resilient, and maintainable code  Experience developing and deploying ML solutions in a public cloud such as AWS, Azure, or Google Cloud Platform  Master's or doctoral degree in computer science, electrical engineering, mathematics, or a similar field  3+ years of experience with distributed file systems or multi-node database paradigms  Contributed to open source ML software  Authored/co-authored a paper on a ML technique, model, or proof of concept  3+ years of experience building production-ready data pipelines that feed ML models  Experience designing, implementing, and scaling complex data pipelines for ML models and evaluating their performance \n\n At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "RequirementsBachelor's degree or equivalent experience in finance, economics or data analytics preferred2-4 years work experience of relevant industry experience with extensive financial modeling / data analytics experienceExtensive experience with building reporting & analytic solutions in Power BISophisticated judgment and problem-solving skills based on advanced analytical capabilities in situations with complex scenarios and alternativesExtraordinary interpersonal and communication skills both internally and externally that show a capability of influencing a wide range of audiences on sometimes complicated or delicate issues for potentially major impact on the development function and potentially on Tide Services as a wholeAdvanced proficiency in Microsoft ExcelAbility to work in teams, particularly cross-functionalAdvanced project management skillsEffective conflict resolution skillsAmbition, ownership mentality, and entrepreneurial spirit\nThe Value of a Career with Tide CleanersOngoing coaching and career development – you will work with passionate people and have access to training & knowledgeable managers & peers.We provide a market-competitive salary benchmarked against the finest companies and you'll be able to spend your paid vacation time doing more of the things you love with the people you love.\nAdditional InformationImmigration sponsorship is not available for this role.Agile Pursuits, Inc. d/b/a Tide Services participates in e-verify as required by law.Qualified individuals will not be disadvantaged based on being unemployed.All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor."}
{"text": "experience levels as their sales increase and they confirm product-market fit.\nWhile being good at what you do are table stakes, we believe the following will make you successful in a startup:\nOptimistic individuals are persistent, determined, and unwilling to give up in the face of adversity. They keep working towards a goal or objective despite setbacks or failures. Optimism often involves a combination of perseverance, resilience, and determination.Growth-oriented individuals embrace challenges, persist in the face of setbacks, and see effort as a key to success. They are willing to take risks and push themselves outside of their comfort zones. Growth-oriented individuals are curious, see learning as a lifelong process, and embrace feedback. They are willing to try new things, and are not afraid to change direction if something isn’t working.Effective individuals collaborate well, work consistently and purposefully towards achieving their goals, efficiently manage their time, and are self-motivated. They are able to identify problems, analyze them critically, and develop effective solutions.\nWe are looking for candidates who have at least 2 years of professional experience. They are unable to sponsor visas of any kind currently. \nIf you are interested in working at an early stage company, please apply to learn more."}
{"text": "skills:\n BA/BS degree in finance-related field and/or 2+ years working in finance or related field Strong working knowledge of Microsoft Office (especially Excel) Ability to work in a fast-paced environment and attention to detail. This role includes reviews and reconciliation of financial information.\nGeneral Position Summary\nThe Business Analyst performs professional duties related to the review, assessment and development of business systems and processes as well as new client requirements. This includes reviewing existing processes to develop strong QA procedures as well as maximizing review efficiencies and internal controls through process re-engineering. The Business Analyst will assist with the development of seamless solutions for unique requirements of new clients, delivered and implemented on time and within scope. This role will ensure that all activity, reconciliation, reporting, and analysis is carried out in an effective, timely and accurate manner and will look for continued process improvement and innovation.\nPerks\n Medical, FSA & HSA, Dental, Vision + More! 401k - 100% vested once you start contributing. Generous company match! Regular employee health, wellness & engagement activities! Pet Insurance, because fur babies are important to us too!\nAbout Vervent\nAs one of the pre-eminent Lending as a Service (LaaS) companies, Vervent sets the global standard for outperformance by delivering superior expertise, future-built technology, and meaningful services. We support our industry-leading partners with primary strategic services including Loan & Lease Servicing, Call Center Services, Backup Servicing/Capital Markets Support, Credit Card Servicing, and Card Marketing & Customer Acquisition. Vervent empowers companies to accelerate business, drive compliance, and maximize service.\nIf you’re interested in reviewing the full job description, continue reading below…\nPrimary Responsibilities\nDefine and document client business functions and processes and ensure adherence to investor guidelines and contractual agreements.Develop and flawlessly execute reconciliation and reporting through coordination with clients and internal resources that embodies the mission and policies of the company.Perform ongoing evaluation of process and reconciliation effectiveness for new client onboarding and portfolio updates for existing clients.Develop strong knowledge of sFTP and Sharefile interfaces and utilize tools such as Excel and Power Pivots to ensure continuous process and efficiency improvements.Build strong working relationships with clients, stakeholders, vendors, and team members through effective communication throughout client life cycle.Deliver analytics on the largest clients using Power BI and EDW tools and communicate results and trends to internal stakeholders.Plan, organize and conduct business process reengineering/improvement projects and/or management reviews thorough gap analysis and develop multiple solutions for identified gaps.Refine tools, techniques, and standardization to ensure repeatable results, enhance company effectiveness, client satisfaction, and overall cost efficiency.\nRequirements\nBachelor’s in business management, Finance, Computer Science, or related field and/or 2-5 years of experience in finance or related field, or combination of relevant experience and education.Ability to communicate effectively with various audiences including clients, team members, and vendors, through written and verbal means.Must possess proven leadership skills with the ability to influence key decision makers and collaborate across business lines.Must demonstrate strong analytical skills and ability to translate data into action.Strong working knowledge of computer software including Microsoft Office and Loan Servicing Software required.\nPhysical Requirements\nThe work is of an intellectual nature. While performing the functions of this job, the employee is required to stand and sit for prolonged periods. Specific vision abilities required include close and medium distance vision and the ability to adjust focus. Must be able to hear normal sounds, distinguish sound as voice and communicate through human speech. This position requires the ability to operate a keyboard, computer mouse, telephone, fax, copier, writing tools, and other standard office equipment. On an occasion, an employee will be asked to lift items weighing up to 35 lbs.\nOther Duties\nPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.\nSalary\nSalary range for this role is $66,560 - $73,000 per year"}
{"text": "SKILLS AND EXPERIENCEA successful Lead Data Engineer within this role will likely have the following skills and experience:·Bachelor's degree in Business Administration, Computer Science, Data Systems or Data Analysis preferred·Minimum of 8+ years experience with Snowflake (Columnar MPP Cloud data warehouse)·Experience within DBT·Design and development of Azure/AWS Data Factory Pipelines preferred·Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence\nHOW TO APPLYPlease register your interest by sending your resume to Ben Bochner via Apply link on this page."}
{"text": "experience and skills in our business. That means the role requirements here should be seen as a guide, not a checklist. If you have more, less or different experiences, but really relevant skills, we’d love to hear from you.We work flexibly and will help you to find a healthy balance of remote working and time in our fantastic Southampton office, collaborating, taking part in events and getting to know people that makes working with us so rewarding. We welcome the opportunity to discuss reduced hours and job share arrangements.\nThe RoleWorking as part of the Central Data Management Team the Master Data Analyst will be key to maintaining the accuracy of our corporate data enabling analytics and trending of corporate asset management data to enable key strategic improvements.Reporting into the Master Data Manager the Master Data Analyst will be responsible for the maintenance, development and implementation related to the mastering of core and referential global data elements for Carnival Corporation. The Master Data Analyst will liaise with data owners within Carnival Corporations brands to allow them to gain further insights.Accountable for maintaining corporation data attributes, implementing decisions made by the Maritime Review Acceptance Board (MRAB) council represented by the BrandsEnsure consistent data alignment between vessels. Change requests to be assessed on an individual basis with a view to keeping fleet standardisation as a priorityOrganize change requests and prioritize, ensuring service levels to our customers remains at the highest possible levelsProvide input to stakeholders to enable metadata for Carnival Corporation and the respective brandsOur role categories range from CUK15 (entry level) to CUK1 (Brand President) so you can clearly see internal development opportunities. This role is a CUK09 and is offered on a full time permanent basis. The role can be based in either Southampton, Miami, Seattle or Hamburg, but within each location you would need to be able to access the Carnival Corporation office and work accordingly to our hybrid working (3 days a week on site).\nAbout YouFresh ideas and different perspectives are what excite us most and help us to succeed. Alongside bringing these to the role, you’ll also need:Experience on technical data teams and/or projects Experience in use of master data management systems Expert in Microsoft Office Programs, specifically Microsoft Excel Strong analytical, problem solving and critical thinking skills Experience managing data in the AMOS/ MDM system \nBeing part of our team has its advantages…We’re a holiday company so we know there’s more to life than work. Our comprehensive range of benefits are designed to help your personal and financial health and wellbeing.Home and office-based hybrid working (minimum two office days)Recognition scheme with prizes and awardsEmployee Discounted Cruising plus Friends and Family offersRegular office events including live entertainment, lifestyle events and charity partner fundraisersExtensive learning and development opportunitiesEmployee-led networksEmployee Assistance and Wellbeing programmesCompany paid Health Cash Plan and health assessmentIn-house Occupational Health help and access to digital GPLife AssuranceParental and adoption leaveEmployee Shares PlanElectric Car and Cycle to Work schemesOnsite restaurant offering range of healthy cooked and grab and go mealsDiscounted retail and leisure via discounts portalMinimum 25 days leave, bank holiday allowance and holiday trading schemeContributory Defined Contribution Pension schemeA friendly welcome with help settling inPlease note: Being able to create unforgettable holiday happiness is a brilliant opportunity so we often receive high volumes of applications for our roles. In these cases we may close our job adverts early and aren’t able to consider applications once this happens. \n#LI-Hybrid #LI-JG1About UsHolidays are one of life’s greatest pleasures. Having the chance to relax, escape and explore is a magical thing. And there is no better holiday than a cruise.\nNo one knows cruising like Carnival UK, where talented people from across the globe come together to create unforgettable holiday happiness. As part of the world’s largest holiday travel and leisure company, we take enormous pride in bringing to life two of the most iconic brands from Britain’s rich seafaring heritage, P&O Cruises and Cunard. Collectively they have been delivering unbridled joy, boundless adventure and lifelong memories to millions of people for over 350 years. And in a multi-million pound global holiday market, where cruising has barely scratched the surface, we have the opportunity to do that for many, many more people.\nOur diverse yet tight knit teams share high standards, heartfelt values and passion for our purpose. Our Culture Essentials describe the expectations we have for ourselves and of each other, in building a culture that supports safe, sustainable, compliant operations and celebrates diversity, equity and inclusion.\nIt’s through the successful delivery of these extraordinary travel experiences for our target markets and our distinctive"}
{"text": "skills and training in predictive modeling, data mining and other quantitative and research analytics (Non-Linear Regression Analysis, Multivariate Analysis, Bayesian Methods, Generalized Linear Models, Decision Trees, Non Parametric estimations, etc.).Ability to apply various predictive modeling techniques to develop solutions to various real-world problems.Hands-on experience developing and delivering structured, methodology projects.Exceptional programming ability in SAS, SQL, R, Python or other programming languages.Excellent written and oral communication and presentation skills.In-depth understanding of database principles and experience working with large databases.Ability to influence and guide across departmental boundaries.\nQualifications and Education3 or more years of experience developing and implementing multivariate predictive models using GLM and other statistical methods. PhD in economics, statistics, or related field required.Or in the alternative, a Master’s degree in Statistics, Engineering, Mathematics, Economics, or a related field (foreign educational equivalent accepted) and five (5) years of experience as indicated above.High level of organizational and project management experience handling multiple projects simultaneously.\nAbout the CompanyThe Plymouth Rock Company and its affiliated group of companies write and manage over $2 billion in personal and commercial auto and homeowner’s insurance throughout the Northeast and mid-Atlantic, where we have built an unparalleled reputation for service. We continuously invest in technology, our employees thrive in our empowering environment, and our customers are among the most loyal in the industry. The Plymouth Rock group of companies employs more than 1,900 people and is headquartered in Boston, Massachusetts. Plymouth Rock Assurance Corporation holds an A.M. Best rating of “A-/Excellent”."}
{"text": "Qualifications\nMaster's degree is preferred in a Technical Field, Computer Science, Information Technology, or Business ManagementGood understanding of data structures and algorithms, ETL processing, large-scale data and machine-learning production, data and computing infrastructure, automation and workflow orchestration.Hands-on experience in Python, Pyspark, SQL, and shell scripting or similar programming languagesHands-on Experience in using cloud-based technologies throughout data and machine learning product development.Hands-on experience with code versioning, automation and workflow orchestration tools such as Github, Ansible, SLURM, Airflow and TerraformGood Understanding of data warehousing concepts such as data migration and data integration in Amazon Web Services (AWS) cloud or similar platformExcellent debugging and code-reading skills.Documentation and structured programming to support sustainable development.Ability to describe challenges and solutions in both technical and business terms.Ability to develop and maintain excellent working relationships at all organizational levels."}
{"text": "requirements and develop solutions that meet those needs.· Design and implement scalable and reliable software architectures that can handle large volumes of data and traffic.· Develop and maintain automated testing frameworks to ensure the quality and reliability of software applications.· Stay up-to-date with the latest AI and cloud-native technologies and trends, and apply them to improve software development processes and outcomes.· Work closely with cross-functional teams, including product managers, designers, and other engineers, to deliver high-quality software products.· Participate in code reviews, design reviews, and other team activities to ensure the quality and consistency of software development practices.· Design and implement cloud-based solutions using Azure services such as Azure Functions, Azure App Service, Azure Storage, and Azure Cosmos DB.· Implement and manage Azure DevOps pipelines for continuous integration and deployment of software applications.· Implement and maintain security and compliance controls for Azure resources, including network security groups, Azure Active Directory, and Azure Key Vault.· Collaborate with other teams, including operations and security, to ensure the availability, reliability, and security of Azure-based applications. Selection CriteriaMinimum Education/Experience:· A Master’s degree with 5 years of relevant experience, or a bachelor’s degree with 7 years of relevant experience. Technical Requirements:a) Strong proficiency in data modelling techniques and best practices, with a focus on designing models for AI applications.b) Extensive experience in implementing and optimizing data pipelines using Azure cloud technologies, such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.c) In-depth knowledge of Azure Machine Learning for model deployment, management, and operationalization.d) Proficiency in programming languages commonly used in AI development, such as Python, R, and/or Scala.e) Experience with AI-specific development frameworks and libraries, such as TensorFlow, PyTorch, or scikit-learn.f) Familiarity with Azure Cognitive Services for integrating AI capabilities, such as natural language processing, computer vision, and speech recognition, into applications.g) Strong understanding of SQL and NoSQL databases, particularly Azure SQL Database and Azure Cosmos DB, for efficient data storage and retrieval.h) Experience in data cleansing, reformatting, and transforming tasks, including handling various file formats (CSV, JSON, Parquet, etc.), content types, and structures.i) Proficiency in data profiling techniques and tools to identify data quality issues and anomalies.j) Knowledge of data anonymization and data masking techniques to ensure data privacy and compliance with regulations.k) Familiarity with version control systems, such as Git, for managing code and collaboration.l) Experience in implementing and optimizing machine learning algorithms and models.m) Strong problem-solving skills and the ability to troubleshoot and resolve technical issues related to data engineering and AI development.n) Excellent understanding of cloud computing principles and distributed computing concepts.o) Familiarity with DevOps practices and CI/CD pipelines for automated deployment and testing.p) Strong knowledge of software engineering principles and best practices, including code documentation, testing, and maintainability.q) Ability to work collaboratively in cross-functional teams and effectively communicate technical concepts to non-technical stakeholders."}
{"text": "Skills:2 intermediate analytics skills (BQ/SQL)"}
{"text": "experienced ML engineers and scientists, and define team best practices and processesLead in the ML space by introducing new technologies and techniques, and applying them to Workiva's strategic initiativesCommunicate complex technical issues to both technical and non-technical audiences effectivelyCollaborate with software, data architects, and product managers to design complete software products that meet a broad range of customer needs and requirements\n\n\nEnsure Reliability and Support\n\nDeliver, update, and maintain machine learning infrastructure to meet evolving needsHost ML models to product teams, monitor performance, and provide necessary supportWrite automated tests (unit, integration, functional, etc.) with ML solutions in mind to ensure robustness and reliabilityDebug and troubleshoot components across multiple service and application contexts, engaging with support teams to triage and resolve production issuesParticipate in on-call rotations, providing 24x7 support for all of Workiva’s SaaS hosted environmentsPerform Code Reviews within your group’s products, components, and solutions, involving external stakeholders (e.g., Security, Architecture) \n\n\nWhat You’ll Need\n\nRequired Qualifications\n\nBachelor’s degree in Computer Science, Engineering or equivalent combination of education and experienceMinimum of 4 years in ML engineering or related software engineering experienceProficiency in ML development cycles and toolsets\n\n\nPreferred Qualifications\n\nFamiliarity with Generative AIStrong technical leadership skills in an Agile/Sprint working environmentExperience building model deployment and data pipelines and/or CI/CD pipelines and infrastructureProficiency in Python, GO, Java, or relevant languages, with experience in Github, Docker, Kubernetes, and cloud servicesProven experience working with product teams to integrate machine learning features into the productExperience with commercial databases and HTTP/web protocolsKnowledge of systems performance tuning and load testing, and production-level testing best practicesExperience with Github or equivalent source control systemsExperience with Amazon Web Services (AWS) or other cloud service providersAbility to prioritize projects effectively and optimize system performance\n\n\nWorking Conditions\n\nLess than 10% travelReliable internet access for remote working opportunities \n\n\nHow You’ll Be Rewarded\n\n✅ Salary range in the US: $120,000.00 - $204,000.00\n\n✅ A discretionary bonus typically paid annually\n\n✅ Restricted Stock Units granted at time of hire\n\n✅ 401(k) match and comprehensive employee benefits package\n\nThe salary range represents the low and high end of the salary range for this job in the US. Minimums and maximums may vary based on location. The actual salary offer will carefully consider a wide range of factors, including your skills, qualifications, experience and other relevant factors.\n\nWhere You’ll Work\n\nOur values drive how we work and who we hire. You will see these values ingrained in how we support our customers, work with team members, build our products and in the work environment we’ve created.\n\nWe believe our people are our greatest asset, and our unique culture gives employees the opportunity to make an impact everyday. We give our employees the freedom and resources they need—backed by our culture of collaboration and diverse thought—to continue innovating and breaking new ground. We hire talented people with a wide range of skills and experiences who are eager to tackle some of today’s most challenging problems.\n\nAt Workiva, you’ll enjoy\n\nFantastic Benefits: With coverage starting day one, choose from competitive health, dental, and vision plans on the largest physician networks available.Casual Dress: Workiva has a casual work environment, most people wear jeans to the office.Involvement: Ability to participate in Business Employee Resource Groups (Black, Hispanic, Asian, Women, Rainbow (LGBTQIA+), Veterans, Disabilities), Volunteering, Company wide celebrations, and moreWork-life Balance: We have competitive PTO, VTO and Parental Leave. We encourage employees to spend time enjoying life outside of work.\n\n\nLearn more about life at Workiva: https://www.linkedin.com/company/workiva/\n\nLearn more about benefits:  https://www.workiva.com/careers/benefits\n\nWorkiva is an \n\nWorkiva is committed to working with and providing reasonable accommodations to applicants with disabilities. To request assistance with the application process, please email talentacquisition@workiva.com.\n\nWorkiva employees are required to undergo comprehensive security and privacy training tailored to their roles, ensuring adherence to company policies and regulatory standards.\n\nWorkiva supports employees in working where they work best - either from an office or remotely from any location within their country of employment."}
{"text": "Skills – Python– 6+ Yrs of Exp – Pyspark –6+ Yrs of Exp – Pytorch–6+ Yrs of Exp – GCP –3 + Yrs of Exp – Web development – Prior experience 3+ Years Docker – 4+ Years KubeFlow - 4+ Years   Description: Client is looking for a highly energetic and collaborative Senior Data Scientist with experience building enterprise level GenAI applications, designed and developed MLOps pipelines . The ideal candidate should have deep understanding of the NLP field, hands on experience in design and development of NLP models and experience in building LLM-based applications. Excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and IT leadership team is key to be successful in this role. We are looking for candidates with expertise in Python, Pyspark, Pytorch, Langchain, GCP, Web development, Docker, Kubeflow etc.  Key requirements and transition plan for the next generation of AI/Client enablement technology, tools, and processes to enable Client to efficiently improve performance with scale.  Tools/Skills (hands-on experience is must): • Ability to transform designs ground up and lead innovation in system design • Deep understanding of GenAI applications and NLP field • Hands on experience in the design and development of NLP models • Experience in building LLM-based applications • Design and development of MLOps pipelines • Fundamental understanding on the data science parameterized and non-parameterized algorithms. • Knowledge on AI/Client application lifecycles and workflows. • Experience in the design and development of an Client pipeline using containerized components. • Have worked on at least one Kubernetes cloud offering (EKS/GKE/AKS) or on-prem Kubernetes (native Kubernetes, Gravity, MetalK8s) • Programming experience in Python, Pyspark, Pytorch, Langchain, Docker, Kubeflow • Ability to use observability tools (Splunk, Prometheus, and Grafana ) to look at logs and metrics to diagnose issues within the system. • Experience with Web development Education & Experience: - • 6+ years relevant experience in roles with responsibility over data platforms and data operations dealing with large volumes of data in cloud based distributed computing environments. • Graduate degree preferred in a quantitative discipline (e.g., computer engineering, computer science, economics, math, operations research). • Proven ability to solve enterprise level data operations problems at scale which require cross-functional collaboration for solution development, implementation, and adoption. Regards,Nisha JhaTechnical RecruiterNisha.j@colinatech.com"}
{"text": "skills into a unified team. We seek the best talent to just us in our mission to bring affordable, sustainable, clean energy to all. Come change the world with us.\n\nJob Description\n\nAt Eos Energy Storage, Inc. we believe outstanding employees build the foundation necessary for excellence in our journey toward a carbon-neutral power grid. It is with this belief that we are always looking for the best qualified and most talented employees to become part of our team. Eos Energy Storage offers advancement opportunities.\n\nResponsibilities\n\n Support analysis of project portfolio risks & opportunities Assist Project Director in day to day tasks Assist Projects group in developing & automating excel based tools to measure cost, revenue, other KPI’s Support Project Controls team creating tools to track project & services progress and plan Support Project Controls analytics needs\n\n\nKnowledge, Skills, And Abilities\n\nData analytical skillsAdvanced excelProblem Solving capabilities\n\n\nEducation and Experience\n\nHigh school diploma or equivalent Required\n\nSecond Year Bachelor's Student Majoring in Business Analytics\n\n0 - 6 Months Experience Required\n\nTRAVEL\n\nOvernight/North America Travel Required\n\nLess than 10% Percentage of time spent traveling\n\nWork Hours & Benefits\n\nWe can work with you on your schedule understanding that you are a student first. You can plan on being a part of our 300+ person team working 3 to 5 days per week during normal business hours on your assigned days. A highlight that sets Eos apart is you will be gaining experience in the Renewable Energy Sector and have an opportunity to meet our Senior Leaders as part of your internship. This network building aspect can be vital for the coming days as you approach graduation."}
{"text": "requirements.\n\n Qualifications\n \nStrong analytical skills, with experience in data analysis and statistical techniquesProficiency in data modeling and data visualization toolsExcellent communication skills, with the ability to effectively convey insights to stakeholdersExperience in business analysis and requirements analysisProject management skillsDatabase administration knowledgeBackground in Data Analytics and StatisticsExperience with Big Data technologies like Hadoop"}
{"text": "experience, we offer a uniquely collaborative approach. Clients look to Simtra as an extension of their own companies.\n\nIt is very rewarding industry to work in. Our teams are driven to help clients scale, innovate, and bring-life changing medicines to patients worldwide.\n\nYour Role At Simtra BioPharma Solutions\n\nSimtra Bio Pharma Solutions is looking for an enthusiastic, fast paced data engineer for a ground up implementation of data and BI platform getting data from many Sales, Financial, Planning, Manufacturing and Factory applications to provide best in class analytics and reporting for all levels in a pharma manufacturing company. This position sits on site in Bloomington, Indiana.\n\nThis will report to a Principal Architect and will lead the end-to-end solution of building a cloud data warehouse platform, implementing ETL tools to pull data from source systems, model the data in data warehouse, understanding the business analytics requirement across multiple business functions and delivering the best-in-class enterprise reporting.\n\nWhat You’ll Do\n\nWork with multiple business partners and cross functional IT teams in various functions sales, marketing, finance, purchasing, supply chain and manufacturing to understand their data and reporting needs.Use Azure DevOps to create data stories and demonstrate weekly progress.Create design documents and data modeling.Build data pipelines from the source system to snowflake using Fivetran, DBT cloud, python, PySpark and airflow.Work with offshore consultants explain design and create development tasks for them as well as review their work for completeness and qualityCreate Power BI reports.Test data pipeline code and reports with IT and Business usersSetup DevOps framework using DBT and DevOps repos/Github.Build a DevOps framework in Azure to promote code from lower environment to higher environment.Troubleshoot production issues and provide code fixes.Mentor junior engineers.\n\nWhat You’ll Bring\n\nBachelors degree in Computer Science, Software Engineering, Business Analytics or equivalent combination of education and experience5-7 years of experience in data engineering and business intelligence5+ years of experience in building data pipelines into Azure Data Lake and Snowflake using ETL tools like Fivetran, DBT and Airflow3-5 years of writing code in python to create data pipelines between source system and target data lake.Excellent written and verbal communication skillsExperience in managing onsite and offshore consultants for timely and quality delivery.Strong, hands-on experience in cloud data platforms like SnowflakeHands-on experience with batch and real-time data ingestion with snowflake and ADLS using variety of data setsStrong understanding of data warehouse concepts and hands-on experience in data modeling using Kimball or Data Vault methodologyExperience in building Power BI reports and/or setting up the user administration in Power BI in a big plus.Experience in understanding PLC data in manufacturing facility and building data platforms and analytics on it is a big plus.\n\n#IND-USOPS\n\n\n\nSimtra is \n\n\n\n\n\nPay Transparency Policy\n\nReasonable Accommodations\n\nSimtra is committed to working with and providing reasonable accommodations to individuals with disabilities globally. If, because of a medical condition or disability, you need a reasonable accommodation for any part of the application or interview process, please click on the link here and let us know the nature of your request along with your contact information.\n\nData Privacy\n\nTo learn more about Simtra's approach to data privacy, please review the Simtra Recruitment Platform Global Privacy Policy:\n\nhttps://biopharmasolutions.baxter.com/simtra-recruitment-platform-privacy-policy"}
{"text": "skills will be difficult. The more aligned skills they have, the better.Organizational Structure And Impact:Describe the function your group supports from an LOB perspective:Experienced ML engineer to work on universal forecasting models. Focus on ML forecasting, Python and Hadoop. Experience with Python, ARIMA, FB Prophet, Seasonal Naive, Gluon.Data Science Innovation (DSI) is a very unique application. It is truly ML-driven at its heart and our forecasting models originally looked singularly at cash balance forecasting. That has all changed as we have now incorporated approximately 100 additional financial metrics from our new DSI Metrics Farm. This allows future model executions to become a Universal Forecasting Model instead of being limited to just cash forecasting. It’s a very exciting application, especially since the models have been integrated within a Marketplace concept UI that allows Subscriber/Contributor functionality to make information and processing more personal and with greater extensibility across the enterprise. The application architecture is represented by OpenShift, Linux, Oracle, SQL Server, Hadoop, MongoDB, APIs, and a great deal of Python code.Describe the current initiatives that this resource will be impacting:Working toward implementation of Machine Learning Services.Team Background and Preferred Candidate History:Do you only want candidates with a similar background or would you like to see candidates with a diverse industry background?Diverse industry background, finance background preferred. Manager is more focused on the skillset.Describe the dynamic of your team and where this candidate will fit into the overall environment:This person will work with a variety of titles including application architects, web engineers, data engineers, data scientists, application system managers, system integrators, and Quality Engineers.Will work with various teams, but primarily working with one core team - approx 15 - onshore and offshore resources.Candidate Technical and skills profile:Describe the role and the key responsibilities in order of which they will be doing daily:Machine Learning Engineer that work with Data Scientists in a SDLC environment into production.Interviews:Describe interview process (who will be involved, how many interviews, etc.):1 round - 1 hour minimum, panel style"}
{"text": "SKILLS AND EXPERIENCE4+ years of experience in machine learning and software engineeringMultiple years of experience deploying machine learning and statistical models into real world applicationsExperience writing production level codeGood communication skills and experience working cross functionally with non technical teamsExperience with techniques such as classification, regression, tree-based methods, or anomaly detectionHuge Plus: Experience in pricing or automotive industry!Tools: Python, Spark, Pyspark THE BENEFITSAs a Senior Machine Learning Engineer, you can expect a base salary between $150,000 to $180,000 (based on experience) plus competitive benefits. HOW TO APPLYPlease register your interest by sending your CV to Kristianna Chung via the Apply link on this page"}
{"text": "requirements and data specificationsDevelop, deploy and maintain data processing pipelines using cloud technology such as AWS, Databricks, Kubernetes, Airflow, Redshift, EMRDevelop, deploy and maintain serverless data pipelines using Event Bridge, Kinesis, AWS Lambda, S3 and GlueDefine and manage overall schedule and availability for a variety of data setsWork closely with other engineers to enhance infrastructure, improve reliability and efficiencyMake smart engineering and product decisions based on data analysis and collaborationAct as in-house data expert and make recommendations regarding standards for code quality and timelinessArchitect cloud-based data infrastructure solutions to meet stakeholder needs\n\nSkills & Qualifications\n\nBachelor’s degree in analytics, statistics, engineering, math, economics, science or related discipline4+ years of professional experience in any one of the Cloud data spaces such as AWS, Azure or GCP4+ years experience in engineering data pipelines using big data technologies (Python, pySpark, Real-time data platform like Active MQ or Kafka or Kinesis) on large scale data setsStrong experience in writing complex SQL and ETL development with experience processing extremely large data setsDemonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutionsFamiliarity with AWS Services (S3, Event Bridge, Glue, EMR, Redshift, Lambda)Ability to quickly learn complex domains and new technologiesInnately curious and organized with the drive to analyze data to identify deliverables, anomalies and gaps and propose solutions to address these findingsThrives in fast-paced startup environmentsExperience using Jira, GitHub, Docker, CodeFresh, TerraformExperience contributing to full lifecycle deployments with a focus on testing and qualityExperience with data quality processes, data quality checks, validations, data quality metrics definition and measurement\n\nAt GoodRx, pay ranges are determined based on work locations and may vary based on where the successful candidate is hired. The pay ranges below are shown as a guideline, and the successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, and other relevant business and organizational factors. These pay zones may be modified in the future. Please contact your recruiter for additional information.\n\nSan Francisco And Seattle Offices\n\n$143,000.00 - $229,000.00\n\nNew York Office\n\n$131,000.00 - $210,000.00\n\nSanta Monica Office\n\n$119,000.00 - $191,000.00\n\nOther Office Locations:\n\n$107,000.00 - $172,000.00\n\nGoodRx also offers additional compensation programs such as annual cash bonuses and annual equity grants for most positions as well as generous benefits. Our great benefits offerings include medical, dental, and vision insurance, 401(k) with a company match, an ESPP, unlimited vacation, 13 paid holidays, and 72 hours of sick leave. GoodRx also offers additional benefits like mental wellness and financial wellness programs, fertility benefits, generous parental leave, pet insurance, supplemental life insurance for you and your dependents, company-paid short-term and long-term disability, and more!\n\nWe’re committed to growing and empowering a more inclusive community within our company and industry. That’s why we hire and cultivate diverse teams of the best and brightest from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has a seat at the table and the tools, resources, and opportunities to excel.\n\nWith that said, research shows that women and other underrepresented groups apply only if they meet 100% of the criteria. GoodRx is committed to leveling the playing field, and we encourage women, people of color, those in the LGBTQ+ communities, and Veterans to apply for positions even if they don’t necessarily check every box outlined in the job description. Please still get in touch - we’d love to connect and see if you could be good for the role!\n\nGoodRx is America's healthcare marketplace. The company offers the most comprehensive and accurate resource for affordable prescription medications in the U.S., gathering pricing information from thousands of pharmacies coast to coast, as well as a telehealth marketplace for online doctor visits and lab tests. Since 2011, Americans with and without health insurance have saved $60 billion using GoodRx and million consumers visit goodrx.com each month to find discounts and information related to their healthcare. GoodRx is the #1 most downloaded medical app on the iOS and Android app stores. For more information, visit www.goodrx.com."}
{"text": "Qualifications:Qualifications1. Experience in the followinga. Database: T-SQL, SQL Server,b. Report development in: SSRS and PowerBIc. SCRUM project management practicesd. Data dictionary and requirements documentation2. Strong communication skillsDuties and Responsibilities1. Perform data validation and sanitation to ensure quality, accuracy, and consistency with thebusiness requirements2. Develop data requirements for the new AHIS system.3. Develop and maintain data reports and dashboards to meet the business requirements.4. Work with the department leads to gather data requirements.5. Work with AHIS developers in developing, testing, and maintaining data interfaces.6. Work with AHIS developers and DBA in developing the new AHIS database.7. Provide user support on data issues.8. Perform data analysis9. Maintain data dictionary10. Test and validate data in the new system.\n\nBest RegardsRAM"}
{"text": "Qualifications • Experience in Data Visualization (Tableau, Python required; Splunk a plus), if Javascript and its libraries (e.g., D3, ReactJS, Next.JS) a plus• Strong experience and knowledge of data wrangling with proficient SQL (Trino, Postgres, Oracle required; SparkSQL, Teradata a plus)• Experience using statistics to identify trends and anomalies in datasets using statistical techniques required.• Experience in building robust and scalable data pipelines and ETL jobs with Python, Pandas required; Pyspark and Scala (desired)• Experience in querying data through API (RESTful or GraphQL), using JSON, ProtocolBuffers, or XML desired; if with API development experience a plus• Experience or working knowledge with Big Data technologies such as Hadoop, Hive, HDFS, Parquet, PySpark, and Spark desired\nDescriptionDevelop interactive data visualizations, data pipelines/ETL jobs, and reporting to analyze and present data related to video contents, asset reviews, metadata curations, and operational supports.Closely partner with the internal teams within the AMP Video QC & Metadata Operations organization to define metrics, KPIs, and automation strategy while meeting the teams’ data and reporting needs.Automate and optimize existing data processing workloads by recognizing complex data structures and technology usage patterns and implementing solutions.Focus on scale and efficiency — build and implement innovative data solutions and establish best practices with a start-to-end workflow in mind.\nEducation & ExperienceBachelor or Master's degree in a related field, such as Data Science, Computer Science, Statistics, Mathematics, Business Analytics, Business Administration, or meaningful industry experience preferred\nEqual Opportunity Employer/Veterans/Disabled \nBenefit offerings include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits and 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs which are direct hire to a client\nTo read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.modis.com/en-us/candidate-privacy/ \nThe Company will consider qualified applicants with arrest and conviction records."}
{"text": "Qualifications:Bachelor’s degree or higher in Computer Science, Data Science, Engineering, Mathematics, Applied Statistics, or related field.8 years of experience in building data science and machine learning solutions using Python, Scala, Spark DataBricks, SQL, or similar technologies.Experience in text GenAI & LLM.Deep understanding of probability, statistics, machine learning, anomalies/outliers’ detection, and data correlation/feature analysis.Strong problem-solving skills and algorithm design capabilities.Proficiency in Python coding and familiarity with relevant ML packages.\nMainz Brady Group is a technology staffing firm with offices in California, Oregon and Washington. We specialize in Information Technology and Engineering placements on a Contract, Contract-to-hire and Direct Hire basis. Mainz Brady Group is the recipient of multiple annual Excellence Awards from the Techserve Alliance, the leading association for IT and engineering staffing firms in the U.S.\nMainz Brady Group is"}
{"text": "experience in a highly analytical roleDegree in a quantitative field (e.g., Maths, Engineering)Expert-level proficiency in writing complex SQL queries across large datasetsExpertise in designing metrics and diagnosing data inconsistenciesExperience working with marketplace experiments (causal inference)Proficiency in Python\n\nCompensation packages at Scale include base salary, equity, and benefits. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position, determined by work location and additional factors, including job-related skills, experience, interview performance, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Scale employees are also granted Stock Options that are awarded upon board of director approval. You’ll also receive benefits including, but not limited to: Comprehensive health, dental and vision coverage, retirement benefits, a learning and development stipend, and generous PTO. Additionally, this role may be eligible for additional benefits such as a commuter stipend.\n\nThe base salary range for this full-time position in the locations of San Francisco, New York, Seattle is:\n\n$148,000 — $177,600 USD\n\nAbout Us:\n\nAt Scale, we believe that the transition from traditional software to AI is one of the most important shifts of our time. Our mission is to make that happen faster across every industry, and our team is transforming how organizations build and deploy AI. Our products power the world's most advanced LLMs, generative models, and computer vision models. We are trusted by generative AI companies such as OpenAI, Meta, and Microsoft, government agencies like the U.S. Army and U.S. Air Force, and enterprises including GM and Accenture. We are expanding our team to accelerate the development of AI applications.\n\nWe believe that everyone should be able to bring their whole selves to work, which is why we are proud to be an affirmative action employer and inclusive and equal opportunity workplace. We are committed to \n\nWe are committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities. If you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at accommodations@scale.com. Please see the United States Department of Labor's Know Your Rights poster for additional information.\n\nWe comply with the United States Department of Labor's Pay Transparency provision .\n\nPLEASE NOTE: We collect, retain and use personal data for our professional business purposes, including notifying you of job opportunities that may be of interest and sharing with our affiliates. We limit the personal data we collect to that which we believe is appropriate and necessary to manage applicants’ needs, provide our services, and comply with applicable laws. Any information we collect in connection with your application will be treated in accordance with our internal policies and programs designed to protect personal data."}
{"text": "Job Title: Business AnalystDuration: 3+ Months (Temp. To Perm.)Location: 6 Quakerbridge Plaza, Trenton, NJ 08619Pay Rate: $35/hr. on W2\nNeed to analyze and query dataResearch information related to the dataDevelop understanding of the data to draw meaningful insights and create reports for management presentationCreate an aesthetically pleasing report to give to exec summaryMay also require data entry processing as needed to support business needs"}
{"text": "Qualifications\n\nAWS tools and solutions including Sagemaker, Redshift, AthenaExperience with Machine learning libraries such as PyTorchHands-on experience with designing, developing and deploying workflows with ML models with feedback loops; Uses Bitbucket workflows and has experience with CI/CDDeep experience in at least two of the following languages: PySpark/Spark, Python, CWorking knowledge of AI/ML algorithms. Large language models (LLMs), Retrieval-augmented generation (RAN), Clustering algorithms (such as K-Means), Binary classifiers (such as XGBoost)High level of self-starter, learning, and initiative behaviors Preferred:Background as a software engineer and experience as a data scientistFeatures Stores\n\nWhy Teaching Strategies\n\nAt Teaching Strategies, our solutions and services are only as strong as the teams that create them. By bringing passion, dedication, and creativity to your job every day, there's no telling what you can do and where you can go! We provide a competitive compensation and benefits package, flexible work schedules, opportunities to engage with co-workers, access to career advancement and professional development opportunities, and the chance to make a difference in the communities we serve.\n\nLet's open the door to your career at Teaching Strategies!\n\nSome additional benefits & perks while working with Teaching Strategies\n\nTeaching Strategies offers our employees a robust suite of benefits and other perks which include:\n\nCompetitive compensation package, including Employee Equity Appreciation ProgramHealth insurance benefits401k with employer match100% remote work environmentUnlimited paid time off (which includes paid holidays and Winter Break)Paid parental leaveTuition assistance and Professional development and growth opportunities100% paid life, short and long term disability insurancePre-tax medical and dependent care flexible spending accounts (FSA)Voluntary life and critical illness insurance\n\nTeaching Strategies, LLC is committed to creating a diverse workplace and is proud to be"}
{"text": "skills that will propel your career forward. Your daily routine will include participating in standup meetings, managing work items based on your capacity, collaborating with the team’s Program Managers to define new projects or initiatives, and, most importantly, engaging in development activities. In addition to traditional Data Engineering tasks, you will directly interact with the teams developing the tools we utilize, enabling you to provide direct product feedback and witness your input driving changes in the products over time. Our team is dedicated to reporting on the health of Azure Data products and contributing to their overall effectiveness and utility.\n\nWe do not just value differences or different perspectives. We seek them out and invite them in so we can tap into the collective power of everyone in the company. As a result, our customers are better served.\n\nMicrosoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond\n\nResponsibilities\n\n Extract and transform disparate data into actionable insights that will drive business decisions. Applying industry best practice transformation and modeling techniques – validating the output to ensure reliable and accurate results for consumption by downstream data teams, and end users.Drive development projects to evolve our platform and operations, leveraging the newest Microsoft technologies available to us.Contribute to the success of Azure Data by trying out and testing new products and features, contributing to the design of major feature enhancements, and dedicating time to provide direct feedback to the Product Group.Work directly with business stakeholders to refine requirements, iterate and finalize design, deliver working proofs of concept, and develop final data solution.Live site and on call DRI duty for maintaining high availability of our analytics solutions.Collaborate with your peers and maintain flexibility on domain ownership as business priorities evolve.Work with other data teams within Microsoft on understanding their data products and onboarding them. \n\nEmbody our Culture and Values\n\nQualifications\n\nRequired/Minimum Qualifications\n\nBachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 2+ years experience in business analytics, data science, software development, data modeling or data engineering work.OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering or related field AND 1+ year(s) experience in business analytics, data science, software development, or data engineering work.OR equivalent experience.1+ years experience as a Data Engineer manipulating and transforming data in Spark SQL, PySpark, or Spark Scala.OR 1+ years experience manipulating and transforming data in T-SQL.1+ years experience translating business requirements to technical requirements.\nOther Requirements\n\nAbility to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check:\n\nThis position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.\n\nAdditional Or Preferred Qualifications\n\nBachelor's Degree in Computer Science , Math, Software Engineering, Computer Engineering , or related field AND 5+ years experience in business analytics, data science, software development, data modeling or data engineering work.OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 3+ years of business analytics, data science, software development, data modeling or data engineering work experience.OR equivalent experience.Knowledge and experience with Microsoft data tools including Microsoft Fabric, Azure Data Factory, Azure Synapse, and Azure Databricks.Knowledge and experience with GIT operations and CICD flows.Experience using a work management tool such as Azure DevOps.Experience in time management and prioritization.Effective written & verbal communication skills.Passion for data and the desire to learn & adopt new technologies\nData Engineering IC3 - The typical base pay range for this role across the U.S. is USD $94,300 - $182,600 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $120,900 - $198,600 per year.\n\nCertain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay\n\nMicrosoft will accept applications for the role until April 23, 2024.\n\n#azdat\n\n#azuredata\n\n#fabric\n\nMicrosoft is"}
{"text": "Experience\n\nBachelor’s degree in Economics or equivalentEnglish - highly proficient Ability to work in shifts, Mon - Fri Proficient in Excel Organized and detail oriented SQL/VB – an advantage Knowledgeable in global derivatives markets \n\nSchedule\n\nThis role offers work from home flexibility of up to 2 days per week."}
{"text": "requirements and industry practices for data integrity, security, and accessibility.Develop data set processes for data modeling, mining, and production.Integrate new data management technologies and software engineering tools into existing structures.Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision-making across the organization.Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Participate in data architecture decisions and partner with technology teams to implement data engineering practices and solutions.\n\nQualifications\n\nProven experience as a data engineer, software developer, or similar role, with a strong foundation in data structures, algorithms, and software design.Proficiency in SQL, Python, and other programming languages commonly used in data engineering.Experience with big data tools (Hadoop, Spark, Kafka, etc.) and data pipeline and workflow management tools.Knowledge of cloud services (AWS, Google Cloud Platform, Microsoft Azure) and understanding of database technologies (SQL and NoSQL).Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.A passion for creating data infrastructure that is accessible to individuals of all backgrounds and levels of technical expertise.Excellent communication and teamwork skills, with a willingness to share knowledge and collaborate with team members.A commitment to the values and mission of Believe in Me.\n\nWhy Volunteer With Us?\n\nMak\n\nThis is a volunteer opportunity provided by VolunteerMatch, in partnership with LinkedIn for Good."}
{"text": "Qualifications\n\nEducation and Years of Experience: \n\n Bachelors in related field required  5+ years of data migration experience  Ideally has experience on at least 1 EHR migration project \n\nRequired And Desired Skills/Certifications\n\n Ability to build and optimize data sets, ‘big data’ data pipelines and architectures  Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions  Excellent analytic skills associated with working on unstructured datasets  Ability to build processes that support data transformation, workload management, data structures, dependency and metadata  Confidentiality is a must on this project \n\nAdditional Requirements\n\nMin Citizenship Status Required:\n\nPhysical Requirements: No Physical requirement needed for this position.\n\nLocation:  Baltimore/DC Metro area. Mainly Remote\n\nWorking at DMI\n\nBenefits\n\nDMI is a diverse, prosperous, and rewarding place to work. Being part of the DMI family means we care about your wellbeing. We offer a variety of perks and benefits that help meet various interests and needs, while still having the opportunity to work directly with several of our award-winning, Fortune 1000 clients. The following categories make up your DMI wellbeing:\n\n Convenience/Concierge - Virtual visits through health insurance, pet insurance, commuter benefits, discount tickets for movies, travel, and many other items to provide convenience.  Development – Annual performance management, continuing education, and tuition assistance, internal job opportunities along with career enrichment and advancement to help each employee with their professional and personal development.  Financial – Generous 401k matches both pre-tax and post-tax (ROTH) contributions along with financial wellness education, EAP, Life Insurance and Disability help provide financial stability for each DMI employee.  Recognition – Great achievements do not go unnoticed by DMI through Annual Awards ceremony, service anniversaries, peer-to-peer acknowledgment, employee referral bonuses.  Wellness – Healthcare benefits, Wellness programs, Flu Shots, Biometric screenings, and several other wellness options. \n\nEmployees are valued for their talents and contributions. We all take pride in helping our customers achieve their goals, which in turn contributes to the overall success of the company. The company does and will take affirmative action to employ and advance in employment individuals with disabilities and protected veterans, and to treat qualified individuals without discrimination based on their physical or mental disability or veteran status. DMI is \n\n***************** No Agencies Please *****************\n\nApplicants selected may be subject to a government security investigation and must meet eligibility requirements for access to classified information. US citizenship may be required for some positions.\n\nJob ID:  2024-26595"}
{"text": "skillsMust be U.S. citizen\n\nPreferred Requirements\n\nCoursework and/or experience with one or more of the following: signal processing, electromagnetics, computer vision, machine learning or neural networksExperience with Python and MATLAB\n\nWhat Sets SRC, Inc. Apart?\n\nSRC, Inc., a not-for-profit research and development company, combines information, science, technology and ingenuity to solve “impossible” problems in the areas of defense, environment and intelligence. Across our family of companies, we apply bright minds, fresh thinking and relentless determination to deliver innovative products and services that are redefining possible®. When you join our team, you’ll be a part of something truly meaningful — helping to keep America and its allies safe and strong. You’ll collaborate with more than 1,400 engineers, scientists and professionals — with 20 percent of those employees having served in the military — in a highly innovative, inclusive and equitable work environment. You’ll receive a competitive salary and comprehensive benefits package that includes four or more weeks of paid time off to start, 10 percent employer contribution toward retirement, and 100 percent tuition support.\n\nTotal compensation for this role is market competitive. The anticipated range for this position based out of Syracuse, NY is estimated at $22.00 to $26.00/hour. The hourly rate will vary based on applicant’s experience, skills, and abilities, geographic location as well as other business and organizational needs. SRC offers competitive benefit options, for more details please visit our website.\n\nEqual Opportunity\n\nIndividuals seeking employment are considered without regard to race, color, religion, sex, sexual orientation, gender identify, national origin, age, status as a protected veteran, or disability. You are being given the opportunity to provide the following information in order to help us comply with federal and state \n\nAbout Us\n\nLearn more about SRC:\n\nEmployee Benefits\n\nDiversity, Equity & Inclusion\n\nAwards & Recognition\n\nSecurity Clearances\n\n Location Syracuse, New York  Employment Type Intern/Co-Op  Experience Required 2+ Years  Education Required High School Diploma/GED  Security Clearance Requirement Must meet eligibility requirements  Travel % 10"}
{"text": "requirements that meet deliverables. Strategically collaborate and consult with client personnel.\n\nQualifications:\n\n﻿Applicants must be authorized to work for ANY employer in the U.S. This position is not eligible for visa sponsorship.Strong Risk Data Analyst experience.Data Management and Data Reporting skills.Strong Excel skills with the ability to create Pivot Tables and conduct VLookups.Experience using Service Now.Previous experience in Financial Services."}
{"text": "requirements.Identify and document inconsistencies or errors in authoritative data sources information.Provide database design, database dictionary, and other documentation to assist in an ITFM platform configuration.Help initialize and participate in an ITFM committee that will make recommendations for and maintain/update processes and procedures, make recommendations for strategic goals and tactical objectives to achieve those goals, metrics to measure those objectives, and new/other IT financial governance issues.Design, develop, and deliver communications and communications documentation designed to provide an efficient and effective implementation of an ITFM solution.\nQualificationsBachelor's Degree in a business management or related technical discipline, or the equivalent combination of education, technical certifications or training, or work experience.4 years or more of experience in Federal IT Financial Management, database development, data analysis, design, reporting, and documentation.Active or ability to obtain a Public Trust clearance.Knowledge, Skills and Abilities:Federal IT Financial Management systems experienceDashboard development and maintenanceData structure design, development and managementPivot table design and developmentSQL command structureData ETL design and developmentGoogle Suite experience\nFinal salary determination based on skill-set, qualifications, and approved funding.\nMany of our jobs come with great benefits – Some offerings are dependent upon the role, work schedule, or location, and may include the following:Paid Time OffPTO / Vacation – 5.67 hours accrued per pay period / 136 hours accrued annuallyPaid Holidays - 11California residents receive an additional 24 hours of sick leave a yearHealth & WellnessMedicalDentalVisionPrescriptionEmployee Assistance ProgramShort- & Long-Term DisabilityLife and AD&D InsuranceSpending AccountFlexible Spending AccountHealth Savings AccountHealth Reimbursement AccountDependent Care Spending AccountCommuter BenefitsRetirement401k / 401aVoluntary BenefitsHospital IndemnityCritical IllnessAccident InsurancePet InsuranceLegal InsuranceID Theft Protection\nTeleworking Permitted?\nYesTeleworking Details\nRemote - Quarterly Onsite MeetingsEstimated Salary/Wage\nUSD $145,000.00/Yr. Up to USD $165,000.00/Yr."}
{"text": "Skills - Apache Spark, Hadoop, Scala, Azure Synapse, Azure Databricks\n\nSecondary Skills - SSIS\n\nJob Description -\n\nOverall IT experience: 10+ yearsNeed a Sr Data Engineer who has 5+ years of experience in Azure native services with good exposure to ADF, Synapse, ADLS Gen2, Strong SQL skills, spark.Experience in analyzing/reverse engineering SSIS packages to re-platform solution on AzureDesigning Synapse tables and implementing data solutions within the Azure ecosystem.Design , develop and implement Synapse tables to support data ingestion, transformation and storage processes.Utilize Spark Scala / SQL to build scalable and efficient data pipelines within Azure Synapse.Optimize data storage, ensuring high performance and reliability in Synapse environment.Provide expertise in troubleshooting and resolving data related issues within Azure Synapse.Collaborate with cross-functional teams to understand data requirements and translate them into technical solutions.Proven experience working with Azure Synapse Analytics.Proficiency in Spark Scala/SQL for data processing and transformation.Strong understanding of data modelling concepts and database design principles within Synapse.Ability to optimize and tune Synapse tables for performance and scalability.Excellent communication skills and the ability to work collaboratively in a team environment.\n\nWipro is an \n\nAzure Data Factory"}
{"text": "skills to provide best-in-class analytics to the business\n\nRequired Qualifications, Capabilities, And Skills\n\nBachelor’s and Master’s degree in a quantitative discipline (Data Science/Analytics, Mathematics, Statistics, Physics, Engineering, Economics, Finance or related fields)3+ years of experience in applying statistical methods to real world problems3+ years of experience with SQL and at least one of the following analytical tools: SAS, Python, R Experience with visualization techniques for data analysis and presentationExperience with web analytics tools (Google Analytics, Adobe/Omniture Insight/Visual Sciences, Webtrends, CoreMetrics, etc.)Superior written, oral communication and presentation skills with experience communicating concisely and effectively with all levels of management and partners\n\nPreferred Qualifications, Capabilities, And Skills\n\nTableau and Python preferredIntellectually curious and eager to become subject matter expert in their focus areaA strategic thinker with the ability to focus on business goalsHighly organized and able to prioritize multiple tasks\n\nABOUT US\n\nChase is a leading financial services firm, helping nearly half of America’s households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.\n\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are \n\nEqual Opportunity Employer/Disability/Veterans\n\nAbout The Team\n\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We’re proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions – all while ranking first in customer satisfaction.\n\nThe CCB Data & Analytics team responsibly leverages data across Chase to build competitive advantages for the businesses while providing value and protection for customers. The team encompasses a variety of disciplines from data governance and strategy to reporting, data science and machine learning. We have a strong partnership with Technology, which provides cutting edge data and analytics infrastructure. The team powers Chase with insights to create the best customer and business outcomes."}
{"text": "QUALIFICATIONSMust-Have:Bachelor’s Degree in Computer Science, Information Systems, or related field.A minimum of 3-5 years of experience as a data engineer or in a similar role (SQL, Python, etc.)Experience working in cloud environments (AWS, Azure, etc.)Solid understanding of data governance principles and practices.Knowledge of a Data Catalog, Data Lineage, and Data Quality frameworksPrior experience with Data governance tools such as Atlan, Collibra, Alation, Manta, etc. is highly desired.Strong analytical and technical problem-solving skills.Excellent interpersonal and communication skills.Takes ownership and pride in end-to-end delivery of projects and initiatives.Comfort with a data-intensive and high transaction volume environmentDeadline-driven mindsetNice-to-have:Prior experience in Finance and Asset management domain is a plus.Prior experience with Snowflake and DBT is a plus"}
{"text": "requirements and ensure alignment with business needs. - Utilize SQL for data validation and verification, ensuring the accuracy and reliability of data reports and feeds. - Manage documentation across various platforms including Jira, Azure DevOps, Word, and Excel to support data governance and project tracking. - Leverage and improve the existing BA framework, proposing process improvements as necessary. - Conduct research and investigative analysis to uncover data ownership and establish sources of truth within a complex organizational structure.\nQualifications:- 10+ years of proven experience as a Business Systems Data Analyst, 2+ years within a Treasury services environment. - Strong foundation in data management practices, with hands-on experience in data validation, verification, and governance. - Comfort with ambiguity and a proven ability to research and resolve complex data issues within a large organization. - Enterprise proficiency in SQL, Jira, Azure DevOps, Microsoft Word, and Excel. - Excellent analytical, problem-solving, and interpersonal communication skills. - Ability to work independently and collaborate effectively with both business and IT teams. - Knowledge of visualization and business intelligence tools is a plus but not required.\nInteractions:- Will work closely with the Enterprise Data Team, IT team (specifically with Vamsi who owns the Treasury hub), various managers across business and IT, and Business Treasury and Regulatory Reporting VPs.\nUnable to partner with 3rd party vendors (Corp-to-Corp/C2C) for this opportunity. We are unable to sponsor at this time. Relocation is not provided."}
{"text": "requirements. Ensure that data is accessible to those who need it for analysis and reporting. Budget and Resource Management:Develop and manage the data engineering budget. Allocate resources effectively to meet project and organizational goals. \nExperiences necessary for success:\n\nBachelor’s degree in data science, mathematics, economics, statistics, engineering or information managementDemonstrated experience in Data Management/Reporting/ETL Development tool sets and knowledge and hands-on experience in rolling out to an enterprise organizationDemonstrated experience with various project management methodologies, including Agile methodologies and supporting techniquesExperience and proficiency with various technology packages related to AI/ML and BI domainsExperience building and leading a large team (including managed services – onshore and offshore) in the development and rapid delivery of platform capabilitiesProven track record of recruiting and retaining strong talent. Experience in working with external vendors and developing SOW’s to explore funding opportunities through those partnershipsMust demonstrate an ability to establish relationships and build rapport to influence colleagues at all levels, uncover business or technical issues, and facilitate their resolution. Must be able to present information concisely and clearly to all levels of management, including financial, technical, and executive level audiences. Takes initiative to complete critical tasks in a team setting; effortlessly navigates the inter-departmental structures to achieve success. Challenges the status quo to generate new ideas, is open to challenges, and implements unique solutions; focuses on the best outcome for the companyHas a comprehensive knowledge base in his or her technical field and continually augments the team’s experiences and skills with the latest training, technologies, and techniques. Experience with Cloud data platforms. Azure Cloud experience a strong plus. A history of embracing and incubating emerging technology and open-source productsFlexible and adaptable to changing priorities\n\n\nSkills and competencies necessary for success:\n\nStrong leadership and team management skillsClarity of thoughtAbility to influenceStrong listening and communication skillsFinancial and business acumenCustomer obsessionExcellent problem-solving and analytical abilitiesKnowledge of data privacy and security regulationsStrong programming skills (Python, Java, Scala)Data engineering tools and technologies (e.g., SQL, data warehouses, ETL tools)Big data technology, pipeline and orchestration (e.g.: Hadoop, Spark, Kafka)\n\n\nThe following qualifications are desired, but not necessarily essential:\n\nExperience working in a portfolio company with multi-business units; preferably a manufacturing company. Experience with Microsoft AzureKnowledge of digital concepts and ways of doing things and successful track record of leveraging them to enable proactive decision making and the use of data to make better and faster decisions. MBA and/or PhD\n\n\nOshkosh is committed to working with and offering reasonable accommodations to job applicants with disabilities. If you need assistance or an accommodation due to a disability for any part of the recruitment process, please contact our reception desk by phone at +1 (920) 502.3009 or our talent acquisition team by email corporatetalentacquisition@oshkoshcorp.com.\n\nOshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application.\n\nOshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information.\n\nCertain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be \"U.S. Persons,\" as defined in these regulations. Generally, a \"U.S. Person\" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum."}
{"text": "requirements Build data cleansing and standardization routines from source systems Produce data views and flows for different client demands Translate business data stories into technical breakdown structures Implement production processes to monitor data quality\n\nWhat Is Required (Qualifications)\n\n Undergraduate studies in computer science, management information systems, business, statistics, math, or related field 5-8 years of experience with data quality rules and data management 3-5 years of experience in data warehousing and queries Strong problem-solving and communication skills Advanced skills in Python and SQL\n\nHow To Stand Out (Preferred Qualifications)\n\n Experience in healthcare, insurance, or financial services industry Knowledge of Cyber Security Experience with AI/Machine Learning Familiarity with Google Dataflow or Dataproc Experience with sensitive data handling and Collibra\n\n#HealthcareServices #CyberSecurity #DataEngineering #CareerOpportunity #CompetitivePay\n\nAt Talentify, we prioritize candidate privacy and champion equal-opportunity employment. Central to our mission is our partnership with companies that share this commitment. We aim to foster a fair, transparent, and secure hiring environment for all. If you encounter any employer not adhering to these principles, please bring it to our attention immediately. Talentify is not the EOR (Employer of Record) for this position. Our role in this specific opportunity is to connect outstanding candidates with a top-tier employer.\n\nTalentify helps candidates around the world to discover and stay focused on the jobs they want until they can complete a full application in the hiring company career page/ATS."}
{"text": "Skills RequiredTechnical Requirements: • Strong written, oral, and interpersonal communications skills; • Able to prioritize and coordinate multiple projects to meet deadlines; • Working knowledge of all, but not limited to, the following: o Processes for leveraging data from data warehousing / data mart / data lake environments; o Visualization Development - Generate analysis through data visualizations from multiple data sets using standard best-in-class analytics software; o Query complex data structures and derive information for reporting, visualizations, and statistical analysis; o Requirements gathering and analysis; o Basic Analytics - Perform basic data analysis to include data profiling, data quality, joining of data tables, graphing, basic trend analysis, data segmentation; o Ad Hoc Query Development - Quickly develop, test, and provide ad hoc (one-time) information based on a business request leveraging internal or external data and using standard querying toolsets; o Report Development - Create reports from multiple data sets using standard best-in-class reporting software; o SQL - basic query and data manipulation skills including selects, inserts, updates, table joins, and grouping; o Visualization (Qlik, PowerBI, Cognos, Tableau) - advanced skills in a best-in-class data visualization tool to include data preparation, rationalization of visualization type, standard charting (time series, Pareto, bar, area, multi-axis, geospatial, scatter plots, etc.), filtering, drill-downs, drill-throughs, navigation, dashboard creation, deep understanding of user interface and effective presentation; o Excel - advanced skills including graphing, Pivot Tables, VLOOKUP, and multi-sheet references; o Experience working with a best-in-class DBMS (Oracle, SQL Server, etc.) to extract and transform data for reporting, analysis, or data science; • Familiarity with all, but not limited to, the following: o Enterprise resource planning (ERP) software (JD Edwards EnterpriseOne) and specialty software programs used to assemble business operations data in the functional area of assignment (billing, budget, accounting, workforce management, etc.); o Familiar with a data warehouse / data mart OLAP environment leveraging data in star schemas, snowflake schemas, and similar data structures; o Familiar with data modeling in the context of transforming data from an OLTP system to an OLAP or other data warehouse related structure. Familiar with the importance of how data is modeled to support the needs of a data reporting and analysis environment; o Familiarity with generally accepted data and information privacy standards (GDPR, PCI, PII, HIPAA, etc.); o Familiarity with leveraging large data sets for data science, machine learning and related analysis; o Dashboard Development - Gather requirements, identify metrics and goals, leverage data sources, select appropriate dashboard objects, and implement a dashboard using a best-in-class tool; o Project Management - Facilitate, create, implement, and manage a project or projects using MS Project or a similar project tracking tool; ability to define, document, and communicate a project charter, resource assignments, risks, issues, and status over the course of a project; o Query Optimization – ability create / modify SQL or other query code to ensure request has minimal impact on the target database and executes in the most efficient manner possible; o Knowledge / application of related industry, organizational, and departmental policies, practices and procedures, legal guidelines, ordinances and laws; o Predictive Model Development - Leverage historic internal and external data to generate predictive business models forecasting trends and providing insights with relevant statistical confidence measures and using appropriate statistical methods; o Process flow documentation; o Related industry, organizational and departmental policies, practices and procedures; legal guidelines, ordinances and laws. • Ability to: o Strong attention to detail; o Ability to apply data quality assurance and troubleshooting to data profiling, analysis, and reporting; o Ability to apply appropriate data cleansing and transformation techniques to prepare data for reporting and analysis; o Demonstrate strong analytical ability to identify appropriate analysis, data anomalies, trends, etc.; o Advanced presentation skills leveraging appropriate software, adapting to audience, and excellent written and grammatical skills; o Work with minimal supervision; self-directed; seeks assistance when needed; o Excellent written and verbal communications skills; o Use advanced Microsoft Office Suite (Excel, PowerPoint, Word, Outlook, etc.) and standard office equipment (telephone, computer, copier, etc.); o Make arithmetic computations using whole numbers, fractions and decimals, rates, ratios, and percentages; o o MS Access - advanced skills including relational table joins, data transformation through joins, filtering, updates, and summarization, reporting (preferred); o Reporting (Cognos, OBIEE, Crystal) - advanced skills in standard columnar reporting, requirements gathering, data preparation requirements, report creation, testing, scheduling, and deployment. (preferred)"}
{"text": "skills and ability to manage and prioritize multiple projects or assignments at one time, including the ability to follow assignments through to completion and meet deadlines independentlyAdvanced proficiency with Microsoft Excel, and the use of formulas, data queries a BI Connectors, API endpoints as well as databases to analyze dataExperience creating sophisticated queries, analyzing data, and finding opportunities to improve data integrity, creating data-related dashboardsProficiency in creating presentations to communicate results and recommended solutions or process improvements\n\nMinimum Qualifications:\n\nBachelor's degree in Mathematics, Statistics, Computer Science, Data Science, Machine Learning, Artificial Intelligence or a related field.3+ years of proven experience as a Data Analyst, Data Scientist, Data Engineer, Machine Learning & Artificial Intelligence Engineer, or similar role.Proficiency in SQL.Proficiency in data visualization tools such as Tableau, PowerBI, Looker, or Qlik.Proficiency in Python and hands-on experience with Data Science libraries (e.g. Pandas, NumPy, Scikit-Learn, Mathplotlib, etc.)Knowledge of statistics including hypothesis testing and probability distributions.Knowledge in data architecture, data warehousing and ETL pipelines.Excellent data storytelling and data communication skills in English.Ability to write technical documentation with data requirements and define metrics relevant to projects’ objectives.Strong analytical and problem-solving skills.Excellent collaboration skills.\n\nPreferred Qualification\n\nApplicant tracking systems (ATS) Avature and Greenhouse, CRMs and recruitment software/systemsWorking knowledge of Human Resource and Talent Acquisition standard methodologiesKnowledge of Office of Federal Contract Compliance Programs (OFCCP) and audits\n\nLife at Cisco Meraki: Work How You Feel Most Empowered\n\nOur hybrid work model prioritizes work-life balance by offering employees the flexibility and autonomy to work outside of the office, or in-person with their team. We believe that if employees work how they feel most empowered, everyone benefits, including our customers.\n\nWe offer exciting benefits and perks, including Paid-Time-Off (PTO), Volunteer-Time-Off (VTO), and on-site health and wellness opportunities, among many other perks. Our goal is to support the whole you.\n\nTo learn more about benefits and perks offered at Cisco Meraki click here.\n\nAt Cisco Meraki, we’re challenging the status quo with the power of diversity, inclusion, and collaboration. When we connect different perspectives, we can imagine new possibilities, inspire innovation, and release the full potential of our people. We’re building an employee experience that includes appreciation, belonging, growth, and purpose for everyone.\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nCompensation Range:\n\n$88,400—$157,500 USD\n\nMessage to applicants applying to work in the U.S. and/or Canada:\n\nWhen available, the salary range posted for this position reflects the projected hiring range for new hire, full-time salaries in U.S. and/or Canada locations, not including equity or benefits. For non-sales roles the hiring ranges reflect base salary only; employees are also eligible to receive annual bonuses. Hiring ranges for sales positions include base and incentive compensation target. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications, or training. Applicants may not be eligible for the full salary range based on their U.S. or Canada hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.\n\nU.S. employees have access to quality medical, dental and vision insurance, a 401(k) plan with a Cisco matching contribution, short and long-term disability coverage, basic life insurance and numerous wellbeing offerings. Employees receive up to twelve paid holidays per calendar year, which includes one floating holiday, plus a day off for their birthday. Employees accrue up to 20 days of Paid Time Off (PTO) each year and have access to paid time away to deal with critical or emergency issues without tapping into their PTO. We offer additional paid time to volunteer and give back to the community. Employees are also able to purchase company stock through our Employee Stock Purchase Program.\n\nEmployees on sales plans earn performance-based incentive pay on top of their base salary, which is split between quota and non-quota components. For quota-based incentive pay, Cisco pays at the standard rate of 1% of incentive target for each 1% revenue attainment against the quota up to 100%. Once performance exceeds 100% quota attainment, incentive rates may increase up to five times the standard rate with no cap on incentive compensation. For non-quota-based sales performance elements such as strategic sales objectives, Cisco may pay up to 125% of target. Cisco sales plans do not have a minimum threshold of performance for sales incentive compensation to be paid."}
{"text": "experience in machine learning, distributed microservices, and full stack systems  Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment  Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance \n\nBasic Qualifications: \n\n Bachelor’s Degree  At least 4 years of experience in application development (Internship experience does not apply)  At least 1 year of experience in big data technologies \n\nPreferred Qualifications: \n\n 5+ years of experience in application development including Python, SQL, Scala, or Java  2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)  3+ years experience with Distributed data computing tools (Kafka, Spark, Flink etc)  2+ year experience working on real-time data and streaming applications  2+ years of experience with NoSQL implementation (DynamoDB, OpenSearch)  2+ years of data warehousing experience (Redshift or Snowflake)  3+ years of experience with UNIX/Linux including basic commands and shell scripting  2+ years of experience with Agile engineering practices \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "requirements for various data and analytics initiatives.Ensure clear communication of project progress and results to stakeholders.Collaborate with data engineers across the wider OP stack and ecosystem to enable open source and publicly available datasets.\nWhat skills do you bring?\n\n4+ years of professional data engineering experienceAdvanced working knowledge of SQL, Python, and experience with relational databasesExperience in building and optimizing 'big data' data pipelines, architectures, and data setsExperience with big data tools: Hadoop, Spark, Kafka, etc.Experience with workflow orchestration management such as Airflow, dbt etc.Experience with Cloud Services such as Google Cloud Services, AWS, etc. Strong analytic skills related to working with unstructured datasets, we are looking for an engineer who can understand the business and how to build to requirementsExcellent communication skills with the ability to engage, influence, and inspire partners and stakeholders to drive collaboration and alignmentSelf-starter who takes ownership, gets results, and enjoys the autonomy of architecting from the ground upExperience with web3 and blockchain protocols is a plus\n\n\nWhat will you like about us?\n\nWe take care of our employees. Competitive compensation, fully paid medical, dental, and vision, and a 4% 401K match—learn more about our benefits, culture, and all recruiting FAQ here.We take pride in the accomplishments of our teammates and support each other in doing the best work of our careers.Our team is a diverse group of people from varied backgrounds. We cherish our eclecticism and consider it a great strength.We’re fully remote, deeply engaged, highly skilled, and like to have fun.We think long-term. Our founders have been scaling Ethereum since 2015.\n\n\nWe strongly encourage candidates of all different backgrounds to apply. We believe that our work is stronger with a variety of perspectives, and we’re eager to further diversify our company. If you have a background that you feel would make an impact at Optimism, please consider applying. We’re committed to building an inclusive, supportive place for you to do the best work of your career."}
{"text": "experienced Technical Data Analyst as part of a team that is supporting the law enforcement community in providing data to a state-wide criminal justice data platform. The first of its type in the nation, the platform will integrate data from police departments, the courts, and the state s custodial institutions to better understand and improve pathways through the criminal justice system, reduce recidivism, and improve public reporting of criminal justice data. This role will entail working with the Data Management Team to review CJIS Data and law enforcement data, to include arrest, summons, and citation data for accuracy and facilitating the delivery of work products to the Data Management team.\n\nWho We Are Looking For\n\n6+ years of experience in one or more relational or columnar databases like Oracle, MS SQL Server, PostgreSQL etc. Fluency with SQL data manipulation, preferably using ETL tools and experience with building models in Excel. Able to create dashboards in a BI framework such as Tableau, PowerBI, Looker, Looker Studio, or comparable tools. Highly efficient technical hands-on resource who can both oversee and provide the delivery of large-scale data and analytics projects on-time and contribute independent data analysis and reporting. Demonstrated experience working with vendor partners, cross functional and development teams in terms of end-to-end technical delivery, including data stewards, architects, data governance, data modeler, data quality, development, solution leads to address data and mapping questions. Performed ad hoc, live research to demonstrate/present data issues, patterns, relationships, findings and/or recommendations, risks, and assumptions. Analysts who can support the development of innovative, accurate products that meet business users needs. Team oriented specialists who work collaboratively with business leaders, project managers,�Data Champions and engineers to build the right thing. \n\nWhat you will do\n\nUnder the direction of the Data Management Team, review data sets for accuracy. Compare data from law enforcement agencies but not limited to against data from other data sources. Work with internal and vendor technical staff for assigned projects to ensure quality oversight and on time delivery. Develop and review standard operating procedures to meet high standards for data organization, quality, and security. Propose and supervise strategies to improve data quality, including within source systemsDevelop and maintain productive business relationships with internal and external partners, including state and local criminal justice agencies, the Trial Court, and vendor partners. Deliver consistent and reliable processes and high-quality output. Use statistical and other software applications, graphics tools, and business intelligence (BI) applications to analyze results, evaluate performance, and project future trends and needsWork with team members and outside departments to access data for ongoing projects and business intelligence needsSupport the development of public facing data analytics and reporting to meet statutory compliance\n\nKnowledge, skills & abilities\n\nExperience as a data steward or other equivalent position to support the enforcement and monitoring of data standards. Strong leadership collaboration skills and abilities to work with a variety of stakeholders. Working, up to-date knowledge of best practices for keeping data separated and secure. Develops adhoc analytics and reporting based on analysis of existing data sources, utilizing a variety of tools (e.g., Looker, Tableau, PowerBI, etc.)Proficient in scripting medium to complex SQL queries. Proficiency in programming languages like Python and VBA and data base programming (SQL Server, Oracle, MySQL)Working experience in Data modellingStrong analytics and critical thinking skills; ability to organize and synthesize large datasets and use data to identify patterns and problems and formulate solutions. Experience with structured and unstructured data formats including csv, txt, json, and xmlExperience with basic applied statistical techniques and experience in different types of analytics for better business decision making. Ability to produce consistently high-quality output under deadlines. Ability to communicate methods and findings verbally, in writing, and visually to technical and nontechnical audiences. Capable of working both independently and as part of a diverse, multiskilled teamStrong intellectual curiosity; drive to master new skills, systems, and subject matterAbility to handle multiple projects simultaneously: superb time management and prioritization abilityAbility to work independently and autonomously, as well as part of a team"}
{"text": "Qualifications for the Data Engineer include: \n\n 6 years of experience in data engineering Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python 3+ years cloud experience: Azure, AWS, or GCP\n\nCompensation for the Data Engineer include:\n\n Salary: $135,000-145,000 Benefits: Full Health/Dental/Vision, 401K, Pension, Annual Bonus"}
{"text": "experiences tailored to each individual's unique learning style. Our platform enables learners to have a customized and adaptive learning journey.\n Role Description\n This is a full-time remote role for a Machine Learning Engineer at eduPhoria.ai. As a Machine Learning Engineer, you will be responsible for developing and implementing machine learning algorithms and models, conducting data analysis, and improving the overall performance of our learning platform. You will collaborate with cross-functional teams, including data scientists and software developers, to create innovative solutions and enhance the learning experience for our users.\n Qualifications\n Strong understanding of pattern recognition, computer science, and statisticsExperience in developing and implementing neural networks and algorithmsProficiency in programming languages such as Python, R, or JavaExperience with data preprocessing, feature engineering, and model evaluationKnowledge of machine learning frameworks and libraries, such as TensorFlow or KerasAbility to analyze and interpret large datasetsExcellent problem-solving and critical thinking skillsStrong communication and collaboration skillsMaster's degree or higher in computer science, data science, or related field"}
{"text": "Qualifications)Experience supporting and troubleshooting complex data systems and integrations.Experience writing SQL queries.Experience reading/debugging code. What Will Put You AheadExperience debugging or modifying code (.NET / typescript/python) or database procedures.Experience in event-based and API integration.Experience with AWS Technologies (Lambda, S3, Cloudwatch, AWS X-Ray).Experience with Neo4J Graph Databases and Cypher queriesFamiliarity with data integration tools such as Denodo, Alteryx, MatillionExperience with monitoring tools and effective alerting practices (SPLUNK, Automic, LogicMonitor, or others)"}
{"text": "skills to spearhead high impact initiatives that optimize operational performance and generate value by turning data assets into actionable insights. You will collaborate with senior leaders in this high-visibility role that thrives at the intersection of data, technology, and finance.\n\nWe are looking for a data enthusiast with a track record of developing analytics capabilities, a passion for quantitative problem solving, and the artistry to distill complex data insights into crystal clear concepts. In addition, the candidate should be able to harmoniously integrate with business and functional stakeholders throughout the organization.\n\nJob Responsibilities\n\nPlay a lead role in data analytics and insight generation initiatives through your applied data science expertise.\n\nManage a comprehensive analytics strategy, from data acquisition to deliveryLeverage business acumen plus a variety of technical skills to design, develop and deliver valuable data productsConstruct models/data applications tuned to yield salient, actionable insightsDesign compelling visualizations that simplify complex analytics messaging for all stakeholdersEngage with business and functional stakeholders on analytics initiatives, developing strong partnerships grounded on data-driven solutionDistill substantial business needs into potential data solutionsPresent data-driven insights, business value propositions, and standout next steps to senior stakeholders\n\nQualifications\n\nMSc. Or higher degree in a scientific field (Computer Science, Engineering, Operations Research, etc.) plus 3 years or more of experience in producing advanced analytics work with an emphasis in optimizationStrong proficiency in statistical software packages and data tools, including Python and SQLStrong proficiency in Advanced Statistical methods and concepts, predictive modeling, time series forecasting, text miningFundamental data engineering experience in designing and building experimental data pipeline for data acquisition and deliveryStrong proficiency in Data Mining & Visualization (Tableau experienced preferred)Strong story telling capabilities including communicating complex concepts into digestible information to be consumed by audiences of varying levels in the organizationFamiliarity with MDLC best practices, with strong commitment to iterative development, ability to engage and update stakeholders as necessary, ensuring alignment, feedback incorporation, and transparency throughout the project lifecycleTeam player with a demonstrated strong work ethic. Demonstrates a high level of professionalism in behavior and performance and an ability to work well and productively with cross-functional teams\n\nLocation – Chicago (primary), Plano (secondary)\n\nAbout Us\n\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, we offer discretionary incentive compensation which may be awarded in recognition of firm performance and individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are \n\nJPMorgan Chase is \n\nAbout The Team\n\nCommercial Banking is focused on helping our clients succeed and making a positive difference in our communities. We provide credit and financing, treasury and payment services, international banking and real estate services to clients including corporations, municipalities, institutions, real estate investors and owners, and nonprofit organizations."}
{"text": "requirements and gather information using judgment and statistical tests. \n * Use programming and evaluation tools, including open-source programs to plan models and extract insights. \n * Apply modeling and optimization methods to improve business performance. \n * Develop ad-hoc reporting based on the review of existing data sources using programs, such as Power BI. \n * Exhibit rigor, judgment, and ability to present a detailed 'data story' to a business line. \n * Confirm the quality and integrity of existing data sources.\n * Collaborate with the agile development team to provide recommendations and communications on enhancing existing or new processes and programs.\n * Have some knowledge of standard principles with limited practical experience in applying them.\n * Lead by example and model behaviors that are consistent with CBRE RISE values. \n * Impact the quality of own work.\n * Work within standardized procedures and practices to achieve objectives and meet deadlines.\n * Exchange straightforward information, ask questions, and check for understanding."}
{"text": "Experience with Tableau2. Ability to present data with meaningful insight.3. Ability to function effectively in a fast-paced environment.\nQUALIFICATIONSBachelor’s degree required; advanced degree preferred2+ years of analytics and reporting experience required, preferably in a pharmaceutical industry, - consumer packaged goods, or marketing organization consideredAble to create effective reports that present data in meaningful and cohesive story telling presentation for commercial team consumption – not just data dumpExperience manipulating and analyzing complex datasets including specialty pharmacy, Komodo, IQVIA (Xponent, DDD), Patient Level Claims Data, and other proprietary sourcesProficiency working with Tableau, Excel and other data manipulation and analysis platformsStrong PowerPoint and Excel skillsDemonstrated ability to work on multiple projects with conflicting deadlinesPositive and eagerness to learn, collaborate and partner with internal business colleaguesDemonstrated ability to work on multiple projects with conflicting deadlinesExcellent verbal, writing, presentation, project management and effective influencing skills requiredWork independently - Gather input about needs from business partners and set own direction accordingly\nESSENTIAL FUNCTIONSDATA MANAGEMENT, ANALYSIS AND REPORTING:Updates and processes secondary data feeds from industry sources (e.g. Komodo, IQVIA, Decision Resources, prescriber, payer, patient and promotional sources, etc.) and internal data (call files, sales data, etc.) into excel and/or other custom programsManages and employs data to create market trend and product performance analysis Design, develop and maintain recurrent monthly /quarterly and ad-hoc reports using advanced query techniques for customers; provides analysis and interpretation of the report dataEnsure distribution pre-established on-going (weekly, monthly) reports and dashboards in line with business needs and objectivesMaintains and develops (in close collaboration with internal data strategy) databases and analytic applications in environments such as Tableau, SAP, Excel/Access, and/or other proprietary environmentsContribute to hypothesis driven investigations into detailed channel dynamics, prescriber, patient and transactional dataMaintains catalogue of standardized market definitions and analytical business rules used within the analyses and reports, while proactively monitoring areas of concern or opportunity for enhancementsLeverages internal data systems and tools to efficiently maintain data and reporting processes to minimize manual data updatesEnsures timely receipt of data from vendors and ensures data integrity and quality throughout all processes\nCOLLABORATION & COMMUNICATION:Develop and sustain close working relationships with key business partners, including commercial, medical and operations business colleagues (US & Global) as requiredProactively monitors areas of concern or opportunity for enhancements and provides recommendations/proposals, supports relevant analysis as neededDevelopment and execution of presentations with clear storylines, grounded on holistic business concepts and communicate key conclusions and business recommendations /implications (what/so what and now what) to business partners and peers through formal and informal channelsAnalytical thinker and creative problem solver, ability to influence others with verbal and written communicationCrafts clear narrative, uses it to develop own perspective and recommendations, even if others may disagree, and takes a stand\nAbout Us: Established in 2004, SPECTRAFORCE® is one of the largest and fastest-growing diversity-owned staffing firms in the US. The growth of our company is a direct result of our global client service delivery model that is powered by our state-of-the-art A.I. proprietary talent acquisition platform, robust ISO 9001:2015/ISO 27001 certified processes, and strong and passionate client engaged teams. We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 140 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation. SPECTRAFORCE is built on a concept of “human connection,” defined by our branding attitude of NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people and clients can reach their highest potential. Learn more at: http://www.spectraforce.com Benefits: SPECTRAFORCE offers ACA compliant health benefits as well as dental, vision, accident, critical illness, voluntary life, and hospital indemnity insurances to eligible employees. Additional benefits offered to eligible employees include commuter benefits, 401K plan with matching, and a referral bonus program. SPECTRAFORCE provides unpaid leave as well as paid sick leave when required by law.\nEqual Opportunity Employer: SPECTRAFORCE is"}
{"text": "experience:\n\nGS-14:\n\nSupervisory/Managerial Organization Leadership\n\nSupervises an assigned branch and its employees. The work directed involves high profile data science projects, programs, and/or initiatives within other federal agencies.Provides expert advice in the highly technical and specialized area of data science and is a key advisor to management on assigned/delegated matters related to the application of mathematics, statistical analysis, modeling/simulation, machine learning, natural language processing, and computer science from a data science perspective.Manages workforce operations, including recruitment, supervision, scheduling, development, and performance evaluations.Keeps up to date with data science developments in the private sector; seeks out best practices; and identifies and seizes opportunities for improvements in assigned data science program and project operations.\n\n\nSenior Expert in Data Science\n\nRecognized authority for scientific data analysis using advanced statistical techniques to determine desired statistical computations, analyses, and acceptable output format based on network constraints.Applies expert data science knowledge to consult with and advise partner agencies in modern/current methodologies, tools, techniques, and ways of thinking to create enduring change and advocate for and propagate adoption and application of innovative data science tools and techniques throughout government. Provides training and expertise on determining appropriate data science products or services with clients or customers to define project scopes, requirements, and deliverables for a full array of data science functions to include defining data requirements, implementing databases, analyzing data, developing data standards, building artificial intelligence (AI)/machine learning (NL) models, etc.Facilitates a work environment that encourages creative thinking and innovation and designs and implements agile, human-centered, cutting-edge data science projects, and/or services. \n\n\nRequirements\n\n Conditions of Employment\n\nUS Citizenship or National (Residents of American Samoa and Swains Island)Register with Selective Service if you are a male born after 12/31/1959Meet all eligibility criteria within 30 days of the closing dateDirect Deposit of salary check to financial organization required\n\n\nIf selected, you must meet the following conditions:\n\nReceive authorization from OPM on any job offer you receive, if you are or were (within the last 5 years) a political Schedule A, Schedule C or Non-Career SES employee in the Executive Branch.Serve a one year supervisory or managerial probationary period, if requiredUndergo and pass a background investigation (Tier 4 investigation level).Have your identity and work status eligibility verified if you are not a GSA employee. We will use the Department of Homeland Security’s e-Verify system for this. Any discrepancies must be resolved as a condition of continued employment.\n\n\nQualifications\n\nFor each job on your resume, provide:\n\nthe exact dates you held each job (from month/year to month/year)number of hours per week you worked (if part time). \n\n\nIf you have volunteered your service through a National Service program (e.g., Peace Corps, Americorps), we encourage you to apply and include this experience on your resume.\n\nFor a brief video on creating a Federal resume, click here .\n\nThe GS-14 step 1 salary range starts at $122,198 per year. The total salary will be determined upon selection and based on the associated GS locality pay table for your assigned duty location.\n\nIf you are a new federal employee, your starting salary will likely be set at the Step 1 of the grade for which you are selected.\n\nBASIC REQUIREMENTS:\n\nDegree: Mathematics, statistics, computer science, data science or field directly related to the position. The degree must be in a major field of study (at least at the baccalaureate level) that is appropriate for the position -OR-Combination of education and experience: Courses equivalent to a major field of study (30 semester hours) as shown in paragraph A above, plus additional education or appropriate experience\n\n\nThis position has a positive education requirement: Applicants must submit a copy of their college or university transcript(s) and certificates by the closing date of announcements to verify qualifications. If selected, an official transcript will be required prior to appointment.\n\nIn addition to the Basic Requirements listed above, to qualify at the GS-14 grade level, you must have at least one year of specialized experience equivalent to the GS-13 level or higher in the Federal service. Specialized experience is serving as a subject matter expert in leading and coordinating the efforts of a team of technical professionals in solving data science and analytics issues and problems for systems, applications, and customers. This experience must include:\n\nManaging or supervising staff that provide or support delivery of modern IT product(s), platform(s), or service(s).Leading the overall planning, management, and direction of a program or of projects.Serving as a data science and analytics practitioner in a technical environment.Working with agile or iterative development methodologies.Building relationships and alignment with partners and/or stakeholders.\n\n\nAdditional Information\n\nBargaining Unit Status: This position is ineligible for the bargaining unit.\n\nRelocation Not Paid: Relocation-related expenses are not approved and will be your responsibility.\n\nManagement Rights: Additional vacancies may be filled through this announcement in this or other GSA organizations within the same commuting area as needed; through other means; or not at all.\n\nOn a case-by-case basis, the following incentives may be approved:\n\nRecruitment incentive if you are new to the federal governmentRelocation incentive if you are a current federal employeeCredit toward vacation leave if you are new to the federal government\n\n\nGSA is committed to diversity, equity, inclusion and accessibility that goes beyond our compliance with \n\nValuing and embracing diversity, promoting equity, inclusion and accessibility, and expecting cultural competence; andFostering a work environment where all employees, customers and stakeholders feel respected and valued.\n\n\nOur commitment is:\n\nReflected in our policies, procedures and work environment;Recognized by our employees, customers and stakeholders; andDrives our efforts to recruit, attract, retain and engage the diverse workforce needed to achieve our mission."}
{"text": "QualificationsRaw data and PED systems support Requests for Information (RFI), rapid scripting, process improvement, technique discovery, and validation of raw data.Experience in designing, building, and maintaining data pipelines and data systemsStrong problem-solving and analytical skillsProficiency in Python programming and ElasticsearchExperience with data integration and data manipulationExperience with cloud-based data platformsTS/SCI clearance is requiredPIR raw data processing and analysis techniques\nEducationBachelor-level degree in a STEM field and at least 5 years of related experience"}
{"text": "requirements.Design, develop, and implement ETL (Extract, Transform, Load) processes to integrate data from various sources into data warehouses or data lakes.Ensure the reliability, scalability, and efficiency of ETL pipelines for large-scale data processing.Identify and resolve data quality issues through data profiling, cleansing, and normalization techniques.Design and maintain dimensional data models for data warehouses to support reporting and analytics requirements.Work closely with data architects and analysts to understand data requirements and translate them into effective data models.\n\nQualifications\n\nBachelor's degree in Computer Science, Information Technology, or a related field.Hands-on experience with SQL Server, MySQL, PostgreSQL, and Snowflake.Proficiency in writing complex SQL queries and optimizing database performance.Strong understanding of data warehousing concepts and dimensional modeling techniques.Excellent problem-solving skills and attention to detail.Effective communication and collaboration skills in a team environment.\n\nAdditional Information\n\nLife at Xplor\n\nYou’ll be part of a global network of talented colleagues who support your success. We look for commonalities and shared passions and give people the tools they need to deliver great work and grow at speed.\n\nSome Of The Perks Of Working With Us\n\n12 weeks Gender Neutral Paid Parental Leave for both primary and secondary career#GiveBackDays/Commitment to social impact – 3 extra days off to volunteer and give back to your local communityOngoing dedication to Diversity & Inclusion initiatives such as D&I Council, Global Mentorship ProgramAccess to free mental health supportFlexible working arrangements\n\nThe average base salary pay range for this role is between $70,000-$90,000 USD\n\nMay be considered for a discretionary bonus \n\nMore About Us\n\nXplor Technologies is a global platform integrating SaaS solutions, embedded payments, and Commerce Accelerating Technologies to help businesses succeed. Xplor provides enterprise-grade SaaS solutions for businesses in “everyday life” verticals: Childcare & Education; Fitness & Wellbeing, Field Services and Personal Services – and a global cloud-based payment processing platform.\n\nXplor Technologies serves over 78,000 customers that processed over $36 billion in payments, operating across 20 markets in 2022.\n\nGood to know\n\nTo be considered for employment, you must be legally authorized to work in the location (country) you're applying for. Xplor does not sponsor visas, either at the time of hire or at any later time.\n\nWe kindly ask you to apply through our careers portal or external job boards only. Please don't send your application via email.\n\nTo learn more about us and our products, please visit www.xplortechnologies.com/us/careers.\n\nWe also invite you to check out our Candidate FAQs for more information about our recruitment process www.xplortechnologies.com/us/recruitment-faqs.\n\nXplor is proud to be an \n\nAll Information will be kept confidential according to \n\nXplor is committed to the full inclusion of all qualified individuals. In keeping with our commitment, Xplor will take the steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact us via talent@xplortechnologies.com.\n\nWe are a 2024 Circle Back Initiative Employer – we commit to respond to every applicant."}
{"text": "requirements, collect data, lead cleansing efforts, and load/support data into SAPthe gap between business and IT teams, effectively communicating data models and setting clear expectations of deliverablesand maintain trackers to showcase progress and hurdles to Project Managers and Stakeholders\nQualifications\nknowledge of SAP and MDGcommunication skillsto manage multiple high-priority, fast-paced projects with attention to detail and organizationan excellent opportunity to learn an in-demand area of SAP MDGa strong willingness to learn, with unlimited potential for growth and plenty of opportunities to expand skills\nThis role offers a dynamic environment where you can directly impact IT projects and contribute to the company’s success. You will work alongside a supportive team of professionals, with ample opportunities for personal and professional development. \nIf you’re ready to take on new challenges and grow your career in data analytics and SAP, apply now and be part of our journey toward excellence."}
{"text": "experience for hundreds of thousands of end customers around the world\n\n“Provide the insight, advice and services to help our partners and prospects succeed and grow with their e-commerce platform”\n\nYou will also be helping to drive our profitability while continuously improving the services we deliver. Your insights will have a direct contribution to Ocado’s long-term goal of changing the way the world shops.\n\nGenerate actionable insights to support our partners in optimizing their e-commerce platforms, leveraging Adobe Analytics and Adobe Customer Journey Analytics as your primary tools.Conduct deep-dive analyses into website and app performance to identify trends and behavior patterns. Use your findings to propose enhancements that elevate the user experience and drive conversions. Expertise in A/B and multivariate testing (MVT) is essential for leading conversion rate optimization (CRO) initiatives.Establish foundational web and app analytics reporting for our partners. Ensure data integrity in reports and segments, and work collaboratively to address any data collection or implementation gaps in Adobe's ecosystem.Forge strong relationships with our partners, acting as the contact between data insights and e-commerce optimization. You'll facilitate regular meetings, workshops, and presentations with our partners to ensure they're fully leveraging their e-commerce platform's potential. Comfort and proficiency in engaging with partner technology teams to refine data collection strategies are key. Collaborate with partners to ensure effective utilization of customer data for marketing and e-commerce reporting within Adobe Analytics as a single source of truthAbility to quantify incremental uplift of our partner’s marketing campaigns and present back findings Assisting our Partners' worldwide e-commerce operations. This entails providing valuable insights and recommendations, developing informative visualization dashboards using LookerML, conducting ad-hoc analysis, and conducting business reviewsProblem Solving, with our department and business growing rapidly, and with even bigger growth plans ahead, new challenges and insights will develop for you to tackleCreating data to represent Ocado Solutions’ objectives and interests to other functions (e.g. Product Continuous Improvement) Project Management and Support. Whether it is your own idea or a project managed by another department, you will be involved in supporting different projects across the business Engage with various teams across our partner's organization to uncover opportunities for growth and improvement. Your efforts will contribute to enhancing customer acquisition, increasing average order frequency and value, and minimizing churn\n\nWhat We Would Like To See\n\nExtensive knowledge using Adobe Analytics and Adobe Customer Journey AnalyticsLeading projects within the CRO activities including AB and MVT testing.Experience with data analysis and visualization tools (e.g. SQL, Excel/Google Sheets, VBA/AppScript, Data Studio/Tableau/Looker) Ability to quantify incremental uplift of our partner's marketing campaigns Strong communication with technical and non-technical senior management and key stakeholdersPython and Google Colab experience would be beneficial to the role but not essential2.1 degree in a STEM subject or equivalent work experience in a quantitative roleComfortable communicating with technical and non-technical senior management and key stakeholdersAnalytical thinker who can break down complex problems Python experience would be beneficial to the role but not essentialAble to use data to create compelling stories and achieve positive outcomesTenacity to follow ideas and problems through to resolutionExperience adapting to a fast-changing environmentWillingness to travel internationally to meet with our OSP Partners\n\nWhat We Can Offer You.\n\n401k Plan; 100% match up to 5% of earnings; Paid Vacation and Sick Days; 10 Paid Public Holidays; Medical, Dental, and Vision Insurance; Medical and Dependent Care; Flexible Spending Accounts Health Reimbursement Account; Company Contribution of 50% of Annual Deductible; Company Paid Life Insurance; Short and Long Term Disability Insurance; Employee Assistance Program.\n\nAlthough this role will be based primarily at our Office in Cincinnati, we will be able to offer flexibility with regard to work location and working hours.\n\nDue to the energizing nature of Ocado's business, vacancy close dates, when stated, are indicative and may be subject to change so please apply as soon as possible to avoid disappointment. \n\nPlease note: If you have applied and been rejected for this role in the last 6 months, or applied and been rejected for a role with a similar skill set, we will not re-evaluate you for this position. After 6 months, we will treat your application as a new one. \n\nBe bold, be unique, be brilliant, be you. We are looking for individuality and we value diversity above gender, sexual orientation, race, nationality, ethnicity, religion, age, disability or union participation. We are an equal opportunities employer and we are committed to treating all applicants and employees fairly and equally.\n\nAbout Ocado Group\n\nWhen our journey started, we were all about changing the way people shop for groceries. 20 years later, we’re developing our own ground breaking technology, opening multiple sites at an accelerating rate and engaging in exciting new business partnerships around the world!\n\nTransforming an industry also means transforming the way we do business. As we continually develop new technologies and customer propositions, we’re looking for the Everyday Pioneers to lead us into the next stage of our evolution: enhancing our capabilities, inspiring our teams and developing new ways of working.\n\nChanging the way the world shops, for good."}
{"text": "experience with the following technologies.\nQuicksite Data VisualizationSQL Data Warehousing toolsData Dashboard OrganizationAWS Data Tools\nThis is a contract-to-hire position, looking for folks onsite in Aurora, Illinois for 2-3 days a week."}
{"text": "Experience/Skills/Training:Minimum and Preferred. Inclusive of Licenses/Certs (include functional experience as well as behavioral attributes and/or leadership capabilities)Basic Qualifications • 8 years of data engineering experience developing large data pipelines• Strong algorithmic problem-solving expertise• Strong fundamental Scala and Python programming skills• Basic understanding of AWS or other cloud provider resources (S3)• Strong SQL skills and ability to create queries to analyze complex datasets• Hands-on production environment experience with distributed processing systems such as Spark• Hands-on production experience with data pipeline orchestration systems such as Airflow for creating and maintaining data pipelines• Some scripting language experience• Willingness and ability to learn and pick up new skillsets• Self-starting problem solver with an eye for detail and excellent analytical and communication skills  Preferred Qualifications • Experience with at least one major Massively Parallel Processing (MPP) or cloud database technology (Snowflake, Redshift, Big Query)• Experience in developing APIs with GraphQL• Deep Understanding of AWS or other cloud providers as well as infrastructure as code• Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices• Familiar with Scrum and Agile methodologies Required Education: Bachelor’s or Master’s Degree in Computer Science, Information Systems equivalent industry experience\n\nThanks and Regards,Malaysis Roymalaysis.roy@infoservices.com"}
{"text": "experiences and business solutions capable of powering the most high-scale websites in the world.\n\nMotivation\n\nYou value world-class journalism and are eager to immerse yourself in features and projects that support that mission.You are a technically savvy critical thinker who enjoys problem-solving.You receive feedback enthusiastically and thrive in a collaborative environment.You are a born self-starter with an earnest work ethic.You are eager to evolve and grow your skill-set by learning and sharing the latest machine learning technologies.\n\nResponsibilities\n\nCollaborate with cross-functional agile teams of data scientists, machine learning engineers, software engineers, and others in building machine learning infrastructure that best supports the ML needs.Have a strong problem-solving ability and a knack for statistical analysis.Apply machine learning technologies to build statistical models with large amounts of data.Analyze large and complex data sets to derive valuable insights, inform feature and product development.Deploy ML models under the constraints of scalability, correctness, and maintainability.Monitor and ensure the quality of machine learning solutions by implementing process and control disciplines as well as tooling to govern.Proven ability to communicate with business and know business needs, and align our data products with our business goals.\n\nQualifications:\n\nMinimum Qualifications\n\nBachelor’s degree in Computer Science, Mathematics, Statistics, or related field2 year+ machine learning and data science experience1 year+ professional experience with Python\n\nPreferred Qualifications\n\n2 year+ experience with AWS2 year+ experience with clustering, classification, sentiment analysis, time series, and deep learningKnowledge in Economics, Finance, or related fieldPh.D. degree in Computer Science, Mathematics, Statistics, or related field\n\nWherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:\n\nCompetitive medical, dental and vision coverageCompany-paid pension and 401(k) matchThree weeks of vacation and up to three weeks of paid sick leaveNine paid holidays and two personal days20 weeks paid parental leave for any new parentRobust mental health resourcesBackup care and caregiver concierge servicesGender affirming servicesPet insuranceFree Post digital subscriptionLeadership and career development programs\n\nBenefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.\n\nThe Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed. \n\nThe innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?\n\n#washpostlife"}
{"text": "Skills: Python, Pyspark, SQL, Datawarehouse, Databricks, Snowflake (Minimal), etcType of Role: Full-Time on the client's W2Visa: Any Visa that can do Full-Time only can apply ONLYLocation: New York City, NY and/or Iselin, NJExperience: Over 9+years of relevent Experience\nWe are seeking a Senior Cloud Data Engineer to join our team in NYC, NY/ Iselin, NJ (Need Onsite day 1, Hybrid 3 days from office, No Remote). Full Time Salary Range: $120K - $140K Need local candidates based in NYC, NY/Iselin, NJ only. Mandatory skills: Python, Pyspark, SQL, Datawarehouse, Databricks, Snowflake (Minimal) Job Description: This position is for a Senior Cloud Data Engineer with a background in Python, Pyspark, SQL and data warehousing for enterprise level systems. The position calls for someone that is comfortable working with business users along with business analyst expertise. Experience of Delta Lake, DWH, Data Integration, Cloud, Design and Data Modelling.Proficient in developing programs in Python and SQLExperience with Data warehouse Dimensional data modeling.Working with event based/streaming technologies to ingest and process data.Working with structured, semi structured and unstructured data.Optimize Databricks jobs for performance and scalability to handle big data workloads. Monitor and troubleshoot Databricks jobs, identify and resolve issues or bottlenecks. Implement best practices for data management, security, and governance within the Databricks environment. Experience designing and developing Enterprise Data Warehouse solutions.Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process.Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards. Qualifications: 5+ years Python coding experience.5+ years - SQL Server based development of large datasets5+ years with Experience with developing and deploying ETL pipelines using Databricks Pyspark.Experience in any cloud data warehouse like Synapse, Big Query, Redshift, Snowflake.Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills.Experience with Cloud based data architectures, messaging, and analytics.Cloud certification(s).Any experience with Airflow is a Plus"}
{"text": "Skills:o Big Data: Spark, Scala, Pyspark, HDFSo Microsoft Stack: MS-SQL with strong knowledge in RDBMS conceptso Scripting Languages: Batch Script, Shell Script, PythonAdditional Skills:o Cloudera Data platform (CDP)o Agile, Scrum, Jira, Git, SVN, Liquibase\nRole & experience in database migration to MS SQL Server/HadoopExtensive experience in database query tuning, performance tuning, and troubleshooting application issues on OLTP/OLAP systems.RDBMS Architecture, T-SQL query and Query Optimization knowledge and work experienceProvide support to team members and helping them to understand the projects and requirements and guiding them to create the optimized solution of it.team player and proven track record of working in various team sizes performing cross-functional roles.Setup CICD pipeline for database changes using GitHub, Jenkin & Liquibase)Good to have experience data migration from Hadoop to CDP"}
{"text": "experience a little bit better.\" - Jeff Bezos, Founder & CEO.\n\nWe didn’t make Amazon a multi-billion dollar company, our customers did and we want to ensure that our customers always have a positive experience that keeps them coming back to Amazon. To help achieve this, the Worldwide Defect Elimination (WWDE) team relentlessly focuses on maintaining customer trust by building products that offer appropriate resolutions to resolve issues faced by our customers. WWDE engineers solve complex problems and build scalable, cutting edge solutions to help our customers navigate through issues and eliminate systemic defects to prevent future issues.\n\nAs a Senior Data Engineer, you will partner with Software Developers, Business Intelligence Engineers, Scientists, and Program Managers to develop scalable and maintainable data pipelines on both structured and unstructured (text based) data. The ideal candidate has strong business judgment, good sense of architectural design, written/documentation skills, and experience with big data technologies (Spark/Hive, Redshift, EMR, +Other AWS technologies). This role involves both overseeing existing pipelines as well as developing brand new ones for ML). The operating environment is fast paced and dynamic, however has a strong team spirited and welcoming culture. To thrive, you must be detail oriented, enthusiastic and flexible, in return you will gain tremendous experience with the latest in big data technologies as well as exposure (as desired) to statistical and Natural Language modeling through collaboration with scientist on global issue detection models and development.\n\nA day in the life\n\nIf you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply!\n\nAmazon offers a full range of benefits that support you and eligible family members, including domestic partners and their children. Benefits can vary by location, the number of regularly scheduled hours you work, length of employment, and job status such as seasonal or temporary employment. The benefits that generally apply to regular, full-time employees include:\n\n Medical, Dental, and Vision Coverage Maternity and Parental Leave Options Paid Time Off (PTO) 401(k) Plan\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nSeattle, WA, USA\n\nBasic Qualifications\n\n 5+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Experience providing technical leadership and mentoring other engineers for best practices on data engineering Bachelor's Degree\n\nPreferred Qualifications\n\n Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience providing technical leadership on high-impact cross-fucntional technical project\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $123,700/year in our lowest geographic market up to $240,500/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2617769"}
{"text": "skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers.  ResponsibilitiesAnalyze raw data: assessing quality, cleansing, structuring for downstream processingDesign accurate and scalable prediction algorithmsCollaborate with engineering team to bring analytical prototypes to productionGenerate actionable insights for business improvements\nQualifications\nDegree 1-3 Years of Experience (industry experience required for years) or Ph.D. Degree 0-2 Years of Experience (in school experience will be considered)with scientists to define/understand work and data pipelines in-labBenchling protocols and templates to capture necessary data and align across teams.Have coding experience SQL, Python, and LIMS Lab Information Systemexperience, industry setting (biotech)Experience (or Gene Data or comparable), Bench Experience in Molecular Biology"}
{"text": "experiences for athletes, coaches, players and fans. We equip more than 30MM players, coaches, athletes, sports administrators and fans in 40 countries with more than 25 sports solution products, including SportsEngine, the largest youth sports club, league and team management platform; GolfNow, the leading online tee time marketplace and provider of golf course operations technology; GolfPass the ultimate golf membership that connects golfers to exclusive content, tee time credits, and coaching, tips; TeamUnify, swim team management services; and GoMotion, sports and fitness business software solutions.\n\nAt NBC Sports Next we’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology that provides the ultimate in immersive experiences.\n\nGolf fuses the team behind products and services like GolfNow, T\n\nCome join us as we work together as one team to innovate and deliver what’s Next. \n\nJob Description\n\nGolfNow has an exciting opportunity for an experienced Data Engineer II. In this role as part of the Data Engineering Team, you work to manage the full lifecycle of our data warehousing needs. You will read and write complex queries, demonstrate the ability to create database objects (tables, views, stored procedures, user-defined functions) and create and maintain ETL pipelines. Our data warehouse and data operations are built on top of Microsoft and AWS technologies including MS SQL Server, SSIS, PowerShell, and other AWS cloud technologies. To perform this job successfully, an individual would need to be able to understand complex business processes, gather requirements, work efficiently, and verify their results.\n\nResponsibilities Include But Are Not Limited To\n\nWork within a small team of passionate data engineers and data scientists.Compile user requirements and specifications for reports.Contribute to the management of the day-to-day operations of running our Data Warehouse.Build, analyze and manage reports and dashboards for business stakeholders.Respond to users to troubleshoot and/or improve existing reports.Collaborate with internal QA on customer acceptance testing.Develop SQL scripts and objects to support reporting functionality and performance.Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis.Assist in building scalable data models to support reporting and tracking of key business and product metrics.Help identify better practices, tools, and relevant trends that can positively influence the data operations across the business.Other duties may be assigned as needed by management.\n\nQualifications\n\nAll candidates must meet the qualifications below:\n\nA minimum of 3 years of data engineering experience is required.Bachelor’s Degree in Computer Science or related field/relevant industry experience in data engineering.Strong experience with SQL Server database and related technologies such as SSIS, SSRS and SSMSAdvanced knowledge of TSQL tuningExperience in the Azure Cloud Environment including ETL processingExperience in the AWS Cloud Environment including ETL processingAdvanced experience and knowledge of T-SQL Microsoft SQL Server Database Platforms.Working experience developing and refactoring SQL Stored Procedures.Experience using source control with Git or Team Foundation Server.Experience with modeling data structures in both transactional and analytical platforms.Experience with one of the following BI Tools: Tableau, Power BI\n\nDesired Qualifications Are As Follows\n\nExperience with AWS resources including Glue, S3, Lambda functions and Step Functions are a plusExperience with Datadog is a plusExperience with Apache Airflow is a plusExperience with PowerShell scripting is a plusExperience working in Agile environmentExperience managing SDLC process with Atlassian tools. (Jira, Confluence)Able and eager to learn new technologies.Able to easily transition between high-level strategy and day-to-day implementation.Excellent teamwork and collaboration skills.Results-oriented and self-motivated.\n\nFully Remote: This position has been designated as fully remote, meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence.\n\nAdditional Information\n\nNBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.\n\nIf you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726."}
{"text": "Requirements:\n\nBachelor's degree in Computer Science, Computer Engineering, Financial Engineering or a related field.Proficiency in Python and SQL. Strong analytical and quantitative skills.Excellent problem-solving skills.Ability to thrive in a fast-paced and collaborative team environment.No trading industry required. 1-2 years\n\n2+ Years"}
{"text": "experience to our users, and in doing so we are partnering with the Global Business Solutions Research and Insights team. The Analysts/Insights Partner global community under the Research and Insights (R&I) teams across the globe, is a thriving and critical resource for TikTok's Global Business Solutions team providing custom analysis of consumer behavior through 1P content data hosted on data tables spread across different teams/owners. We are looking for a Data Engineer with the skills and curiosity to see the human being behind the swipes, views, clicks and likes and build the data infrastructure to organize and access this data in a privacy compliant, aggregated and anonymized way, so Research and Insights' Analysts can query it to draw consumer insights for our clients.\n\nIn order to enhance collaboration and cross-functional partnerships, among other things, at this time, our organization follows a hybrid work schedule that requires employees to work in the office 3 days a week, or as directed by their manager/department. We regularly review our hybrid work model, and the specific requirements may change at any time.\n\nResponsibilities\n- Collaborate with cross-functional teams, including analysts, and software engineers, to understand data requirements and develop scalable solutions\n- Design, build, and maintain efficient and reliable data pipelines from our data lake to our data marts, ensuring data quality and integrity\n- Define metrics and create / maintain dashboards for measuring and reporting key performance indicators \n- Build and manage data inventories and data flow mappings by collecting and aggregating datasets from multiple data source systems\n- Implement data governance and security measures to protect sensitive information and comply with industry regulations\n- Monitor and optimize the performance of data infrastructure, troubleshoot issues, and propose enhancements to ensure maximum efficiency and reliability\n- Stay up to date with emerging technologies and trends in data engineering and make recommendations for their implementation when relevant. \n- Contribute to developing and maintaining documentation for data pipelines, processes, and systems \n\nQualifications\n\n Minimum Qualifications\n- Bachelor’s degree in computer science, Engineering, or a related field.\n- Proficiency in programming languages such as Python, SQL, and experience with ETL tools\n- Proficiency working with multiple large and linked databases\n- Strong understanding of data modeling and database design principles.\n- Experience with big data technologies such as PostgreSQL databases. Familiarity with data governance, privacy, and security practices. \n- Proficiency in writing and communicating in Mandarin, due to cross functional partnerships with Mandarin speaking colleagues\n\nPreferred Qualifications\n- 3 years of experience operating within a data engineer facet or a related field. \n- Excellent problem-solving skills and ability to work independently and in a team environment. \n- Strong communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams and present technical concepts to non-technical stakeholders.\n\n\nD&I Statement\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\nAccommodation Statement\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/ktJP6\n\nData Security Statement\nThis role requires the ability to work with and support systems designed to protect sensitive data and information. As such, this role will be subject to strict national security-related screening. \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $108300 - $168800 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "experiences Spectrum is known for.\n\nBE PART OF THE CONNECTION\n\nAs a Data Scientist in the Credit Services department, you’ll work in a fast-paced, collaborative environment to develop data-driven solutions to Charter’s business problems. You’ll be empowered to think of new approaches, use analytical, statistical and programming skills to analyze and interpret data sets, and learn new skills while growing your career with Spectrum.\n\nWhat Our Data Scientists Enjoy Most\n\nLeveraging knowledge in analytical and statistical algorithms to assist stakeholders in improving their businessPartnering on the design and implementation of statistical data quality procedures for existing and new data sourcesCommunicating complex data science solutions, concepts, and analyses to team members and business leadersPresenting data insights & recommendations to key stakeholdersEstablishing links across existing data sources and finding new, interesting data correlationsEnsuring testing and validation are components of all analytics solutions\n\nYou’ll work in a dynamic office environment. You’ll excel in this role if you are a self-starter who can work independently as well as in a team. If you’re comfortable presenting data and findings in front of team members & stakeholders and have excellent problem-solving skills, this could be the role for you.\n\nRequired Qualifications\n\nWHAT YOU’LL BRING TO SPECTRUM\n\nExperience: Data analytics experience: 3 years, programming experience: 2 yearsEducation: Bachelor’s degree in computer science, statistics, or operations research, or equivalent combination of education and experienceTechnical skills: Python, R, comprehensive SQL skill, Spark, HiveSkills: Experience with analytics and modeling on large datasets encompassing millions of records; Experience with the full model development and implementation cycle from ideation; Research, train and test models to model implementationAbilities: Perform in-depth & independent research and analysis; Experience using a data science toolkit such as Python or R, command of statistical techniques and machine learning algorithms; Ability to work with minimum supervision; Effective communication, verbal and written, relationship management, and customer service skills with a focus on working effectively in a team environmentTravel: As required (10%)\n\nPreferred Qualifications\n\nEducation: Graduate degree in statistics, mathematics, analytics or operations researchExperience: Experience in working with large consumer data to discern consumer behaviors and risk profiles, ideally in telecommunication or banking industries.\n\nSPECTRUM CONNECTS YOU TO MORE\n\nDynamic Growth: The growth of our industry and evolving technology powers our employees’ careers as they move up or around the companyLearning Culture: We invest in your learning, and provide paid training and coaching to help you succeedSupportive Teams: Be part of a strong community that gives you opportunities to network and grow, and wants to see you succeed Total Rewards: See all the ways we invest in you—at work and in life\n\nApply now, connect a friend to this opportunity or sign up for job alerts!\n\nBDA303 2023-25170 2023\n\nHere, employees don’t just have jobs, they build careers. That’s why we believe in offering a comprehensive pay and benefits package that rewards employees for their contributions to our success, supports all aspects of their well-being, and delivers real value at every stage of life.\n\nA qualified applicant’s criminal history, if any, will be considered in a manner consistent with applicable laws, including local ordinances.\n\nGet to Know Us Charter Communications is known in the United States by our Spectrum brands, including: Spectrum Internet®, TV, Mobile and Voice, Spectrum Networks, Spectrum Enterprise and Spectrum Reach. When you join us, you’re joining a strong community of more than 101,000 individuals working together to serve more than 32 million customers in 41 states and keep them connected to what matters most. Watch this video to learn more.\n\nWho You Are Matters Here We’re committed to growing a workforce that reflects our communities, and providing equal opportunities for employment and advancement."}
{"text": "requirements, identify key business needs and translate those into technical specifications. Integrate seamlessly with client teams, adapting to various business environments and challenges.Develop and optimize SQL queries, stored procedures, and scripts for data transformation and extraction.Collaborate with data scientists and analysts to understand data requirements and ensure data availability.Implement data quality checks and data validation processes to ensure data accuracy and consistency.Utilize Databricks for advanced data processing, transformation, and analytics.Manage and optimize data storage.Build and maintain data warehouses and analytics solutions.Create interactive reports and dashboards using Power BI for data visualization and insights.Monitor and troubleshoot data pipelines, addressing any issues in a timely manner.Stay up-to-date with the latest Azure data technologies and best practices.\n\nRequired Qualifications\n\nBachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience). 3+ years of experience as a Data Analyst, BI Developer, or similar role.  Experience in data analysis and business intelligence with technical experience and/or certifications with technologies such as Microsoft Power BI. Exceptional communication skills, with the ability to understand complex analytics requirements and to clearly communicate and demonstrate  A proactive approach to problem-solving and a continuous improvement mindset.Experience with data exploration and data profiling. \n\nPreferred Qualifications\n\nFamiliarity with SQL, Python, or other languages used for data exploration. Familiarity with leading data platforms such as Snowflake, Databricks, Microsoft Fabric as a data source for BI tools. Familiarity with data modeling concepts and the ability to recognize when data is in the right format to meet analytical needs. Experience in a consulting role and understanding of the dynamics of professional services. General acumen with AI technologies to enhance data solution development.\n\nThe Perks\n\nComprehensive medical, dental and vision plans for you and your dependents401(k) Retirement Plan with Employer Match, 529 College Savings Plan, Health Savings Account, Life Insurance, and Long-Term DisabilityCompetitive CompensationTraining and development programsStocked kitchen with snacks and beveragesCollaborative and cool cultureWork-life balance and generous paid time offTo all recruitment agencies: Trace3 does not accept unsolicited agency resumes/CVs. Please do not forward resumes/CVs to our careers email addresses, Trace3 employees or any other company location. Trace3 is not responsible for any fees related to unsolicited resumes/CVs.\n\nActual salary will be based on a variety of factors, including location, experience, skill set, performance, licensure and certification, and business needs. The range for this position in other geographic locations may differ. Certain positions may also be eligible for variable incentive compensation, such as bonuses or commissions, that is not included in the base salary.\n\nEstimated Pay Range\n\n$106,300—$143,800 USD"}
{"text": "skills, and a deep understanding of mortgage banking.\n\nThe role can take advantage of Lakeview’s offices in Coral Gables, FL hybrid or fully-remote anywhere in the United States.\n\nResponsibilities\n\nGather and analyze data from various sources to identify trends, opportunities, and areas for improvement within the sales and marketing departments.Manage data-driven decision-making processes and support the implementation of BI tools.Collaborate and partner with cross-functional teams, provide recommendations, and contribute to the enhancement of business strategies.Work closely with Sr. Leaders in Sales and Marketing to develop reports to track and monitor the success of the business. Examples of reports would be marketing campaigns, recapture metrics, and MLO performance.Extracting, transforming, and loading data from various sources to create cohesive datasets in PowerBI.Writing code and designing dashboards for report requestors using SQL and PowerBI.Oversee and manage tickets within Lakeview’s Originations and Correspondents report pipeline, and complete in a timely manner.Research and analyze key business requirements needed for report requests to translate into analytical solutions.Conduct ad-hoc analyses to support business initiatives and address specific inquiries from stakeholders.Maintain data integrity and ensure accuracy of reports by performing regular quality checks and data validation.Act as a subject matter expert on reporting tools and methodologies.Provide guidance and education to other Business Intelligence Analysts as necessary.Other duties as assigned.\n\nQualifications\n\nBachelor’s degree in Computer Science, Finance, Business Administration, Mathematics, Business Intelligence, or related field.\n\nMinimum of 5 years of experience in business intelligence, analytics, or related field. Preferably with a focus on sales and marketing within the mortgage banking industryStrong knowledge of SQL, DAX and PowerQuery.Proficient in using MS Excel, PowerBI, Snowflake, and SSMS.Strong analytical skills.Experience working with large datasets and data warehousing concepts.Excellent communication and presentation skills, with the ability to effectively communicate findings to both technical and non-technical audiences.Ability to interact and communicate with stakeholders, particularly executives or senior leaders.Ability to work independently and manage multiple priorities in a fast-paced environment.\n\nCertifications, Licenses, and/or Registration\n\nN/A\n\nPhysical Demands and Work Environment\n\nWhile performing the duties of this job, the employee is regularly required to sit and use hands to handle, touch or feel objects, tools, or controls. The employee frequently is required to talk and hear. The noise level in the work environment is usually moderate. The employee is occasionally required to stand; walk; reach with hands and arms. The employee is rarely required to stoop, kneel, crouch, or crawl. The employee must regularly lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, color vision, and the ability to adjust focus.\n\n\n\nBayview is an"}
{"text": "experiences and affordable and transparent financial services products. Coast's mission is to deliver them at a transformational scale, and to improve working lives in one of the country’s biggest industry sectors. The company is backed by top fintech and mobility venture funds.\nCoast is competing and winning with software, and we are building a world-class engineering team. This is a unique opportunity to get in on the ground level early on, contribute and learn in myriad ways, make key decisions, expand your impact as the business grows, have fun, and learn a ton in the process. If you're practical and business-minded, smart and creative, and excited about the rewards and the energy of an early-stage venture-backed startup, we'd love to hear from you.\nWe believe in re-using existing tools as much as possible so that we can focus on building products for our clients. At the same time we do not hesitate to roll up our sleeves and build our own if needed. Since we benefit from Open Source Software so much, we try to contribute back, e.g. https://github.com/loyada/dollarx and https://github.com/loyada/typedpy, and are excited about the potential of more contributions in the future.\nWe are looking for a founding member of the Coast data engineering team to shape our company’s data culture and underlying infrastructure.\nWe have followed the path of least resistance so far, creating a data warehouse and pulling in both operational and vendor data, adding BI and other SaaS tools on top of it.\nNow as we are growing, we recognize the need for a dedicated leader for all things data at Coast - someone that can work with our business users, establish company-wide self-serve data infrastructure, and enable product engineering teams to build data products well.\nWe are looking for someone that can champion data-aware culture within the company, as well as roll up their sleeves and build out the technical pieces behind it. Looking ahead, we need to position ourselves well for feature engineering work that will power our AI/ML use cases. This means metadata, automation, observability, and quality.\nWe need you to help us establish a vision for the data ecosystem evolution while satisfying day to day demands of a rapidly growing early stage startup.\nThe Data Engineer will:lead design and implementation of all aspects of our data ecosystem — from obtaining third party data to building our own data products, from infrastructure architecture to end-user BI and data exploration toolchain;evangelize and implement the best practices, from reasoning about statistical significance to implementing headless BI, from source control and change management to database migrations;establish guardrails for self-serve ecosystem for the business users;help our product engineering teams evolve from treating data as exhaust to building DDD-based data products;establish ETL/ELT patterns, from landing zone to semantic layers;ensure that our metrics are built on top of consistent, curated data with clear stewardship;oversee our connected SaaS data landscape;own the budget for the data infrastructure and develop a sensible cost allocation model;remain relentlessly pragmatic and balance the daily demands or a fast-growing startup business with the needs of a well-managed platform.\nThe Data Engineer must:have 7-10+ years experience working with first-class engineering teams with a proven track record;have hands-on experience working across the data ecosystem, from modern ETL/ELT and orchestration to data warehouses and columnar stores, from BI tooling for less-technical business users to SQL optimization;have software engineering mindset, leading with the principles of source control, infrastructure as code, testing, modularity, automation, and observability;bring in a strong professional network, since it is impossible to know everything, and one must be able to tap others for advice;have experience working directly with product engineers as well as business users;be proficient in Python, since you would be expected to contribute data platform aspects into product engineering code as well as write your own tools;have experience with one of Terraform/CloudFormation/Pulumi/CDK since we treat our infrastructure as code;be able to figure stuff out - the modern data space is deep and complex, and there are many ways of solving the same problem; you need to be able to go off on your own, research and design a solution, implement technical spikes, and then deliver it through responsible change management;have an owner mindset and continuously look for, notice, and implement improvements to our data infrastructure, because small continuous improvements matter;be a thought-leader that keeps a finger on the pulse of the industry - vendor landscape, industry trends;eventually hire and run a data org as we scale.\nCompensation:Our salary ranges are based on paying competitively for our size and industry, and are one part of our total compensation package that also includes benefits, signing bonus, and equity. Pay decisions are based on a number of factors, including scope and qualifications for the role, experience level, skillset, and balancing internal equity relative to other Coast employees. We expect the majority of the candidates who are offered roles at Coast to fall healthily within the range based on these factors.Salary range: $185,000 - $220,000 annuallySigning bonusEquity grant: commensurate with level determined at the discretion of the company, with meaningful potential upside given the company’s early stageBenefits overview:Medical, dental and vision insuranceUnlimited paid time off (vacation, personal well being, paid holidays)Paid parental leave$400 accessories allowance for home office setup to be spent on a keyboard, mouse, headphones, etc.Free lunch every Friday\nAbout CoastCoast is founded and led by Daniel Simon, who previously cofounded Bread (breadpayments.com), a leading payments and credit technology firm backed by some of the world’s top VCs which was acquired for $500MM+ in 2020.\nCoast recently raised $27.5M in a Series A financing co-led by Accel and Insight Partners. We're also backed by top fintech and mobility venture funds – including Better Tomorrow Ventures, Bessemer Venture Partners, BoxGroup, Foundation Capital, Greycroft, and Colle – and premier angel investors – including Max Levchin (Affirm), Josh Abramowitz (Bread), Jason Gardner (Marqeta), William Hockey (Plaid), Ryan Petersen (Flexport), and many others.\nCoast is committed to diversity, equity, and inclusion. We are building a diverse and inclusive environment, so we encourage people of all backgrounds to apply. We’re"}
{"text": "SKILLS AND EXPERIENCE· Prior experience within hyper growth start up environment or construction industry· Extensive LLM experience necessary· Experience working with an array of models (recommendation models, NLP, LLM, search) in an full end -to-end setting, seeing the models through deployment· Proven success driving a project 0-1, from ideation through deployment· RAG experience is a plus· Currently working with large unstructured data sets· Bachelors or Masters in STEM field ; computer science, engineering, mathematics· Proficiency and experience within PowerBI, Python, SQL, Microsoft Dataverse\n\nBENEFITSAs a Senior MLOPS Engineer, you can expect to earn up to $200,000 (depending on experience), highly competitive benefits and equity.\nHOW TO APPLYPlease register your interest by sending your Resume to Grace McCarthy via the Apply link on this page\nKEYWORDSMachine Learning | MLOPS | LLM | AI | Python | SQL | Deep Learning | Data Modeling | RAG | NLP | Start Up | Recommendation Systems"}
{"text": "Experienced Candidates \n﻿We need a Data Scientist with demonstrated expertise in training and evaluating transformers such as BERT and its derivatives.\nJD:\nProject Scope and Brief Description:\nNext-generation Artificial Intelligence for Genomics will use more complex datatypes and be applied to new crop contexts. We need a Data Scientist with demonstrated expertise in training and evaluating transformers such as BERT and its derivatives.\nSkills/Experience:\nRequired: Proficiency with Python, pyTorch, Linux, Docker, Kubernetes, Jupyter. Expertise in Deep Learning, Transformers, Natural Language Processing, Large Language ModelsPreferred: Experience with genomics data, molecular genetics. Distributed computing tools like Ray, Dask, Spark."}
{"text": "qualifications:\n\nBachelor's degree in Computer Science, Information Systems, related technical field, or equivalent practical experience.3 years of experience developing/deploying machine learning and time series forecasting models using statistical software (e.g., Python, R).3 years of experience applying statistical modeling, hypothesis testing, and experimentation.3 years of experience analyzing data, and working with SQL and databases.\n\nPreferred qualifications:\n\nMaster's degree or PhD in Computer Science, Data Science, Mathematics, Economics, Physics, Engineering, Management Information Systems, Statistics, Accounting, or a similar field.Experience in product analytics, customer support, business intelligence, data science, or data warehousing.Ability to collaborate with internal stakeholders across different functions and geographies to manage change and drive improvement initiatives.Excellent communication skills, with the ability to influence team members and manage business priorities.\n\nAbout The Job\n\nThe Google Cloud team helps companies, schools, and government seamlessly make the switch to Google products and supports them along the way. You listen to the customer and swiftly problem-solve technical issues to show how our products can make businesses more productive, collaborative, and innovative. You work closely with a cross-functional team of web developers and systems administrators, not to mention a variety of both regional and international customers. Your relationships with customers are crucial in helping Google grow its Cloud business and helping companies around the world innovate.\n\nAs part of the Cloud Support Data Science team, you’ll play a key role in using data and machine intelligence to empower data-driven execution of strategy and operations for Google customers. The team works collaboratively with Sales, Engineering, and other Cloud Support teams to build analytics solutions that enable actionable insights to provide an effortless customer experience. In this role, you’ll work on a variety of stakeholder projects with opportunities to address problems that require innovative solutions and data products.\n\nGoogle Cloud accelerates every organization’s ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.\n\nThe US base salary range for this full-time position is $150,000-$223,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\n\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .\n\nResponsibilities\n\nBuild and maintain data pipelines and time series models to generate support case volume forecasts that enable long-term capacity planning and short-term scheduling decisions.Lead monthly business reviews with senior stakeholders, sharing insights on drivers of change across a dynamic organization.Engage with the organization to identify, prioritize, frame, and structure ambiguous challenges.Define the analytical direction for the team, and influence the direction of the associated engineering and infrastructure work.\n\n\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to"}
{"text": "requirements to support data-driven solutions/decisions.complex data insights in a clear and effective manner to stakeholders across the organization, which includes non-technical audience.informed and stay current on all the latest data science techniques and technologies.for exploring and implementing innovative solutions to improve data analysis, modeling capabilities, and business outcomes.use case design and build teams by providing guidance/ feedback as they develop data science models and algorithms to solve operational challenges. The incumbent must bring these skills/qualifications:Master’s or PhD in Computer Science, Statistics, Applied Mathematics.If degree is in non-related field, must have at least 5 – 7 years’ experience in data science or a similar role.Must be proficient in at least one analytical programming language relevant for data science, such as Python. R will be acceptable. Machine learning libraries & frameworks are a must. Must be familiar with data processing and visualization tools (e.g., SQL, Tableau, Power BI).Must have experience with full Machine Learning lifecycle - feature engineering, training, validation, scaling, deployment, monitoring, and feedback loop.Expertise in advanced analytical techniques (e.g., descriptive statistics, machine learning, optimization, pattern recognition, cluster analysis, etc.)Experience in Supervised and Unsupervised Machine Learning including classification, forecasting, anomaly detection, pattern recognition using variety of techniques such as decision trees, regressions, ensemble methods and boosting algorithms.,Experience with cloud computing environments (AWS, Azure, or GCP) and Data/ML platforms (Databricks, Spark)."}
{"text": "Experience You'll Bring\n\nMaster's degree in Computer Science, Statistics, Mathematics, or a related quantitative field (we will consider exceptional candidates without advanced degrees)3+ years of hands-on experience in developing and deploying AI/ML models in a production environmentStrong expertise in machine learning algorithms, deep learning frameworks (e.g., TensorFlow, PyTorch), and statistical modeling techniquesProficient in programming languages such as Python, R, and SQL for data manipulation and analysisExperience with big data technologies (e.g., Snowflake, Google BigQuery) and cloud computing platforms (e.g., AWS, GCP, Azure)Excellent problem-solving skills and ability to think creatively to develop innovative AI/ML solutionsStrong communication and collaboration skills to effectively work with cross-functional teams and stakeholdersProven track record of delivering high-impact AI/ML projects in a fast-paced and dynamic environmentDemonstrated ability to persevere and iterate through solutions in the face of technical challenges and setbacksExperience with embedding AI/ML models into web and mobile applicationsHands-on experience in building solutions leveraging the latest AI capabilities and advancements, including large language models, generative AI, and other state-of-the-art techniques\n\nWhat We Offer\n\nA dynamic, innovative, and collaborative work environment that fosters creativity and encourages out-of-the-box thinkingOpportunity to work on cutting-edge AI/ML projects that have a significant impact on various industries, including fleet & EV, Health Benefits, and Corporate PaymentsAccess to state-of-the-art technology, tools, and resources to support your work and professional developmentMentorship from experienced AI/ML professionals who are leaders in their field and dedicated to helping you grow and succeedCollaborative opportunities with cross-functional teams, allowing you to expand your skills and knowledge across various domainsContinuous learning and development opportunities, including workshops, conferences, and training programs to keep you updated with the latest advancements in AI/MLFlexible work arrangements that promote work-life balance and support your personal and professional well-beingCompetitive compensation package, including salary, benefits, and performance-based bonusesJoin a purpose-driven organization that is committed to making a positive impact on society through innovation and technologyBe part of a collaborative, inclusive, and supportive team that values diversity, integrity, and excellenceEnjoy a culture that promotes work-life balance, fosters continuous learning, and celebrates individual and collective achievementsAlign yourself with a company that prioritizes customer-centricity, sustainability, and ethical business practicesA diverse and inclusive workplace that values and celebrates individual differences and fosters a sense of belonging\n\nThe base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the \"About Us\" section.\n\nSalary Pay Range: $113,000.00 - $150,000.00"}
{"text": "skills in the broader Capital One team  Promote a culture of engineering excellence, using opportunities to reuse and innersource solutions where possible  Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization  Operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in an unified manner  Lead the way in creating next-generation talent for Tech, mentoring internal talent and actively recruiting external talent to bolster Capital One’s Tech talent \n\nBasic Qualifications:\n\n Bachelor’s Degree  At least 9 years of experience in data engineering  At least 5 years of experience in data architecture  At least 2 years of experience building applications in AWS \n\nPreferred Qualifications:\n\n Masters’ Degree  3+ years of data modeling experience  2+ years of experience with ontology standards for defining a domain  2+ years of experience using Python, SQL or Scala  1+ year of experience deploying machine learning models  3+ years of experience implementing data processing solutions on AWS \n\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "Qualifications\n Data Science, Statistics, and Data Analytics skillsData Visualization and Data Analysis skillsExperience with machine learning algorithms and predictive modelingProficiency in programming languages such as Python or RStrong problem-solving and critical thinking abilitiesExcellent communication and presentation skillsAbility to work independently and remotelyExperience in the field of data science or related rolesBachelor's degree in Data Science, Statistics, Computer Science, or a related field"}
{"text": "experienced staffDocument new system components, or modifications to existing componentsImprove how we deliver solutions with process innovation, application of new technologies, and task automationEnsure the security and integrity of system and product solutions including compliance with Navy Federal, industry engineering and Information Security principles and practicesEnsure the quality of deliverables by developing automated controls and performing unit, integration, and user acceptance testingPerform engineering technology research, procurement, deployment, and configuration for new and modified systemsPresent clear, organized and concise information to all audiences through a variety of media to enable effective business decisionsPerform other duties as assigned\n\n\nQualifications\n\nSignificant experience building Data Warehouses (Azure Synapse Analytics or similar), Data Lakes (Azure Data Lake or similar), ETL/ELT pipelines (Databricks or similar), and data streaming (Azure Event Hub, Kafka, Cosmos, MongoDB, or similar)Expert knowledge of SQLExperience in data engineering programming languages. (Python, Spark, or similar) Knowledge of data engineering requirements and design practices including fact qualifier matrices, dimensional modelingData modeling, including knowledge of modeling tools and experience building consumption layer models with Erwin or similar modeling toolsExperience modeling and designing no-SQL streaming solutions with Hackolade or similar toolsExperience in error handling, data validation and reconciliationExperience working in retail banking or credit unionsAdvanced research, analytical, and problem solving skillsAdvanced verbal and written communication skillsExperience with Agile software development practicesBachelors in Information Technology, Data Science, Analytics, Computer Science, Engineering or the equivalent combination of training, education, and experience\n\n\nHours: Monday - Friday, 8:00AM - 4:30PM\n\nLocation: 820 Follin Lane, Vienna, VA 22180\n\nAbout Us\n\nYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:\n\n Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services\n\n\n\n\nDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market position\n\nBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."}
{"text": "requirements Bachelor’s degree in Computer Science, Data Science, Math, Engineering or related field plus 4 years of related experience. Prior experience must include Engage in multifaceted collaboration with stakeholders across various functions to convert complex business requirements into customized data-driven, modeling, and analytical resolutions; Frame and lead projects based on key steps in data, analytics, and machine learning lifecycle; Facilitate communication and present modeling results and findings to stakeholders including non-technical audiences; Implement and develop descriptive, predictive, prescriptive analytical/data science models to solve business questions; Mentor and aid fellow data scientists and data science community through projects and events, fostering the growth and development of data science within the organization; Formulate operational procedures to guarantee the delivery of data science work at a consistently high standard of quality. Up to 70% remote work allowed.\n\nApply on-line at www.takedajobs.com and search for Req #R012122\n\nTakeda Compensation And Benefits Summary\n\nWe understand compensation is an important factor as you consider the next step in your career. We are committed to equitable pay for all employees, and we strive to be more transparent with our pay practices.\n\nFor Location \n\nBoston, MA\n\nU.S. Base Salary Range \n\n$108,500.00 - $170,500.00\n\nThe estimated salary range reflects an anticipated range for this position. The actual base salary offered may depend on a variety of factors, including the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the location in which the applicant lives and/or from which they will be performing the job. The actual base salary offered will be in accordance with state or local minimum wage requirements for the job location.\n\nU.S. based employees may be eligible for short-term and/ or long-term incentives. U.S. based employees may be eligible to participate in medical, dental, vision insurance, a 401(k) plan and company match, short-term and long-term disability coverage, basic life insurance, a tuition reimbursement program, paid volunteer time off, company holidays, and well-being benefits, among others. U.S. based employees are also eligible to receive, per calendar year, up to 80 hours of sick time, and new hires are eligible to accrue up to 120 hours of paid vacation.\n\n\n\nTakeda is proud in its commitment to creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law.\n\nLocations\n\nBoston, MA\n\nWorker Type\n\nEmployee\n\nWorker Sub-Type\n\nRegular\n\nTime Type\n\nFull time"}
{"text": "skills and supercharge careers. We help discover passion—the driving force that makes one smile and innovate, create, and make a difference every day. The Hexaware Advantage: Your Workplace BenefitsExcellent Health benefits with low-cost employee premium.Wide range of voluntary benefits such as Legal, Identity theft and Critical Care CoverageUnlimited training and upskilling opportunities through Udemy and Hexavarsity\nExperienced Data ScientistVery Strongly in AI and ML Primary Skills - Minimum 4 to 6 years of experience in AI/ML application build Experience in Generative AI with at least one major cloud LLM Experience in gathering requirements from the client Experience in designing the architecture Should have managed multiple PODs - Product Oriented Development Teams Experience in delivering back the application for continuous operation Manages the continuous usage of the application Exposure in Agile practices Secondary Skills - Certifications in Generative AI Certifications in Agile project Management Member position in Architectural Review Board"}
{"text": "requirements, plan and execute projects, and produce actionable data to support strategic initiatives and drive continuous performance improvement. Design data modeling processes, create algorithms and predictive models to extract key data, and analyze the data and share insights with clinical, operational, and business stakeholders. The right candidate will have a passion for discovering solutions hidden in large data sets and working with customers to improve outcomes and performance.\n\nRequired Education/Experience/Specialized Skills\n\nBachelor's degree.Experience with different healthcare data sources (medical claims, authorizations, pharmacy, provider, membership).Experience with statistical modeling and interpretations of complex data.Experience in analyzing any of the following: Medical expenses & utilization trends, RAF Scores, Comorbidities & disease burden, Healthcare quality measures.Excellent critical and analytical thinking, customer service skills, and strong written and verbal communication skills.Experience in R, Python, or SAS programming.Intermediate knowledge of SQL, ETL, data modeling, and reporting technologies. Intermediate knowledge of Power BI or Tableau.Must have knowledge and skills necessary to explain complex concepts to team members and leadership.Ability to work both independently and collaboratively as part of a team.\n\nPreferred Education/Experience/Specialized Skills/Certification\n\nMaster's Degree in related field (Data Science, Quantitative, Clinical, or Engineering).Experience in working with financial datasets (e.g., accounting, health plan claims, P&L).Prior experience with Epic data structures and reporting environments.Experience in predictive modeling for clinical intelligence.Experience in market intelligence and financial projections.Background or interest in AI, machine learning intelligence.\n\nScripps Health is \n\nPosition Pay Range: $46.88-$70.33/hour"}
{"text": "Experience with cloud platforms such as AWS, Azure, or Google Cloud.Knowledge of containerization technologies (Docker, Kubernetes).Familiarity with deep learning techniques and frameworks.Previous experience in industries such as healthcare, finance, or manufacturing is a plus."}
{"text": "Skills :\na) Azure Data Factory – Min 3 years of project experiencea. Design of pipelinesb. Use of project with On-prem to Cloud Data Migrationc. Understanding of ETLd. Change Data Capture from Multiple Sourcese. Job Schedulingb) Azure Data Lake – Min 3 years of project experiencea. All steps from design to deliverb. Understanding of different Zones and design principalc) Data Modeling experience Min 5 Yearsa. Data Mart/Warehouseb. Columnar Data design and modelingd) Reporting using PowerBI Min 3 yearsa. Analytical Reportingb. Business Domain Modeling and data dictionary\nInterested please apply to the job, looking only for W2 candidates."}
{"text": "requirements of analyses and reports.Transform requirements into actionable, high-quality deliverables.Perform periodic and ad-hoc operations data analysis to measure performance and conduct root cause analysis for Claims, FRU, G&A, Provider and UM data.Compile, analyze and provide reporting that identifies and defines actionable information or recommends possible solutions for corrective actions.Partner with other Operations areas as needed to provide technical and other support in the development, delivery, maintenance, and enhancement of analytical reports and analyses.Collaborate with Operations Tower Leaders in identifying and recommending operational performance metrics; map metrics against targets and the company’s operational plans and tactical/strategic goals to ensure alignment and focus.Serve as a liaison with peers in other departments to ensure data integrity.Code and schedule reports using customer business requirements from Claims, FRU, G&A, Provider and UM data.\n\nPrincipal Accountabilities\n\nConduct operational data analyses to identify root causes; develop actionable information (recommendations, conclusions, and possible solutions); produce reports to evaluate operational efficiencies and effectiveness.Prepare dashboards and other management reports, soliciting information from business teams and serve as liaison for their submissions; ensure quality control; provide oversight to staff when necessary.Identify and collect internal historical data; research and collect external benchmark data; devise more efficient and accurate approaches to vet and prepare metric reports; use sound reasoning and judgment for identifying and applying appropriate analytical approach.Recommend and implement accuracy, efficiency, and productivity enhancements.Maintain documentation library to promote efficient knowledge transfer of data collection strategies and data quality protocols.Work with other areas as needed to ensure recommended solutions meet business requirements.Manage multiple, simultaneous team-based projects along with other individually assigned projects.Provide support in developing & expanding the scope of dashboards and other management reports for distribution to middle and upper management; organize and maintain report methodology documentation.Communicate and collaborate with internal and external stakeholders as needed to support overall EmblemHealth objectives.Perform other related tasks/projects as directed or required.\n\nEducation, Training, Licenses, Certifications\n\nBachelor’s Degree in Business, Data Management, or other related quantitative analysis field of study required\n\nRelevant Work Experience, Knowledge, Skills, And Abilities\n\n4 – 6+ years of relevant work experience including Data Analysis and reporting requiredBusiness Intelligence Experience – Cognos or Tableau; proficiency with SAS requiredProject management experience preferredProficient with MS Office (Word, Excel, Access, PowerPoint, Outlook, Teams, etc.) requiredExperience working with large volumes of data requiredEnergy, drive and passion for End-to-End excellence and customer experience improvement requiredExcellent collaborative skills and the ability to influence management decisions requiredStrong problem solving and analytical skills that be applied across all types of business problems requiredStrong communication skills (verbal, written, presentation, interpersonal, facilitation) with all audiences required\n\nAdditional Information\n\nRequisition ID: 1000001321Hiring Range: $63,000-$110,000"}
{"text": "requirements, consult on display, and aid in customer inquiries\n\nWhat We’re Looking For\n\nProduct owner or business analyst experienceExperience leading cross team initiativesExceptional problem solving and analytical skills; willingness to get into the details and drive clarity from ambiguityThe ability to work in a dynamic environment with shifting priorities and focusExperience in interpreting, and explaining estimate/repair order dataExperience in eliciting requirements and identifying process improvementsHighly collaborative with excellent communication skillsProven ability to push forward on multiple projects simultaneously Ability to work independently within defined metrics and game planStrong organizational and project management skillsMarketing experience – understanding of customers’ needs/wantsExperience working with Agile software development teamsAdvanced Microsoft Excel skillsBA/BS preferred\n\nWhat’s In It For You\n\nCompetitive compensation, benefits and generous time-off policies4-Day summer work weeks and a winter holiday break401(k) / DCCP matchingAnnual bonus programCasual, dog-friendly, and innovative office spaces\n\nDon’t Just Take Our Word For It\n\n10X Virginia Business Best Places to Work9X Washingtonian Great Places to Work9X Washington Post Top WorkplaceSt. Louis Post-Dispatch Best Places to Work\n\nAbout CARFAX\n\nCARFAX, part of S&P Global Mobility, helps millions of people every day confidently shop, buy, service and sell used cars with innovative solutions powered by CARFAX vehicle history information. The expert in vehicle history since 1984, CARFAX provides exclusive services like CARFAX Used Car Listings, CARFAX Car Care, CARFAX History-Based Value and the flagship CARFAX® Vehicle History Report™ to consumers and the automotive industry. CARFAX owns the world’s largest vehicle history database and is nationally recognized as a top workplace by The Washington Post and Glassdoor.com. Shop, Buy, Service, Sell – Show me the CARFAX™. S&P Global Mobility is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets.\n\nCARFAX is an Affirmative Action/Equal Opportunity Employer. It is the policy of CARFAX to provide"}
{"text": "Qualifications:Extensive experience in finance or project management, proficiency in tools like KNIME for data analytics, and familiarity with software like SharePoint and Microsoft Office.\nSkills: 10+ years of experience in finance/ project management.Experience and proficiency building data pipelines and performing analytics using KNIME (or similar software).Experience creating team SharePoint sites and maintaining content to make information and documents easily accessible.Proficiency with Visual Basic for Applications (VBA) for Microsoft Office.Proficiency with SQL and relational database management systems.Strong proficiency with Microsoft Excel.Significant experience building end-user tools with Microsoft Access.Experience in using Lynx UI, Optima Cognos Reporting Tool, and extracting data from Data Globe (especially data schemas: DGSTREAM, DGFU, DGREF & DGLOBE).Good understanding on Loan data hierarchy (Request/Credit Agreement/Facility/GFRN) in Lynx.\nEducation:Bachelor's/University degree.\nJob ID 79226"}
{"text": "SkillsExpertise and hands-on experience on Spark, and Hadoop echo system components – Must HaveGood and hand-on experience* of any of the Cloud (GCP or AWS) – Must HaveGood knowledge of HiveQL & Spark SQL – Must HaveGood knowledge of Shell script & Java/Scala/python – Good to HaveGood knowledge of SQL – Good to HaveGood knowledge of migration projects on Hadoop – Good to HaveGood Knowledge of one of the Workflow engines like Oozie, Autosys – Good to HaveGood knowledge of Agile Development– Good to HavePassionate about exploring new technologies – Good to HaveAutomation approach – Good to Have"}
{"text": "Resource should be able to visualize and explain the Data Models.Should be able to compare and validate the differences.Should be strong in ExcelShould be strong in SQLShould be strong in TableauIntermediate – Python (NumPy, Data Analytics)"}
{"text": "Qualifications: At least 3 years of experience within financial markets. 1+ years of experience using SQL.Professional data experience with futures, foreign exchange, crypto assets, OTC derivatives, and bonds. Bloomberg terminal background.\nThey are looking to make this immediate hire as soon as possible!"}
{"text": "Qualifications\n\nrequirements and partner with the product team to provide a strategic solutionCollaborate with front-end engineers to design or modify the schema for software performanceAdvanced database administration and development, including stored procedures, user-defined functions, triggers, and ETL packages, and security and rolesOptimization and tuning of existing T-SQL stored procedures to improve performanceTroubleshoot database issues, identify causes, and implement solutionsExtract, transform, and load data from multiple data sources using ETL tools such as SSIS or TalendDevelop, Test, Debug, Monitor, and Troubleshoot ETL and software processesRecommend solutions to existing or anticipated issuesFollow implementation standardsBe an escalation point for any problems that may ariseDesign testing requirements and prepare test plans and test modulesDevelop documentation and transfer of knowledgeMinimum Qualifications:Minimum Bachelor's Degree in Computer Sciences, Information Technology, or its equivalentMinimum 3+ years' experience with ETL and databases.Minimum 2+ years' experience with SSIS or Talend.Minimum 2+ years' experience with Python.Familiarity with cloud technologies such as Amazon Web Services (AWS) and Microsoft Azure.Experience with complex processing logicExcellent data modeling experience working with multiple datasetsPosses Testing SkillsAdvanced T-SQL programming skills (stored procedures, functions)Python programming skills.Experience with Databricks.Familiarity with code versioning tools such as GitExperience working with big data technologies such as Hadoop(Hive) and MPP\n\nAdditional Information\n\nOur Publicis Groupe motto \"Viva La Différence\" means we're better together, and we believe that our differences make us stronger. It means we honor and celebrate all identities, across all facets of intersectionality, and it underpins all that we do as an organization. We are focused on fostering belonging and creating equitable & inclusive experiences for all talent.\n\nPublicis Groupe provides robust and inclusive benefit programs and policies to support the evolving and diverse needs of our talent and enable every person to grow and thrive. Our benefits package includes medical coverage, dental, vision, disability, 401K, as well as parental and family care leave, family forming assistance, tuition reimbursement, and flexible time off.\n\nIf you require accommodation or assistance with the application or onboarding process specifically, please contact hrcompliance_usms@publicis.com.\n\nAll your information will be kept confidential according to \n\nCompensation Range: $81,500.00 - $137,500.00 annually. This is the pay range the Company believes it will pay for this position at the time of this posting. Consistent with applicable law, compensation will be determined based on the skills, qualifications, and experience of the applicant along with the requirements of the position, and the Company reserves the right to modify this pay range at any time. For this role, the Company will offer medical coverage, dental, vision, disability, 401k, and paid time off. The Company anticipates the application deadline for this job posting will be 05/30/2024.\n\n\n\nVeterans Encouraged to Apply"}
{"text": "skills will be valued.\n\nRepresentative Responsibilities\n\nData Pipelines:\n\nDesign, develop, and manage data pipelines of varying complexity, ensuring smooth flow from acquisition sources to integration and consumption for key stakeholders like business analysts, data scientists, etc. Ensure compliance with data governance and security standards while operationalizing data pipelines, collaborating effectively with platform engineers and database administrators. Implement best practices for agile development, facilitating seamless updates from development to production environments. Support upgrades and testing initiatives as needed, exploring additional data extraction and analysis options from source systems to meet diverse customer needs. Develop solutions to enhance data analytic capabilities without disrupting transactional systems. \n\nMetadata Management & Data Modeling\n\nCreate and implement data models to support organizational strategies, working closely with stakeholders to ensure scalability and efficiency. Document data models and extraction processes for reference by team members and customers, leveraging modern tools and techniques to automate repetitive tasks and improve productivity. Continually refine solutions to deploy best practices across reports, database structures, and extraction methods, collaborating with vendors as necessary. Address complex reporting requests and contribute to deeper understanding of source system data models. \n\nTechnical & Business Skill\n\nDemonstrate proficiency in Data Management practices and architectures such as Data Modelling, Data Warehousing, Data Lake, etc., with foundational knowledge of others. Proficiency in Python and Pyspark for forward-looking data processing and analysis. Experience with SSIS or other ETL tools for data extraction, transformation, and loading. Flexibility with ETL tool experience, demonstrating adaptability to different technologies and systems. Understand core clinical, business, and research processes to develop tailored data solutions, obtaining relevant certifications as needed and pursuing continuous education. \n\nProject Execution & Management\n\nExecute project tasks efficiently with a focus on quality, communicating status effectively using project management tools. Participate actively in project meetings and serve as technical lead for moderately complex projects, ensuring effective management of scope, risk, and budget. Provide technical support and customer service, ensuring adherence to Service Level Agreements and promoting change management policies. Collaborate closely with stakeholders, particularly data science teams and business analysts, to refine data requirements and support data consumption needs. \n\nEducation/Experience\n\nRequired:\n\nBachelor's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field. 1-2 + years of work experience in a related job discipline. \n\nPreferred\n\nExperience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative. \n\nCincinnati Children's is proud to be \n\nJob\n\nInformation Technology\n\nPrimary Location\n\nUnited States-Ohio-Cincinnati-Vernon Place\n\nSchedule\n\nFull-time\n\nShift\n\nDay Job\n\nJob Type\n\nStandard\n\nDepartment\n\nHealth Network 2.0 IS\n\nEmployee Status\n\nRegular\n\nFTE \n\n1.0\n\nWeekly Hours\n\n40\n\nSalary Range\n\n42.17"}
{"text": "Qualifications:\n\nQualifications\n\nCandidates are currently pursuing a Bachelor, Master, or PhD degree in Biostatistics or Statistics, Mathematics, Computer Science, Data Science, Machine Learning, or a related discipline.Candidates must be available to work 10-12 weeks from May 2024 – September 2024 and are required to work full-time or 20 hours per week during that time.Candidates must be legally authorized to work in the United States, as Permanent Residents or United States Citizens, and not require sponsorship for employment visa status (e.g., H1-B status).Candidates must be detail-oriented, highly organized, and able to manage multiple tasks.Candidates must be a self-motivated and have the ability to work independently as well as collaboratively with a team.Ability to demonstrate strong analytical, quantitative, and programming skills is required.Proficiency in at least one of the following languages SAS, R, or Python is preferred.Excellent communication, presentation, and leadership skills are preferred.\n\nJoin us in transforming the future of medical devices and empowering patients to live their best life possible. Apply now to reimagine healthcare with J&J MedTech!"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Manager/ ArchitectData Engineering- AWS Cloud to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating them to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and subsystems on production servers\n\n• Mentor, support and manage team members\n\nYour Skills & Experience:\n\n• 8+ years of demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platform- AWS (Amazon Web Services)\n\n• Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension concepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAnnual base pay range: $123,000 - $184,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.\n\nWe’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!\n\nThe Adobe Digital Imaging Data Science Team (Photoshop & Lightroom) is looking for a Data Scientist who is passionate about data and has the desire to provide an outstanding product experience for our customers. Using extensive product usage data sets, you will partner directly with product managers, product marketing managers, and software engineers to harness our data, derive significant insights, and help lay the foundation for robust and reliable data-centric decision-making. You will have the opportunity to focus on new and intriguing initiatives spanning, product analytics, data engineering, GTM analytics, growth, and more. Your primary focus will be to develop and maintain a robust data architecture to advance our reporting, analytics, and experimentation capabilities, and on a longer term you will use predictive modeling and machine learning methods to allow the broader organization to understand, lead, and optimize our customer experiences. Join our innovative team and make an impact in the most exciting areas of Adobe!\n\nRequirements:\n\nMS or Ph.D. in data science, computer science, statistics, applied mathematics, engineering, or economics. 3 - 5+ years of relevant data science experience. Experience translating business questions into data analytics approaches. Strong proficiency in querying and manipulating large datasets using SQL-like languages (Hive, Spark, etc.). Experience developing and operationalizing consistent approaches to experimentation, using appropriate statistical techniques to reduce bias and interpret statistical significance. Proficiency with descriptive and inferential statistics (i.e., t-test, chi-square, ANOVA, correlation, regression, etc.) to understand customer engagement and generate hypotheses. Experience crafting data visualizations and storytelling to efficiently communicate analysis results to both technical and non-technical audiences. Knowledge of relevant tools in this field such as Hadoop, Hive, Splunk, Spark, Tableau, Excel (Charting and Pivot-Tables), and Power BI. Experience in product instrumentation is a plus. Possess natural curiosity and technical competence, being capable of asking critical questions and always ready to address any challenges. Experience addressing an executive level audience. Excellent communication, relationship skills, and a strong teammate. \n\nOur compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $107,900 -- $192,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.\n\nAt Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).\n\nIn addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.\n\nAdobe is proud to be an \n\nAdobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.\n\nAdobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees."}
{"text": "experience\nABOUT THE ROLE:Skills: Bachelor’s degree in computer science or related technical field with 5+ years of experienceExperience with big data technologies (e.g. Apche Hadoop, spark, Kafka, Flink) and workingStrong knowledge of SQL and experience working with relational and non-relational databases.Proficiency in programming languages such as Python, Java, Go etc.Extensive knowledge of networking, operation system, database system and container technology.Effective communication skills and ability to collaborate effectively with cross-functional teams.Fluent in English and Mandarin Chinese (both verbal and written) to communicate with external global stakeholdersExcellent problem-solving skills, attention to detail, and ability to thrive in a fast-paced environment.requirements and deliver data solutions that meet business needsDesign, build, and optimize scalable data pipelines to ingest, process and transform large volumes of dataEnsure the data integrity, accuracy and consistency of data by implementing data quality checks, validation processes and monitoring mechanismsEvaluate, implement, and maintain infrastructure tools and technologies to provide automation capabilities for projects, thereby enhancing work efficiency and qualityDesign and implement rovust data models and visualization to track project progress, task SLAs and quality metricsProvide rapid response to SLA oncall support to business critical data pipelinesCreate and maintain high – quality technical design documentation and promote best practices for data governance within the data user communityEstablish excellent communication habits to ensure that progress and risks are accurately and promptly sharedProactively communicate with stakeholders about the context of any blocks and seek assistance to drive the completion of objectives\nBenefit offerings available for our associates include medical, dental, vision, life insurance, short-term disability, additional voluntary benefits, EAP program, commuter benefits and a 401K plan. Our benefit offerings provide employees the flexibility to choose the type of coverage that meets their individual needs. In addition, our associates may be eligible for paid leave including Paid Sick Leave or any other paid leave required by Federal, State, or local law, as well as Holiday pay where applicable.To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.modis.com/en-us/candidate-privacy/\nThe Company will consider qualified applicants with arrest and conviction records."}
{"text": "Qualifications:\n\nMS or PhD in computer science or EE.4+ years of experience in machine learning and statistics, preferably in leading internet companies.Solid understanding of ML technologies, mathematics, and statistics.Proficiency in Java, Python, Scala, Spark, SQL, and large-scale ML/DL platforms.Passion for understanding the ad business and seeking innovation opportunities.Experience thriving in a fast-paced, data-driven, collaborative, and iterative environment.\n\n\nMainz Brady Group is a technology staffing firm with offices in California, Oregon and Washington. We specialize in Information Technology and Engineering placements on a Contract, Contract-to-hire and Direct Hire basis. Mainz Brady Group is the recipient of multiple annual Excellence Awards from the Techserve Alliance, the leading association for IT and engineering staffing firms in the U.S.\n\nMainz Brady Group is"}
{"text": "experience on data analysis/ data integrity/ data governance;Need experience in analytical tools including PowerBI development, Python, coding, Excel, SQL, SOQL, Jira, and others."}
{"text": "Skills:\nPython, R programming, PostgresBackground in Postgres, python, R programming and bioinformatics and genomics dataYour expertise in PostgreSQL for database management and Python and R for scripting and automation will be crucial in developing.Bachelor's degree in computer science, bioinformatics,related field +3 years of experience."}
{"text": "experience will begin at $133,300 USD ($114,700 CAD) and go up to $207,300 USD ($160,600 CAD). Full-time employees are also eligible for a bonus, competitive equity package, and benefits. The actual base salary offered may be higher, depending on your location, skills, qualifications, and experience.\n\nIn this role, you can expect to\n\nImplement new forecasting processes and continue to improve and refine system over timeScenario plan with leadership under different assumptions (i.e. how does investing in one area translate into business metric improvement)Keep pulse on financial KPIs and unit economics. Unlike anyone else, you will be aware of changes in Chime’s key financial and business performance metrics and will be building the understanding for why.Be an arbiter of truth with data. You’ll use data to help the organization understand how members are interacting with Chime.Partner widely. You’ll work hand-in-hand with the Finance team and collaborate with stakeholders across Marketing, Product, Operations & Member Experience, and Risk. The stronger these partnerships, the more successful you and Chime will be.Ask why a lot. Your proactive inquisitiveness, supported by your data dexterity, means you’re leading first-of-its-kind analyses.\n\nTo thrive in this role, you have\n\n5+ years in data-focused roles, performing analyses or building data tools that support an executive audience and facilitate decisions of trade-offs.Proficiency in financial metrics and how investors evaluate business performance.Extensive stakeholder management experience and a Northstar for how Analytics partners with business units.Expertise in SQL - you innately translate business questions to queries, understand the edge cases of joins, and with dexterity explore a warehouse to find data most appropriate to the problem.Expertise in R or python - you write reproducible code and have a tendency toward automation.Sound statistical inference skills, with the ability to communicate uncertainty appropriately to business partners.A focus on impact - you don’t stop with just recommendations but ensure to see work through to changing the business.\n\nA Little About Us\n\nAt Chime, we believe that everyone can achieve financial progress. We’re passionate about developing solutions and services to empower people to succeed. Every day, we start with empathy for our members and stay motivated by our desire to support them in ways that make a meaningful difference.\n\nWe created Chime—a financial technology company, not a bank*-- founded on the premise that basic banking services should be helpful, transparent, and fair. Chime helps unlock the access and ability our members need to overcome the systemic barriers that block them from moving forward. By providing members with access to liquidity, rewards, and credit building, our easy-to-use tools and intuitive platforms give members the ability to have more control over their money and to take action toward achieving their financial ambitions.\n\nSo far, we’re well-loved by our members and proud to have helped millions of people unlock financial progress, whether they started a savings account, bought their first car or home, opened a business, or went to college. Every day, we’re inspired by our members’ dreams and successes, big and small.\n\nWe’re uniting everyday people to unlock their financial progress—will you join us?\n\nChime partners with The Bancorp Bank and Stride Bank, N.A., Members FDIC, that power the bank accounts used by Chime Members.\n\nWhat We Offer\n\n💰 Competitive salary based on experience✨ 401k match plus great medical, dental, vision, life, and disability benefits🏝 Generous vacation policy and company-wide Take Care of Yourself Days🫂 1% of your time off to support local community organizations of your choice🧠 Mental health support with therapy and coaching through Modern Health👶 16 weeks of paid parental leave for all parents and an additional 6-8 weeks for birthing parents👪 Access to Maven, a family planning tool, with up to $10k in reimbursement for egg freezing, fertility treatments, adoption, and more.💻 Hybrid work perks, like UrbanSitter and Kinside for backup child, elder and/or pet care, as well as a subsidized commuter benefit🏢 A thoughtful hybrid work policy that combines in-office days and trips to team and company-wide events depending on location to ensure you stay connected to your work and teammates, whether you’re local to one of our offices or remote🎉 In-person and virtual events to connect with your fellow Chimers—think cooking classes, guided meditations, music festivals, mixology classes, paint nights, etc., and delicious snack boxes, too!💚 A challenging and fulfilling opportunity to join one of the most experienced teams in FinTech and help millions unlock financial progress\n\nWe know that great work can’t be done without a diverse team and inclusive environment. That’s why we specifically look for individuals of varying strengths, skills, backgrounds, and ideas to join our team. We believe this gives us a competitive advantage to better serve our members and helps us all grow as Chimers and individuals.\n\nWe hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. Chime is proud to be \n\nTo learn more about how Chime collects and uses your personal information during the application process, please see the Chime Applicant Privacy Notice."}
{"text": "experience in the industries we serve, and to partner with diverse teams of passionate, enterprising SVBers, dedicated to an inclusive approach to helping them grow and succeed at every stage of their business.\n\nJoin us at SVB and be part of bringing our clients' world-changing ideas to life. At SVB, we have the opportunity to grow and collectively make an impact by supporting the innovative clients and communities SVB serves. We pride ourselves in having both a diverse client roster and an equally diverse and inclusive organization. And we work diligently to encourage all with different ways of thinking, different ways of working, and especially those traditionally underrepresented in technology and financial services, to apply.\n\nResponsibilities\n\nSVB’s Foreign Exchange business is one of the largest FX providers to the Innovation economy. We support the transactional and risk management needs of our fast-growing clients as they expand and do business internationally.\n\nLocated close to one of our Hubs in SF, NYC or Raleigh and reporting to the Managing Director of FX Strategy, this Business Data Analyst will be an integral part of the Product Strategy and Business Management team, supporting and driving the insights that will be used to formulate, drive and validate our strategic and business effectiveness.\n\n \n\nYou will take part in complex, multi-disciplinary projects to further enable the Product, Trading and Sales teams. You will be a fast learner who is comfortable in the weeds with analytics and data manipulation whilst developing the story for leadership.\n\nThis role would be a great fit for a creative, curious and energetic individual and offers the right candidate the opportunity to grow while creating significant business value by continuously improving business intelligence/reporting, processes, procedures, and workflow.\n\nThe ideal candidate will have 3-5 yrs experience in Financial Services or Fintech, preferably with FX, Trading or Cross Border Payment experience.\n\n \n\nrequirements.Become familiar with the evolving FX, Fintech and Banking landscape to overlay industry insights.Drive continued evolution of our business analytics/data framework in order to inform MI and product evaluation.Assist with maintenance and accuracy of company data within SVB’s data repositories.\n\nQualifications\n\nBasic Requirements:\n\nBS/BA Degree – preferably in a quantitative discipline (e.g., Economics, Mathematics, Statistics) or a HS Diploma or GED with equivalent work experience3-5 years’ experience in financial services or fintech, ideally within FX or Cross Border Payments\n\nPreferred Requirements:\n\nStrong attention to detail with an eye for data governance and compliance\n\nAptitude for framing business questions in analytic terms and translating requirements into useful datasets and analyses with actionable insights."}
{"text": "experience in Natural Language Processing (NLP). In this role, you will play a crucial role in designing and deploying optimized models in production environments and developing acoustical and lexical ML Pipelines, Speech Analysis, and other AI-based systems.\nIn this role, you will:- Collaborate with the research team to prototype ML models, focusing on areas such as acoustical and lexical ML Pipelines.- Build and deploy scalable, maintainable ML models into production environments.- Utilize Flask and FastAPI frameworks to create REST microservices and APIs.- Automate and orchestrate ML and data pipelines.- Collaborate with researchers, engineers, product managers, and designers to introduce new features and research capabilities.- Foster a diverse, equitable, and inclusive culture that encourages open dialogue and challenges conventional thinking.\nYou might thrive in this role if you:- Have extensive experience building and maintaining production ML systems.- Have expertise in neural networks, ML frameworks, pattern recognition, and algorithm development.- Own problems end-to-end and are willing to learn to get the job done.- Have the ability to work in a fast-paced environment where things are sometimes loosely defined and may have competing priorities or deadlines.\n🌐 Benefits at InSpace:🏥 Medical Insurance: Your well-being matters, and we've got you covered with comprehensive medical insurance.🎓 Trainings: Invest in your professional growth with tailored training packages to enhance your skills.🕒 Flexible Working Schedule: Enjoy the flexibility to balance work and life with our accommodating schedules.✈️ Business Trips: Seize opportunities for travel and collaboration with business trips.💼 Annual Salary Review: Your hard work deserves recognition; benefit from our annual salary review process."}
{"text": "Requirements:7-8 years of systems analysis experienceExtensive experience working directly with business and detailing their requirementsStrong SQL and Data skillsETL experience (Informatica or Snaplogic)Can connect with the developers / QA directly and ensure they can understand what the requirements areGood Communication skillsWorked in an agile teamFinancial domain experience would be very advantageous\nSkills, experience, and other compensable factors will be considered when determining pay rate. The pay range provided in this posting reflects a W2 hourly rate; other employment options may be available that may result in pay outside of the provided range.  W2 employees of Eliassen Group who are regularly scheduled to work 30 or more hours per week are eligible for the following benefits: medical (choice of 3 plans), dental, vision, pre-tax accounts, other voluntary benefits including life and disability insurance, 401(k) with match, and sick time if required by law in the worked-in state/locality. \nPlease be advised- If anyone reaches out to you about an open position connected with Eliassen Group, please confirm that they have an Eliassen.com email address and never provide personal or financial information to anyone who is not clearly associated with Eliassen Group. If you have any indication of fraudulent activity, please contact InfoSec@eliassen.com.\nJob ID: 383882"}
{"text": "skills, and become a part of our global community of talented, diverse, and knowledgeable colleagues.\n\nPerficient is always looking for the best and brightest talent and we need you! We're a quickly growing, global digital consulting leader, and we're transforming the world's largest enterprises and biggest brands. You'll work with the latest technologies, expand your skills, experience work-life balance, and become a part of our global community of talented, diverse, and knowledgeable colleagues.\n\nMachine Learning Development\n\nMaintains, as well as furthers, enhances existing machine learning modules for automotive applications including autonomous vehicles.Designs and implements new machine learning based approaches based on existing frameworks.Keeps up to speed with the state of the art of academic research and AI/ML technology in the Automotive industry.Applies industry and technology expertise to real business problems.Coordinates with automotive engineers and autonomous driving software experts.Transfers technologies and solutions to automotive OEM development divisions.\n\nData Engineering and Pipelines:\n\nUnderstand business context and wrangles large, complex datasets.Create repeatable, reusable code for data preprocessing, feature engineering, and model training.Build robust ML pipelines using Google Vertex AI, BigQuery and other Google Cloud Platform services.\n\nResponsible AI and Fairness:\n\nConsider ethical implications and fairness throughout the ML model development process.Collaborate with other roles (such as data engineers, product managers, and business analysts) to ensure long-term success.\n\nInfrastructure and MLOps:\n\nWork with infrastructure as code to manage cloud resources.Implement CI/CD pipelines for model deployment and monitoring.Monitor and improve ML solutions.Implement MLOps using Vertex AI pipelines on the Google Cloud Platform platform.\n\nProcess Documentation and Representation\n\nDevelops technical specifications and documentation.Represents the Customer in the technical community, such as at conferences.7 - 10 years of professional experience REQUIRED5+ years' Deep Learning experience REQUIREDMaster's Degree in Computer Science or equivalent.PhD Strongly Preferred. \n\nRequired Skills\n\nStrong communication skills must be able to describe and explain complex AI/ML concepts and models to business leaders.Desire and ability to work effectively within a group or team.Strong knowledge of different machine learning algorithms.Deep Learning: Proficiency in deep learning techniques and frameworksMachine Learning: Strong understanding of traditional machine learning algorithms and their applications.Computer Vision: Expertise in computer vision, including object detection, image segmentation, and image recognitionProficiency in NLP techniques, including sentiment analysis, text generation, and language understanding models. Experience with multimodal language modeling and applications.Neural Network Architectures: Deep understanding of various neural network architectures such as CNNs, RNNs, and Transformers.Reinforcement Learning: Familiarity with reinforcement learning algorithms and their applications in AI.\\Data Preprocessing: Skills in data cleaning, feature engineering, and data augmentation.Model Training And Tuning: Experience in training, fine-tuning, and optimizing AI models.Model Deployment: Knowledge of model deployment techniques, including containerization (Docker) and orchestration (Kubernetes).Understanding of Generative AI concepts and LLM Models tailored to a wide variety of automotive applications.Strong documentation skills for model architecture, code, and processes.\n\nDesired Skills\n\nAI Ethics: Awareness of ethical considerations in AI, including bias mitigation and fairness.Legal And Regulatory Knowledge: Understanding of AI-related legal and regulatory considerations, including data privacy and intellectual property.Data Management: Proficiency in data storage and management systems, including databases and data lakes.Cloud Computing: Familiarity with Google Cloud Platform. Experience with Google Cloud Platform, Vertex AI and BigQuery is a plus.\n\nThe salary range for this position takes into consideration a variety of factors, including but not limited to skill sets, level of experience, applicable office location, training, licensure and certifications, and other business and organizational needs. The new hire salary range displays the minimum and maximum salary targets for this position across all US locations, and the range has not been adjusted for any specific state differentials. It is not typical for a candidate to be hired at or near the top of the range for their role, and compensation decisions are dependent on the unique facts and circumstances regarding each candidate. A reasonable estimate of the current salary range for this position is $92,118 to $202,730. Please note that the salary range posted reflects the base salary only and does not include benefits or any potential equity or variable bonus programs. Information regarding the benefits available for this position are in our benefits overview.\n\nWho We Are\n\nPerficient is a leading global digital consultancy. We imagine, create, engineer, and run digital transformation solutions that help our clients exceed customers' expectations, outpace competition, and grow their business. With unparalleled strategy, creative, and technology capabilities, our colleagues bring big thinking and innovative ideas, along with a practical approach to help our clients - the world's largest enterprises and biggest brands succeed.\n\nWhat We Believe\n\nAt Perficient, we promise to challenge, champion, and celebrate our people. You will experience a unique and collaborative culture that values every voice. Join our team, and you'll become part of something truly special.\n\nWe believe in developing a workforce that is as diverse and inclusive as the clients we work with. We're committed to actively listening, learning, and acting to further advance our organization, our communities, and our future leaders... and we're not done yet.\n\nPerficient, Inc. proudly provides equal employment opportunities (\n\nApplications will be accepted until the position is filled or the posting removed.\n\nDisability Accommodations:\n\nPerficient is committed to providing a barrier-free employment process with reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or accommodation due to a disability, please contact us.\n\nDisclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time. Cloud Platform Senior AI Deep Learning Engineer - REMOTE"}
{"text": "requirements and provide efficient solutions for data exploration, analysis, and modeling  Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs  Use cutting edge technologies to develop modern data pipelines supporting Machine Learning and Artificial Intelligence \n\nBasic Qualifications:\n\n Bachelor’s Degree  At least 2 years of experience in application development (Internship experience does not apply)  At least 1 year of experience in big data technologies \n\nPreferred Qualifications:\n\n 3+ years of experience in application development including Python, Scala, or Java  1+ years of experience using Spark  1+ years of experience working on data stream systems (Kafka or Kinesis)  1+ years of data warehousing experience (Redshift or Snowflake)  1+ years of experience with Agile engineering practices  1+ years of experience working with a public cloud (AWS, Microsoft Azure, Google Cloud) \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "experience solutions and technologies.This is a hybrid position, with the ideal candidate located near one of our regional hubs (New York, Chicago, Boston) and able to travel to an office as needed for working sessions or team meetings.\nCurinos is looking for a Senior Data Engineering Manager to lead the build and expansion of our Retail Consumer product suite, relied on by our clients for precision deposit analysis and optimization. Our Retail Consumer business covers the largest suite of Curinos products and this position is a critical role within the Product Development team, combining both hands-on technical work (architecture, roadmap, code review, POC of new/complex methodologies) and team management.In this role, you will lead a cross-functional Product Development team of Software, Data and QA engineers covering all aspects of product development (UI/Middle Tier/API/Backend/ETL). You will collaborate with product owners on business requirements and features, work with the development team to identify scalable architecture and methodologies needed to implement, and own the timely and error-free delivery of those features. You will be expected to be “hands-on-keys” in this role, leading the team by example and helping to establish and model quality software development practices as the team, products and business continues to grow.\nResponsibilitiesBuilding and leading a Product Engineering team consisting of Software, Data and QA EngineersModeling quality software development practices to the team by taking on user stories and writing elegant and scalable codeConducting code reviews and providing feedback to help team members advance their skillsLeading the design and development of performant, extendable and maintainable product functionality, and coaching the team on the principles of efficient and scalable designEngaging with product owner and LOB head to understand client needs and craft product roadmaps and requirementsProviding input into the prioritization of features to maximize value delivered to clientsAnalyzing complex business problems and identifying solutions and own the implementationIdentifying new technologies and tools which could improve the efficiency and productivity of your teamWorking with in the Agile framework to manage the team’s day-to-day activitiesUnderstanding Curinos’ Application, API and Data Engineering platforms and effectively using them to build product featuresUnderstanding Curinos’ SDLC and compliance processes and ensuring the team’s adherence to them\nBase Salary Range: $160,000 to $185,000 (plus bonus)\nDesired Skills & Expertise6+ years professional full stack experience developing cloud based SaaS products using Java, SPA and related technologies with a complex backend data processing system[SW1][NS2]3+ years of experience with SQL Server or Databricks ETL, including hands-on experience developing SQL stored procedures and SQL-based ETL pipelines2+ Years of management experience of engineers/ICsProven ability to grow and lead geographically dispersed and cross-functional teamsA passion for proactively identifying opportunities to eliminate manual work within the SDLC process and as part of product operationA commitment to building a quality and error-free product, via implementation of unit testing, integration testing, and data validation strategiesA desire to design and develop for scale and in anticipation of future use casesDemonstrated intellectual curiosity and innovative thinking with a passion for problem-solvingSelf–discipline and willingness to learn new skills, tools and technologiesExcellent verbal and written communication skillsAdvanced proficiency in Java (including testing frameworks like Junit) and T-SQL (including dynamic sql and the use of control structures) is an assetExperience using Scala is a plusExperience using a templating language like Apache Freemarker is a plusBachelors or advanced degrees (Masters or PhD) degree, preferably in computer science, or a related engineering field\nWhy work at Curinos?Competitive benefits, including a range of Financial, Health and Lifestyle benefits to choose fromFlexible working options, including home working, flexible hours and part time options, depending on the role requirements – please ask!Competitive annual leave, floating holidays, volunteering days and a day off for your birthday!Learning and development tools to assist with your career developmentWork with industry leading Subject Matter Experts and specialist productsRegular social events and networking opportunitiesCollaborative, supportive culture, including an active DE&I programEmployee Assistance Program which provides expert third-party advice on wellbeing, relationships, legal and financial matters, as well as access to counselling services\nApplying:We know that sometimes the 'perfect candidate' doesn't exist, and that people can be put off applying for a job if they don't meet all the requirements. If you're excited about working for us and have relevant skills or experience, please go ahead and apply. You could be just what we need!If you need any adjustments to support your application, such as information in alternative formats, special requirements to access our buildings or adjusted interview formats please contact us at careers@curinos.com and we’ll do everything we can to help.\nInclusivity at Curinos:We believe strongly in the value of diversity and creating supportive, inclusive environments where our colleagues can succeed. As such, Curinosis proud to be"}
{"text": "experience with speech interfaces Lead and evaluate changing dialog evaluation conventions, test tooling developments, and pilot processes to support expansion to new data areas Continuously evaluate workflow tools and processes and offer solutions to ensure they are efficient, high quality, and scalable Provide expert support for a large and growing team of data analysts Provide support for ongoing and new data collection efforts as a subject matter expert on conventions and use of the data Conduct research studies to understand speech and customer-Alexa interactions Assist scientists, program and product managers, and other stakeholders in defining and validating customer experience metrics\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nBoston, MA, USA | Seattle, WA, USA\n\nBasic Qualifications\n\n 3+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience 2+ years of data scientist experience Bachelor's degree Experience applying theoretical models in an applied environment\n\nPreferred Qualifications\n\n Experience in Python, Perl, or another scripting language Experience in a ML or data scientist role with a large technology company Master's degree in a quantitative field such as statistics, mathematics, data science, business analytics, economics, finance, engineering, or computer science\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $111,600/year in our lowest geographic market up to $212,800/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2610752"}
{"text": "experiences. OpenWeb works with more than 1,000 top-tier publishers, hosting more than 100 million active users each month.\nFounded in 2015, OpenWeb has over 300 employees in New York City, Tel Aviv, Kyiv, San Diego, Canada, London, and Paris and is backed by world-class investors including Georgian, Insight Partners, , Entrée Capital, The New York Times, Samsung Next, Dentsu, and ScaleUp. To date, the company has raised $393 million in funding and is currently valued at $1.5 billion.\nTo learn more about OpenWeb's platform visit OpenWeb.com, or follow @OpenWebHQ on LinkedIn and Twitter.\nOpenWeb is looking for a Data Engineer for Business Insights and Reporting engineering. This role has a heavy emphasis on cloud data engineering aiming to build a performant, concise, fault tolerant, and cost effective data pipeline processing 750M+ user events per day into useful forms for downstream consumers.\nWhat you'll Do:Work primarily in the AWS cloud to transform raw data sources into curated datasets for BI and Reporting.Work secondarily in the Azure cloud to consume the curated datasets for user facing Reporting.Work with BI users to build aggregated summary tables for efficient business dashboards.Build automated data quality checks to ensure BI and Reporting have correct data.Maintain data dictionaries for BI and Reporting users consuming the curated datasets.Maintain documentation covering the design and implementation of the data systems for internal engineering.\nWhat you'll Bring:3+ years of data engineering experience, with a minimum of 2 years focused on AWS technologies including Glue, Airflow, and Athena.Experience working with complex SQL models and queries.Experience creating and automating data integrity checks, maintaining documentation, and dictionaries.\nNice to haves:Knowledge of Java/ScalaExperience in AdTechExperience working remotely\nWhat You'll Get:Company Stock Options - we believe that every employee should benefit from the company’s success, as we all contribute to it. Hence, we offer stock options to all our employees and continue to reward with more stock options over time.Unlimited PTO401K matching - the company is matching dollar for dollar, up to 4% or $5,000 (whichever is higher)Very generous health benefits: Medical, Dental, and Vision - for employees and their dependents $50 per month for wifi fee upon submitting a receipt$100 one-time fee for home office for new employees - one-offInsurance policies covered by the company: Short term Disability (STD), Long Term Disability (LTD), Life insurance, AD&D insuranceFully remote work environment\nThe OpenWeb Culture:We offer a dynamic and unconventional work environment that spans from NYC to Tel Aviv, bringing together a diverse group of world-class and high-caliber techies, wordsmiths, entrepreneurs, and creative thinkers. We empower every individual across our global team to be a catalyst for change and strive to create a work environment where you can have the utmost autonomy over your role and projects from start to finish. If you want to join an innovative tech company where you can challenge yourself, have the freedom to own your work, and make a lasting impact, then you have a spot within our growing community!\nOpenWeb is committed to building diverse teams and upholding an equal employment workplace free from discrimination. We hire amazing individuals regardless of race, color, ancestry, religion, sex, gender identity, national origin, sexual orientation, age, citizenship, marital status, pregnancy, medical conditions, genetic information, disability, or Veteran status.\nApply today to build your skills with us as you build a better web for everyone.We care about your privacy. Please take a moment to review OpenWeb's Privacy Practices."}
{"text": "requirements and deliver tailored data solutions.Implement data governance policies and procedures to ensure data quality, consistency, and security.Monitor and troubleshoot data issues, ensuring timely resolution and minimal impact on business operations.Stay updated with the latest trends and technologies in data management, cloud computing, and big data analytics.Provide technical guidance and mentorship to junior data management team members.\nQualifications:Bachelor's degree in Computer Science, Information Systems, or related field; Master's degree preferred.Minimum of 5 years of experience in data management, data engineering, or related role.Strong programming skills in Python and experience with PySpark for data processing and analytics.Hands-on experience with DataBricks for building and optimizing data pipelines.Proficiency in managing and administering cloud-based data platforms such as Snowflake and/or Redshift.Solid understanding of data modeling, ETL processes, and data warehousing concepts.Excellent analytical, problem-solving, and communication skills.Ability to work effectively in a fast-paced environment and manage multiple priorities.\nPreferred Qualifications:Certifications in PySpark, DataBricks, Snowflake, or Redshift.Experience with other data management tools and technologies.Knowledge of machine learning algorithms and techniques.Experience working in banking and payments domain"}
{"text": "experience with Snowflake.Sound understanding of Python programming.Strong experience with AWS.Strong knowledge and understanding of PL/SQL.Provide progress reports, proposals, requirements documentation, and presentations as needed."}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Manager, Data Engineer to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating\n\nthem to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues\n\nthat translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the\n\nlifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards,\n\nadhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and\n\nsubsystems on production servers\n\n• Mentor, support and manage team members\n\n\n\nYour Skills & Experience:\n\n• 7+ years demonstrable experience in enterprise level data platforms involving\n\nimplementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data\n\nplatforms - Amazon Web Services (AWS)\n\n• Experience with column-oriented database technologies (i.e. Big Query,\n\nRedshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable,\n\nCosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle,\n\nMySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and\n\nbatch integrations using tools/frameworks like Glue ETL, Lambda, Google\n\nCloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and\n\ncontributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension\n\nconcepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\nPay Range: $117,000 - $165,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "requirements, and assist in data structure implementation planning for innovative data visualization, predictive modeling, and advanced analytics solutions.* Unfortunately, we cannot accommodate Visa Sponsorship for this role at this time.\nESSENTIAL JOB FUNCTIONS\nMine data covering a wide range of information from customer profile to transaction details to solve risk problems that involve classification, clustering, pattern analysis, sampling and simulations.Apply strong data science expertise and systems analysis methodology to help guide solution analysis, working closely with both business and technical teams, with consideration of both technical and non-technical implications and trade-offs.Carry out independent research and innovation in new content, ML, and technological domains. Trouble shooting any data, system and flow challenges while maintaining clearly defined strategy execution.Extract data from various data sources; perform exploratory data analysis, cleanse, transform, and aggregate data.Collaborate with New Product Strategy, Decision Science, Technology Development, Business Intelligence, and business leaders to define product requirements, provide analytical support and communicate feedback.Assess the efficiency and accuracy of new data sources and optimize data gathering techniques.Communicate verbally and in writing to business customers with various levels of technical knowledge, educating them about defined solutions, as well as sharing insights and recommendations.\n\nCANDIDATE REQUIREMENTS\nMS in Data Science, Data Engineering, mathematics, Computer Science, Statistics, or related field, or equivalent working experience5+ years of relevant experience in Data Science, Data Analytics, Applied Statistics, or another quantitative field preferred2+ years using R, Python or SQL to manipulate data and draw insights from large data setsExperience working in cloud environments for data science workloadsPrevious experience working within banking and / or other financial services industries a plusStrong creative thinking and problem-solving skillsExcellent oral and written communication and presentation skills\nWHO WE ARE \nVALID Systems is comprised of two differentiating ingredients. Our Technology and our Team. VALID’s core capabilities are driven by our fully automated transaction processing and patented risk decision engine, and our team of data scientists, technologists, risk analysts, innovators, and industry leaders bring these capabilities to life for our clients. This enables VALID to offer the most highly customized solutions that execute on the near impossible mission of minimizing risk, enhancing the customer experience, all at a profit for our clients. We are meticulous about our data, relentless in solving problems, and maniacal in the pursuit of our clients’ success. \nTHE TECHNOLOGY Our technology allows our clients to make the right transactional decisions, in real-time, and drive revenue. Leapfrogging the conventional static set of industry based risk rules and 2 day old account status responses, VALID leverages a proprietary risk modeling architecture that employs predictive analytics. Focusing on the key predictive data attributes and behavioral patterns, each decision, positive pay, and return are fed back into the automated decision engine, thereby creating a self-learning model that remains in a state of perpetual refinement. While the principles of VALID’s risk modeling are increasingly technical, extensive emphasis has been placed on both up front data attribute and decision response flexibility that allows for client specific tailoring. We provide this level of sophistication on each decision not only in sub-second real-time transaction speeds, but with industry leading security within our platform and data management. \nTHE TEAM Since 2003 VALID has focused on acquiring talent with an expertise that reflects its client base. Equal to that focus has been equipping that talent with the ability to execute against major initiatives and deliver on the objectives of our partners and clients. To that end VALID has fostered a culture that encourages our world-class talent to push the edges of conventional processes and think outside the box when facing problems. We develop solutions not to simply fix a problem, but looking ahead to better an industry. \nOUR CULTURE Google meets Wall-Street. We are casual in dress, but exceptionally professional in our expectations of our employees. We are all experts in our own business areas. We rely on one another, and trust has to be high for this to be successful. We value accountability in the workplace and family. We may not be monitoring you but we expect you to monitor yourself. \nIf you ask the people who work here, we’d tell you none of us has ever worked at a company quite like VALID Systems!"}
{"text": "experience1. Experience in working with big data in a cloud environment (Azure-Databricks) 2. Experience with PowerBI and Cognos visualization tools (PowerBI Pro experience is a plus) 3. Experience writing advanced SQL\nTechnical Overview  The Data Analyst will provide technical support for the Agile Development Team in their efforts to create Consumable Data Sets (CDS) using Azure Cloud data via Databricks (DBX) and PowerBI cloud reports. They serve the team but also will take on some development tasks as time allows.  Tech Leader Duties  1. Provide Operational and Technical Leadership for the Agile Development Team a. Assist the team with development needs and/or questions b. Knowledge in Data Engineering with DataBricks, Hadoop and spark SQL to ensure code is optimized as per request if needed. c. Review BI product to ensure that the requirements are met d. Validate data e. Quick Daily Stand up and see any Open issues or blockers team is facing f. Responsible to ensure the EXL team is following processes as defined by the Team and Tech leaders (updating task hours, updating task description and status). g. Recognize when EXL development team needs to collaborate on user stories or issues on their own (try to find own solution before announcing in DSU). 2. Participate in New requirements /pre-refinement, refinement sessions with business requestors leads and EXL Contractors a. Support the Product Manager, Scrum Leader, and Architect with requirements b. Set up meetings and take notes c. Knowledge sharing with the team 3. Enable User Acceptance Testing a. Review product that are ready to test b. Set up meetings with the requestor, business owner, and their delegates to introduce the product and begin UAT c. Follow up to ensure UAT is complete 4. Coaches team in best practices a. Support the Agile Framework by identifying anti-patterns and working with the scrum master to coach the team in best agile practices b. Support DE and BI deployments (Build /release pipeline) c. Version control is maintained in development d. Documentation is stored in the GitHub or appropriate location (Mapping / Tech doc). e. All testing and validation should first peer review by Tech Lead 5. Provides Development support as part of the team a. Develops CDS and BI reports 6. After-hours Operational Support a. Monitoring all intraday reports after noon ET b. Take any actions necessary due to morning report issues 7. Conducts quarterly usage audits a. Identifies the number of unique users and report executions and provides recommendations to management on low usage reports  Requirements  1. Experience in working with big data in a cloud environment (Azure-Databricks) 2. Experience with PowerBI and Cognos visualization tools (PowerBI Pro experience is a plus) 3. Agile development experience 4. Experience writing advanced SQL\n#LI-AD1"}
{"text": "requirements.Testing measure configuration changes and working with the vendor to implement corrections if needed.On an annual basis, conduct a detailed review of all energy efficiency measures to ensure that they are correctly configured for the next program year.\nWork with reporting team members to update system process improvement and training manuals to include current processes and controls for all residential, commercial, and industrial programs.Support the implementation teams in their evaluation of energy efficiency initiative spending and energy efficiency savings by ensuring that data in the Tracksys system is accurate and reported on a timely basis. Assist with creation of reports and dashboards as needed to provide insight regarding energy efficiency program and measure spending and savings trends.Provide support to business operations resources, vendors, and implementation staff on data uploads as it relates to TrackSys energy efficiency measure configurations. For example, assist vendors with understanding measure mapping, savings calculations, and upload template information.Responsible for, demonstrating expertise in organization, schedule development, prioritization, and deadline management.\n\n\n\n\nQualifications\n\nTechnical Knowledge/Skill:\nKnowledge of energy efficiency engineering concepts related to measures and measure calculations. (i.e., energy engineering formulas to calculate savings from measures that impact end uses such as lighting, heating, cooling, refrigeration, motors, process)Knowledge of IT product management concepts and experience with working in a project role on IT implementation and or software project implementationStrong knowledge, experience and demonstrated ability in data analysis, and database management. Must be customer driven, display initiative, accepts responsibility, holds others accountable, participates in and facilitates team effectiveness, thinks, and acts analytically.Demonstrated ability to make sound decisions to support the mission, work independently and apply knowledge and skill to solve problems.Develop and maintain an excellent working relationship with management.Demonstrated proficiency in Microsoft Excel, in addition to other Microsoft Office applications (MS Power point, MS Word) and other business system applications.Demonstrated technical proficiency in running queries in various systems and data gathering. Effective written and oral communication skills.\n\n\n\nEducation:\nBachelor’s degree in engineering, Engineering Technology, Statistics, Economics/Mathematics or a related discipline or equivalent experience. \n\nExperience: \nFive (5) plus years related experience. Energy Efficiency, Statistics, Economics/Mathematics \n\n \n\nCompensation and Benefits:\nEversource offers a competitive total rewards program. The annual salary range for this position is $86,000 - $96,000 plus incentive. Salary is commensurate with your experience. Check out the career site for an overview of benefits.\n\n\n\n#cengajd\n\n\n\nWorker Type:\nRegular\nNumber of Openings:\n\n1\n\n\n\nEversource Energy is an Equal Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to age, race, color, sex, sexual orientation, gender identity, national origin, religion, disability status, or protected veteran status.\n\nVEVRRA Federal Contractor\n\n\n\n\n\n\nEmergency Response:\n\nResponding to emergency situations to meet customers’ needs is part of every employee’s role. If employed, you will be given an Emergency Restoration assignment. This means you may be called to assist during an emergency outside of your normal responsibilities, work hours and location."}
{"text": "experience who share our company values.\n\nTruveta was born in the Pacific Northwest, but we have employees who live across the country. Our team enjoys the flexibility of a hybrid model and working from anywhere. In person attendance is required for two weeks during the year for Truveta Planning Weeks.\n\nFor overall team productivity, we optimize meeting hours in the pacific time zone. We avoid scheduling recurring meetings that start after 3pm PT, however, ad hoc meetings occur between 8am-6pm Pacific time.\n\nWho We Need\n\nTruveta is rapidly building a talented and diverse team to tackle complex health and technical challenges. Beyond core capabilities, we are seeking problem solvers, passionate and collaborative teammates, and those willing to roll up their sleeves while making a difference. If you are interested in the opportunity to pursue purposeful work, join a mission-driven team, and build a rewarding career while having fun, Truveta may be the perfect fit for you.\n\nThis Opportunity\n\nThis Data Analyst will report to our Sr. Director of Research Solutions on the Partner team. They will focus on supporting strategic engagements with its Life Sciences partners. Leveraging technical skillsets, they will deliver collateral that will be used by members of the team to demonstrate the value of Truveta to prospective customers and drive adoption of the Truveta Studio in the Life Sciences space. The Analyst will have a strong background in health care, real world data (RWD), and leveraging programming skills to analyze Real World Data.\n\nResponsibilities Will Include\n\nUse technical skills and domain expertise to receive incoming customer requests and produce feasibility analyses and other value demonstration collateral Build demos, use cases, and other content designed to support Business Development to advance prospective customer engagements Collaborate closely with other Truveta teams to conduct investigations of data sources to demonstrate value of the Truveta Studio to prospective customers Deliver feedback to internal teams based on customer requests to inform Truveta’s product roadmap Provide subject matter expertise across the Truveta Partner team, including assisting in program design and coordinating well across Truveta teams \n\nKey Qualifications\n\nBS in Computer Science, Data Analysis, or equivalent 3+ years of experience analyzing Electronic Health Record data or other Real World Data sources in healthcare Advanced skills using SQL and R to conduct data analysis Knowledge of electronic health record or claims data for data analysis Proven ability to understand clinical research questions and translate into data analyses workflows Proven ability with to simplify complex clinical research and other domain-specific topics into synthesized, structured, and simplistic content for a non-research audience Start-up mindset that allows you to shift gears quickly \n\nWhy Truveta? \n\nBe a part of building something special. Now is the perfect time to join Truveta. We have strong, established leadership with decades of success. We are well-funded. We are building a culture that prioritizes people and their passions across personal, professional and everything in between. Join us as we build an amazing company together.\n\nWe Offer\n\nInteresting and meaningful work for every career stage Great benefits package Comprehensive benefits with strong medical, dental and vision insurance plans 401K plan Professional development for continuous learning Work/life autonomy via flexible work hours and flexible paid time off Generous parental leave Regular team activities (virtual and in-person as soon as we are able) The base pay for this position is $94,000 to $156,000. The pay range reflects the minimum and maximum target. Pay is based on several factors including location and may vary depending on job-related knowledge, skills, and experience. Certain roles are eligible for additional compensation such as incentive pay and stock options. \n\nIf you are based in California, we encourage you to read this important information for California residents linked here.\n\nTruveta is committed to creating a diverse, inclusive, and empowering workplace. We believe that having employees, interns, and contactors with diverse backgrounds enables Truveta to better meet our mission and serve patients and health communities around the world. We recognize that opportunities in technology historically excluded and continue to disproportionately exclude Black and Indigenous people, people of color, people from working class backgrounds, people with disabilities, and LGBTQIA+ people. We strongly encourage individuals with these identities to apply even if you don’t meet all of the requirements."}
{"text": "experienced Senior Machine Learning Engineer to join our rapidly growing Enterprise Data team. The ideal candidate will have a strong background in machine learning, MLOps, and data engineering, with a passion for leading teams and collaborating with data scientists to develop innovative solutions. The Senior Machine Learning Engineer will be responsible for creating, maintaining, and improving our ML pipeline and establishing MLOps practices within our data science group. Experience with Neo4j and working with graph databases is essential for this role.\n\nEssential Functions\n\nLead the design, development, and deployment of machine learning models and solutions, ensuring they meet business objectives and performance requirements.Establish and maintain a robust ML pipeline, including data ingestion, feature engineering, model training, evaluation, and deployment.Implement MLOps practices to streamline the ML lifecycle, including version control, testing, continuous integration, and continuous deployment.Collaborate closely with data scientists, data engineers, and other stakeholders to understand requirements, provide technical guidance, and ensure successful delivery of machine learning solutions.Stay current with the latest advancements in machine learning, MLOps, and data engineering, and drive the adoption of new technologies and best practices within the team.Develop and maintain documentation related to machine learning models, pipelines, and processes.Provide mentorship, guidance, and support to junior team members, fostering a collaborative and inclusive team culture.\n\nMinimum Requirements\n\nBachelor's or Master's degree in Computer Science, Data Science, or a related field.At least 5 years of experience as a Machine Learning Engineer, with a proven track record of leading teams and working with data science groups.Strong experience with MLFlow, Databricks, and other machine learning platforms.Deep understanding of machine learning algorithms, data structures, and optimization techniques.Proficiency in Python and experience with ML libraries such as TensorFlow, PyTorch, or Scikit-learn.Familiarity with MLOps tools and practices, including version control (e.g., Git), CI/CD, and containerization (e.g., Docker, Kubernetes).Extensive experience working with Neo4j and other graph databases, along with a solid understanding of graph algorithms and data modeling.\n\nWorking Conditions And Physical Requirements\n\nAbility to work for long periods at a computer/deskStandard office environment\n\nAbout The Organization\n\nFullsight is an integrated brand of our three primary affiliate companies – SAE Industry Technologies Consortia, SAE International and Performance Review Institute – and their subsidiaries. As a collective, Fullsight enables a robust resource of innovative programs, products and services for industries, their engineers and technical experts to work together on traditional and emergent complex issues that drive their future progress.\n\nSAE Industry Technologies Consortia® (SAE ITC) enables organizations to define and pilot best practices. SAE ITC industry stakeholders are able to work together to effectively solve common problems, achieve mutual benefit for industry, and create business value.\n\nThe Performance Review Institute® (PRI) is the world leader in facilitating collaborative supply chain oversight programs, quality management systems approvals, and professional development in industries where safety and quality are shared values.\n\nSAE International® (SAEI) is a global organization serving the mobility sector, predominantly in the aerospace, automotive and commercial-vehicle industries, fostering innovation, and enabling engineering professionals. Since 1905, SAE has harnessed the collective wisdom of engineers around the world to create industry-enabling standards. Likewise, SAE members have advanced their knowledge and understanding of mobility engineering through our information resources, professional development, and networking."}
{"text": "experience in ITCollaborate with local business users to understand business processes, gather technical requirements, design, develop, perform testing and support analytical solutions using TIBCO Spotfire, SQL, HTML, Redshift etc.Research, recommend, develop, train functional groups on reportingEnd to end implementation experience in building analytical solutions using data visualization tools like TIBCO Spotfire.Proficient in SQL and ability to design efficient queries with a focus on high performing solutions.Strong knowledge of Spotfire Iron python for customization and enhanced UI design.Ideal candidates need to have at least the top 3 skills of strong TIBCO Spotfire, SQL experience, AWS Redshift (at least 5+ years of Spotfire & SQL) Coordinate with teams for UAT and prod deployment"}
{"text": "experience in designing and developing data warehouse and data lake ETL/ELT pipelines,4+ years building Enterprise Data Warehouse (EDW) from multiple sources,4+ years of experience with Cloud technologies (AWS / Azure / Snowflake)4+ years of experience with data transformation tools and services. (Azure Data Factory,)4+ years of SQL scripting experience and excellent understanding of SQL joins, ranks, nested queries, complex CDC logic, etc.Experience with DevOps, CI/CD pipeline technologies (GitHub, etc.)Understanding of Agile methodologies (Scrum and Kanban)\n\nThe Skills You Bring\n\nExperience leading a team of developers with multiple skills,Experience writing technical design documents, and documenting implementation plans,Advanced SQL knowledgeAbility to perform data analysis on legacy systems such as Teradata, SQL Server, MongoDB, Oracle, etc.Pulling data into Snowflake using ETL/ELT data integration patterns Expertise in data modeling concepts such as dimensional modeling, slow-changing- dimension (SCD) types, Data Vault modeling, Normalized/Denormalized architectures, etc.Strong Interpersonal skills in teamwork, facilitation, communication, and negotiation.Excellent written and verbal communication skills, especially within the IT community.Excellent planning and organizational skills.\n\nRequirements:\n\nADF data pipelines (ETL/ELT)SQLSnowflakeData Modeling\n\nThanks & Regards,\n\nGowthami Paramasivam\n\nSenior Talent Acquisition Specialist\n\nMobile:\n\nEmail:\n\nWeb:\n\nJob Opening for \"Technical Lead- Data Engineering\" #Fulltime Role# Boston, MA Local Candidates Only"}
{"text": "Experience with LLMs and PyTorch: Extensive experience with large language models and proficiency in PyTorch.Expertise in Parallel Training and GPU Cluster Management: Strong background in parallel training methods and managing large-scale training jobs on GPU clusters.Analytical and Problem-Solving Skills: Ability to address complex challenges in model training and optimization.Leadership and Mentorship Capabilities: Proven leadership in guiding projects and mentoring team members.Communication and Collaboration Skills: Effective communication skills for conveying technical concepts and collaborating with cross-functional teams.Innovation and Continuous Learning: Passion for staying updated with the latest trends in AI and machine learning.\n\nWhat We Offer\n\nMarket competitive and pay equity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual lifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming and fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including company holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully stocked kitchens, and geo-specific commuter benefits\n\nBase pay for the successful applicant will depend on a variety of job-related factors, which may include education, training, experience, location, business needs, or market demands. The expected salary range for this role is based on the location where the work will be performed and is aligned to one of 3 compensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For other locations not listed, compensation can be discussed with your recruiter during the interview process.\n\nZone 1 (Menlo Park, CA; New York, NY; Bellevue, WA; Washington, DC)\n\n$187,000—$220,000 USD\n\nZone 2 (Denver, CO; Westlake, TX; Chicago, IL)\n\n$165,000—$194,000 USD\n\nZone 3 (Lake Mary, FL)\n\n$146,000—$172,000 USD\n\nClick Here To Learn More About Robinhood’s Benefits.\n\nWe’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing finance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you feel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people invigorated by our mission, values, and drive to change the world, not just those who simply check off all the boxes.\n\nRobinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants and employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and skills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone. Additionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy rights. To review Robinhood's Privacy Policy please review the specific policy applicable to your country."}
{"text": "skills and expertise in cloud-based technologies while contributing to the success of our team. Additionally, you will be responsible for legislative and regulatory reporting for claims, ensuring compliance and transparency.\n\nIf you are passionate about transforming data into actionable insights, thrive in a collaborative environment, and are eager to be part of a team that is driving innovation, then we want you on our team. Join us and make a significant impact on our organization and the industry.\n\nQualifications\n\nCompetitive AWS QuickSight Developers will possess the following qualifications:\n\nKnowledge of AWS: Familiarity with Amazon Web Services (AWS) is essential, including understanding the various AWS services, such as S3, Redshift, Athena, and IAM.Data Visualization Skills: Proficiency in data visualization concepts and techniques using AWS QuickSight.Strong understanding of SQL (Structured Query Language)Data Modeling concepts and techniques required. Python programming skillsPrior experience with BI tools such as Power BI, or QlikView will provide a good foundation for working with AWS QuickSight.AWS Developer Certification, a plus.\n\n**Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g., H-1B visa) for this opportunity**\n\nS\n\nSFARM\n\nBI Data Analyst - AWS QuickSight"}
{"text": "skills to drive real world impact. You will lead end-to-end machine learning projects, driving impact from project scoping through deployment while rigorously adhering to scientific best practices . You will collaborate closely with stakeholders to ensure alignment and design solutions that meet the business’s specific goals. You will stay at the forefront of ML and AI advances, regularly exploring research papers, experimenting with novel techniques, and providing technical mentorship to fellow data scientists. Through your work and daily interactions, you will foster a culture of curiosity, respect, and excellence.\n\nResponsibilities\n\n Project Ownership: Lead end-to-end machine learning projects from scoping through deployment, applying scientific rigor and best practices throughout the project lifecycle. Stakeholder Engagement: Collaborate closely with product managers, engineers, and other cross functional partners to integrate data-driven products into existing products and processes, ensure alignment, and proactively identify new opportunities for impact. Modeling Expertise: Spearhead the design, training, and deployment of advanced algorithms to drive employee awareness and utilization of their benefits. Statistical Analysis and Experimentation: Use statistical modeling and controlled experiments to deliver actionable insights for business strategies and product development aimed at driving benefit awareness and utilization. Accelerate team productivity by providing technical mentorship, contributing to internal tooling, and helping team members stay up to speed with recent advancements in ML and AI.\n\nMake a lasting impact through your technical contributions, and foster a culture of curiosity, respect, and excellence.\n\nQualifications\n\n Advanced degree (PhD or Master’s) in a quantitative field (science, math, engineering, etc.) 3 + years of experience driving impact on cross functional data science projects. Excellent communication skills and thought partnership to build trusting relationships with stakeholders. Demonstrated ability to work autonomously, taking ownership of projects and driving them forward. Advanced python and SQL skills. Strong proficiency in traditional machine learning. NLP and deep learning experience a plus. Experience designing, deploying, and evaluating generative AI applications. Familiarity with common model and service deployment frameworks. Experience mentoring other data scientists and fostering a culture of curiosity, respect, and excellence. Passion for continuous learning and the ability to quickly understand the complex space of benefits administration.\n\n Flexible Working \n\nSo that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and “Top 100 Company for Remote Jobs” 5 years in a row.\n\nBenefits\n\nWe offer programs and plans for a healthy mind, body, wallet and life because it’s important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as a number of voluntary benefit options.\n\nBy applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight’s employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.\n\nOur commitment to Diversity and Inclusion\n\nAlight is committed to diversity, equity, and inclusion. We celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.\n\nAt Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.\n\nDiversity Policy Statement\n\nAlight is an \n\nAlight provides reasonable accommodations to the known limitations of otherwise qualified employees and applicants for employment with disabilities and sincerely held religious beliefs, practices and observances, unless doing so would result in undue hardship. Applicants for employment may request a reasonable accommodation/modification by contacting his/her recruiter.\n\nAuthorization to work in the Employing Country\n\nApplicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.\n\nNote, this job description does not restrict management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\n\np&tjobs\n\nWe offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.\n\nSalary Pay Range\n\nMinimum :\n\n102,400 USD\n\nMaximum :\n\n162,600 USD\n\nPay Transparency Statement: Alight considers a variety of factors in determining whether to extend an offer of employment and in setting the appropriate compensation level, including, but not limited to, a candidate’s experience, education, certification/credentials, market data, internal equity, and geography. Alight makes these decisions on an individualized, non-discriminatory basis. Bonus and/or incentive eligibility are determined by role and level. Alight also offers a comprehensive benefits package; for specific details on our benefits package, please visit: https://bit.ly/Alight_Benefits\n\nDISCLAIMER:\n\nNothing in this job description restricts management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\n\nAlight Solutions provides equal employment opportunities ("}
{"text": "experience is influenced by physical, social, and economic environments at the city scale. Your efforts will be focused on multiple aspects of research execution and data analyses, including, but not limited to, survey question design, testing, post-research data analyses, database analysis and the development of insights for publication in research white papers and reports. Using various kinds of data (survey data, socio-economic data, built environment data), develop, evaluate, validate models to develop and execute various indices at the city scale. Additionally, you will work with the Data Development Team to help develop and enhance a data lake and data warehouse solution, develop the infrastructure for a suite of dashboards, and develop and deploy Machine Learning solutions. Qualified candidates should be collaborative, flexible, self-motivated, quick learners, and have an entrepreneurial spirit. This position is based on site in either our DC or NYC office. \n\nWhat You Will Do\n\nCollaborate with Gensler Research Institute project leads, subject matter experts, and analysts to develop and refine research instruments focused on measuring the human experience related to the built environment. Lead the creation and analysis of large datasets for the development and launch of various indices, with an initial focus on the refinement and creation of an index focused on Cities and the urban experience. Create and implement strategies to account for cross cultural biases in multi-national, multi-cultural studies Improve survey data collection through optimized question design and advisory on development of survey questions by other practitioners. Work both independently and collaboratively as needed to mine data sets, both structured and unstructured, for insights Develop, evaluate, and deploy machine learning solutions on a broad range of structured and unstructured data sets in contexts that call for supervised and unsupervised approaches. Partner with Data engineering teams to develop and improve our data lake and data warehouse Work with analysts and product managers to deliver data models for Tableau and/or PowerBI dashboards Maintain excellent documentation, conform to literate coding practice, and commit to strong version control \n\nYour Qualifications\n\nBachelor’s degree (master’s preferred) in a quantitative field like statistics, physics, math, economics, finance, computer science, etc. 10+ years professional experience working with data Fluency in Python (numpy/pandas/sklearn/TensorFlow) and SQL in a Data Science setting Fluency in statistical packages and data management tools (e.g., R, SPSS, SQL etc.) a plus Demonstrated ability to develop and evaluate machine learning models from real data sets. Experience with NLP, particularly topic modeling, sentiment analysis, and text classification a plus Expert in survey research and methodology, including:Survey question design Survey data analysis Evaluation of survey instrument for further refinement Strong data visualization skills, and a history of deploying data visualization tools in a modern dashboarding software (Tableau, Qlik, PowerBI, etc.)Exceptional data munging skills including comfort with transaction level data A flexible mindset and a willingness to work at all levels of the analytics value chain is a must. \n\n**If this position is based in New York, the base compensation range will be $100k-130k, plus bonuses and benefits and contingent on relevant experience**\n\nLife at Gensler\n\nGensler is committed to Diversity, Equity and Inclusion. Through our Five Strategies to Fight Racism, we continue to create a just and equitable future for our colleagues and our communities with our clients. We are a solid voice for social change as we impact people’s lives and fight racism. At Gensler, we celebrate diversity and are committed to creating an inclusive workplace environment.\n\nAt Gensler, we are as committed to enjoying life as we are to delivering best-in-class design. From curated art exhibits to internal design competitions to “Well-being Week,” our offices reflect our people’s diverse interests.\n\nWe encourage every person at Gensler to lead a healthy and balanced life. Our comprehensive benefits include medical, dental, vision, disability, wellness programs, flex spending, paid holidays, and paid time off. We also offer a 401k, profit sharing, employee stock ownership, and twice annual bonus opportunities. Our annual base salary range has been established based on local markets."}
{"text": "Qualifications:Relevant educational qualification or degree in Data analytics or Data Science or Statistics or Applied Mathematics or equivalent qualification. (Required)Experience with Tableau.(Optional)Familiar with Python, Big Data. (Optional)Proficient in SQL.Candidates who are missing the required skills, might be provided an option to enhance their skills, so that they can also apply for the role and can make a career in the IT industry.***Freshers can also apply***"}
{"text": "experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with our US Delivery Center - we are breaking the mold of a typical Delivery Center.\n\nOur US Delivery Centers have been growing since 2014 with significant, continued growth on the horizon. Interested? Read more about our opportunity below ...\n\nWork you'll do\n\nThe Generative AI Engineer will, as part of several client delivery teams, be responsible for developing, designing, and maintaining cutting-edge AI-based systems, ensuring smooth and engaging user experiences. Additionally, the Generative AI Engineer will participate in a wide variety of Natural Language Processing activities, including refining and optimizing prompts to improve the outcome of Large Language Models (LLMs), and code and design review. The kinds of activities performed by the Prompt Engineer will also include, but not be limited to:\n\nWorking across client teams to develop and architect Generative AI solutions using ML and GenAIDeveloping and promoting standards across the communityEvaluating and selecting appropriate AI tools and machine learning models for tasks, as well as building and training working versions of those models using Python and other open-source technologiesWorking with leadership and stakeholders to identify AI opportunities and promote strategy.Developing and conducting trainings for users across the Government & Public Services landscape on principles used to develop models and how to interact with models to facilitate their business processes.Building and prioritizing backlog for future machine-learning enabled features to support client business processes.You'll design and build generative models, selecting the most suitable architecture (e.g., GANs, VAEs) based on the desired output (text, images, code). This involves writing code using Python libraries like TensorFlow or PyTorch.Once your model is built, you'll train it on the prepared data, fine-tuning hyperparameters to achieve optimal performance. You'll then evaluate the model's outputs to assess its effectiveness and identify areas for improvement.You'll collaborate with other engineers to integrate your generative AI solution into existing systems or develop new applications. This might involve deploying the model on cloud platforms for scalability.The field of generative AI is rapidly evolving. Staying abreast of the latest research, advancements, and ethical considerations in AI development is an ongoing process.\n\nThe TeamArtificial Intelligence & Data Engineering\n\nIn this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.\n\nThe Artificial Intelligence & Data Engineering team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.\n\nArtificial Intelligence & Data Engineering will work with our clients to:\n\nImplement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platformsLeverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actionsDrive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements\n\nQualifications\n\nRequired:\n\n6+ years of experience programming in Python or R.Knowledge of Python libraries like Pandas, Scikit-Learn, Numpy, NLTK is required5+ years of experience with Natural Language Processing (NLP) and Large Language Models (LLM) 5+ years of experience building and maintaining scalable API solutionsExperience working with RAG technologies and LLM frameworks (Langchain, Claude and LLamaIndex), LLM model registries (Hugging Face), LLM APIs, embedding models, and vector databases (FAISS , Milvus , OpenSearch, Pinecone etc.)Experience working with Retrieval Augmented Thoughts (RAT) and chain of thoughts.Experience building scalable data models and performing complex relational databases queries using SQL (Oracle, MySQL, PostGres), etc.Experience working with cloud computing platforms (e.g., AWS, Azure, Google Cloud) and containerization technologies (e.g., Docker, Kubernetes).Utilize tools such as Docker, Kubernetes, and Git to build and manage AI pipelinesExperience driving DevOps and MLOps practices, covering continuous integration, deployment, and monitoring of AIExperience with machine learning libraries and services like TensorFlow, PyTorch, or Amazon SageMaker.Experience integrating GenAI solution on cloud platform (e.g., AWS, Azure, Google Cloud) 5+ years of experience designing solutions to address client requirements3+ years of experience with the design and implementation (building, containerizing, and deploying end to end automated data and ML pipelines) of automated cloud solutions5+ years of experience in developing algorithms using data science technologies to build analytical models5+ years of data extraction/manipulation experience using scripts specific to AI/ML5+ years of modeling experience using a variety of regression and supervised and unsupervised learning techniques.5+ years of experience in data wrangling/cleansing, statistical modeling, and programming5+ years of extensive experience working in an Agile development environment5+ years of experience for fluency in both structured and unstructured data (SQL, NOSQL)5+ years of production experience with Apache Spark5+ years of hands-on experience with web APIs, CI/CD for ML, and Serverless Deployment3+ years of experience with presentation and data analysis software such as: SAS, R, SPSS, MATLAB, QlikView, Excel and Access1+ years of experience to have familiarity with Linux OS and Windows servers1+ years of experience to have knowledge of Docker, Jenkins, Kubernetes, and other DevOps toolsMust be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the futureMust live in a commutable distance (approximately 100-mile radius) to one of the following Delivery locations: Atlanta, GA; Charlotte, NC; Dallas, TX; Gilbert, AZ; Houston, TX; Lake Mary, FL; Mechanicsburg, PA; Philadelphia, PA; with the ability to commute to assigned location for the day, without the need for overnight accommodationsExpectation to co-locate in your designated Delivery location up to 30% of the time based on business needs. This may include a maximum of 10% overnight client/project travelBachelor's degree, preferably in Computer Sciences, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience\n\nPreferred:\n\nPrevious Government Consulting and/or professional services experienceIn depth understanding of AI protocols and standardsUnderstanding of technology risks and the ability to assess and mitigate themDeep knowledge of a specific domain or industry, with a focus on applying NLP/LLM solutions in that contextExperience with debugging and troubleshooting software or solutions design issuesProven ability to stay current with best practices and new technology solutions in the fieldAbility to display both breadth and depth of knowledge regarding functional and technical issuesExperience presenting to clients or other decision makers to present and sell ideas to various audiences (technical and non-technical)Certification from any of the three major cloud platforms (AWS / Azure / GCP) in Cloud Architecture / Engineering / DevOps / ML.Familiarity with Kubeflow or MLflowExperience with machine learning pipelines (Azure ML)Familiarity with the latest Natural Language Processing or Computer Vision related algorithms\n\nInformation for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html"}
{"text": "experienced professionals to support informed business choices.Key requirements and perform data analysis to uncover trends, patterns, and anomalies.Assist in data cleansing and transformation to ensure accurate and reliable data for reporting.Data Visualization:Work with the team to design compelling visualizations that provide clear insights into key performance indicators (KPIs) and business metrics.Assist in creating interactive dashboards and reports that make complex data easily understandable.Data Reporting:Contribute to the development and maintenance of data reports, ensuring they align with business needs.Assist in optimizing data reporting processes to enhance efficiency.Collaboration:Work closely with cross-functional teams to understand data requirements and assist in creating solutions that address business needs.Participate in discussions to gather feedback and refine data analysis and reporting based on user input.Quality Assurance:Assist in validating the accuracy of data used in analysis and reporting.Perform thorough testing to ensure the functionality and reliability of data solutions.Learning and Growth:Stay updated on the latest data analysis tools, best practices, and industry trends.Proactively seek opportunities to enhance technical skills and contribute to the team's success.Qualifications:Bachelor's degree in a relevant field such as Business Analytics, Computer Science, or Information Systems.Basic understanding of data analysis concepts and principles.Familiarity with data analysis tools is a plus, but not required.Strong analytical skills and a passion for transforming data into actionable insights.Excellent communication skills, both written and verbal.Enthusiasm for learning and a proactive attitude toward professional development.Ability to work collaboratively within a team and adapt to changing priorities.Join our team as an Entry-Level Data Analyst and gain hands-on experience in extracting valuable insights from data to drive our business forward. Be part of a dynamic environment where you'll contribute to data-driven decision-making and make a meaningful impact."}
{"text": "Skills:\n\nBachelor's or Master's degree in Computer Science, Information Systems, Data Engineering, or related field.Proven experience in data engineering, data analysis, or a related role, preferably in a fast-paced, technology driven environment.Proficiency in programming languages such as Python, SQL, or Java, and experience with data manipulation and transformation frameworks (e.g., Pandas, Spark).Strong understanding of database technologies (e.g., SQL, NoSQL, BigQuery), data warehousing concepts, and cloud platforms (e.g., AWS, Azure, GCP).Experience with data integration tools (e.g., Apache NiFi, Talend, Informatica) and workflow management systems (e.g., Apache Airflow, Luigi).Familiarity with data visualization and BI tools (e.g., Tableau, Power BI) is a plus.Excellent analytical, problem-solving, and communication skills, with the ability to collaborate effectively across teams and communicate technical concepts to non-technical stakeholders.Detail-oriented mindset with a focus on data quality, accuracy, and consistency.Strong organizational skills and the ability to manage multiple tasks and priorities in a dynamic environment.Prior experience with Smartsheet, Air Table, Power Query, and Sharepoint is highly desirable due to the specific data organization, tracking and collaboration requirements of the role."}
{"text": "skills and current Lubrizol needs):\n\nCreate predictive models by mining complex data for critical formulating or testing insights Implement and assess algorithms in R, Python, SAS, JMP or C#/C++ Collaborate with data science team, as well as, scientists and engineers, to understand their needs, and find creative solutions to meet those needs \n\nPrevious Intern Projects Include\n\nPredictive modeling using Bayesian and machine learning methods R/Shiny tool development to enable model predictions and formulation optimization Creation of an interactive visualization tool for monitoring predictive models \n\nWhat tools do you need for success?\n\nEnrolled in a Bachelor’s program such as statistics, data analytics, machine learningExcellent programming skills with the ability to learn new methods quicklySignificant course work in statistics or data analytics; experience using advanced statistical software such as R or PythonDemonstrated computer programming skills, such as formal course work in C/C++, Java, or PythonExposure to database systems and the ability to efficiently manipulate complex data Strong problem solving and deductive reasoning skillsCuriosity and creativity\n\nBenefits Of Lubrizol’s Chemistry Internship Programs\n\nRewarding your hard work!Competitive payHoliday pay for holidays that fall within your work periodFUN! We host a variety of events and activities for our students. Past events include a Cleveland Cavaliers game, paid volunteering days, professional development and networking events, and even a picnic hosted by our CEO!\nWhile headquartered in the United States, Lubrizol is truly a global specialty chemical company. We have a major presence in five global regions and do business in more than 100 countries. Our corporate culture ensures that Lubrizol is one company throughout the world, but you will find each region is a unique place to work, live and play.\n\nLubrizol is"}
{"text": "Requirements:Python (Pandas, Numpy, SciKit-learn, etc.)Timeseries forecastingBuilding algorithmic decision tools (e.g. mixed-integer linear programming)Exposure to energy markets or battery systems modellingBS in Computer Science, Machine Learning or related technical field\nNice to Have:MS/PHD in Computer Science, Machine Learning or related technical fieldAWSCI/CD"}
{"text": "RequirementsMaster’s degree or PhD in computer science, artificial intelligence, applied mathematics, statistics, machine learning or related discipline5-7 years of applied experience in machine learning, deep learning methods, statistical data analysis and complex data visualization; experience in life science industry would be a plusDeep experience with PythonDesign and implement AI solutions working within a Software Engineering Life Cycle (SDLC (Software Development Life Cycle))Experience with the more recent large language models (GPT-4, Stable Diffusion models, others, other more focused language models)Experience or strong interest in working with cloud computing systems (preferably AWS (Amazon Web Services))Experience with AI platforms such as SageMaker, MLFlow, others, preferredExperience with building machine/deep learning models with at least one common framework such as PyTorch, Tensorflow, Keras, Scikit learn etc.Knowledge of relational database architecture and data management with expertise in SQLFamiliarity with software development practices such as unit testing, code reviews, and version controlExcellent analytical skills and presentation skillsStrong verbal and written communication skills and ability to work independently and cooperativelyProficiency in EnglishUS Work Visa - Will not require employer sponsorship now or in the futureSalary range $170,000-$240,000+ DOE + 20% bonusHybrid work schedule: Able to be in San Francisco office, in-person at least 3 days per week, option to work from home 2 days per week"}
{"text": "requirements/deliverables.\n\n Life with Us \n\n Your Career:  We are quickly growing our team and this opportunity will provide ample opportunity for career growth and skillset development. You will have the opportunity to work closely with leadership to help set your own goals and ensure you are on a path to achieving them.\n\nWe offer:\n\nDedicated professional development time.Peer groups.Education reimbursement.Student loan forgiveness.and much more...\n\n Day- to-Day with your Colleagues: \n\nWork closely with a tight-knit team of data scientists, as well as a larger team of software developers, network engineers, senior investigators, program managers, researchers, and data analysts to design, build, and optimize a Data Science platform to produce and analyze results, disseminate findings, and contribute to publications and presentations. Work on small projects analyzing a variety of big data covering national security, cyber security, business intelligence, online social media, human behavior and more. Support multiple simultaneous projects and take open-ended or high-level guidance, independently and collaboratively make discoveries that are mission-relevant, and package and deliver the findings to a non-technical audience.Bring your mix of intellectual curiosity, quantitative acumen, and customer-focus to identify novel sources of data across a range of fields, to improve the performance of predictive algorithms, and to encourage user adoption of high-end data analytics platforms in partnership with a highly qualified, highly motivated team. Leverage your strong background in research design, exploratory analysis, quantitative methods, user interface application design, and experience with customer outreach and engagement.\n\nMinimum Requirements: \n\nB.S. Degree in a quantitative or analytical field such as Computer Science, Mathematics, Economics, Statistics, Engineering, Physics, or Computational Social Science; or Master's degree or equivalent graduate degree including certificate-based advanced training courses.B.S. with 8+ years of experience OR Master's degree with 6+ years of experience in data science, analytics or quantitative intelligence analysis, and demonstrating progressive technical development and outcomes. Must have an active Top Secret clearance and must be able to achieve a TS/SCI clearance with PolygraphProficiency in one or more scripting languages such as R or PythonExperience working with a hybrid team of analyst, engineers, and developers to conduct research, and build and deploy complex, but easy-to-use algorithms and analytical platformsPrevious experience performing Research in data analytics or big data;Track record of active learning and creative problem solvingAbility to analyze and assess software development or data acquisition requirements and determine optimum, cost-effective solutions.\n\nDesired Skills\n\nData analytics experience in direct support if military or intelligence community customers, demonstrating progressive technical development and mission-focused outcomes;Significant experience dealing with at least two of the following data classes: open source, publicly available information (PAI); forensic media (i.e. DOMEX); measurement and signatures intelligence (MASINT).Significant experience with Knowledge Graphs and KG tech such as neo4jPrevious experience developing predictive algorithmsSocial network analysis, supply chain analysis, forensic accounting, pattern of life, natural language processing, social media analysis, classification algorithms, and/or image processing;Experience blending analytical methodologies and leveraging existing COTS/GOTS/OS tools in an unconventional manner;Familiarity utilizing virtualization and distributed field systems, such as Hadoop (or similar distributed file systems) in development and deployment environments;Familiarity using git, svn, JIRA, or other version control technologies;Experience with Amazon Web Services (AWS/C2S);Familiarity with hardware platforms, e.g., CPUs, GPUs, FPGAs, etc.\n\nOur salary ranges are market-driven and set to allow for flexibility. Individual pay will be competitive based on a candidate's unique set of knowledge, skills, and geographic diversity, with earnings potential commensurate with experience. The range for this position is:\n\n$130,000.00 - $218,000.00 annually.\n\nMaxar employees must follow all applicable Maxar policies and COVID-19 requirements as well as those of Maxar customers and third parties. Individual job requirements may vary, and Maxar reserves the right to modify its policies and requirements as it deems appropriate in accordance with applicable law.\n\nMaxar Technologies  values diversity in the workplace and is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.  Data Scientist - TS/SCI Hybrid"}
{"text": "experiences have formed a powerful engine for growth that activates audiences, drives real connections, and diversifies revenue for companies around the world.\n\nOur global organization of InMobians are excited to continue discovering and developing impactful technologies that will continue to transform people, businesses, and society.\n\nOverview\n\nThere are trillions of events a day in our system. That means that whatever models we use must be run at a tremendous scale with milliseconds in latency. We see the success of our models and experiments astonishingly quickly – our learning loop is not measured in weeks or days. It is hours and minutes. We live in what might be the fastest model-learning playgrounds in the world. We have built an infrastructure that enables model deployment at scale and speed. As data scientists, we sit alongside engineering colleagues who enable our models to deploy. Combine this with our growing variable set of hundreds of potential features (and growing!), and this is a highly fertile environment for building, experimenting, refining and achieving real impact from your models. If models fire, the bottom-line impact to our teams is immediate – you see the value of your work incredibly fast.\n\nThe Experience You'll Need\n\n The core foundation we look for is an aptitude with Mathematics, Statistics, Algorithms, Optimization and a competent ability in coding and with data science languages and tools, such as Python or Apache Spark. Most importantly, we look for a passion to investigate and learn about the world from data, to ask interesting and provocative questions, and be driven to put real models into production that drive real business value.  Basics of big data processing and cloud computing will be critical to succeed in this environment.  We are open to diverse academic backgrounds, providing an intent to think and problem-solve like a data scientist. Our team includes engineers, mathematicians, computer scientists, statisticians, physicists, economists and social scientists – a rock-star data scientist can come from any academic field. We are looking for a Staff level Data Scientist, but depending on the experience we may hire at a higher or lower level. \n\n\nRequired\n\n Master’s in a quantitative field such as Computer Science, Statistics, Electrical Engineering, Statistics, Mathematics, Operations Research or Economics, Analytics, Data Science. Ph.D. is a huge plus.  Depending on the level we are looking for experience in the Ad Tech Industry working in Data Science teams. You would have applied algorithms and techniques from Machine Learning, Statistics, Time Series or other domains in solving real world problems and understand the practical issues of using these algorithms especially on large datasets.  You are passionate about Mathematics, Algorithms, Machine Learning and eager to learn and apply cutting edge Science to Inmobi business problems. You are excited when you see the real world impact of your models in production. You are fast to execute.  You have intellectual depth to translate fuzzy business problems into rigorous mathematical problem statements and algorithms.  You have experience and passion in figuring out what to do when ML models don't produce any production lift.  Comfortable with software programming and statistical platforms such as R,Python etc. Comfortable with the big data ecosystem. Experience in Apache Spark will be a bonus.  Comfortable collaborating with cross-functional teams.  Excellent technical and business communication skills and should know how to present technical ideas in a simple manner to business counterparts.  Possess a high degree of curiosity and ability to rapidly learn new subjects and systems. \n\n\nThe Impact You'll Make\n\n You will be responsible for leading the data science efforts for one of the biggest in-app programmatic exchange in the world. This involves project ideation and conceptualization, solution design, measurement and solution iteration, coaching, deployment and post deployment management.  This will also include designing, development, testing of product experiments. You will need to guide the team in practical experiments, product design, model development and model evaluation. It is vital to be agile and iterate fast across experiment to deliver go-to-market ready products.  You are expected to be a hands-on part of the role where you will also actively analyse data, design and develop models, and problem-solve solutions with the rest of the team.  Additionally, stakeholder management is needed. It will involve being the interface with internal stakeholders such as our Product, Engineering, Data, Infrastructure, and Business teams.  Our team strives for thought leadership in the sector. We encourage and support all team members to write blogs, commentary and case studies published on the InMobi blog.  We also support team members across our ML/AI team to speak at industry conferences and represent InMobi’s work.  You will learn how to design and build models for specific business problems. Even before that, you will be responsible for identifying the problem areas where AI can be applied to best business impact. You will learn to start a model design by anchoring in the business context and end user needs. You will learn how to connect model impact with real and measurable business impact.  You will work in a multi-functional team environment. You will collaborate and benefit from the skills of a diverse group of individuals from teams such as engineering, product, business, campaign management and creative development.  You will have the opportunity to experiment with multiple algorithms. Enduring learning comes from building, launching and reviewing performance of a particular algorithm; from asking why something worked or why it did not work; from asking how to tailor techniques to fit the problem at hand. We have an environment that makes this possible at speed.  Importantly, you will learn to become creative in designing models to be successful. Model design is not one-size-fits. Our models need to fit our particular problems and be modified to perform. Tougher problems require layers of models, and feedback mechanisms in a dynamic environment such as ours.  We are a company that innovates and demonstrates our thought leadership to the world, whether in products, research papers or conferences – there are many opportunities for you to shine. \n\n\nAbout Us\n\nInMobi is the leading provider of content, monetization, and marketing technologies that fuel growth for industries around the world. Our end-to-end advertising software platform, connected content and commerce experiences activate audiences, drive real connections, and diversify revenue for businesses everywhere. With deep expertise and unique reach in mobile, InMobi is a trusted and transparent technology partner for marketers, content creators and businesses of all kinds.\n\nIncorporated in Singapore, InMobi maintains a large presence in San Mateo and Bangalore and has operations in New York, Delhi, Mumbai, Beijing, Shanghai, Jakarta, Manila, Kuala Lumpur, Sydney, Melbourne, Seoul, Tokyo, London and Dubai. To learn more, visit inmobi.com.\n\nOur Purpose \n\nInMobi creates transformative mobile experiences and software platforms to positively impact people, businesses, and societies around the world.\n\nWe believe that our innovations at the intersection of artificial intelligence, commerce, and the creator economy will revolutionize the way consumers use their mobile devices. Our mission is to power our customers’ growth with innovative content and commerce experiences that help them activate their audiences and drive real connections. How do we do it?\n\n An End-to-End Content, Monetization, & Marketing Platform the fuels industry growth  AI-Powered Audience Activation for the open content, media and marketing ecosystem  New Content and Commerce experiences for a world of connected devices \n\n\nAward-winning Culture, Best-in-class Benefits\n\nOur compensation philosophy enables us to provide competitive salary that drives high performance while balancing business needs and pay parity. We determine compensation based on a wide variety of factors including role, nature of experience, skills and location.\n\nThe base (fixed) pay range for this role would range from what $168,630 USD to $240,901 USD (Min and Max of Base Pay range). This salary range is in applicable for our offices located in California and New York*.\n\nOur ranges may vary basis final location / region / or fully remote roles in accordance to the geographical differentiation in pay scales in the country.\n\n\nIn addition to cash compensation, based on the position, an InMobian can receive equity in the form of Restricted Stock Units. We believe that our employees/personnel should have the ability to own a part of the entity they are a part of. Therefore, the entity employing you may elect to provide such stocks to you. Ownership of stock aids us to treat our employer company as our own and base our decisions on such a company’s best interest at heart. To encourage a spirit of shared ownership, we grant InMobians relevant company stock(s). As you contribute to the growth of your company, certain stocks may be issued to you in recognition of your contribution.\n\nA Quick Snapshot Of Our Benefits\n\nCompetitive salary and RSU grant (where applicable) High quality medical, dental, and vision insurance (including company-matched HSA) 401(k) company match Generous combination of vacation time, sick days, special occasion time, and company-wide holidays Substantial maternity and paternity leave benefits and compassionate work environment Flexible working hours to suit everyone Wellness stipend for a healthier you! Free lunch provided in our offices daily Pet friendly work environment and robust pet insurance policy - because we love our animals! LinkedIn Learning on demand for personal and professional developmentEmployee Assistance Program (EAP) \n\n\nInMobi is \n\nInMobi is a place where everyone can grow. Howsoever you identify, and whatever background you bring with you, we invite you to apply if this sounds like a role that would make you excited to work.\n\nInMobi provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\n\nInMobi has implemented a mandatory COVID vaccination policy for all employees in the U.S. Employees who are unable to be vaccinated may request an exemption under certain circumstances."}
{"text": "experience the youth sports moments important to our community, we are helping families elevate the next generation through youth sports.\nSo if you love sports and their community-building potential, or building cool products is your sport, GameChanger is the team for you. We are a remote-first, dynamic tech company based in New York City, and we are solving some of the biggest challenges in youth sports today.\nThe Position:We are looking for a Senior Data Engineer to lead our data governance and security efforts. This role sits on the Core Data Team, which is focused on delivering high-quality data and tooling on a reliable and scalable platform. You’ll work closely with your Core Data and Platform Engineering colleagues to enhance the discoverability and accessibility of our data while ensuring compliance with GameChanger and industry standards and regulations. The solutions you build will benefit everyone including Analysts, Product Managers, Marketers, Developers, Executives, Coaches, Players, and Parents!\nWhat You’ll Do:Design and build Data Governance services, systems, and product features to classify data, track data lineage and provenance, secure its access and audit its usageUtilize Python, SQL, GitHub, Airflow, Snowflake, and DBT to build data integration workflows and observability tools.Collaborate within the Core Data team to develop, update and maintain secure data infrastructure, including designing and implementing data pipelines, ETL processes, and data access controlsCollaborate with Platform and Security teams to achieve compliance with internal cybersecurity standards (inspired by NIST) throughout the governance lifecycle, including conducting regular audits and risk assessmentsLead efforts to identify and mitigate potential vulnerabilities and security risks within data systems and infrastructureLead supporting data initiatives, providing insights and guidance on data governance, security and complianceStay updated with the latest industry trends, tools, and technologies related to data governance and security and compliance, and make recommendations for process improvements and enhancementsShare your knowledge through technical documentation, code reviews, and mentoring\nWho You Are:5+ years of software development experience, preferably as a data or backend engineer focused on the data governance and/or security spaceExperience with a data governance platform like DataHub or homegrown equivalentExpertise in Python and/or Spark for the processing of dataExpertise with data warehouse management in SnowflakeExperience with containers and orchestration toolsProven experience with cloud security primitives in AWS such as IAM and Security GroupsStrong experience with version control systems like GitHub and working collaboratively in a team environmentExcellent communication and collaboration skills, with the ability to work effectively across cross-functional teamsDrive to help others learn and improve themselves as engineersEvangelist for adopting robust data governance and security practices across the company\nBonus Points:Experience with privacy compliance and regulationsExperience working with Kafka, Scala, Typescript and Node.jsExperience with IaC tools like TerraformExperience with Github Actions, DataDogExperience with dbt and Airflow\n\nPerks:Work remotely throughout the US* or from our well-furnished, modern office in Manhattan, NY.Unlimited vacation policy.Paid volunteer opportunities.WFH stipend - $500 annually to make your WFH situation comfortable.Snack stipend - $60 monthly to have snacks shipped to your home office.Full health benefits - medical, dental, vision, prescription, FSA/HRA., and coverage for family/dependents.Life insurance - basic life, supplemental life, and dependent life.Disability leave - short-term disability and long-term disability.Retirement savings - 401K plan offered through Vanguard, with a company match.Company paid access to a wellness platform to support mental, financial and physical wellbeing.Generous parental leave.DICK’S Sporting Goods Teammate Discount.\nWe are \nThe target salary range for this position is between $150,000 and $190,000. This is part of a total compensation package that includes incentive, equity, and benefits for eligible roles. Individual pay may vary from the target range and is determined by several factors including experience, internal pay equity, and other relevant business considerations. We constantly review all teammate pay to ensure a great compensation package that is fair and equal across the board.\n*DICK'S Sporting Goods has company-wide practices to monitor and protect us from compliance and monetary implications as it pertains to employer state tax liabilities. Due to said guidelines put in place, we are unable to hire in AK, DE, HI, IA, LA, MS, MT, OK, and SC."}
{"text": "experience in developing and deploying AI models and algorithmsYou are proficient in Python: You possess strong programming skills in Python, with the ability to write clean, efficient, and scalable codeYou have Foundational Model Experience: You have hands-on experience working with foundational models such as RAG (Retrieval-Augmented Generation), and you are well-versed in their applications and limitationsYou are proficient in API Building: You have experience building and designing APIs using frameworks like Flask, ensuring seamless integration and interaction between different systems and componentsYou are proficient in AWS tools: You are proficient in utilizing AWS services and tools for data storage, processing, and deployment of AI modelsYou are proficient in Big Data: You have expertise in large-scale data processing and analysis, and you are comfortable working with big data technologiesYou have Vector and Embedded Vector Knowledge: You have a solid understanding of vector spaces and embedded vectors, and you can apply this knowledge to develop efficient and accurate AI modelsYou are proficient in Data Architecture and ETL: You have experience designing and implementing robust data architectures and ETL (Extract, Transform, Load) pipelines to ensure seamless data flow and integrityYou have expertise in SQL and NoSQL Databases: You are proficient in working with both SQL and NoSQL databases, and you can efficiently query and manipulate data to support AI model development and deploymentYou leverage Testing and CI/CD: You are well-versed in testing methodologies and have experience implementing continuous integration and continuous deployment (CI/CD) pipelines to ensure code quality and streamline the development processYou are an expert in Code Review and Collaboration: You have a strong understanding of code review best practices and enjoy collaborating with team members to maintain high-quality code and share knowledgeYou know Agile Methodologies: You are familiar with Agile development methodologies, such as Scrum or Kanban, and can effectively work in an Agile environment to deliver iterative and incremental valueYou live in Cross-Team Collaboration: You thrive in a collaborative environment, working effectively with cross-functional teams, including data scientists, software engineers, and product managers, to deliver high-quality AI solutionsYou are Continuous Learning and Adaptable: You stay up to date with the latest advancements in AI, Client, and data engineering, and you are eager to learn and adapt to new technologies and methodologies\nTHINGS YOU MAY DO:Develop AI Models and Algorithms: Design, implement, and optimize advanced AI models and algorithms using Python, foundational models like RAG, and other relevant technologiesBuild and Integrate APIs: Design and develop robust APIs using frameworks like Flask to facilitate seamless integration and interaction between AI models, tools, and other systemsIntegrate AI into Tools and Applications: Collaborate with cross-functional teams to integrate AI capabilities into user-friendly tools and applications, focusing on enhancing functionality and user experienceUtilize AWS for AI Deployment: Leverage AWS services and tools to deploy, scale, and manage AI models and applications in a secure and efficient mannerPerform Large-Scale Data Processing: Use big data technologies to process and analyze vast amounts of data, enabling the development of accurate and insightful AI modelsDesign Data Architectures and ETL Pipelines: Develop robust data architectures and ETL pipelines to ensure smooth data flow, data quality, and data integrity throughout the AI development lifecycleImplement Testing and CI/CD: Develop and execute comprehensive testing strategies and implement CI/CD pipelines to ensure code quality, reliability, and efficient deployment of AI models and applicationsConduct Code Reviews: Actively participate in code reviews, providing constructive feedback and ensuring adherence to coding best practices and standardsOptimize AI Performance and Scalability: Continuously monitor and optimize the performance and scalability of AI models and applications, ensuring they meet the highest standards of efficiency and reliabilityCollaborate and Innovate: Work closely with UI/UX engineers, software engineers, and product managers to ideate, prototype, and implement innovative AI solutions that push the boundaries of what's possibleStay at the Forefront of AI Research: Keep abreast of the latest research and advancements in AI, Client, and data engineering, and apply this knowledge to drive innovation and improve our AI-powered tools and GenAI solutions\nPERKS:Paid flexible time off & holidaysCompetitive medical, dental, vision benefitsFun, get-things-done work environment\nreq24-00458"}
{"text": "experience in machine learning, distributed microservices, and full stack systems  Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment  Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance \n\nBasic Qualifications: \n\n Bachelor’s Degree  At least 6 years of experience in application development (Internship experience does not apply)  At least 2 years of experience in big data technologies  At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud) \n\nPreferred Qualifications:\n\n 7+ years of experience in application development including Java, Python, SQL, Scala  4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)  4+ years experience with Distributed data computing tools (Flink, Kafka, Spark etc)  4+ year experience working on real-time data and streaming applications  4+ years of experience with NoSQL implementation (DynamoDB, OpenSearch)  4+ years of data warehousing experience (Redshift or Snowflake)  4+ years of experience with UNIX/Linux including basic commands and shell scripting  2+ years of experience with Agile engineering practices \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "Requirements:\n\n5+ years of related work experience in data science, analytics, or related quantitative fieldAccomplished technical contributor in data science & advanced analytics teams, preferably in a product-centric organizationExperience in machine learning & statistics needed, along with demonstrated proficiency in scalable coding (SQL, and/or R or Python) and data science tools (Git, Docker)Elevated level of business acumen with experience in digital & traditional marketing preferred.Bonus if you have deep expertise in development & applications of marketing & customer measurement methods (e.g. Media Mix Models, Attribution Modeling, Customer Lifetime Value, Segmentation, etc.); experience with Bayesian approaches preferred.Demonstrated ability to think independently, lead and influenceAbility to communicate clearly and lead discussions with technical and non-technical stakeholders, including clients & executives\n\n\nPay Transparency\n\nAt Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.\n\nFor our Manager positions, our compensation ranges from $79,000 to $132,000, which is inclusive of a 20% bonus.\n\nBenefits Of Working At Ovative Group\n\nWe provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.\n\nCulture: \n\nCulture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.\n\nCompensation and Insurance: \n\nWe strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.\n\nWe’re rewarded fairly and when the company performs well, we all benefit.\n\nTangible amenities we enjoy: \n\nAccess to all office spaces in MSP, NYC, and CHI Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams. Flexible paid vacation policy 401k match program Top-notch health insurance options Monthly stipend for your mobile phone and data plan Sabbatical program Charitable giving via our time and a financial match program Shenanigan’s Day \n\n\nWorking at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!"}
{"text": "experience in:\n-Expert level SQL skills.-Very good Python skills, focused on data analysis, adaptation, enhancement.-Expert level in Mines and interprets data.-Expert level in Performs root causes analysis with an ability to learn industry data and understands how to conduct ab testing.-Very good at Translates data into meaningful insights and being able to present them so that others can take action or make decisions.\nThe main responsibilities for this position are:\n-Information Analysis-Performs data analysis and validation.-Translates data into meaningful insights.-Mines and interprets data.-Performs root causes analysis with an ability to learn industry data and understands how to conduct ab testing.-Monitors and provides insight into key metrics, including metric performance.-Handles data preparation and analysis for various projects.-Writes, maintains, and updates SQL queries in response to questions from internal customers.-Assists development teams in creating and maintaining data reporting models.-Customer Reporting-Creates customer facing reports through our analytics tools.-Creates reports for internal customers using a separate set of tools.-Develops reports and data visualizations to present insights in the most effective way for decision making.-Coordinates data feeds and sources additional data required for analysis.-Determines the value of new internal and external data sources.-Merges disparate data sets and transforms the data to derive metrics and usable information.-Assists in the development and implementation of data monitoring and measurement systems."}
{"text": "experience\nStored Procs (AWS Postgres) to API integration which is the critical need for the project.Pycharm, Pytest, API Data, TDD API Gateway"}
{"text": "QUALIFICATIONS: \n\nEducation:\n\n12 years of related experience with a Bachelor’s degree; or 8 years and a Master’s degree; or a PhD with 5 years experience; or equivalent experience\n\nExperience:\n\nWork experience in biotech/pharmaceutical industry or medical research for a minimum of 8 years (or 4 years for a PhD with relevant training)Experience in clinical developmentExperience in ophthalmology and/or biologic/gene therapy a plus\n\nSkills:\n\nStrong SAS programming skills required with proficiency in SAS/BASE, SAS Macros, SAS/Stat and ODS (proficiency in SAS/SQL, SAS/GRAPH or SAS/ACCESS is a plus)Proficiency in R programming a plusProficiency in Microsoft Office Apps, such as WORD, EXCEL, and PowerPoint (familiar with the “Chart” features in EXCEL/PowerPoint a plus)Good understanding of standards specific to clinical trials such as CDISC, SDTM, and ADaM, MedDRA, WHODRUGExperience with all clinical phases (I, II, III, and IV) is desirableExperience with BLA/IND submissions is strongly desirableGood understanding of regulatory requirements for submission-related activities (e.g., CDISC, CDASH, eCTD) and CRT packages (e.g., XPTs Define/xml, reviewer’s guide, analysis metadata report, executable programs) is desirableAble to run the P21 checks is a plusKnowledge of applicable GCP/FDACHMP//ICH/HIPPA regulationsDisplays excellent organization and time management skills, excellent attention to detail, and ability to multi-task in a fast-paced environment with shifting priorities and/or conflicting deadlinesExcellent written and verbal communication skills and strong team player with demonstrated track record of success in cross-functional team environmentProven conceptual, analytical and strategic thinkingGood interpersonal and project management skillsProactively identifies risks, issues, and possible solutions\n\nBase salary compensation range:\n\nOutside of Bay Area Range: $176,000/yr - $198,000/yr\n\nBay Area Range: $178,000/yr - $211,000/yr\n\nPlease note, the base salary compensation range and actual salary offered to the final candidate depends on various factors: candidate’s geographical location, relevant work experience, skills, and years of experience.\n\n4DMT provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, genetic information, marital status, status as a covered veteran, and any other category protected under applicable federal, state, provincial and local laws.\n\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities"}
{"text": "experience.Six or more years of relevant work experience. Demonstrated knowledge or experience with Machine Learning, Data Science, and Data Engineering principles and related technologies such as R, Python, SQL etc. Experience delivering and managing AI/ML based software products or models as an engineer or product owner/manager. \nEven better if you have one or more of the following:\nMaster's degree or Ph.D. in data analytics or similar field Experience with agile software development methodology Experience working with Pega Decisioning platform. Pega certification(s) a plusProven experience with data engineering and ETL techniques using data from a wide variety of data sources Rigorous understanding of statistics/machine learning and ability to discern appropriate analytics techniques to problem-solve Knowledge of data warehouse, data lakes, and cloud architecture (Teradata, GCP, AWS etc.)Ability to learn new analytics tools and platforms quickly. Excellent communication skills with ability to interpret and explain analytics results and implications to non-technical audience. Ability to work in a fast-paced environment, managing multiple projects and stakeholders. Passion for applying data-driven approaches and quantitative reasoning for business decision making\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don't meet every \"even better\" qualification listed above.\n\nWhere you'll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\n\n\nWe're proud to be"}
{"text": "experience in bash and cloud concepts such as (EC2, EMR, Glue, ECS, Lambda, IAM, Security Groups, S3, etc.)Utilize programming languages like Python, Java and Open-Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake.Collaborate with Product owner and Tech lead to solve Business user questions on the data pipelines team supports and resolve infrastructure issues. Required Qualifications:5+ years’ experience using programming languages like Python, Java5+ years Distributed data/computing tools (MapReduce, Hive, Spark, EMR, Kafka)3+ years’ experience in AWS tech stack (EC2, EMR, Glue, ECS, Lambda, IAM, Security Groups, S3, etc.) Preferred Qualifications:3+ years AgileFlexible in experimenting with and learning new technologies.﻿RegardsPuja Kumari(O) 630-7802001 | pujak@rcginfosoft.comwww.rcginfosoft.com"}
{"text": "experience.•Deep Learning.•Preferable experience/certification in Automation tools like Xceptor, Appian or KNIME."}
{"text": "Requirements/Must Have Skills:Experience working in AWS environment (S3, Snowflake, EC2, APIs)Skilled in coding languages (Python, SQL, Spark)Ability to thrive in a fast paced, evolving work environment Experience with BI tools like Tableau, QuicksightPrevious experience building and executing tools to monitor and report on data quality.Excellent communication skills (written and verbal)Having a sense of ownership and craftsmanship around the code base Open to learning about new technologies and sharing your knowledge with others\nrequirements.Build well-managed data solutions, tools, and capabilities to enable self-service frameworks for data consumers Partner with the business to provide consultancy and translate the business needs to design and develop tools, techniques, metrics, and dashboards for insights and data visualization Troubleshooting, debugging, maintaining, and improving existing reporting solutions.Demonstrate ability to explore and quickly grasp new technologies to progress varied initiatives.Drive analysis that provides meaningful insights on business strategiesDrive an understanding and adherence to the principles of data quality management including metadata, lineage, and business definitions Build and execute tools to monitor and report on data quality."}
{"text": "Requirements:Minimum 4 years of experience in tier-1 Tech companies as a Data Engineer Excellent understanding of large-scale Data Pipelines Ability to simplify complex topics and explain them in an engaging mannerParticipation in interview panels and hiring committees High levels of empathy to understand the challenges faced by students and willingness to help them outShould be willing to work on weekends/evenings and be available as per the US time zonePreferred languages/tools: SQL, PySpark, Kafka, Airflow"}
{"text": "REQUIREMENTS:Prior experience in solutions architecture or software architectureGood experience in Big dataStrong skills inPySparkAirflow,Hive\nRegards,\nRamdas SUS IT Recruiterramdas@themesoft.com | Themesoft Inc"}
{"text": "Skills & Experience:Professional experience with Python and a JVM language (e.g., Scala) 4+ years of experience designing and maintaining ETL pipelines Experience using Apache SparkExperience with SQL (e.g., Postgres) and NoSQL databases (e.g., Cassandra, ElasticSearch, etc.)Experience working on a cloud platform like GCP, AWS, or Azure Experience working collaboratively with git \nDesired Skills & Experience:Understanding of Docker/Kubernetes Understanding of or interest in knowledge graphsExperienced in supporting and working with internal teams and customers in a dynamic environmentPassionate about open source development and innovative technology\nBenefits: Limitless growth and learning opportunitiesA collaborative and positive culture - your team will be as smart and driven as youA strong commitment to diversity, equity & inclusionExceedingly generous vacation leave, parental leave, floating holidays, flexible schedule, & other remarkable benefitsOutstanding competitive compensation & commission packageComprehensive family-friendly health benefits, including full healthcare coverage plans, commuter benefits, & 401K matching Sayari is"}
{"text": "experience in Technical Leadership in a Big Data Environment Teradata experienceETL Experience with Glue and Lambda Functions RedShift Serverless with AWS Experience creating Materialized Views in Amazon RedshiftRelease Management and Performance Management within Big Data SQL Experience- Queries and Joins Experience with Data Lakes\nPlusses Cognos Experience Experience working on a modernization projectProcess Improvement Experience AI/ML Knowledge"}
{"text": "skills and handling big data along with real time streamingGraph Ontology and semantic modeling with GraphQL or SPARQL experience is desirable.Proactive, self-driven, works independently and collaborates wellExpertise in Python, PysparkUse of databricks is a mustclient - AT&T"}
{"text": "experience on Data Analysis.Tableau and sql experience  If you’re interested in learning more, I’d love to connect. Would you be available for a quick phone call at your preferred time? I look forward to hearing from you soon."}
{"text": "Qualifications)\n\n Minimum two years of experience with DOMO MajorDOMO Certification Required 3+ years experience in data analysis, reporting, business intelligence or financial analysis Bachelor’s Degree in Business, Statistics, Mathematics, Analytics, Computer Sciences or related field Experience with SQL, and DOMO\n\nHow To Stand Out (Preferred Qualifications)\n\n 2 years experience in providing people analytics reporting to organizations\n\n#SolarEnergy #DataAnalysis #RemoteWork #CareerGrowth #CompetitivePay\n\nAt Talentify, we prioritize candidate privacy and champion equal-opportunity employment. Central to our mission is our partnership with companies that share this commitment. We aim to foster a fair, transparent, and secure hiring environment for all. If you encounter any employer not adhering to these principles, please bring it to our attention immediately.\n\nTalentify is not the EOR (Employer of Record) for this position. Our role in this specific opportunity is to connect outstanding candidates with a top-tier employer.\n\nTalentify helps candidates around the world to discover and stay focused on the jobs they want until they can complete a full application in the hiring company career page/ATS."}
{"text": "requirements and contribute to the development of data architectures.Work on data integration projects, ensuring seamless and optimized data flow between systems.Implement best practices for data engineering, ensuring data quality, reliability, and performance.Contribute to data modernization efforts by leveraging cloud solutions and optimizing data processing workflows.Demonstrate technical leadership by staying abreast of emerging data engineering technologies and implementing industry best practices.Effectively communicate technical concepts to both technical and non-technical stakeholders.Collaborate with the team to address unique challenges in talent attraction, development, and retention.\n\n\nMinimum Requirements:\n\n4+ years of experience of relevant experience\n\nCritical Skills:\n\n4+ years of experience of professional experience in IT data and analytics fieldProven experience as a Data Engineer or in a similar role.Deep technical expertise in building and optimizing data pipelines and large-scale processing systems.Experience working with cloud solutions and contributing to data modernization efforts.Experience working with Databricks, Snowflake, Azure SQL, Azure ADF, Big Query, GCP, Power BI/Tableau, Azure ADFStrong programming skills (e.g., Python, Java, Scala) for data manipulation and transformation.Excellent understanding of data engineering principles, data architecture, and database management.\n\n\nAdditional Experience:\n\nStrong problem-solving skills and attention to detail.Excellent communication skills, with the ability to convey technical concepts to both technical and non-technical stakeholders.Knowledge of the healthcare, distribution, or software industries is a plus.Strong technical aptitude and experience with a wide variety of technologiesAbility to rapidly learn and if required evaluate a new tool or technology.Strong verbal & written communication skillsDemonstrated technical experience.Be an innovative thinker.Must have a strong customer and quality focus.\n\n\nEducation:\n\nBachelor's degree in a related field (e.g., Computer Science, Information Technology, Data Science) or equivalent experience\n\n\nWork Environment/Physical Demands:\n\nGeneral Office Requirements\n\nAt McKesson, we care about the well-being of the patients and communities we serve, and that starts with caring for our people. That’s why we have a Total Rewards package that includes comprehensive benefits to support physical, mental, and financial well-being. Our Total Rewards offerings serve the different needs of our diverse employee population and ensure they are the healthiest versions of themselves. For more information regarding benefits at McKesson, please click here.\n\nAs part of Total Rewards, we are proud to offer a competitive compensation package at McKesson. This is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. In addition to base pay, other compensation, such as an annual bonus or long-term incentive opportunities may be offered.\n\nOur Base Pay Range for this position\n\n$112,200 - $187,000\n\nMcKesson is an Equal Opportunity/Affirmative Action employer. \n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.Qualified applicants will not be disqualified from consideration for employment based upon criminal history.\n\nMcKesson is committed to being an \n\nCurrent employees must apply through the internal career site.\n\nJoin us at McKesson!"}
{"text": "skills to redesign and enhance these procedures.Communication (10%): Liaise with different departments to discuss outcomes and reporting needs.\nSkills and Qualifications:Required Education: Bachelor’s degree in Statistics, Computer Science, Mathematics, Business, Healthcare, or a related field; OR a 2-year degree in a related field plus 2 years of reporting and data analysis work experience; OR 4 years of related experience.Required Skills: Strong organizational, customer service, and analytical abilities. Proficient in Microsoft Office and capable of learning new software platforms.Preferred Skills: Experience with SAS and/or DB2, negotiation skills, and familiarity with ICD9/CPT4 coding.\nAdditional Requirements:Fluent in English with excellent ability to read, write, and speak.Familiarity with “Lean Management” and “Six Sigma” methodologies.Proficient in creating value stream maps and using Microsoft Visio.Eligibility for C2 clearance is required. The role is onsite.\nSchedule:Hours per week: 40Hours per day: 8Start Date: 5/6/23"}
{"text": "experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters.\nLocation:\nAt LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together.\nThis is a full-time engineering role based in Sunnyvale, CA\nTeam Overview: \nThe mission of the Marketplace Monetization AI team is to help LinkedIn create economic value while helping members seek job opportunities and customers to find best candidates to the platform. The team is at the center of the largest LinkedIn business line to drive billion-dollar-level business impact and help hundreds of millions of members find their next job, a complex problem-space with exciting opportunities to revolutionize the digital job marketplace. \nWe develop cutting-edge AI technologies with a wide range of technologies such as deep learning, generative AI, large language models, recommender systems, ranking, search, advertising, auction theory and much more in our solutions, and support many areas of member and customer success within LinkedIn including Jobs-You-May-Be-Interested-In (JYMBII), Job Search, Jobs Notifications, LinkedIn Coach, etc. We are closely partnering with many products, engineering and infrastructure teams to build the next generation AI-first product experience for our members. \nQualifications:\n• 7+ years of relevant professional experience • 3+ years of management experience • BA/BS in Computer Science or other technical discipline, or related practical technical experience• Hands on experience in data modeling and machine learning engineering\nPreferred Qualifications: \n• 10+ years of relevant professional work experience• 5+ years of experience leading engineering teams. • At least one year of experience managing other managers and technical leads. • Domain experience in Ads AI or Marketplace AI • MS or PhD in Computer Science, Machine Learning, Statistics or related fields\nSuggested Skills:\n• Machine Learning & AI • Engineering Leadership • Change Management\nYou will Benefit from our Culture:\nWe strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels.\n--\nCompensation:\nLinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $220,000-$300,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits.\nEqual Opportunity Statement\nLinkedIn is committed to diversity in its workforce and is proud to be \n LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful.\n If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation.\n Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to:\n -Documents in alternate formats or read aloud to you  -Having interviews in an accessible location -Being accompanied by a service dog -Having a sign language interpreter present for the interview\n A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response.\n LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information.\n Pay Transparency Policy Statement As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency.\n Global Data Privacy Notice for Job Candidates This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice"}
{"text": "requirements to pull required data to measure the current state of these assets, set up usage metrics for internal and external stakeholders.Table Metadata to improve documentation coverage for tables, including table descriptions, column definitions, and data lineage.Implement a centralized metadata management system to maintain and access asset documentation.Ensure that all existing and new data assets are properly documented according to established standards.Pipeline Clean-up and ConsolidationConsolidate and streamline pipelines by eliminating redundancies and unnecessary elements according to the set of provided rules.Clean up and restructure data tables, ensuring consistent naming conventions, data types, and schema definitions.Retire or archive obsolete dashboards and workflows.Implement monitoring and alerting mechanisms for critical workflows to ensure timely issue detection and resolution.Set up a foundation for scalable Data Model for the Stock Business - Implement and build performant data models to solve common analytics use-Knowledge Transfer and DocumentationThoroughly document the work performed, including methodologies, decisions, and any scripts or tools developed.Provide comprehensive knowledge transfer to the data team, ensuring a smooth transition and the ability to maintain the optimized data environment.\nSkills: Proven experience in data engineering and data asset management.Proficiency in SQL, Python, and other relevant data processing languages and tools.Expertise in data modeling, ETL processes, and workflow orchestration (e.g., Airflow, Databricks).Strong analytical and problem-solving skills.Excellent communication and documentation abilities.Familiarity with cloud data platforms (e.g., Azure, AWS, GCP) is a plus.\nPride Global offers eligible employee’s comprehensive healthcare coverage (medical, dental, and vision plans), supplemental coverage (accident insurance, critical illness insurance and hospital indemnity), 401(k)-retirement savings, life & disability insurance, an employee assistance program, legal support, auto, home insurance, pet insurance and employee discounts with preferred vendors."}
{"text": "Skills' Details \n\nPassion for Machine Learning and Data Science and their fundamentals\n\nResearch and quantitative analysis of AI risk management\n\nDevelopment of data science algorithms using Python\n\nDocumentation\n\nWould prefer someone who has a strong understanding or at least a passion for AI Risk Management.\n\n Description \n\nThis is a Data Scientist role in Chat and Voice Technology team. The team builds next generation AI and Search platforms for the bank, enabling smart virtual assistants across multiple channels and platforms. This position requires candidate to be well versed with various machine learning algorithms and NLP techniques, including LLM and Generative AI techniques. Role offers an opportunity to work with seasoned architects, PhDs in Machine Learning and NLP, Software Engineers, and Risk Management partners. Candidate should be able to work independently and collaboratively to take ownership of prepare models for validation and monitoring. Candidate must possess passion for machine learning and data analysis, creatively solving how to assess risk, conduct and summarize research, and prepare technical white papers to support Machine Learning and Software Engineers through the model development lifecycle. This role is unique, in that candidate must be 100% AI Risk Management (50% Research and Quantitative Analysis, 25% Development, and 25% White Paper Documentation).\n\n Required Skills \n\n Bachelor Degree in Computer Science, Data Science, Mathematics, or related field Knowledge of machine learning and related techniques Knowledge of recent developments in AI space including but not limited to transformers, LLMs, Generative AI Good understanding of a version control system like git to be able to efficiently collaborate with colleagues. Strong Python development skills and knowledge of Java/C++ Adept at leveraging ML/AI techniques to solve critical business problems with good understanding of Supervised, Unsupervised and Reinforcement Learning. Excellent interpersonal communication skills for tech, business, and risk conversations Good analytical skills to break down requirements, solve complex problems, and challenge the approach, build, and test of AI models and model components\n\n\n Skills \n\nPython, Data science, Data, java, Algorithm, risk management, artificial intelligence, Machine learning, Predictive modelling, Data analysis, Predictive analytics\n\n Top Skills Details \n\nPython, Data science, Data, java Algorithm, risk management, artificial intelligence\n\n Additional Skills & Qualifications \n\nThere will be a heavy research and analysis component to this job, especially around risk management related to artificial intelligence and GenAI. They will be diving into an understanding of the biases of AI and the algorithms created by other data scientists on the team, how the data flows through the algorithm, and the risks associated to the outcomes. They'll be developing their own algos a smaller percentage of their time, but need to have a strong background in Python to be able to read the code of the 18 existing AI models and their architecture. They'll be spending a lot of time trying to break what exists and raising questions around why certain things were done that way. From a documentation perspective, they'll need to be able to create graphical representations of their findings so a lay person could understand them.\n\n About TEKsystems \n\nWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.\n\nThe company is"}
{"text": "Skills/Attributes: Data Analysis, Loan Processing, Microsoft Office, SQL Job Description\n**Only those lawfully authorized to work in the designated country associated with the position will be considered.**\n**Please note that all Position start dates and duration are estimates and may be reduced or lengthened based upon a client’s business needs and requirements.**\nRequired Education:• Bachelors/University degree\nRequired Qualifications:• 10+ years of experience in finance/ project management• Experience and proficiency in building data pipelines and performing analytics using KNIME (or similar software• Experience creating team SharePoint sites and maintaining content to make information and documents easily accessible• Proficiency with Visual Basic for Applications (VBA) for Microsoft Office• Proficiency with SQL and relational database management systems• Strong proficiency with Microsoft Excel• Significant experience building end-user tools with Microsoft Access• Additional experience and knowledge for Internal Candidate• Experience in using Lynx UI, Optima Cognos Reporting Tool, (Facility Management, Collateral), and extracting data from Data Globe (especially data schemas: DGSTREAM, DGFU, DGREF & DGLOBE)• Good understanding of Loan data hierarchy (Request/Credit Agreement/Facility/GFRN) in Lynx"}
{"text": "Qualifications6+ years of experience in: Data Integration, data analysis, analytics, or modelingIdeal candidate would have Metadata/data lineage, data dictionary, and data controls or catalog experience.5+ years of SQL (Oracle, SQL server or Teradata) experienceMust have hands on experience in Data profiling, Data quality, and Data Reporting or Data scorecardsWill be responsible to define data requirements, provide recommendations, develop Data assessment, data mapping and lineage, and participate in data validation.Perform data assessment and profile of the data from multiple, cross business sources.Should be able to communicate and ask the questions that are needed to gather the details required for the project.Need to understand data from a technical prospective but must be able to define and discuss from a business prospective.Need to understand databases do not have to develop.Ability to write basic SQL QueriesMS Excel skills must be very strongDay to Day:Responsible for ensuring compliance with Enterprise Data Management Policy.Collect business metadata, artifacts supporting Data Quality Control Assessment and application preventative controls.Support the understanding data management concepts, such as core data, data element descriptions and business criticality.Escalate to team lead if timeline risks arise which might impact deliverables.Partner with QA to ensure artifacts meet Data Management Standards."}
{"text": "experience for any student pursuing their career. This is a compensated internship. \nResponsibilities include and are not limited to:\nInterpreting data, analyzing results using statistical techniques and providing ongoing reports. Acquiring data from primary or secondary data sources and maintaining databases/data systems.Identifying, analyzing, and interpreting trends or patterns in complex data sets.Filtering and “cleaning” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems.Working with management to prioritize business and information needs.Locating and defining new process improvement opportunities.Using Tableau and visualization packages to create and support KPI and operations dashboards Ad-hoc report writing. Using advanced Excel function or coding languages to support data analytics.\nThis internship comes with the possibility of continuing into the fall semester and beyond."}
{"text": "Requirements:\nExperience on data projects and advanced reporting tools (SQL, Azure, PowerBI, is required. Extensive knowledge of Azure & Azure BI. Expertise in Power BI, DAX & Power Automate solutions. Expertise in data ETL process and performance optimization of data warehouseExperience with Microsoft Dynamics 365 FO or Dynamics AX a big plus.Must be a US Citizen or GC Holder"}
{"text": "Qualifications Education/Certifications: Master’s degree in data science, statistics, mathematics, econometrics, engineering, or other quantitative field of study\nExperience: 5+ years of relevant work experience\nKnowledge, Skills, and Abilities: Expert knowledge of statistics and machine learning methods, including regression, classification, time series analysis, clustering, simulation, dimension reductionDemonstrated aptitude distilling complex business problems into clear data science and advanced analytics models and solutions that can and will be adopted and implemented Ability to effectively lead projects from conception to conclusion, both independently and in a team environment (including working with both internal and external business partners)Expertise developing and applying machine learning algorithms Strong verbal and written communication and presentation skills, with the ability to translate complex concepts into actionable business insights Proficient in Python, R, SQLExperience working with data science tools (e.g., Databricks); distributed compute; manipulating, analyzing, and interpreting large-scale data; and scaling analytics in the cloud (e.g., Azure)Proven track record of leading projects and cross-functional teamsStrong problem-solving and analytical thinking skillsExperience working with Agile methodologiesExperience using data visualization tools (e.g., Power BI)Experience with IOT, software-defined customer experiences, private equity backed companies"}
{"text": "skills and professional experience would be a good fit for a position with one of our premier clients. Please review the job description below. If you are interested in this position, please forward your updated resume for immediate consideration and preferred time to discuss this opportunity further.\nTitle- Data Analyst/Senior Data Analyst (W2 Only)Location- Remote\nRequired Skills – \nAWS, Python, Scala & Health domain (claims / payers)Excellent Communication Skills"}
{"text": "requirements of the machine learning platform.\n\nResponsibilities\n\nImplementing and operating machine learning algorithmsRunning AI systems experiments and testsDesigning and developing machine learning systemsPerforming statistical analyses\n\nData Collection And Preprocessing\n\nGather, clean, and preprocess large datasets to make them suitable for machine learning tasks.Collaborate with data engineers and data scientists to ensure data quality and availability.Model Development:Design, build, and train machine learning models using state-of-the-art techniques and frameworks.Experiment with different algorithms and architectures to achieve optimal results.\n\nFeature Engineering\n\nCreate and select relevant features from data to improve model performance.Utilize domain knowledge to engineer features that capture important patterns.Model Deployment:Deploy machine learning models into production environments.Work closely with DevOps teams to ensure scalability, reliability, and security.\n\nMonitoring And Optimization\n\nImplement monitoring solutions to track model performance in real-time.Continuously fine-tune and optimize models to maintain or improve accuracy.Collaboration and Communication:Collaborate with cross-functional teams, including data scientists, software engineers, and domain experts.Clearly communicate technical findings and insights to both technical and non-technical stakeholders.\n\nResearch And Innovation\n\nStay up-to-date with the latest developments in machine learning and artificial intelligence.Experiment with emerging technologies and techniques to drive innovation within the organization.\n\nQualifications\n\nThe ideal candidate will have a background in Python, have experience working with large data sets, annotating and formatting data for ML, and have experience in building Machine Learning Platforms, applying Machine Learning, and deploying data-driven solutionsMaster's or PhD degree in Computer Science, Machine Learning, Data Science, or a related field (Ph.D. preferred).Proven experience (7+ years) as a Machine Learning Engineer or a similar role.Proficiency in machine learning frameworks (e.g., TensorFlow, PyTorch) and programming languages like Python, Java, Golang, and Scala. Strong knowledge of machine learning algorithms, deep learning, and statistical modeling.Experience with data pre-processing, feature engineering, and model deployment.Demonstrated strength in data modeling, ETL development, and data warehousingExperience using big data technologies (PostgresDB, Airflow, Kubernetes, Docker, Spark, Data Lakes, TensorFlow)Experience delivering end-to-end projects independently.Experience using business intelligence reporting tools (SuperSet, Power BI, Tableau, etc.).Knowledge of data management fundamentals and data storage principles.Experience with data pipelines and stream-processing systems Knowledge of distributed systems as it pertains to data storage and computing.Proven success in communicating with end-users, technical teams, and senior management to collect requirements, and describe data modeling decisions and data engineering strategy.Knowledge of software engineering best practices across the development life-cycle, including agile methodologies, coding standards, code reviews, version control, build processes, testing, and observability.\n\nSalary: $200k - $250/yr plus bonus and equity."}
{"text": "Qualifications\n\nPossess knowledge and skills related to data processes and database design.Knowledge and skills related to coordinating work in a team-based setting. Good communication skills.Ability to learn skills related to Microsoft SQL, SAS, Tableau, and Microsoft Reporting Services as well as other tools used for data reporting. \n\nPreferred Qualifications\n\nBachelor's degree in social service or data analysis related field.Four (4) or more years of progressive research/data analysis experience pertaining to mental health/substance use disorder programs/services may substitute.\n\nLack of post‐secondary education will not be used as the sole basis denying consideration to any applicant.\n\nThe State of Missouri offers an excellent benefits package that includes a defined pension plan, generous amounts of leave and holiday time, and eligibility for health insurance coverage. Your total compensation is more than the dollars you receive in your paycheck. To help demonstrate the value of working for the State of Missouri, we have created an interactive Total Compensation Calculator. This tool provides a comprehensive view of benefits and more that are offered to prospective employees. The Total Compensation Calculator and other applicant resources can be found here .\n\nIf you have questions about this position, please contact: hrmail@dmh.mo.gov"}
{"text": "experience in SQL required. Experience in the health care preferred. Experience in python/R preferred."}
{"text": "requirements. Employee will also be required to review, analyze and report on data from multiple internal and external sources. The employee must be able to communicate the results of their findings and make recommendations to management in the form of highly-digestible, easy-to-understand, expert-quality reports and visualizations.\nEssential Job Results\ncomplex datasets to derive meaningful insights and trends.data and discovers patterns, meaningful relationships, anomalies and trendsand maintain data models, databases, and dashboards for reporting purposes.patterns, trends, and correlations in data to aid decision-making processes.with cross-functional teams to gather requirements and define key performance indicators (KPIs).transform, and organize data for analysis using tools like SQL, Python, or R.statistical analysis and hypothesis testing to validate findings.data analysis findings as high-quality custom reports, dashboards and visualizationsvisualizations and presentations to effectively communicate insights to stakeholders.in the development and implementation of data-driven strategies and initiatives.support for ad-hoc data requests and troubleshooting data-related issues.data accuracy, integrity, and security across all data-related activitiesAlteryx Designer to automate data preparation, blending, and analysis tasks.and maintain Alteryx workflows to streamline data processes and improve efficiency.transform, and load (ETL) data from various sources including databases, spreadsheets, and APIs using Alteryx tools.advanced analytics, predictive modeling using Alteryx predictive tools.with stakeholders to understand business requirements and translate them into Alteryx workflows.and implement data validation processes to ensure accuracy and consistency of output.interactive dashboards and reports using Alteryx outputs integrated with visualization tools like Tableau or Power BI.training and support to team members on Alteryx best practices and techniques.issues and optimize Alteryx workflows for performance and scalability.updated on the latest features and capabilities of Alteryx platform to leverage its full potential for data analysis and automation.cleansingupdatesduties as assigned\nSupervisory Responsibilities\nNo\nJob Requirements\ndegree in Computer Science, Information Systems or Data Science related field. Equivalent experience is acceptable.years of work experience in data analysissupporting full Agile and Waterfall software development lifecycles (including understanding business processes, gathering user requirements, design, testing, deployment and training)analytical capabilitywritten, verbal and interpersonal skillshave ability to communicate, motivate and organize projects throughout the companySQL knowledge and experienceAlteryx experience requiredexperience in Jira, Confluence, Excel, Tableau and VBA preferred"}
{"text": "Skills Required:\nAzure , Python, AIML, Kubernetes, Devops\n\n\nLooking for a positive response and fruitful alliance :)Dushyant ChaudharySenior Executive Talent AcquisitionCell No: +1 (201) 448-1094Email ID: dushyant.chaudhary@okayainc.com"}
{"text": "Qualifications:Bachelor's degree in HR, Business Administration, or related field preferred2+ years of HR Operations experienceProficiency in Workday or similar HR cloud based platformsStrong analytical, communication, customer service and prioritization skills"}
{"text": "Skills MS Excel, MS PowerPoint, Data Analysis  Tech Skills: 1. Intermediate Level MS Excel (Pivot & Macros knowledge helpful) 2. Intermediate Level MS PowerPoint (Presentation Slides & Charts) 3. Familiarity with Data Storage platforms, directories and network drives.  Soft Skills: 1. Punctuality is required due to the reporting deadlines & on time delivery of data. 2. Organized 3. Team player 4. Curious & Quick Learner  Summary: The main function of the Data Analyst is to provide business intelligence support and supporting areas by means of both repeatable and ad hoc reporting delivery reports (charts, graphs, tables, etc) that enable informed business decisions.  Job Qualifications: • Associates or related Certifications • VBA Concepts • SQL Basic • Data Visualization Concepts  Education/Experience: Associate Degree in a technical field such as computer science, computer engineering or related field required. 2 -3 years of experience required. Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI."}
{"text": "Requirements:\n- Good communication; and problem-solving abilities- Ability to work as an individual contributor; collaborating with Global team- Strong experience with Data Warehousing- OLTP, OLAP, Dimension, Facts, Data Modeling- Expertise implementing Python design patterns (Creational, Structural and Behavioral Patterns)- Expertise in Python building data application including reading, transforming; writing data sets- Strong experience in using boto3, pandas, numpy, pyarrow, Requests, Fast API, Asyncio, Aiohttp, PyTest, OAuth 2.0, multithreading, multiprocessing, snowflake python connector; Snowpark- Experience in Python building data APIs (Web/REST APIs)- Experience with Snowflake including SQL, Pipes, Stream, Tasks, Time Travel, Data Sharing, Query Optimization- Experience with Scripting language in Snowflake including SQL Stored Procs, Java Script Stored Procedures; Python UDFs- Understanding of Snowflake Internals; experience in integration with Reporting; UI applications- Strong experience with AWS tools such as S3, Athena, Glue, Lambda, SNS, SQS, etc.- Experience with application and libraries packaging and distribution like Wheel packages, Zipapp and Pyinstaller and Docker Containerization- Experience working in financial services preferably buy side firms\nGood to have:\n- Familiarity with building reports using reporting tools such as Tableau- High level understanding of ETL tools such as Informatica- Familiarity with batch schedulers such as Active Batch- Experience with Real time data streaming using message queues- Python Libraries Kivy, Dash, PyTorch and Poetry Tool- Experience in Python building UI interface with libraries such as Matplotlib, plotly, streamlit- Devops experience specially utilizing Azure Devops for deploying Python applications- Experience with scripting such as Power Shell, Unix Shell"}
{"text": "experience with Databricks, PySpark, SQL, Spark clusters, and Jupyter Notebooks.- Expertise in building data lakes using the Medallion architecture and working with delta tables in the delta file format.- Familiarity with CI/CD pipelines and Agile methodologies, ensuring efficient and collaborative development practices.- Strong understanding of ETL processes, data modeling, and data warehousing principles.- Experience with data visualization tools like Power BI is a plus.- Knowledge of cybersecurity data, particularly vulnerability scan data, is preferred.- Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nrequirements and deliver effective solutions aligned with Medallion architecture principles.- Ensure data quality and implement robust data governance standards, leveraging the scalability and efficiency offered by the Medallion architecture.- Design and implement ETL processes, including data cleansing, transformation, and integration, optimizing performance within the delta file format framework.- Build and manage data lakes based on Medallion architecture principles, ensuring scalability, reliability, and adherence to best practices.- Monitor and optimize data pipelines, integrating CI/CD practices to streamline development and deployment processes.- Collaborate with cross-functional team members to implement data analytics projects, utilizing Jupyter Notebooks and other tools to harness the power of the Medallion architecture.- Embrace Agile methodologies throughout the development lifecycle to promote iterative and collaborative development practices, enhancing the effectiveness of Medallion-based solutions."}
{"text": "Skills:-SQL, SharePoint, Financial Services, Experience and proficiency building data pipelines and performing analytics using KNIME (or similar software), 10+ Years of experience \n\nLoans Transformation Data Analyst\nFunction background / context:The Institutional Credit Management (ICM) team is a critical component of Client's First Line of Defense for wholesale lending and counterparty credit risk. ICM partners with businesses Client-wide to ensure we have best-in-class risk and control capabilities. ICM also plays a key role in Client's Transformation efforts by helping to drive a Client-wide focus on wholesale credit risk management. Through ongoing investment in processes, controls, systems, and governance, ICM continues to further embed consistency and best practices across Client, driving closer alignment between our business and regulatory goals.ICM Lending Transformation is looking for an experienced Data Analyst, who is proactive, independent, and comfortable with identifying and resolving blockers. Role includes creating and maintaining centralized SharePoint site and associated content for the overall Data Remediation Transformation Program. Develop and maintain automated workflow tools to facilitate regulatory remediation efforts. Support BAU and analytics processes.You will interact and work closely with multiple areas across the organization, including the broader Institutional Credit Management (ICM) function and the business lines supported by ICM, as we enhance our processes and technology to better deliver for our clients. You will provide data management support to the Transformation team's initiatives.Qualifications:10+ years of experience in finance/ project managementExperience and proficiency building data pipelines and performing analytics using KNIME (or similar software)Experience creating team SharePoint sites and maintaining content to make information and documents easily accessibleProficiency with Visual Basic for Applications (VBA) for Microsoft OfficeProficiency with SQL and relational database management systemsStrong proficiency with Microsoft ExcelSignificant experience building end-user tools with Microsoft AccessEducation:"}
{"text": "requirements, and general interpretation of dataMentor, teach, share knowledge and analytic techniques with your colleagues\n\nExperience And Preferred Qualifications\n\nMinimum of three years of relevant experience in developing analytic solutions with proficiency in SQL, Microsoft Excel, Power BI, or similar data analysis and ETL toolsBachelor's degree (B.S./B.A.) in an appropriate field from an accredited college or universityStrong verbal and written communication skills with the ability to convey highly complex technical concepts down to actionable objectives to advise stakeholders including attorneys, firm management, and firm colleaguesExperience in project management including planning, organizing, and supervising clients and colleagues towards successful project completionDemonstrated ability to learn and succeed in a fast-paced environmentExpert level of proficiency with T-SQL or equivalent including a high level of proficiency in database administrationHigh proficiency with Microsoft Excel including an ability to create pivot tables, power pivots & queries, formulas, and external data connectionsAbility to design and implement ETL solutionsExperience in developing client facing visualizations and reports using Power BI, SSRS or similar visualization tools is a plusKnowledge of coding in Python, R, DAX and/or MExperience in developing SSIS and/or SSAS solutions\n\nQualified candidates must apply online by visiting our website at www.morganlewis.com and selecting “Careers.”\n\nMorgan, Lewis & Bockius LLP is committed to \n\nPursuant to applicable state and municipal Fair Chance Laws and Ordinances, we will consider for employment qualified applicants with arrest and conviction records.\n\nCalifornia Applicants: Pursuant to the California Consumer Privacy Act, the following link contains the Firm's California Consumer Privacy Act Privacy Notice for Candidates which explains the categories of personal information that we collect and the purposes for which we use such personal information. CCPA Privacy Notice for Candidates\n\nMorgan Lewis & Bockius LLP is also \n\nIf You Are Interested In Applying For Employment With Morgan Lewis And Need Special Assistance Or An Accommodation To Use Our Website Or To Apply For a Position, Please Call Or Email The Following Contacts\n\nProfessional Staff positions – 1.888.534.5003 / talent.acquisition@morganlewis.com \n\nMorgan, Lewis & Bockius, LLP reasonably accommodates applicants and employees who need them to perform the essential functions of the job because of disability, religious belief, or other reason protected by applicable law. If you believe you need a reasonable accommodation during the application process, please contact Talent Acquisition at talent.acquisition@morganlewis.com."}
{"text": "Requirements\n\nBachelor’s Degree in Computer Science, Computer Information Systems, Information Technology and Management, Engineering or related field and 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture.\n\nOR\n\nMaster’s degree in Computer Science, Computer Information Systems, Information Technology and Management, Engineering or related field and 1 year of experience in data engineering or architecture.\n\nMust have 1 year of experience in each of the following:\n\n SQL and data analysis Python, Scala or Java Developing and maintaining data warehouses in cloud or other large scale data platforms Big Data development experience using Hadoop with any of the following: Hive, BigQuery, SQL, Impala OR Spark Designing, creating, coding, and supporting an ETL solution, including at least one of the following: Talend Studio, Kafka, Jira, SAP Data Services, SAP or HANA. Tableau, Power BI, Looker or Shiny Data and analytics including at least one of the following: dimensional modeling, ETL, reporting tools, data governance, data warehousing or structured and unstructured data\n\nBackground check and drug testing required.\n\nContact: Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #26017 | 20330.291.8.\n\nThe salary range for this position $129,147-174,600. At General Mills we strive for each employee’s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance.\n\nCompany Overview\n\nWe exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next."}
{"text": "requirements. Lead client meetings, both online and onsite, to discuss new and current functionality.\n\nIf you are a seasoned data scientist with a passion for leadership, ready to guide a team in implementing impactful initiatives, we invite you to lead innovation at Blue Yonder.\n\nQualifications\n\n Bachelor’s Degree in Computer Science or any other related field is required Min 10 years of experience with strong foundation in data science and deep learning principles. Proficient in Python programming with a solid understanding of data structures. Experience with frameworks and libraries like Pandas, NumPy, Keras, TensorFlow, Jupyter, Matplotlib, etc. Expertise in a database query language, preferably SQL. Familiarity with Big Data technologies like Snowflake, Apache Beam/Spark/Flink, and Databricks. Solid experience with major cloud platforms, preferably Azure and/or GCP. Knowledge of modern software development tools and best practices, including Git, Github Actions, Jenkins, Docker, Jira, etc. Familiarity with deep learning, time series, NLP, reinforcement learning, and combinatorial optimization. Proven experience in team leadership, mentoring junior data scientists in an official or unofficial capacity. Desired knowledge of Kafka, Redis, Cassandra, etc.\n\nThe salary range for this positions is $150,385 - $189,615\n\nThe salary range information provided, reflects the anticipated base salary range for this position based on current national data. Minimums and maximums may vary based on location. Individual salary will be commensurate with skills, experience, certifications or licenses and other relevant factors. In addition, this role will be eligible to participate in either the annual performance bonus or commission program, determined by the nature of the position.\n\nBenefits\n\nAt Blue Yonder, we care about the wellbeing of our employees and those most important to them. This is reflected in our robust benefits package and options that includes:\n\nComprehensive Medical, Dental and Vision 401K with Matching Flexible Time Off Corporate Fitness Program Wellbeing Days A variety of voluntary benefits such as; Legal Plans, Accident and Hospital Indemnity, Pet Insurance and much more\n\nAt Blue Yonder, we are committed to a workplace that genuinely fosters inclusion and belonging in which everyone can share their unique voices and talents in a safe space. We continue to be guided by our core values and are proud of our diverse culture as \n\nOur Values\n\nIf you want to know the heart of a company, take a look at their values. Ours unite us. They are what drive our success – and the success of our customers. Does your heart beat like ours? Find out here: Core Values\n\nDiversity, Inclusion, Value & Equality (DIVE) is our strategy for fostering an inclusive environment we can be proud of. Check out Blue Yonder's inaugural Diversity Report which outlines our commitment to change, and our video celebrating the differences in all of us in the words of some of our associates from around the world.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status."}
{"text": "qualifications, skills, competencies, competencies, experience, location and end client requirements).\n\nBenefits and Ancillaries:\n\nMedical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee."}
{"text": "Skills - Nice to Havessnowflakebig dataJob Description- Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies - Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems - Utilize programming languages like Python, Spark, PySpark and Open Source RDBMS and Cloud based data warehousing services such as SnowflakeAdditional Skills & QualificationsThe Card Data and Analytics Team at Capital One is building data features for their depersonalization platform to onboard new external data providers. They want to be able to depersonalize data from their data partners that they can then consume."}
{"text": "skills and analytical mindset to present your findings and make recommendations to upper management. More than that, this role is about constant improvement and doing so with our signature all-win approach in mind.\n\nWhat You’ll Do\n\nInterpret data using sound statistical methods while considering how the data can tell a story for an audience with a different area of expertiseExtract relevant data from enterprise data storage systems by using Structured Query Language (SQL) and other available tools and techniquesProvide expertise to create reporting and analysis that improves and automates the financial data collection processWork closely with end users to determine business rules and requirements that must be followed during report creation and validate that extracted information is accurateProvide guidance to less experienced Data Analysts Always act using Integrity, Caring, and Excellence to achieve all-win outcomes\n\nWhat You’ll Need\n\nBachelor's degree in Business, Data Analytics, Statistics or MIS disciplineAdvanced knowledge and skill in SQL tools and techniquesAdvanced experience with report writing systems and the ability to create programs from scratchStrong analytical thinking and problem-solving skillsExcellent written and verbal communication skillsStrong knowledge and understanding of financial and accounting conceptsExperience working in cross-functional teamsProficiency in Microsoft computer applications\n\nAdditional Preferred Skills\n\n2+ years of data analyst experienceExperience in the banking industryExperience with workflow process management or process improvementCompetency with advanced analytics or data science\n\nFrost Benefits\n\nAt Frost, we care about your health, your family, and your future and strive to have our benefits reflect that. This includes:\n\nMedical, dental, vision, long-term, and life insurance401(k) matchingGenerous holiday and paid time off scheduleTuition reimbursementExtensive health and wellness programs, including our Employee Assistance ProgramReferral bonus program + more!\n\nSince 1868, Frost has dedicated their expertise to provide exceptional banking, investment, and insurance services to businesses and individuals throughout Texas. Frost is one of the 50 largest U.S. banks by asset size and is a leader is banking customer satisfaction. At Frost, it’s about being part of something bigger. If this sounds like you, we encourage you to apply and see what’s possible at Frost."}
{"text": "skills, critical thinking and the ability to dig in and work your way backwards on your own. Successful candidates will grasp our infrastructure with ease and also understand data and business rules. If this is you, we look forward to hearing from you. \n\n Location: Lombard, IL \n\n Remote Options: Position is completely remote but may eventually be on Hybrid arrangement. \n\nWhat You’ll Be Doing\n\n Analyze complex data elements and systems, data flows, dependencies, and relationships to troubleshoot data issues across the business and presents solutions to development team. Perform ad-hoc analyses of data stored in Air view and write SQL and/or Python scripts, stored procedures, functions. Design and build scalable pipelines to process terabytes of data. Focus on the design, implementation, and operation of data management systems to meet business needs. This includes designing how the data will be stored, consumed, and integrated into our systems. Developing metrics using data infrastructure to monitor performance of systems. Creation and management of databases to support large scale aggregation processes. Contribute to the vision for data infrastructure, data science, and analytics.\n\n\nWhat We’re Looking For\n\n Bachelor’s Degree or higher 2-4 years of working experience as a database engineering support personnel. Strong knowledge of Python. Experience with MySQL server and administration. Strong SQL skills. Comfortable navigating in a Linux environment, with bash shell scripting a bonus Experience building and deploying on AWS, especially with RDS, EC2, S3, EMR and Redshift. Experience building custom ETL, data warehousing, and pipeline infrastructure. Expertise transforming and standardizing and aggregating large datasets. And validating your work. Comfort with the DevOps side of engineering. Experience with Web Development Frameworks such as Django is a big plus. Interest in machine learning and statistics.\n\n\n This Might Be the Right Place if You…. \n\n Are a team fit; can help advance our global, inclusive culture Are you a self-starter who likes to roll up your sleeves and take initiative with minimal direction Can think about tomorrow, while getting your work done today Are a collaborative team player; the primary audience will be internal teams Are curious and open to learning and finding new solutions Are able to provide and accept feedback in a constructive manner Are organized, have strong communication skills, and do whatever it takes to get things done\n\n\nThe Benefits Of Working Here\n\n Generous 401(k) Matching Company Paid Short-Term & Long-Term Disability Company Paid Life/AD&D Insurance Company Paid Wellness Programs Company Health Savings Account Contributions Employee Assistance Program Flexible Spending Accounts for Dependent Care, Medical & Transit Paid Parental Leave and more!\n\n\n Advanced | Agile | Applied \n\n Anuvu is an equal-opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. \n\n Let Innovation Move You."}
{"text": "experience; familiarity with national security use cases; and success working in an agile development environment. Additionally, the ideal candidate has demonstrated the ability to manage their own efforts over a broad scope of work as an independent contributor. Finally, the candidate should be an independent thinker with the demonstrated ability and willingness to lean in and solve new problems. This position reports to Analytics Manager in the AI/ML team.\nWhile the locations listed in the job posting are ideal, we would love candidates near either our Herndon, VA or Seattle, WA offices. We may also consider remote candidates in certain states.\nQualifications: A minimum of three years of hands-on experience as a machine learning engineer or data scientist.Bachelor’s Degree or higher in one of the following fields: computer science, mathematics, physics, statistics, or another computational field with a strong background of using machine learning/data mining for predictive modeling or time series analysis.Extensive experience developing machine learning based software solutions. In particular, developing models in Python 3, PyTorch, Tensorflow, Keras, or scikit-learn.Working knowledge of a wide range of machine learning concepts including supervised and unsupervised deep learning methods for both classification and regression.Experience performing research in both groups and as a solo effort with a history of implementing algorithms directly from research papers.Experience conducting literature review and applying concepts to programs or products.Strong ability to communicate concepts and analytical results with customers, management, and the technical team, highlighting actionable insights.Hands-on experience working with large data sets including data cleansing/transformation, statistical analyses, and visualization (using Python libraries such as Pandas, NumPy, etc.). Must be a US Citizen.\nPreferred Qualifications: A minimum of five years of hands-on experience as a machine learning engineer or data scientist.PhD./Master's degree in the previously mentioned fields.Experience working with remote sensing data, ideally satellite imagery.Experience with cloud-based MLOps tools such as ClearML, Weights & Biases, or MLFlowExperience with tracking and motion detection algorithms.Experience with maritime data for analysis and modeling.Experience working with geospatial data and geospatial Python libraries (GDAL, shapely, rasterio, etc).Experience developing asynchronous processing algorithms and Cloud-based solutions (especially AWS services like EC2 & S3). \nLife at BlackSky for full-time benefits eligible employees includes:Medical, dental, vision, disability, group term life and AD&D, voluntary life and AD&D insuranceBlackSky pays 100% of employee-only premiums for medical, dental and vision and contributes $100/month for out-of-pocket expenses!15 days of PTO, 11 Company holidays, four Floating Holidays (pro-rated based on hire date), one day of paid volunteerism leave per year, parental leave and more401(k) pre-tax and Roth deferral options with employer matchFlexible Spending AccountsEmployee Assistance and Travel Assistance ProgramsEmployer matching donationsProfessional developmentMac or PC? Your choice!Awesome swag\nThe anticipated salary range for candidates in Seattle, WA is $120,000-140,000 per year. The final compensation package offered to a successful candidate will be dependent on specific background and education. BlackSky is a multi-state employer, and this pay scale may not reflect salary ranges in other states or locations outside of Seattle, WA.\nBlackSky is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer All Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity, disability, protected veteran status or any other characteristic protected by law.\nTo conform to U.S. Government space technology export regulations, including the International Traffic in Arms Regulations (ITAR) you must be a U.S. citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324b(a)(3), or eligible to obtain the required authorizations from the U.S. Department of State."}
{"text": "Skills Required:Health Care Domain (must)AWS Python, Scala, AWS servicesNoSQL storage databases such Cassandra and MongoDBApache Beam and Apache SparkAmazon Redshift, Google BigQuery, and Snowflake\nSecondary:Java, Go languageMicroservices frameworks such as Kubernetes and Terraform."}
{"text": "Skills/Attributes: Banking/Financial, DATA MANAGEMENT, Data Quality, Database, DataStage, ETL, Informatica, Python, Shell Scripting, UNIX Job Description\n**Only those lawfully authorized to work in the designated country associated with the position will be considered.**\n**Please note that all Position start dates and duration are estimates and may be reduced or lengthened based upon a client’s business needs and requirements.**\nThe Application Support Data Engineer will help provide Application support, monitor dashboards, data quality exception reports, investigate and resolve data issues, coordinate resolution of data problems in upstream systems and executing solutions such as data quality reporting as well as automation of data quality monitoring and resolution.\nRequired Skills:• 4 to 8+ years of relevant work experience around automation, data management, data quality, financial or regulatory reporting• Experience with relational and non-relational data stores• Experience in ETL/ELT using tools like Abinitio, Informatica, and DataStage• Experience writing Python• Understanding database performance concepts like indices, segmentation, projections, and partitions• Require shell scripting in a Unix environment\nWhat You’ll Do:• Support data warehouse batch and drive continuous optimization and improvement• Identify and implement process improvements: infrastructure that scales, automating manual processes• Take ownership of the various tasks that will allow us to maintain high-quality data; ingestion, validation, transformation, enrichment, mapping, storage• Improve observability across the data infrastructure to ensure data quality from raw sources to downstream systems• Collaborate with the teams to deploy and support reliable, scalable tooling for analysis and experimentation• Collaborate with the dev teams to anticipate and support changes to the data \nBenefits:For information and details on employment benefits offered with this position, please visit here. Should you have any questions/concerns, please contact our HR Department via our secure website.California Pay Equity:For information and details on pay equity laws in California, please visit the State of California Department of Industrial Relations' website here.Rose International is"}
{"text": "experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters.\n\nLocation: \n\nAt LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. \n\nThis role is based in Sunnyvale, CA. \n\n\nTeam Information:\n\n\nThe mission of the Anti-Abuse AI team is to build trust in every interaction on LinkedIn. The team uses a wide suite of state-of-the-art Machine Learning techniques to identify patterns in large-scale attacks and take down attackers proactively. This engineer will help lead the effort to build detection and prevention algorithms, models, and systems that can stay ahead of attackers in an adversarial space.\n\n\n \nQualifications:\n\n\nBachelor's degree in Computer Science or related technical field or equivalent technical experience\n1+ year(s) experience with machine learning, data mining, and information retrieval or natural language processing \n1+ year(s) of industry experience in software design, development, and algorithm related solutions.\n1+ year(s) experience in programming languages such as Java, Python, etc. \n\nPreferred Qualifications: \n\n\n2+ years of relevant industry experience \nMS or PhD in Computer Science or a related technical discipline\nProficiency with Spark and/or Tensorflow \nExperience in the Trust & Safety domain, specifically account security (account take-overs) and/or fake accounts.\n\n\nSuggested Skills:\n\n\nExperience in Machine Learning and Deep Learning\nExperience in Big Data \nStrong technical background & Strategic thinking\n\n\nYou will Benefit from our Culture:\n\n\nWe strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels\n\nLinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $107,000-$176,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits.\n\n\nEqual Opportunity Statement\n LinkedIn is committed to diversity in its workforce and is proud to be \n\n LinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful.\n\n If you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation.\n\n Reasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to:\n\n -Documents in alternate formats or read aloud to you \n -Having interviews in an accessible location\n -Being accompanied by a service dog\n -Having a sign language interpreter present for the interview\n\n A request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response.\n\n LinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information.\n\n Pay Transparency Policy Statement\n As a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency.\n\n Global Data Privacy Notice for Job Candidates\n This document provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://lnkd.in/GlobalDataPrivacyNotice"}
{"text": "RequirementsMaster's or Ph.D. in Computer Science, Artificial Intelligence, Machine Learning, or a related technical field.Demonstrable experience in computer vision and machine learning with a strong portfolio of relevant work.Proficient in programming with Python, C++, or similar languages and familiar with ML development frameworks.Capable of thriving in a dynamic, fast-paced, and collaborative environment.Excellent analytical, problem-solving, and communication skills.Ability to obtain and hold a Department of Defense Security ClearanceThis role will require up to 25% domestic travel.\nPreferredExperience with Publisher Subscriber architectures.Experience with ROS and similar node-based packages.Experience in handling large amounts of data at high frequency and performing real-time analytics.Experience in data compression, especially image/video (J2K, H.264, H.265, FFMPEG, etc.)DoD Security Clearance is always nice to have.\nWhat They OfferCompetitive compensation and a comprehensive benefits package.An opportunity to work on groundbreaking and impactful technology initiatives.A team-oriented culture with a focus on professional growth and development.Flexible work options including remote work and unlimited PTO.Fun all-expense-paid company retreats at various destinations across the country\nIf this is of interest, please apply with your current resume or reach out for more details."}
{"text": "Hi All,\nThis is Nithya from TOPSYSIT, We have a job requirement for Data Scientist with GenAI. If anyone interested please send me your updated resume along with contact details to nithya@topsysit.com\nAny Visa is Fine on W2 except H1B ,OPT and CPT.If GC holders who can share PPN along with proper documentation are eligible\nJob Title Data Scientist with GenAILocation: Plano, TX-OnsiteEXP: 10 Years Description:Competencies: SQL, Natural Language Processing (NLP), Python, PySpark/ApacheSpark, Databricks.Python libraries: Numpy, Pandas, SK-Learn, Matplotlib, Tensorflow, PyTorch.Deep Learning: ANN, RNN, LSTM, CNN, Computer vision.NLP: NLTK, Word Embedding, BOW, TF-IDF, World2Vec, BERT.Framework: Flask or similar.\nThanks & Regards,Nithya Kandee:nithya@topsysit.comph:678-899-6898"}
{"text": "Qualifications\n• Excellent decision making abilities and effective problem solving skills. Ability to analyze data and make decisions based on the information gathered.• Analytical experience (e.g. data and process analysis, quality metrics, policies, standards, and processes) preferred.• Strong time management skills; organized with strong focus and excellent attention to detail.• Strong verbal and written communication skills.• Experience with Customer data analysis a plus"}
{"text": "Experienced in implementing standardized pipelines with automated testing, Airflow scheduling, Azure DevOps for CI/CD, Terraform for infrastructure as code, and Splunk for monitoring Continuously improve systems through performance enhancements and cost reductions in compute and storage Data Processing and API Integration: Utilize Spark Structured Streaming for real-time data processing and integrate data outputs with REST APIs Lead Data Engineering Projects to manage and implement data-driven communication systems Experienced with Scrum and Agile Methodologies to coordinate global delivery teams, run scrum ceremonies, manage backlog items, and handle escalations Integrate data across different systems and platforms Strong verbal and written communication skills to manage client discussions\nCandidate Profile: 8+ years experience in developing and implementing ETL pipelines from various data sources using Databricks on cloud Some experience in insurance domain/ data is must Programming Languages – SQL, Python Technologies - IaaS (AWS or Azure or GCP), Databricks platform, Delta Lake storage, Spark (PySpark, Spark SQL).o Good to have - Airflow, Splunk, Kubernetes, Power BI, Git, Azure Devops Project Management using Agile, Scrum B.S. Degree in a data-centric field (Mathematics, Economics, Computer Science, Engineering or other science field), Information Systems, Information Processing or engineering. Excellent communication & leadership skills, with the ability to lead and motivate team members"}
{"text": "Qualifications: Good communication verbal/written, Good organization, Good analysis, Customer service, cross team facilitation.Experience with “Lean Management” and/or “Six Sigma” concepts.Be able to analyze processes/workflows and find opportunities to streamline/improve/eliminate waste.Be able to create value stream maps.Experience with Microsoft Visio.Office products (MS Word/MS Excel/Teams) MS AccessBachelors degree Statistics, Computer Science, Mathematics, Business, Healthcare, or other related field. or 2 year degree in Computer Science, Business or related field and 2 years of reporting and data analysis work experience OR 4 years reporting and data analysis experience."}
{"text": "experience:\n\nGS-15:\n\nApplied Data Science\n\nDevelops, administers, controls, coordinates, and executes assigned data science requirements, which requires technical expertise across the data life cycle (e.g., data collection, ingestion, storage, modeling, access, integration, analysis, and decision support). Uses analytic and statistical software to programmatically prepare data for analysis and clean imperfect data including structured, semi-structured, and unstructured sources such as vulnerability scans, configuration scans, the results from manual and automated control testing, and system security plans and other cybersecurity data and documentation. \n\n\nCustomer Communications and Reporting \n\nTranslates complex business logic, analytic findings, and data limitations into concise, plain language reports or other materials such as visualizations and dashboards.Designs presentations and interpretations of analytical outputs tailored to specific audiences including the use of interactivity and narrative storytelling with data where appropriate. Collaborates with teammates, internal and external data consumers, and stakeholders in a reproducible and organized manner.\n\n\nCloud Data Security Support   \n\nSupports emerging IT and IT cybersecurity initiatives including but not limited to cloud computing, DevSecOps (i.e., development, security, and operations), continuous integration and continuous delivery, vulnerability management, and safe integration of emerging technology, ensuring related data needs are appropriately accounted for in the program's strategy.Maintains current knowledge and skill in cloud security, web application security, network architecture, and application development to conduct data science functions within the context of program operations.\n\n\nRequirements\n\n Conditions of Employment\n\nUS Citizenship or National (Residents of American Samoa and Swains Island)Meet all eligibility requirements within 30 days of the closing date.Register with Selective Service if you are a male born after 12/31/1959\n\n\nIf selected, you must meet the following conditions:\n\nCurrent or Former Political Appointees: The Office of Personnel Management (OPM) must authorize employment offers made to current or former political appointees. If you are currently, or have been within the last 5 years, a political Schedule A, Schedule C or NonCareer SES employee in the Executive Branch, you must disclose this information to the HR Office. Failure to disclose this information could result in disciplinary action including removal from Federal Service.Undergo and pass a background investigation (Tier 4 investigation level).Have your identity and work status eligibility verified if you are not a GSA employee. We will use the Department of Homeland Security’s e-Verify system for this. Any discrepancies must be resolved as a condition of continued employment.\n\n\nQualifications\n\nFor each job on your resume, provide:\n\nthe exact dates you held each job (from month/year to month/year)number of hours per week you worked (if part time). \n\n\nIf you have volunteered your service through a National Service program (e.g., Peace Corps, Americorps), we encourage you to apply and include this experience on your resume.\n\nFor a brief video on creating a Federal resume, click here .\n\nThe GS-15 salary range starts at $143,736 per year.\n\nIf you are a new federal employee, your starting salary will likely be set at the Step 1 of the grade for which you are selected.\n\nAll candidates for Data Scientist positions must meet one of the following basic qualification requirements:\n\nBasic Requirement:\n\n Degree: Mathematics, statistics, computer science, data science or field directly related to the position. The degree must be in a major field of study (at least at the baccalaureate level) that is appropriate for the position.\n\n\nOR\n\n Combination of education and experience: Courses equivalent to a major field of study (30 semester hours) as shown above, plus additional education or appropriate experience.\n\n\nSpecialized Experience: In addition to meeting the basic requirements above, applicants must demonstrate that they have at least one year of specialized experience equivalent to the GS-14 level in Federal service. Specialized experience is defined as:\n\nDeveloping and deploying data products and visualizations using data science, statistical, and artificial intelligence/machine learning techniques to facilitate users data-driven decision making;Collaborating across roles and organizations to build strategic relationships, achieve common goals, and resolve sensitive issues;Performing continuous improvement of data products to meet current and evolving user needs by monitoring user feedback, performance, accuracy, and reliability; andDeveloping tooling, models, and visualizations using general-purpose programming languages (such as Python) and/or tools optimized for statistical and data analysis (such as R).\n\n\nThis position has a positive education requirement: Applicants must submit a copy of their college or university transcript(s) and certificates by the closing date of announcements to verify qualifications. If selected, an official transcript will be required prior to appointment.\n\nAdditional Information\n\nBargaining Unit Status: This position is ineligible for the bargaining unit.\n\nRelocation-related expenses are not approved and will be your responsibility.\n\nOn a case-by-case basis, the following incentives may be approved:\n\n Recruitment incentive if you are new to the federal government Relocation incentive if you are a current federal employee Credit toward vacation leave if you are new to the federal government\n\n\nAdditional vacancies may be filled through this announcement in this or other GSA organizations within the same commuting area as needed; through other means; or not at all.\n\nGSA is committed to diversity, equity, inclusion and accessibility that goes beyond our compliance with \n\nValuing and embracing diversity, promoting equity, inclusion and accessibility, and expecting cultural competence; andFostering a work environment where all employees, customers and stakeholders feel respected and valued.\n\n\nOur commitment is:\n\nReflected in our policies, procedures and work environment;Recognized by our employees, customers and stakeholders; andDrives our efforts to recruit, attract, retain and engage the diverse workforce needed to achieve our mission."}
{"text": "experience in ETL development, coupled with a profound understanding of data extraction, transformation, and loading processes, will be considered.\nAs a key player in our data operations, you'll leverage native Azure tools to spearhead ETL tasks and ensure seamless data transformations.\nKey requirements and enhancements.Harness the power of Azure Data Factory, Synapse, and other Azure services to architect end-to-end data pipelines.Standardize and optimize data workflows, adhering to industry best practices and governance standards.Automate and monitor data pipelines, ensuring efficiency and accuracy in data processing.\nQualifications:Bachelor’s degree in information technology or related field, or equivalent experience.Extensive expertise (15+ years) in ETL processes, including end-to-end pipeline development and data loading.Proficiency (10+ years) with Microsoft Azure tools such as Azure Data Factory, Synapse, SQL Database, and more.Strong command of SQL for relational databases and experience with various data formats.Familiarity with data integration and pipeline tools like Informatica PowerCenter, Apache NiFi, and Apache Airflow.Excellent communication skills, both verbal and written, with a focus on customer service.Ability to work independently or collaboratively within a team, demonstrating adaptability and initiative.DatabriclsVisualization and reporting software such as MicroStrategy, Tableau, and Esri ArcGIS ﻿\nThe candidate must live in the DMV."}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across technology, data sciences, consulting and customer obsession to accelerate our clients’ businesses through designing the products and services their customers truly value.\n\nJob Description\n\nPublicis Sapient is looking for a Data Architect -AWS Cloud to join our team of bright thinkers and doers. You will team with top-notch technologists to enable real business outcomes for our enterprise clients by translating their needs into transformative solutions that provide valuable insight. Working with the latest data technologies in the industry, you will be instrumental in helping the world’s most established brands evolve for a more digital\n\nfuture.\n\n\n\nYour Impact:\n\n• Play a key role in delivering data-driven interactive experiences to our clients\n\n• Work closely with our clients in understanding their needs and translating them to technology solutions\n\n• Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs\n\n• Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements\n\n• Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews\n\n• Participate in integrated validation and analysis sessions of components and subsystems on production servers\n\n• Mentor, support and manage team members\n\nYour Skills & Experience:\n\n• 8+ years of demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelines\n\n• Good communication and willingness to work as a team\n\n• Hands-on experience with at least one of the leading public cloud data platform- AWS (Amazon Web Services)\n\n• Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)\n\n• Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.\n\n• Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\n\n• Understanding of data modeling, warehouse design and fact/dimension concepts\n\nQualifications\n\nSet Yourself Apart With:\n\n• Certifications for any of the cloud services like AWS\n\n• Experience working with code repositories and continuous integration\n\n• Understanding of development and project methodologies\n\n• Willingness to travel\n\nAdditional Information\n\nBenefits of Working Here:\n\n\n\n• Flexible vacation policy; time is not limited, allocated, or accrued\n\n• 16 paid holidays throughout the year\n\n• Generous parental leave and new parent transition program\n\n• Tuition reimbursement\n\n• Corporate gift matching program\n\n\n\nAnnual base pay range: $117,000 - $175,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "Qualifications: Data Engineering backgroundExperience with SQL, Teradata, PowerPoint, Snowflake (all of these required) This expert needs to be a \"doer\", comfortable getting in the weeds and able to take directions and run with themAbility to develop and execute an analysis plan to answer business questions\nPlusses:Experience in Pharmacy Operations liaison/SME"}
{"text": "experience projects in Nordstrom’s high-growth Direct online channel. This individual should have a high degree of curiosity about the business and the skills to discover impactful insights from data. They should be able to communicate those insights in a way that builds confidence and enables decisions that drive business value.\n\nA day in the life…\n\nPartner with key stakeholders on data analysis problemsMine complex digital behavioral data and transform it into actionable informationApply common statistical methods, data modeling, and predictive analysis to answer strategic questions for the online businessUse advanced data visualization techniques to communicate solutions to business stakeholdersProvide exceptional customer service through professionalism, ownership and initiativeDemonstrate productivity through the efficient use of time and a commitment to strong results\n\nYou own this if you have…\n\nQuantitative degree (preferably a Master's degree) in mathematics, statistics, computer science, engineering, or similar quantitative analytics discipline1+ year of corporate data analysis experience in online analytics (online retail a plus)1+ year of corporate experience writing SQL queries from scratch1+ year of professional experience using R or Python in an analytics capacityWorking understanding of statisticsGood verbal and written communication skills, ability to convey results to non-experts\n\nNordstrom is able to offer remote employment for this position in: Arizona, California, Colorado, Illinois, Nevada, North Carolina, Texas, Utah and Washington.\n\nWe’ve got you covered…\n\nOur employees are our most important asset and that’s reflected in our benefits. Nordstrom is proud to offer a variety of benefits to support employees and their families, including:\n\nMedical/Vision, Dental, Retirement and Paid Time AwayLife Insurance and DisabilityMerchandise Discount and EAP Resources\n\nA few more important points...\n\nThe job posting highlights the most critical responsibilities and requirements of the job. It’s not all-inclusive. There may be additional duties, responsibilities and qualifications for this job.\n\nNordstrom will consider qualified applicants with criminal histories in a manner consistent with all legal requirements.\n\nApplicants with disabilities who require assistance or accommodation should contact the nearest Nordstrom location, which can be identified at www.nordstrom.com.\n\n© 2022 Nordstrom, Inc\n\nCurrent Nordstrom employees: To apply, log into Workday, click the Careers button and then click Find Jobs.\n\nPay Range Details\n\nThe pay range(s) below are provided in compliance with state specific laws. Pay ranges may be different in other locations.\n\nCalifornia: $87,500- $145,000 annually, Colorado: $76,500- $126,500 annually, Nevada: $76,500- $126,500 annually, Washington: $87,500- $145,000 annually"}
{"text": "skills, knowledge and experience.\n\nEssential Functions\n\nReasonable accommodations may be made to enable individuals with disabilities to perform these essential functions. \n\nLevel 2Work with senior engineers to design and develop data pipelines that extract data from various sources, transform it into the desired format, and load it into the appropriate data storage systems.Support the buildout, migration, and maintenance of Wellabe’s Enterprise Data LakehouseAssist in administering and optimizing Azure Databricks clusters and ensure efficient data processing.Implement and optimize ELT process to ensure data accuracy, consistency, and scalability.Collaborate with analysts and business units to design and implement efficient data models.Work closely with Information Technology teams to implement continuous integration and deployment (CI/CD) pipelines using Azure DevOpsDetermine data tracking and storage needs, perform data auditing and validation, perform data mapping, loading and conversion planning, and resolve data issues or discrepancies.Level 3Designs and develops data pipelines that extract data from various sources, transform it into the desired format, and load it into the appropriate data storage systems.Train, mentor, support, and develop less experienced Data Engineers by providing advice, coaching, and educational opportunities.Lead the buildout, migration, and maintenance of Wellabe’s Enterprise Data LakehouseAdminister and optimize Azure Databricks clusters and ensure efficient data processing.Implement and optimize ELT process to ensure data accuracy, consistency, and scalability.Collaborate with analysts and business units to design and implement efficient data models.Work closely with Information Technology teams to implement continuous integration and deployment (CI/CD) pipelines using Azure DevOpsDetermine data tracking and storage needs, perform data auditing and validation, perform data mapping, loading and conversion planning, and resolve data issues or discrepancies.\nSUCCESS PROFILE\n\nKnowledge, Skills, And Abilities\n\nWorking (Level 2), Advanced (Level 3) knowledge of the principles, processes, and practices related to data engineering and/or data architecture.Moderate (Level 2), Advanced (Level 3) SQL skills.Moderate (Level 2), Advanced (Level 3) knowledge of Python required.Exposure to Azure Data Factory or related tools.Exposure to Power BI consumption or related tools.Working knowledge of data management and transformation processes.Experience in working with Azure Databricks required.Familiarity with Azure DevOps for CI/CD.Experience with version control (e.g. Git).Excellent communication and collaboration skills.Demonstrate the Wellabe core competencies of change, communication, customer focus, financial acumen, innovation, teamwork, critical thinking, and decision making. For a full description of each competency, please visit wellabe.com/core-competencies.Embrace and foster a diverse and inclusive culture that aligns with our commitment to THRIVE Toolkit: We are building resources for employee reference, including a diversity driven book club, leadership training, and a culture of betterment philosophy. Hear: Every employee has a voice. We listen with attention to gain knowledge and to understand. Respect: We empower people by respecting and appreciating their differences. Inclusion: We support a collaborative workforce that promotes belonging and respect that increases participation and contribution from all employees. Value: All employees are appreciated for their uniqueness. Equity: For all.Demonstrate an Agile mindset* of trust, teamwork, and transparency and capability to implement agile tools into daily work, allowing for quick adaption to change. (*An agile mindset is the set of attitudes supporting an agile working environment including trust, teamwork, transparency, respect, improvement, ongoing learning, pride in ownership, focus on delivering value, and the ability to adapt to change.)Embrace our culture of betterment, which surrounds the question: Does this decision fit with our mission and core values while enhancing the outcome for our customers, our business, and our associates?\n\nEducation And Experience\n\nEducation\n\nBachelor's degree in computer science, business/data analytics, management information systems, information technology or related field. Combination of education and/or relevant work experience may be accepted in lieu of degree\n\nExperience\n\nLevel 2: 2+ years related experience.Level 3: 5+ years related experience.\n\nTravel Requirements\n\nTrainings/Seminars/ConferencesMinimal\n\nPhysical Requirements\n\nPrimarily works in an office environment (or in-home office) performing work that is sedentary which involves remaining stationary and working on a computer for long periods of timeMust be able to review, assess, and/or analyze data and figures by viewing a computer screen, including extensive reading.Regularly communicates with others in-person, via video conferencing, electronically (including email), or by phone to exchange accurate information.\n\nThis job description does not list all activities, duties, or responsibilities that may be required. The employee in this position may be assigned other duties at any time with or without notice.\n\nThis job description does not constitute a contract of employment and the company may exercise its employment-at-will rights at any time."}
{"text": "Skills You BringBachelor’s or Master’s Degree in a technology related field (e.g. Engineering, Computer Science, etc.) required with 6+ years of experienceInformatica Power CenterGood experience with ETL technologiesSnaplogicStrong SQLProven data analysis skillsStrong data modeling skills doing either Dimensional or Data Vault modelsBasic AWS Experience Proven ability to deal with ambiguity and work in fast paced environmentExcellent interpersonal and communication skillsExcellent collaboration skills to work with multiple teams in the organization"}
{"text": "requirements and translate them into technical specifications.Ensure data integrity and system reliability by implementing best practices in data security and quality.Optimize data retrieval and develop dashboards for data analytics and monitoring.Mentor junior engineers and promote a culture of technical excellence and innovation.Requirements:Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field.Minimum of 5 years of experience in data engineering, specifically with high volume, high transaction systems.Expertise in Python and big data technologies such as Apache Spark.Strong experience with AWS cloud services, including EMR, Redshift, and other data processing technologies.Proven ability to architect and scale high-availability systems from scratch.Excellent problem-solving skills and attention to detail.Strong communication skills and the ability to work effectively in a team environment.Preferred Qualifications:Experience in the payments industry or financial services.Knowledge of additional programming languages or data processing frameworks.Experience in building systems designed for financial transactions with a focus on security and trust."}
{"text": "skills and handling big data along with real time streaming.Graph Ontology and semantic modeling with GraphQL or SPARQL experience is desirable.Proactive, self-driven, works independently and collaborates well.Expertise in Python, Py sparkUse of data bricks is a must."}
{"text": "experienced enough to ask for help for efficiency.• Ability to understand intent without getting bogged down in the totality of the details.• Capable of delivering high quality software with efficiency and re-usability in mind.\nRequired Qualifications and Skills• Experience in the following programming languages: C++, C#, and Python.• Experience in designing/implementing efficient algorithms, data structures and interfaces.• Proven experience debugging moderately complex software developed by someone else.• Highly motivated and willing to devote time and effort to accelerated learning.• Self-starter with can-do attitude and ability to work with little supervision.• Exposure at the professional level to most, or all, of the software development lifecycle.\nDistinguishing Qualifications and Skills• Experience in neural networks, machine learning and data engineering is not mandatory but a plus.• Experience/demonstrated competence developing OO solutions; concurrency; design patterns; real time application development.• Experience developing applications that are extensible, scalable, performant, and maintainable.• Solid mathematical foundation/educational background.• Prior experience in computer vision and data visualization is a plus.\nMinimum Education and Work Experience• BS or MS Computer Science, Electrical Engineering, Physics, Mathematics or equivalent• Ideally 3-5 years of development experience (C++, C#) preferably in an industrial or commercial setting.\nSalary$120,000-$135,000/year\nBenefits• Medical Insurance plans through United Healthcare• Supplemental GAP Insurance• Vision and Dental plans through Humana• 401(k) plus fully vested match• Employer paid LTD, Life and AD&D insurance• Voluntary STD insurance with no EOI• Voluntary Accident and additional Life/AD&D insurance• Several Employee Assistance Plans to help with a variety of personal needs\nJob TypeFull-time, Exempt\nJob LocationLouisville, CO \nAbout Boulder ImagingBoulder Imaging offers a growing suite of integrated systems and software that \"Inspect the Unexpected™\" by leveraging machine vision technologies perfected for the aerospace and industrial products industries. Our inspection technology provides revealing visual data for our clients, from the manufacturing lines of flooring and ceiling tile producers to the precision printing of banknotes or rugged wind farm environment. Visit www.boulderimaging.com and www.identiflight.com to learn more.\nBoulder Imaging is also proud to be"}
{"text": "requirements and develop concepts for new applications.\nEffectively work in an inter-disciplinary team environment.\nCoordinate with project management, software architects, other engineering and data science teams in determining overall system solutions.\nSupport the scoping and implementation of technical solutions: estimate, prioritize, and coordinate development activities.\nApply both procedural and object oriented techniques and Agile methodologies.\nAuthor technical documentation as needed.\nSupport QA team in developing test plans.\n\nWhere you'll be working...\n\nIn this worksite-based role, you'll work onsite at a defined location, Ashburn, VA.\n\nWhat we're looking for...\n\nWith an eye towards improving performance and predictability, you like the science of analytics. Developing resolutions to complex problems, using your sharp judgment to develop methods, techniques, and evaluation criteria allows you to deliver solutions that make a huge impact. You're able to communicate technical information to non-technical audiences, and you take pride in your ability to share your considerable knowledge with your peers.\n\nYou'll need to have:\nBachelor's or four or more years of work experience\nSix or more years of relevant work experience\nExperience in IT software development with some Big Data software development\nPredictive Analytics model implementation experience in production environments using ML/DL libraries like TensorFlow, H20, Pytorch, Sci-kit Learn.\nExperiences in designing, developing, optimizing, and troubleshooting complex data analytic pipelines and ML model applications using big data related technologies such as Spark or Hive\nMust be able to pass an extensive background investigation as a condition of employment.\n\nEven better if you have one or more of the following:\nBachelor's or advanced degree in computer science, applied math, statistics or other relevant quantitative discipline, or equivalent industry experience\nFour or more years of relevant work experience as a data scientist, analyst, or statistical modeler.\nMaster's/Ph.D in Computer Science or relevant technology field.\nExperience in using NLP, Bi/Visual analytics, Graph Databases like Neo4j/OrientDB/Neptune\nProgramming in Python and R using distributed frameworks like PySpark, Spark, SparkR\nExcellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, etc. and their real-world advantages/drawbacks\nRigorous understanding of statistics and ability to discern appropriate statistical techniques to problem-solve\nProven expertise optimizing extraction, transformation and loading of data from a wide variety of data sources using Apache NiFi\nFamiliarity with virtualization/containerization, DevOps and CI/CD tools for automation of build, packaging, deployment, and testing\nExperience with Atlassian's agile development tools including Bitbucket, Jira and Confluence.\nExperience with programming languages, like Java, Python, or Scala.\nExcellent written and verbal communication skills.\nGood soft skills in working with other engineering and analytical teams to arrive at optimal solutions to technical challenges.\nHigh degree of initiative with a passion for learning technology.\n\nWhy Verizon?\n\nVerizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.\nWe are a 'pay for performance' company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.\nYour benefits are market competitive and delivered by some of the best providers.\nYou are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.\nWe offer generous paid time off benefits.\nVerizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.\nYou will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don't meet every \"even better\" qualification listed above.\n\n#STSERP22\n\nWhere you'll be working\n\nIn this worksite-based role, you'll work onsite at a defined location(s).\n\nScheduled Weekly Hours\n40\n\n\n\nWe're proud to be"}
{"text": "Qualifications:\nFluency in English (native or bilingual)Proficient in at least one programming language (Python, JavaScript, HTML, C++, C# and SQL)Excellent writing and grammar skillsA bachelor's degree (completed or in progress)\nNote: Data Annotation payment is made via PayPal. We will never ask for any money from you. PayPal will handle any currency conversions from USD. This role is considered an independent contractor position."}
{"text": "requirements gathering, feedback on proposed designs and models, and acceptance testing\n\nQualifications of the Data Engineer:\n\n 10 years’ experience, with both hands-on and lead experience in supporting data warehousing solutions Must possess the following technical skills: ETL Tools: Enterprise class ETL tool (Talend is plus) Databases & Utilities: Experience with enterprise relational databases (Snowflake experience preferred) Platforms: Microsoft / Unix Expertise and fluency in SQL language is required Knowledge of scripting languages and job schedulers is required (Powershell, etc.) Experience with various integration patterns (e.g. Flat Files, Web Services, etc.) is required Knowledge of fundamental data modeling concepts (e.g. ER Diagrams, normalization, etc.) is required Familiarity with Python, Snowflake, Talend, XML/XSLT, and Cloud Services (AWS or Azure) are preferred Excellent troubleshooting and problem-solving skills; able to root cause and debug complex code in and efficient manner/with appropriate urgency Bachelor's degree in computer science, information technology or another computer-based discipline\n\nCompensation for the Data Engineer:\n\n Salary of $120K - $150K Hybrid Scheduling Comprehensive Benefits Package: Medical, Dental, Vision, 401K, PTO\n\nKeywords:\n\nData, Data analysis, Engineering, Data Engineering, Data Wrangling, Data Manipulation, Data Automation, SQL, MySQL, SQL Server, RDMS, Relational Databases, Relational Database Management Systems, DBA, Database Management, Schemas, Queries, Query, DA, Extract, Transform, Load, scripting, data reports, data visualization, benefits, medical, dental, vision, 401K, pto, vacation, hybrid"}
{"text": "experience in marketing analytics and are interested in helping us solving tough problems and influence positive change? We’d like to hear from you!\n\nAt Expedia Group (EG) Analytics we are looking for a curious and hard-working individual with a strong statistical and analytical background. The role will be responsible for devising, developing, and maintaining methods and tools to help optimize our capital allocation process. This could involve developing industry leading testing methodologies to gather as much insight as possible from a marketing test or using ML methods to help predict returns where testing is not possible. You will work closely with other highly skilled data scientists across EG, partnering with our digital marketing teams, as well as colleagues across Capital Allocation, Finance, and Product.\n\nThis job is for you if you are comfortable in tackling sophisticated analytical and business problems, you are resilient and strive to make a difference...and are seeking a stimulating role within a fast-paced and fun business!\n\nWho You Are\n\nBachelor's or Master's or PhD degree in Mathematics, Science, Statistics or related Technical field; or equivalent related professional experience in a role focused on analytics or data science (e.g. driving significant and sustained change and performance improvement from data-driven insights)Strong SQL skills, proficiency and experience in coding with R or PythonGood knowledge of statistical modelling techniques (previous experience in predictive analytics is a strong plus)Excellent analytical problem-solving skills and can-do attitudeAbility to communicate sophisticated concepts concisely and clearlyDisplay strong domain knowledge, business acumen and critical reasoning skills\n\nWhat You’ll Do\n\nApply your knowledge with SQL, Python or R, or any other major ML programming languageWorking with statisticians around the business to devise innovative ways to understand marketing efficiency when testing may or may not be possible, including building models to predict the incrementality of marketing campaigns.Work to understand business requirements and problems and find analytical solutions to solve or support them.Constantly assess the status quo, find and discuss opportunities for optimisation, simplification and acceleration of current processes.Clearly and confidently articulate decision-making rationale, solutions, methodologies and frameworks to team members and both technical and non-technical partnersPick analytically valid approaches, appropriate in terms of level of effort, favoring iterative delivery that tackle the objective, not the ask\n\nThe total cash range for this position in Seattle is $86,000.00 to $120,500.00. Employees in this role have the potential to increase their pay up to $137,500.00, which is the top of the range, based on ongoing, demonstrated, and sustained performance in the role.\n\nStarting pay for this role will vary based on multiple factors, including location, available budget, and an individual’s knowledge, skills, and experience. Pay ranges may be modified in the future.\n\nExpedia Group is proud to offer a wide range of benefits to support employees and their families, including medical/dental/vision, paid time off, and an Employee Assistance Program. To fuel each employee’s passion for travel, we offer a wellness & travel reimbursement, travel discounts, and an International Airlines Travel Agent (IATAN) membership.View our full list of benefits.\n\nAbout Expedia Group\n\nExpedia Group (NASDAQ: EXPE) powers travel for everyone, everywhere through our global platform. Driven by the core belief that travel is a force for good, we help people experience the world in new ways and build lasting connections. We provide industry-leading technology solutions to fuel partner growth and success, while facilitating memorable experiences for travelers. Expedia Group's family of brands includes: Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Vrbo®, trivago®, Orbitz®, Travelocity®, Hotwire®, Wotif®, ebookers®, CheapTickets®, Expedia Group™ Media Solutions, Expedia Local Expert®, CarRentals.com™, and Expedia Cruises™.\n\n© 2021 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. CST: 2029030-50\n\nEmployment opportunities and job offers at Expedia Group will always come from Expedia Group’s Talent Acquisition and hiring teams. Never provide sensitive, personal information to someone unless you’re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals to whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs.\n\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization."}
{"text": "skills in data science to analyze and interpret complex data sets, provide insights and recommendations, and collaborate with cross-functional teams to drive data-driven decision making. They will simultaneously improve existing processes and models, build new tools, and deliver a combination of client-facing and internal reports.\nQualificationsData Science, Statistics, and Data Analytics skillsData Visualization and Data Analysis skills (Excel and Tableau)Microsoft ecosystem and licensed software integration experience (e.g. Power Automate, etc)Experience working with large datasetsProficiency in programming languages (SQL, MySQL, Python, and R)Strong problem-solving and critical thinking abilitiesExcellent communication and presentation skillsKnowledge of healthcare data and analytics is a plusBachelor's or Master's degree in Data Science, Statistics, Computer Science, or a related field"}
{"text": "Skills:\n5 years or more experience with the following languages: Python, Java Script, C#, Shell scripting2 years or more experience with databases PostgreSQL: querying (SQL), and data modeling.1 year or more experience with graph data models and graph databasesExperience with web development framework especially Rest API and ORM.Semiconductor knowledge in Product and Test EngineeringExperience in developing data science solution for Hardware Engineering\n\nRequired Education:\nBachelor degree in Computer/Electrical Engineering, Math/Stats, Computer Science, or related field. with 2 years of experience-OR- Masters Degree in Computer/Electrical Engineering, Math/Stats, Computer Science, or related field. with 1 years of experience"}
{"text": "Experience supporting and troubleshooting complex data systems and integrations* Experience writing SQL queries* Experience reading/debugging code"}
{"text": "skills, perform as a self-starter able to timely and efficiently manage multiple tasks, make decisions in ambiguous conditions, while meeting provided deadlines with minimal supervision.\n\nThis position requires a DoD secret clearance with the ability to upgrade to a TS/SCI which requires US citizenship for work on a DoD contract.\n\nEssential Duties & Responsibilities\n\nDevelop plans for Government implementation to shape the environment to achieve customer objectives and support affiliated component units, government organizations, and partner nation missions in the SOUTHCOM AORDemonstrate and share gained experience with working groups, by identifying areas for future implementation of current and arising innovative uncrewed vehicles and technologies, with the final intent of maximizing autonomous operations in support of Maritime Domain Awareness (MDA)Serve as the Data Analytics lead and Subject Matter Expert in terms of Data analytics, policy, and integration into the 4F Mission/AORInterface with Government customer and execute logistics planning and contracting actions with subcontractors in support of technical testing, exercises, and operational activities within the SOUTHCOM AORProvide support to Minotaur Integration lead, capturing user stories, developing data strategies, facilitate knowledge management planning and tool integration\n\nRequired Skills & Experience\n\nRequired clearance: Secret with ability to upgrade to TS/SCI3+ years of experience in military intelligence, cryptologic analysis, information warfare, or cyber operationsBachelors degree in mathematics, data analytics, statistics or geospatial analysisUnderstanding of military data analytics, data policy, and C2 (Command and Control) systems architectureProficiency in data wrangling, database management (including SQL querying)Ability to collect, clean, prepare, analyze, interpret, and archive dataStrong foundation in statistics, probability theory, and applying them to data analysisSkill in automating data workflows using Python or other scripting languagesProven ability to create clear and compelling data visualizations (dashboards, charts, infographics)Excellent communication skills to translate analytical findings into actionable stories for military decision-makersKnowledge of cloud computing service models (SaaS, IaaS, PaaS) and deployment models (public, private, hybrid)Working knowledge of cybersecurity principles, network security methodologies, and common threats/vulnerabilitiesAbility to identify basic coding errors\n\nDesired Skills & Experience\n\nDoD or Government Data Team leadership experienceExperience with AI/ML modeling, applications, and fundamental AI conceptsOperational experience with unmanned systems or intelligence, surveillance, and reconnaissance5 years Academic or technical experience in Data Analytics, Database administration, GIS Processes, software development, sensor payloads or robotics, AI/ML processes and autonomous systemsProficiency using data tools such as Excel functions, VBS, Hadoop, Jupyter, Python, BI, R, SQL, Mongo, Java, JavaScript, NiFi, Tableau, Flare, Google Visualization API Docker, Kubernetes, Neo4j, and/or ArcGISKnowledge of intelligence community directives (ICDs) and standards Certified Defense All Source Analyst (CDASA)-I qualification desired \n\n#CJPOST\n\nAt SMX®, we are a team of technical and domain experts dedicated to enabling your mission. From priority national security initiatives for the DoD to highly assured and compliant solutions for healthcare, we understand that digital transformation is key to your future success.\n\nWe share your vision for the future and strive to accelerate your impact on the world. We bring both cutting edge technology and an expansive view of what’s possible to every engagement. Our delivery model and unique approaches harness our deep technical and domain knowledge, providing forward-looking insights and practical solutions to power secure mission acceleration.\n\nSMX is committed to hiring and retaining a diverse workforce. All qualified candidates will receive consideration for employment without regard to disability status, protected veteran status, race, color, age, religion, national origin, citizenship, marital status, sex, sexual orientation, gender identity or expression, pregnancy or genetic information. SMX is an Equal Opportunity/Affirmative Action employer including disability and veterans.\n\nSelected applicant will be subject to a background investigation."}
{"text": "Qualifications, Capabilities And Skills\n\n4+ years’ experience working in a business strategy and analytics role and a BS degree in a related fieldSubstantial experience with SQL (query/procedure writing)Experience with at least one of the following versatile, cross-technology tools/languages - Python, SAS, R or AlteryxClear and succinct written and verbal communication - able to frame and present messages for different audiencesCritical and analytical thinkingStrong Microsoft Excel skillsAbility to work independently and manage shifting priorities and projectsAbility to maintain detail focus and retain big picture perspectiveExcellent interpersonal skills necessary to work effectively with a variety of individuals, departments and organizations\n\nPreferred Qualifications, Capabilities, And Skills\n\nMaster’s degree in relevant field preferredExperience in Mortgage Banking or Financial Services industry\n\nHybrid 60% in-office\n\nPlease note: Relocation assistance is not offered/supported for this role.\n\nAbout Us\n\nChase is a leading financial services firm, helping nearly half of America’s households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.\n\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are \n\nEqual Opportunity Employer/Disability/Veterans\n\nAbout The Team\n\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We’re proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions – all while ranking first in customer satisfaction."}
{"text": "experiences based on user attributes Identify key trends and build automated reporting & executive-facing dashboards to track the progress of acquisition, monetization, and engagement trends. Extract actionable insights through analyzing large, complex, multi-dimensional customer behavior data sets Monitor and analyze a high volume of experiments designed to optimize the product for user experience and revenue & promote best practices for multivariate experimentsTranslate complex concepts into implications for the business via excellent communication skills, both verbal and writtenUnderstand what matters most and prioritize ruthlesslyWork with cross-functional teams (including Data Science, Marketing, Product, Engineering, Design, User Research, and senior executives) to rapidly execute and iterate\nRequirements\nBachelors’ or above in quantitative discipline: Statistics, Applied Mathematics, Economics, Computer Science, Engineering, or related field8-10+ years experience using analytics to drive key business decisions; examples include business/product/marketing analytics, business intelligence, strategy consultingProven track record of being able to work independently and proactively engage with business stakeholders with minimal directionSignificant experience with SQL and large unstructured datasets such as HadoopDeep understanding of statistical analysis, experimentation design, and common analytical techniques like regression, decision treesSolid background in running multivariate experiments to optimize a product or revenue flowStrong verbal and written communication skillsProficiency in programming/scripting and knowledge of statistical packages like R or Python is a plus\n\nPreferred Qualifications\nAdvanced scripting language experience: R/PythonSuperior skills with TableauExperience with predictive modeling based strategies Experience in handling large data sets and relational databases\nTotal Rewards\nDropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. We target most new hire offers between the minimum up to the middle of the range.\nSalary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs).\nCurrent Salary/OTE Ranges (Subject to change):• US Zone 1: $210,800 - $248,000 - $285,200.• US Zone 2: $189,700 - $223,200 - $256,700.• US Zone 3: $168,600 - $198,400 - $228,200. Dropbox uses the zip code of an employee’s remote work location to determine which metropolitan pay range we use. Current US Zone locations are as follows:• US Zone 1: San Francisco metro, New York City metro, or Seattle metro• US Zone 2: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington (outside Seattle metro), Washington DC metro and West Virginia (DC metro)• US Zone 3: All other US locations \n\n Dropbox is"}
{"text": "requirements, ensuring all model implementations and documentation meet industry standards. \n\n\n\nRequired Education\n\n\n Bachelor's Degree or equivalent combination of education and work experience\n\n\n\nRequired Experience\n\n\n 5 years relevant experience\n\n\n\nPreferred Competencies/Skills\n\n\nExcellent project management, collaboration, and communication skills, capable of leading complex projects and influencing stakeholders at all levels. Excellent all-around software development skill in Python. Experience working in cloud environments such as Azure, AWS, or GCP and knowledge of their AI and ML services. Experience in running a large program or several projects simultaneously. Proficiency in SQL for analysis and data extraction. Advanced knowledge in machine learning engineering practices, including MLOps tools (MLflow, Kubeflow, TFX) to streamline the machine learning lifecycle. Familiarity with containerization and orchestration technologies (Docker, Kubernetes) for scalable ML deployments. Experience with TensorFlow, PyTorch, transformers, LangChain, numpy, pandas, polars, and related. Excellent communication and collaboration skills. \n\n\n\nPreferred Education Specifics\n\n\nDegree qualified (or equivalent) in Computer Science, Engineering, Machine Learning, Mathematics, Statistics, or related discipline 3+ years of experience with design and architecture, data structures, and testing/launching software products. 2+ years in ML engineering with production-level deployments. \n\n\n\nPreferred Licenses/Certifications\n\n\n Certified Specialist in Predictive Analytics (CAS) or other data science related certifications\n\n\n\nPreferred Knowledge\n\n\nStrong understanding of data and model quality monitoring systems, and developing data validation frameworks. Expertise in advanced model optimization techniques, including fine-tuning and the development and deployment of Retrieval-Augmented Generation (RAG) models for enhanced AI performance. Proficient in Git and trunk-based branching strategies. Guide the team in adopting CI/CD practices, code review processes, and automated testing frameworks for ML systems. Strong understanding of software design principles. Skilled in implementing data and model quality monitoring systems and developing data validation frameworks. Proven proficiency in developing and executing Bash scripts for automation and system management tasks. Understand policyholder characteristics and insurance product attributes as needed to improve model performance. Creativity and curiosity for solving complex problems. \n\n\n\nAbout QBE\n\nWe can never really predict what’s around the corner, but at QBE we’re asking the right questions to enable a more resilient future by helping those around us build strength and embrace change to their advantage.\n\nWe’re an international insurer that’s building momentum towards realizing our vision of becoming the most consistent and innovative risk partner.\n\nAnd our people will be at the center of our success. We’re proud to work together, and encourage each other to enable resilience for our customers, our environment, our economies and our communities.\n\nWith more than 12,000 people working across 27 countries, we’re big enough to make a real impact, but small enough to provide a friendly workplace, where people are down-to-earth, passionate, and kind.\n\nWe believe this is our moment: What if it was yours too?\n\nYour career at QBE — let’s make it happen!\n\nhttps://www.linkedin.com/company/qbe-north-america/\n\nUS Only - Travel Frequency \n\n\n Infrequent (approximately 1-4 trips annually)\n\n\n\nUS Only - Physical Demands \n\n\n General office jobs: Work is generally performed in an office environment in which there is not substantial exposure to adverse environmental conditions. Must have the ability to remain in a stationary position for extended periods of time. Must be able to operate basic office equipment including telephone, headset and computer. Incumbent must be able to lift basic office equipment up to 20 lbs.\n\n\n\nUS Only - Disclaimer \n\n\n To successfully perform this job, the individual must be able to perform each essential job responsibility satisfactorily. Reasonable accommodations may be made to enable an individual with disabilities to perform the essential job responsibilities.\n\n\n\nJob Type \n\n\n Individual Contributor\n\n\n\nGlobal Disclaimer \n\n\n The duties listed in this job description do not limit the assignment of work. They are not to be construed as a complete list of the duties normally to be performed in the position or those occasionally assigned outside an employee’s normal duties. Our Group Code of Ethics and Conduct addresses the responsibilities we all have at QBE to our company, to each other and to our customers, suppliers, communities and governments. It provides clear guidance to help us to make good judgement calls.\n\n\n\nCompensation\n\nBase pay offered will vary depending on, but not limited to education, experience, skills, geographic location and business needs.\n\nAnnual Salary Range: $121,000 - $182,000\n\nAL, AR, AZ, CO (Remote), DE, FL, GA, IA, ID, IL (Remote), IN, KS, KY, LA, ME, MI, MN, MO, MS, MT, NC, ND, NE, NH, NV, OH, OK, OR, PA, SC, SD, TN, TX (Remote, Plano), UT, VA, VT, WI, WV and WY\n\n* * * * *\n\nAnnual Salary Range: $133,000 - $200,000\n\nCA (Remote, Fresno, Irvine and Woodland), Greenwood Village CO, CT, Chicago IL, MA, MD, NY (Remote), RI, Houston TX and WA\n\n* * * * *\n\nAnnual Salary Range: $152,000 - $228,000\n\nSan Francisco CA, NJ and New York City NY\n\nBenefit Highlights\n\nYou are more than your work – and QBE is more than a workplace, which is why QBE provides you with the benefits, support and flexibility to help you concentrate on living your best life personally and professionally. Employees scheduled over 30 hours a week will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health.\n\nWe also offer a competitive 401(k) contribution and a paid-time off program. In addition, our paid-family and care-giver leaves are available to support our employees and their families. Regular full-time and part-time employees will also be eligible for QBE’s annual discretionary bonus plan based on business and individual performance.\n\n\n\nAt QBE, we understand that exceptional employee benefits go beyond mere coverage and compensation. We recognize the importance of flexibility in the work environment to promote a healthy balance, and we are committed to facilitating personal and professional integration for our employees. That's why we offer the opportunity for hybrid work arrangements.\n\nIf this role necessitates a hybrid working model, candidates must be open to attending the office 8-12 days per month. This approach ensures a collaborative and supportive work environment where team members can come together to innovate and drive success.\n\nHow to Apply:\n\nTo submit your application, click \"Apply\" and follow the step by step process.\n\n\n\nQBE is \n\nApplication Close Date: 17/04/2024 11:59 PM\n\nHow to Apply:\n\nTo submit your application, click \"Apply\" and follow the step by step process.\n\n\n\nQBE is"}
{"text": "experiences. Through our platform, we help people use their own agency to change who they are — their identity — and to increase their ability to actively change behavior, happiness, resilience, and health.\nOur fully remote company has a culture that emphasizes speed of iteration, open and frank discussion, autonomy, and making decisions with evidence. Our team is passionate about change and values the team’s progress over individual achievement. Come join our journey to revolutionize healthcare.\nRole SummaryOur ideal machine learning engineering candidate is hands-on with a track record of taking ideas from concept to implementation. They are comfortable working with cloud platforms, databases and streaming data, developing algorithms and models, setting up and using APIs, and incorporating developed models into larger production software ecosystems. They excel at identifying testable hypotheses and simplifying experimental solutions to complex problems raised by technical and non-technical staff and are not afraid to pivot when a plan isn’t working – they evolve and build upon learnings from every effort. They can think from both a technical and business perspective, can balance scientific and market risks and rewards, and are passionate in pushing research into development and iterating to bring high-value solutions and applications into production.Our objective is to help users connect, share and reflect upon experiences, and support one another as they try out and work through different lifestyle changes with peers. This role, together with their cross-functional team will support users in having meaningful and productive conversations and exchanging and implementing new ideas and behaviors. That is, you will help facilitate the overall flourishing of our community. \nExperience and Education Requirements:MS or higher in Computer/Information Science, Computational Social Science, Mathematics, Statistics, or a related field8+ years of professional experience in building and deploying machine learning systems; a combination of education and experience is acceptable but at least three years of industry experience is required.Understanding of and experience with the full machine learning lifecycle (explore, train, evaluate, deployment, monitor, iterate, etc.) including with custom datasets.Strong proficiency in Python programming including use of unit and integration tests, version control, etc.Experience working in a collaborative multi-disciplinary team with an agile process.Excellent communication skills, with the ability to communicate technical concepts to non-technical audiences.Ability to rapidly use open-source software and models to create production-ready tools that serve multiple projects and teams.Highly motivated with outstanding organizational skills, effective relationship builder – ability to partner internally and externally with various levels of the organization and personality types.\nPreferred Qualifications:Experience with cloud computing platforms (e.g., GCP, AWS, Azure).Proficient understanding of a range of NLP algorithms and models (e.g. entity extraction and resolution techniques, embeddings, transformers, fine-tuning)Experience building and deploying NLP models and pipelines as API endpoints and scheduled workflows.Experience with LLMs (encoder-decoder, encoder only, decoder only) and fine-tuning for downstream tasks.Understanding of and experience with models that utilize zero-, one-, and few-shot learning.\nTravel Requirements:0-5%"}
{"text": "experience and drive business outcomes is at the heart of everything FanDuel does, the Director of Data Governance will be responsible for defining the strategy for our data governance vertical in providing well defined, quality, consistent and compliant data available to all stakeholder groups throughout FanDuel. We are looking for a passionate, hands-on Data Governance professional to join our team.\n\nTHE GAME PLAN\n\nEveryone on our team has a part to play\n\nDefine, communicate and execute the data governance strategy to meet the needs of the business as it scales over the next 2-3 yearsEvaluate data quality, data profiling, data lineage and metadata managementEstablish and maintain relationships with stakeholders within the organizationInterpret and enforce data requirements for data governance initiativesMonitor and enforce compliance with legal and security policies and standards for access to dataMonitor and enforce compliance with data governance policies and standardsUse data governance tools to access data quality, integrity and completenessProvide guidance and support to teams on data governance best practicesPut in place the right organizational structure to support the strategy and ensure teams can deliver predictably, at pace and to high qualityIdentify opportunities for new approaches and new technologies that can deliver relevant data, faster to the organizationCollaborate with cross-functional partners in product, engineering, Business units, marketing, finance and legal to define and build data definitions and data stewardshipManage 6-8 team members across multiple office locations and continents. Expectation will be to increase the team in size quickly while not sacrificing quality or pace of output.\n\nTHE STATS\n\nWhat we're looking for in our next teammate\n\nMust be able to lead a team with a diverse set of skill sets including product manager, analysts and engineers.A minimum of 5+ years’ experience of leadership positions in data governance within a technical organizationDemonstrable experience in creating a culture of inclusion, ownership and accountability through role modelling company principlesTrack record in being able to inspire people and teams by creating compelling visions for the futureAccomplished in scaling teams, managing multiple geographic locations and inspiring a team to deliver high quality projects at startup paceDeep technical domain knowledge and have the ability to roll up sleeves to teach and develop team membersExperience with data governance tools such as Alation, Collibra, Databricks Unity catalog, Informatica etc. High familiarity with data platform and applications such as S3, Tableau, Databricks, Redshift and AirflowExperience in supporting your managers to identify, develop and grow talent.Be a thought leader and evangelist of data governance practices to drive adoption and knowledge at all levels of the organization\n\nPlayer Benefits\n\nWe treat our team right\n\nFrom our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:\n\nAn exciting and fun environment committed to driving real growthOpportunities to build really cool products that fans loveMentorship and professional development resources to help you refine your gameBe well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another \n\nFanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to"}
{"text": "skills. They will work with data analysis tools like Microsoft Excel, Structured Query Language, and the Minnesota Department of Revenue's integrated tax system to identify potential tax non-compliance and income tax non-filers and make work available to Income Tax and Withholding Division staff.\nWorking at the Department of Revenue offers numerous advantages, including opportunities for personal and professional growth, impactful work, competitive compensation, work-life balance, and continuous learning. We strive to provide a supportive and inclusive work environment that enables our employees to thrive and make a meaningful impact. Join us and be a part of a team that is making a difference!Teleworking employees are required to live in Minnesota or in a state bordering Minnesota.\nMinimum QualificationsTwo (2) years* of full-time professional experience in accounting or auditingORCertification (satisfactory completion of probation) as a Revenue Tax SpecialistApplicants who meet the above requirements will be further evaluated based on the following during the interview process:Knowledge of accounting and auditing principles and practicesTechnical tax knowledge, including court rulings, regulations and administrative policies and procedures.Customer service and interpersonal skillsAbility to plan, direct and review the work of others.Knowledge of personal computer operation and software programs used by the department in its internal and external operations.Communication and presentation skillsKnowledge of audit report processing proceduresKnowledge of electronic filing/processing systemsSkill in reading comprehensionSkill in problem-solving\n*An associate degree may substitute for six (6) months of experience. Bachelor's degree may substitute one (1) year of experience; master’s degree may substitute for eighteen (18) months of experience; PhD may substitute for twenty-four (24) months of experience. Majors in Accounting, Finance, Economics, Business Administration, Legal, Business Management, Marketing, or other closely related fields are acceptable.\nPreferred QualificationsThe ability to gain knowledge and apply it to tax and accounting work.Experience with data analysis tools such as Microsoft Excel and Structured Query Language (SQL).Knowledge of federal (Internal Revenue Code) and state statutes, regulations, rulings, and administrative policies related to individual income tax, withholding tax, and property tax refund.Knowledge of withholding tax and income tax nonfiler and audit processes, policies, and procedures.Knowledge of accounting and auditing principles and practices.Extensive knowledge of the operation of a personal computer, including the use of Microsoft Office software such as Excel, Word, Access, PowerPoint.\nPhysical RequirementsRequires occasional moving of articles such as boxes, accounting records, laptop computer, and portable printer.\nAdditional RequirementsPrior to an offer of employment, a background check will be conducted. This will include, but is not limited to checking degrees and licensures, criminal history, and tax filing and payment history. All individual income tax filing and payment obligations must be current prior to interviewing for this position.Must be legally authorized to work in country of employment without sponsorship for employment visa status (e.g., H1B status).\nAbout Revenue Dept The Minnesota Department of Revenue works to fund the future for all of Minnesotans. We manage over 30 different taxes and collect $26.7 billion annually in state taxes, which funds state programs such as healthcare, transportation, public safety, and early childhood, K-12, and higher education.\nRevenue is dedicated to an inclusive work environment that celebrates and values the diversity of each employee and reflects the communities we serve. We're committed to a culture of inclusion where everyone can bring their authentic selves to work and thrive.\nWe value a work life balance for our employees, and many of our employees telework in a full or hybrid capacity. For those that come to the office, regularly or occasionally, we invested in a state-of-the-art hybrid workspace located at the Stassen building in St. Paul.\nFind out more about us on our website.\nWhy Work for Us Diverse Workforce We are committed to continually developing a workforce that reflects the diversity of our state and the populations we serve. The varied experiences and perspectives of employees strengthen the work we do together and our ability to best serve the people of Minnesota.\nA recent engagement survey of State of Minnesota employees found: 95% of employees understand how their work helps achieve their agency’s mission91% of employees feel trusted to do their jobs88% of employees feel equipped to look at situations from other cultural perspectives when doing their job87% of employees report flexibility in their work schedule\nComprehensive Benefits Our benefits aim to balance four key elements that make life and work meaningful: health and wellness, financial well-being, professional development, and work/life harmony. As an employee, your benefits may include:Public pension planTraining and professional developmentPaid vacation and sick leave11 paid holidays each yearPaid parental leaveLow-cost medical and dental coveragePrescription drug coverageVision coverageWellness programs and resourcesEmployer paid life insuranceShort-term and long-term disabilityHealth care spending and savings accountsDependent care spending accountTax-deferred compensationEmployee Assistance Program (EAP)Tuition reimbursementFederal Public Service Student Loan Forgiveness Program\nPrograms, resources and benefits eligibility varies based on type of employment, agency, funding availability, union/collective bargaining agreement, location, and length of service with the State of Minnesota."}
{"text": "experience with high-performance computing, it would be advantageous, especially in optimizing code for performance and efficiency.of systems and infrastructure concepts, including cloud computing, containerization, and microservices architecture, would be beneficial.with AI-powered developer tools such as Codeium, Copilot, ChatGPT, and others is highly valued. Being able to leverage these tools to enhance productivity and code quality is a strong advantage.\nRequirements:don't impose hard requirements on specific languages or technologies, but we expect you to demonstrate your ability to write clean, efficient, and maintainable code.should have the capability to work on different layers of the software stack, including frontend, backend, IDE extensions, machine learning components, infrastructure, and data processing."}
{"text": "requirements for claims processing on the Pattern platform, including the required workflow stages, computations for award methodologies, and the generation of work products. Define and create data-driven operational protocols and procedures that drive program progress and ensure transparency and visibility throughout settlements. Work with product and engineering teams to implement these protocols and procedures within the Pattern platform, identifying key gaps and driving platform updates where required. Support deliverable management processes through planning, tracking, and reporting deliverables, ensuring projects are delivered on time and to the required quality standards. Support project and program communications. Distribute project status to team members throughout the project lifecycle. Collect, analyze, and interpret data to identify program-related issues to make action-oriented recommendations Prepare detailed reports and presentations using data visualization tools to summarize analysis results and provide actionable recommendationsIdentify areas for product and process improvement Identify and analyze potential associated risks and recommend risk mitigation strategies.Work with databases to organize, query, and retrieve data efficiently\n\n\nWhat You’ll Have\n\nBachelor’s degree in Mathematics, Economics, Accounting, Finance or related analytical fieldsProven experience in data analysis, preferably in a software development or legal setting.Advanced proficiency in Excel and other data analysis tools (e.g., SQL, Python, R)Excellent analytical and problem solving skills to interpret complex data and draw meaningful conclusionsExcellent verbal and written skills, with the ability to communicate with all levels of client personnel. Passion and ability to tell stories with data, educate effectively, and instill confidence, motivating stakeholders to act on insights and recommendationsStrong analytical and problem-solving skills with a keen attention to detailAbility to ensure that projects are delivered on time and produce desired resultsAbility to navigate complexity and ambiguityFamiliarity with AI and machine learning concepts is a plus\n\n\nReady to meet us?\n\nPlease apply directly through our website or Linkedin. We are excited to hear from you!"}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across truly value\n\nJob Description\n\nPublicis Sapient is looking for a Senior Associate Data Engineer (Azure) to be part of our team of top-notch technologists. You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future.\n\nYour Impact:Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our client's businessTranslate client's requirements to system design and develop a solution that delivers business valueLead, designed, develop, and deliver large-scale data systems, data processing, and data transformation projectsAutomate data platform operations and manage the post-production system and processesConduct technical feasibility assessments and provide project estimates for the design and development of the solutionMentor, help and grow junior team membersSet Yourself Apart With:Developer certifications in Azurecloud servicesUnderstanding of development and project methodologiesWillingness to travel\n\nQualifications\n\nYour Technical Skills & Experience:Demonstrable experience in data platforms involving implementation of end to end data pipelinesHands-on experience with at least one of the leading public cloud data platforms (Azure, AWS or Google Cloud)Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Glue ETL, Lambda, Spark, Spark Streaming, etc.Ability to handle module or track level responsibilities and contributing to tasks “hands-on”Experience in data modeling, warehouse design and fact/dimension implementationsExperience working with code repositories and continuous integrationData modeling, querying, and optimization for relational, NoSQL, timeseries, and graph databases and data warehouses and data lakesData processing programming using SQL, DBT, Python, and similar toolsLogical programming in Python, Spark, PySpark, Java, Javascript, and/or ScalaData ingest, validation, and enrichment pipeline design and implementationCloud-native data platform design with a focus on streaming and event-driven architecturesTest programming using automated testing frameworks, data validation and quality frameworks, and data lineage frameworksMetadata definition and management via data catalogs, service catalogs, and stewardship tools such as OpenMetadata, DataHub, Alation, AWS Glue Catalog, Google Data Catalog, and similarCode review and mentorshipBachelor’s degree in Computer Science, Engineering or related field.\n\nAdditional Information\n\nPay Range:$103,000 -$154,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. Actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nBenefits of Working Here:Flexible vacation policy; time is not limited, allocated, or accrued16paid holidays throughout the yearGenerous parental leave and new parent transition programTuition reimbursementCorporate gift matching programAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "Experience with various databases including SQL Server, Teradata, Snowflake, and Synapse.Solid understanding of data engineering principles, data modeling, data warehousing, and ETL/ELT processes, encompassing data testing, validation, and reconciliation procedures.Hands-on experience with data integration and transformation frameworks, tools, and methodologies.Familiarity with version control systems like Git, GitHub, etc.Collaborate with cross-functional and business teams to grasp business requirements and translate them into technical designs and solutions.Develop and maintain data pipelines, integrations, and transformations to facilitate efficient data processing, storage, and retrieval.Optimize data infrastructure and solutions for performance, scalability, and cost-efficiency, ensuring high availability and reliability.Conduct data profiling, validation, and cleansing activities to maintain data integrity and accuracy.Provide mentorship and technical guidance to junior data engineers, interns, and freshers, promoting knowledge sharing and skill development within the team.\n\nThanks & RegardsUtsavManagerChabezTech LLC4 Lemoyne Dr #102, Lemoyne, PA 17043, USAUS Office : +1-717-441-5440 Email: utsav@chabeztech.com | www.chabeztech.com"}
{"text": "experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.\n\nJoin Team Amex and let's lead the way together.\n\nAs part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.\n\nHow will you make an impact in this role?\n\nResponsible for managing data quality initiatives, improving CMDB health, and creating a forward-looking view to collaborate across multiple organizations.\n\nDesign and develop data strategy and database systems that meet the needs of the IT Asset Management and Tech partnersPractical knowledge of Public Cloud asset management and classesCollaborate with multiple stakeholders to deliver technical capabilities, reporting, and automation of processesPerform analytics with Python to improve Data QualityOptimize the processes of CMDB ingestion and sustain CMDB HealthFunction as member of a development team by contributing to database builds through consistent development practices (tools, common components, and documentation)Utilize and define operational best practices, requirements and associated documentation – ensure all processes are clear and auditableInfluence the future direction of IT Asset Management data managementDemonstrate a sense of urgency and enthusiasm in developing best-in-class technology services Demonstrate well-rounded technical understanding of Midrange, Mainframe, Network, and Storage environment including but not limited to hypervisors, operating systems, databases, monitoring, basic network terminology, Active Directory, and storage technologyActively monitor quality of service and drive corrective actions and process enhancements to improve performance and availability\n\nQualifications\n\n2-4 years of database experience in a professional environment and/or comparable experienceFamiliar with Agile or other rapid application development methodsExperience with SQL, Power BI, .Net, API creation and consumption Hands-on experience with data visualization, dashboard design, and software developmentExposure to distributed (multi-tiered) systems, algorithms, and relational databasesFamiliar with ITIL processes, Service Now, and database application programmingExperience with PythonAbility to work independently and collaboratively as a member of a teamBachelors in Computer Science/Management Information Systems or equivalent experience preferredAptitude to develop and maintain strong relationships with both internal and external stakeholders\n\nSalary Range: $85,000.00 to $150,000.00 annually + bonus + benefits\n\nThe above represents the expected salary range for this job requisition. Ultimately, in determining your pay, we'll consider your location, experience, and other job-related factors.\n\nWe back our colleagues and their loved ones with benefits and programs that support their holistic well-being. That means we prioritize their physical, financial, and mental health through each stage of life. Benefits include:\n\nCompetitive base salaries Bonus incentives 6% Company Match on retirement savings plan Free financial coaching and financial well-being support Comprehensive medical, dental, vision, life insurance, and disability benefits Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need 20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy Free access to global on-site wellness centers staffed with nurses and doctors (depending on location) Free and confidential counseling support through our Healthy Minds program Career development and training opportunities\n\nFor a full list of Team Amex benefits, visit our Colleague Benefits Site.\n\nAmerican Express is \n\nWe back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.\n\nUS Job Seekers/Employees - Click here to view the “Know Your Rights” poster and the Pay Transparency Policy Statement.\n\nIf the links do not work, please copy and paste the following URLs in a new browser window: https://www.dol.gov/agencies/ofccp/posters to access the three posters.\n\nEmployment eligibility to work with American Express in the U.S. is required as the company will not pursue visa sponsorship for this position."}
{"text": "Requirements:\n\nAdvanced degree in Computer Science, Machine Learning, or a related fieldExtensive experience in developing and deploying ML/AI-based systemsStrong Python skillsStrong expertise in natural language processing (NLP) and large language models (LLMs)Experience with LLM libraries such as LangChain, Llama Index, Instructor, DsPy, Outlines, or InstructorExperience with Vector Databases such as Chroma, Weaviate, PineConeProficiency in deep learning frameworks such as TensorFlow, PyTorch, Keras, HuggingFaceExperience with fine-tuning LLMs is a plusExperience with KubernetesExperience with model serving technologies a plus, eg KServe, Ray, vLLM, SkyPilot etc.Experience with ML experiment tracking, CometML, Weights and Biases,Excellent problem-solving, analytical, and communication skillsAbility to work collaboratively in a fast-paced, innovative environmentKnowledge of genetics or experience in the healthcare domain is a plus\n\nBusiness Value Add:\n\nBy hiring a talented Machine Learning Engineer with expertise in Generative AI, we will be able to:\n\nAutomate the report writing activity which is currently plagued by the tedious work of template selection and minor edits by our highly trained clinical staff.Enable our clinical team to allocate resources more efficiently and focus on high-value tasks such as variant interpretation and difficult cases.Develop a platform for other generative AI applications, such as automated claim denial rebuttals and literature analysis.Establish our company as a technology leader in the genetic testing industry in the use of AI/ML attracting top talent, investors, and partnerships.\n\nPay Transparency, Budgeted Range\n\n$153,000—$191,300 USD\n\n~\n\nScience - Minded, Patient - Focused. \n\nAt GeneDx, we create, follow, and are informed by cutting-edge science. With over 20 years of expertise in diagnosing rare disorders and diseases, and pioneering work in the identification of new disease-causing genes, our commitment to genetic disease detection, discovery, and diagnosis is based on sound science and is focused on enhancing patient care.\n\nExperts In What Matters Most.\n\nWith hundreds of genetic counselors, MD/PhD scientists, and clinical and molecular genomics specialists on staff, we are the industry’s genetic testing experts and proud of it. We share the same goal as healthcare providers, patients, and families: to provide clear, accurate, and meaningful answers we all can trust.\n\nSEQUENCING HAS THE POWER TO SOLVE DIAGNOSTIC CHALLENGES. \n\nFrom sequencing to reporting and beyond, our technical and clinical experts are providing guidance every step of the way:\n\nTECHNICAL EXPERTISE\n\nHigh-quality testing: Our laboratory is CLIA certified and CAP accredited and most of our tests are also New York State approved.Advanced detection: By interrogating genes for complex variants, we can identify the underlying causes of conditions that may otherwise be missed.\n\nCLINICAL EXPERTISE\n\nThorough analysis: We classify variants according to our custom adaptation of the most recent guidelines. We then leverage our rich internal database for additional interpretation evidence.Customized care: Our experts review all test results and write reports in a clear, concise, and personalized way. We also include information for research studies in specific clinical situations.Impactful discovery: Our researchers continue working to find answers even after testing is complete. Through both internal research efforts and global collaborations, we have identified and published hundreds of new disease-gene relationships and developed novel tools for genomic data analysis. These efforts ultimately deliver more diagnostic findings to individuals.\n\nLearn more About Us here.\n\n~\n\n Benefits include:\n\nPaid Time Off (PTO)Health, Dental, Vision and Life insurance401k Retirement Savings PlanEmployee DiscountsVoluntary benefits\n\nGeneDx is \n\nAll privacy policy information can be found here."}
{"text": "Skills:Ideal candidate should have a degree in a quantitative field (e.g., mathematics, computer science, physics, economics, engineering, statistics, operations research, quantitative social science, etc.).Basic Knowledge on software development principles and architecture.Good analytical and problem-solving abilities.Ability to break down and understand complex business problems, define a solution and implement it using advanced quantitative methods.Familiarity with programming for data analysis; ideally Python, SQL, or R.Solid oral and written communication skills, especially around analytical concepts and methods.Great work ethic and intellectual curiosity.Knowledge of Cloud technologies such as AWS or Google Cloud.Knowledge of any relational database such as My SQL.Must be a team player with excellent communication and problem-solving skills and have experience working with customers across teams."}
{"text": "experienced data engineer to join our Professional Services team. In this role, you will play a pivotal part in the full development life cycle, from conceptualization to implementation, delivering high-quality BI solutions to our clients. You will be responsible for creating scalable and repeatable solutions that can be tailored to meet the unique needs of each client. \n requirements and extend the QuickLaunch data model by identifying additional data sources. Design and develop ETL pipelines to efficiently extract, transform, and load data into the BI system. Utilize DAX to develop complex Tabular models that accurately represent the underlying data and support advanced analytics. Work closely with the development team to stay abreast of new products and technologies, providing front-line technical support when necessary. Serve as a subject matter expert, sharing knowledge and guiding clients and team members on best practices for implementing BI solutions. Ensure the delivery of high-quality services, maintaining our reputation for excellence in customer satisfaction.  \n Who You Are: Possess strong analytical, problem-solving, conceptual, communication, and organizational skills. Demonstrate a customer-centric approach with a focus on delivering effective solutions and achieving results. Display a career trajectory centered on reporting, business intelligence, and analytics applications. Thrive in a fast-paced, collaborative team environment. Bachelor’s degree or equivalent work experience is required, with a preference for disciplines such as Computer Science, MIS, Engineering, Business Administration, or related fields.  \nTechnical Requirements: Proficiency in Advanced SQL across multiple platforms, including SQL Server, Oracle, DB2, Databricks, and Synapse, specializing in performance tuning and complex query optimization. Extensive experience with Databricks ETL/ELT methodologies, Delta Lake, SQL Warehouse, and Delta Sharing, with additional knowledge of AI/ML being advantageous. Expertise in SSIS or a third-party tool for ETL/ELT processes, including utilizing custom components. Fluency in Python \nBonus Points: Knowledge of Data Mart/Warehouse modeling, adept at integrating diverse data sources regardless of coupling. Proficient in SSAS/Power BI Tabular modeling and DAX language. Experience extracting data from ERP systems, ideally JD Edwards or Viewpoint Vista  \nAbout Us: Preferred Strategies (www.preferredstrategies.com) is an Employee-Owned (ESOP) dedicated to helping organizations turn their ERP (JD Edwards, NetSuite, and Viewpoint Vista), CRM (Salesforce), CPM (OneStream) data into decision-ready information. Our mission is to find companies who value data as much as we do, who align on vision, and who want to partner together on their data-driven journey. We are passionate about giving our clients the competitive advantage they need to make smarter business decisions and achieve their business goals. We have spent thousands of hours building a solution called QuickLaunch that enables customers to leverage best-in-class technologies like Power BI, Databricks, Azure Cloud, etc. with their ERP, CRM, and CPM data which becomes the framework and foundation of their Analytics Strategy. Come join our team and gain the opportunity to work with some of the world’s highest performing companies and talented people who share a common vision for the future of data. We seek people who thrive in a team-oriented and collaborative environment and are proud to have this represented by our Team Member Net Promoter Score (NPS) of 92 and a Customer NPS of 81.   Working Location: Remote OR Santa Cruz County, CA office Estimated Compensation Range: $125,000-150,000"}
{"text": "experience in Azure native services. In this role, you will be instrumental in leveraging Azure Data Factory, Synapse Analytics, and Azure Data Lake Storage Gen2 to design and implement scalable data solutions. Your strong SQL skills and proficiency in Spark will be essential for optimizing data pipelines and driving insights from our vast datasets.Key skills and expertise in Spark to optimize data processing and analysis.Analyze and reverse engineer SSIS packages to migrate solutions to Azure.Collaborate effectively with offshore team members, providing guidance and support as needed.Communicate effectively with stakeholders to gather requirements and present solutions.Qualifications:Bachelor's degree in Computer Science, Engineering, or related field.12+ years of overall IT experience, with at least 5 years specializing in Azure data services.Proven experience in designing and implementing complex data solutions in Azure.Strong proficiency in SQL and experience with Spark for data processing.Experience in analyzing and migrating SSIS packages to Azure.Excellent communication skills with the ability to work effectively in a team environment."}
{"text": "experience for yourself, and a better working world for all.\n\nData Analyst, Technology Consulting - Data & Analytics (Data Governance & Controls) - Financial Services Office (Manager) (Multiple Positions), Ernst & Young U.S. LLP, New York, NY. \n\nWork with clients to transform the way they use and manage data by architecting data strategies, providing end-to-end solutions that focus on improving their data supply chain, reengineering processes, enhancing risk control, and enabling information intelligence by harnessing latest advanced technologies. Solve complex issues and drive growth across financial services. Define data and analytic strategies by performing assessments, recommending remediation strategies/solutions based on aggregated view of identified gaps, and designing/implementing future state data and analytics solutions. Manage and coach diverse teams of professionals with different backgrounds. Manage cross functional teams, to ensure project task and timeline accountability. Propose and drive new technologies to enhance or replace existing business processes. Initiate and build thought leadership through white papers, point of views, and proof of concepts. Develop strategies to solve problems logically using creative methods. Engage and influence large teams and functional leaders.\n\nManage and motivate teams with diverse skills and backgrounds. Consistently deliver quality client services by monitoring progress. Demonstrate in-depth technical capabilities and professional knowledge. Maintain long-term client relationships and networks. Cultivate business development opportunities.\n\nFull time employment, Monday – Friday, 40-45 hours per week, 8:30 am – 5:30 pm.\n\nMINIMUM REQUIREMENTS:\n\nMust have a bachelor's degree in Engineering, Computer Science, Business, Economics, Finance, Statistics, Analytics or a related field and 5 years of progressive, post-baccalaureate related work experience. Alternatively, must have a Master’s degree in Engineering, Computer Science, Business, Economics, Finance, Statistics, Analytics or a related field and 4 years of related work experience.\n\nMust have 4 years of experience in the banking, capital markets, insurance or asset management industry.\n\nMust have 3 years of experience in at least 1 of the following:\n\n Distributed Processing (Spark, Hadoop, or EMR); Traditional RDBMS (MS SQL Server, Oracle, MySQL, or PostgreSQL); MPP (AWS Redshift, or Teradata); NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, or Titan); Cloud Platforms (AWS, Azure, Google Platform, or Databricks); Data Governance, Lineage and Quality (Collibra, Solidatus, Informatica, Alation, Snowflake, Ab Initio, One Trust, or Big ID.\n\nMust have 3 years of experience using technologies, frameworks or methodologies for data ingestion, storage, mining or warehousing, big data analytics, manipulation, or visualization.\n\nMust have 3 years of experience in managing teams through a product and/or project management life cycle including requirements, design, development and testing.\n\nMust have 3 years of experience quantifying improvement in business areas resulting from optimization techniques through use of business analytics and/or statistical modeling.\n\nRequires domestic and regional travel up to 60% to meet client needs.\n\nEmployer will accept any suitable combination of education, training or experience.\n\nPlease apply on-line at ey.com/en_us/careers and click on \"Careers - Job Search”, then “Search Jobs\" (Job Number – 1499053).\n\nWhat We Offer\n\nWe offer a comprehensive compensation and benefits package where you’ll be rewarded based on your performance and recognized for the value you bring to the business. The base salary for this job is $168,850.00 per year. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options. Under our flexible vacation policy, you’ll decide how much vacation time you need based on your own personal circumstances. You’ll also be granted time off for designated EY Paid Holidays, Winter/Summer breaks, Personal/Family Care, and other leaves of absence when needed to support your physical, financial, and emotional well-being.\n\nContinuous learning: You’ll develop the mindset and skills to navigate whatever comes next.Success as defined by you: We’ll provide the tools and flexibility, so you can make a meaningful impact, your way.Transformative leadership: We’ll give you the insights, coaching and confidence to be the leader the world needs.Diverse and inclusive culture: You’ll be embraced for who you are and empowered to use your voice to help others find theirs.\n\nIf you can demonstrate that you meet the criteria above, please contact us as soon as possible.\n\nThe Exceptional EY Experience. It’s Yours To Build.\n\nEY | Building a better working world\n\nEY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.\n\nEnabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.\n\nWorking across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.\n\nEY is an equal opportunity, affirmative action employer providing equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law.\n\nEY is committed to providing reasonable accommodation to qualified individuals with disabilities including veterans with disabilities. If you have a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please call 1-800-EY-HELP3, type Option 2 (HR-related inquiries) and then type Option 1 (HR Shared Services Center), which will route you to EY’s Talent Shared Services Team or email SSC Customer Support at ssc.customersupport@ey.com\n\nThis particular position at Ernst & Young in the United States requires the qualified candidate to be a \"United States worker\" as defined by the U.S. Department of Labor regulations at 20 CFR 656.3. You can review this definition at https://www.gpo.gov/fdsys/pkg/CFR-2011-title20-vol3/pdf/CFR-2011-title20-vol3-sec656-3.pdf at the bottom of page 750. Please feel free to apply to other positions that do not require you to be a \"U.S. worker\"."}
{"text": "experience. As an employee, you’re empowered to show up every day as your most authentic self and be a part of something bigger – thriving both personally and professionally. Together, let’s empower people everywhere to live their healthiest lives.\n\nJoin the red-hot field of healthcare technology and enjoy the opportunity to apply technical ingenuity and machine learning expertise to optimize Teladoc Health’s trailblazing member solutions. As our Staff Machine Learning Engineer, your fascinating and effective new approaches to ML will directly impact the health and happiness of our members. Currently we’re working with models and approaches such as LSTMs, VAEs, Doc2Vec, contextual bandits, and reinforcement learning. With these models, your work will drive clinical results and behavior-change outcomes, while reducing healthcare costs. Armed with insightful data, you can personalize member updates and recommendations, while continually improving our platform features.\n\nLeveraging innovative, advanced technology, you’ll contribute to our ability to rapidly expand our product offering, reaching more people, and addressing more health issues. With ML you’ll uncover key insights into member health and behavior, enabling us to personalize the platform for individuals and provide direct, measurable benefits.\n\nEssential Duties And Responsibilities\n\nDesign, prototype and build machine learning systems, frameworks, pipelines, libraries, utilities and tools that process massive data for ML tasks Translate data science prototypes into scalable production implementations Partner with data scientists to troubleshoot and optimize complex data pipelines Deploy machine learning models into production Build model deployment platform that can simplify implementing new models Build end-to-end reusable pipelines from data acquisition to model output delivery Mentor and guide data scientists to deploy their models into production Design & Build ML (engineering) solutions that unlock new ML modeling capabilities for Teladoc Health Work with Scala, Python, Tensorflow, Keras to build real-world products using ML Collaborate closely on intriguing technical projects with data scientists, data engineers, product managers, design specialists, and clinical researchers Identify opportunities and propose new ways to apply ML to solve challenging technical and data engineering problems and thus improve business results Design, develop, deploy, and maintain production-grade scalable data transformation, machine learning, time series models and deep learning code, pipelines, and dashboards; manage data and model versioning, training, tuning, serving, experiment and evaluation tracking dashboards \n\nQualifications\n\n10+ years of full time experience with data and/or backend engineering or equivalent Strong knowledge of computer science fundamentals, including object oriented programming, data structures, and algorithms Experience integrating Machine Learning models in production (batch, streaming and online) Fluent in Machine Learning algorithms Expert in Python, Java, and/or Scala Expert in using offline data storage and processing frameworks such as Hadoop and Hive Superb written and oral communication skills Experience in writing data pipeline and machine learning libraries and utilities Industry experience building and productionizing innovative end-to-end Machine Learning systems Willingness to learn new technologies Willingness to mentor junior data scientists Comfortable in a high-growth, fast-paced and agile environment \n\nThe base salary range for this position is $140,000 - $190,000. In addition to a base salary, this position is eligible for a performance bonus and benefits (subject to eligibility requirements) listed here: Teladoc Health Benefits 2024. Total compensation is based on several factors including, but not limited to, type of position, location, education level, work experience, and certifications. This information is applicable for all full-time positions.\n\nWhy Join Teladoc Health?\n\nA New Category in Healthcare:  Teladoc Health is transforming the healthcare experience and empowering people everywhere to live healthier lives.  \n\nOur Work Truly Matters: Recognized as the world leader in whole-person virtual care, Teladoc Health uses proprietary health signals and personalized interactions to drive better health outcomes across the full continuum of care, at every stage in a person’s health journey.  \n\nMake an Impact: In more than 175 countries and ranked Best in KLAS for Virtual Care Platforms in 2020, Teladoc Health leverages more than a decade of expertise and data-driven insights to meet the growing virtual care needs of consumers and healthcare professionals.  \n\nFocus on PEOPLE:  Teladoc Health has been recognized as a top employer by numerous media and professional organizations. Talented, passionate individuals make the difference, in this fast-moving, collaborative, and inspiring environment. \n\nDiversity and Inclusion:  At Teladoc Health we believe that personal and professional diversity is the key to innovation. We hire based solely on your strengths and qualifications, and the way in which those strengths can directly contribute to your success in your new position.  \n\nGrowth and Innovation:  We’ve already made healthcare yet remain on the threshold of very big things. Come grow with us and support our mission to make a tangible difference in the lives of our Members. \n\nAs \n\nTeladoc Health respects your privacy and is committed to maintaining the confidentiality and security of your personal information. In furtherance of your employment relationship with Teladoc Health, we collect personal information responsibly and in accordance with applicable data privacy laws, including but not limited to, the California Consumer Privacy Act (CCPA). Personal information is defined as: Any information or set of information relating to you, including (a) all information that identifies you or could reasonably be used to identify you, and (b) all information that any applicable law treats as personal information. Teladoc Health’s Notice of Privacy Practices for U.S. Employees’ Personal information is available at this link."}
{"text": "skills and business acumen to drive impactful results that inform strategic decisions.Commitment to iterative development, with a proven ability to engage and update stakeholders bi-weekly or as necessary, ensuring alignment, feedback incorporation, and transparency throughout the project lifecycle.Project ownership and development from inception to completion, encompassing tasks such as gathering detailed requirements, data preparation, model creation, result generation, and data visualization. Develop insights, methods or tools using various analytic methods such as causal-model approaches, predictive modeling, regressions, machine learning, time series analysis, etc.Handle large amounts of data from multiple and disparate sources, employing advanced Python and SQL techniques to ensure efficiency and accuracyUphold the highest standards of data integrity and security, aligning with both internal and external regulatory requirements and compliance protocols\n\nRequired Qualifications, Capabilities, And Skills\n\nPhD or MSc. in a scientific field (Computer Science, Engineering, Operations Research, etc.) plus 6 years or more of experience in producing advanced analytics work with an emphasis in optimizationStrong proficiency in statistical software packages and data tools, including Python and SQLStrong proficiency in Advanced Statistical methods and concepts, predictive modeling, time series forecasting, text miningStrong proficiency in Data Mining & Visualization (Tableau experienced preferred)Experience in Cloud and Big Data platforms such as AWS, Snowflake, Hadoop, Hive, Pig, Apache Spark, etc.Strong story telling capabilities including communicating complex concepts into digestible information to be consumed by audiences of varying levels in the organizationStrong commitment to iterative development, with a proven ability to engage and update stakeholders bi-weekly or as necessary, ensuring alignment, feedback incorporation, and transparency throughout the project lifecycle.\n\nPreferred Qualifications, Capabilities, And Skills\n\nFinancial Service industry experience preferredExperience / Understanding of Cloud Storage (Object Stores like S3, Blob; NoSQL like Columnar, Graph databases) \n\nABOUT US\n\nChase is a leading financial services firm, helping nearly half of America’s households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.\n\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are \n\nEqual Opportunity Employer/Disability/Veterans\n\nAbout The Team\n\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We’re proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions – all while ranking first in customer satisfaction."}
{"text": "requirements and industry practices for mortgage banking.Build high-performance algorithms, prototypes, predictive models, and proof of concepts.Integrate new data management technologies and software engineering tools into existing structures.Create data tools for analytics and line of business that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.\nBenefits:Flexible scheduleFlexible spending accountPaid time offReferral programTuition reimbursement"}
{"text": "Qualifications:\n\nBachelor’s degree or higher in Computer Science, Data Science, Engineering, Mathematics, Applied Statistics, or related field.8 years of experience in building data science and machine learning solutions using Python, Scala, Spark DataBricks, SQL, or similar technologies.Experience in text GenAI & LLM.Deep understanding of probability, statistics, machine learning, anomalies/outliers’ detection, and data correlation/feature analysis.Strong problem-solving skills and algorithm design capabilities.Proficiency in Python coding and familiarity with relevant ML packages.\n\n\nMainz Brady Group is a technology staffing firm with offices in California, Oregon and Washington. We specialize in Information Technology and Engineering placements on a Contract, Contract-to-hire and Direct Hire basis. Mainz Brady Group is the recipient of multiple annual Excellence Awards from the Techserve Alliance, the leading association for IT and engineering staffing firms in the U.S.\n\nMainz Brady Group is"}
{"text": "QualificationsCurrently enrolled in a degree program in the United States and eligible for Optional Practical Training (OPT).Strong understanding of SQL with hands-on experience in writing complex queries.Proficiency in Python programming language.Familiarity with Pyspark or similar distributed computing frameworks is a plus.Solid grasp of data structures, algorithms, and software engineering principles.Excellent problem-solving skills and attention to detail.Ability to work independently as well as collaboratively in a team environment.Eagerness to learn new technologies and adapt to changing requirements."}
{"text": "skills and knowledge in a supportive and empowering environment.\nTechnology StackWe utilize the Google Cloud Platform, Python, SQL, BigQuery, and Looker Studio for data analysis and management.We ingest data from a variety of third-party tools, each providing unique insights.Our stack includes DBT and Fivetran for efficient data integration and transformation.\nKey ResponsibilitiesCollaborate with teams to understand data needs and deliver tailored solutions.Analyze large sets of structured and unstructured data to identify trends and insights.Develop and maintain databases and data systems for improved data quality and accessibility.Create clear and effective data visualizations for stakeholders.Stay updated with the latest trends in data analysis and technologies.\nQualifications and Skills2-3 years of hands-on experience in data.You can distill complex data into easy to read and interpret dashboards to enable leadership / business teams to gather data insights and monitor KPIs.Solid understanding of SQL and Python, along with experience in visualization tools.Basic familiarity with Looker and BigQuery.Basic familiarity with dbt or other data warehouse modeling methods.Strong problem-solving skills and a collaborative mindset.Must be authorized to work in the US. 👋 About UsParallel is the first tech-forward provider of care for learning and thinking differences across the United States. We believe learning differences are parallel ways of thinking that should be celebrated! Our mission is to provide students with the resources and encouragement to succeed in the classroom and beyond. To us, this means helping them build confidence in their unique strengths and create strategies to work around their challenges. Parallel simplifies the process of getting support for learning differences by consolidating providers and resources on a single platform. We connect students with qualified professionals while significantly reducing waiting times, costs, and confusion. We provide a variety of services, including: Psychological Assessment & TherapyCounselingSpeech-Language TherapySpecial EducationAnd more!"}
{"text": "experienced and innovative Data Engineer Manager with expertise in packaged goods products to lead our data engineering team. In this role, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the collection, processing, and analysis of large datasets related to packaged goods products. The ideal candidate will have a strong background in data engineering, cloud technologies, and database management, with a deep understanding of the complexities and challenges specific to the packaged goods industry.\n\nJob Description\n\nKey requirements and implement scalable solutions that meet business needs.Architect and implement cloud-based data platforms and infrastructure using technologies such as AWS, Azure, or Google Cloud Platform, ensuring scalability, reliability, and security.Design and implement data models, schemas, and database structures optimized for efficient data storage, retrieval, and processing in support of analytical use cases.Develop and implement data governance policies, data quality standards, and best practices to ensure data integrity, accuracy, and compliance with regulatory requirements.Evaluate and implement new technologies, tools, and frameworks to enhance data engineering capabilities and drive innovation in data processing and analytics.Establish and maintain data engineering standards, documentation, and best practices to support knowledge sharing and collaboration within the team and across the organization.Provide technical leadership, mentorship, and coaching to team members, fostering a culture of continuous learning and professional growth.Collaborate with external vendors, partners, and industry experts to leverage data engineering best practices and stay abreast of emerging trends and technologies in the packaged goods industry.Drive a culture of operational excellence, performance optimization, and continuous improvement in data engineering processes and practices.\n\nQualifications\n\nBachelor's degree in Computer Science, Engineering, Information Systems, or a related field.Proven 3+ years experience in data engineering, database management, and cloud computing, with a focus on packaged goods products or related industries.Strong proficiency in programming languages such as Python, SQL, or Scala, as well as experience with data processing frameworks such as Apache Spark or Hadoop.Expertise in cloud technologies and services, including AWS, Azure, or Google Cloud Platform, with hands-on experience in building and managing data pipelines and infrastructure.Solid understanding of data modeling, database design, and data warehousing concepts, with experience working with relational databases, NoSQL databases, and data lakes.Strong problem-solving, analytical, and troubleshooting skills, with the ability to diagnose and resolve complex data engineering challenges.Excellent communication and collaboration skills, with the ability to effectively communicate technical concepts to non-technical stakeholders.Proven leadership experience, with the ability to motivate, inspire, and mentor a team of data engineers to achieve excellence and drive results.Ability to thrive in a fast-paced, dynamic environment and manage multiple projects simultaneously.Passion for continuous learning and staying at the forefront of data engineering best practices and technologies in the packaged goods industry.\n\nWe offer a competitive benefits package!\n\n(*Eligibility may vary.)\n\n401(k) Savings PlanPremium Medical Insurance CoverageYear-end Bonus PlanPaid Time Off (PTO) based on seniorityPaid HolidaysOnsite Employee Fitness Center with Indoor Racquetball Court and Yoga RoomSummer FridayComplimentary Gourmet Breakfast, Lunch, and DinnerRelocation Support for New Hires*Work Anniversary RecognitionsCongratulatory & Condolence GiftsEmployee Referral Bonus ProgramLicense/Certification Reimbursements*Corporate Employee DiscountsVisa Sponsorships (100% paid by the company) i.e., New H-1B, H-1B Transfer, O-1, and Green CardCommuter Support (Shuttle Bus Program)*Vehicle Perks*\n\nThe anticipated compensation range is\n\n69,000.00 - 137,000.00 USD Annual\n\nActual compensation will be determined based on various factors including qualifications, education, experience, and location. The pay range is subject to change at any time dependent on a variety of internal and external factors.\n\nKiss Nail Products, Inc., or Kiss Distribution Corporation or Ivy Enterprises, Inc., or AST Systems, LLC, or Red Beauty, Inc., Dae Do, Inc. (collectively, the “Company”) is"}
{"text": "experience on Palantir Foundry8+ years in Data Engineering and must have at least 3-4 years of experience in Palantir Foundry•Strong experience with Palantir Data Engineering features such as, Code Repo, Code Workbook, Pipeline Build, Ontology Manager, migration techniques, Data Connection and Security setup"}
{"text": "experience in machine learning, distributed microservices, and full stack systems  Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake  Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community  Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment  Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance \n\nBasic Qualifications: \n\n Bachelor’s Degree  At least 4 years of experience in application development (Internship experience does not apply)  At least 1 year of experience in big data technologies \n\nPreferred Qualifications: \n\n 5+ years of experience in application development including Python, SQL, Scala, or Java  2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)  3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)  2+ year experience working on real-time data and streaming applications  2+ years of experience with NoSQL implementation (Mongo, Cassandra)  2+ years of data warehousing experience (Redshift or Snowflake)  3+ years of experience with UNIX/Linux including basic commands and shell scripting  2+ years of experience with Agile engineering practices \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.\nAscendion | Engineering to elevate life\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:Build the coolest tech for world’s leading brandsSolve complex problems – and learn new skillsExperience the power of transforming digital engineering for Fortune 500 clientsMaster your craft with leading training programs and hands-on experience\nExperience a community of change makers!\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\nAbout the Role:\nJob Title: Data Analyst\nLocation: Onsite in Seattle, WA\nJob Description:Extracts data from various databases; performs exploratory data analysis, cleanses, massages, and aggregates dataApplies basic statistical concepts and descriptive statistics to understand and describe relationships in dataBuilds predictive models and complex descriptive analytics, such as clustering and market basket analysisParticipates in discussions with business partners to define business questions and to consultCreates impactful visual representations of analytic insights and concise summaries of methodology geared to audience needs; presents selected portions to stakeholdersProvides analytic support (code documentation, data transformations, algorithms, etc.) to implement analytic insights and recommendations into business processes (e.g., automation of process to level up Lab analytics)Contributes to analytic project proposalsPromotes and advocates for value of analytics and data among peersProvides knowledge share and mentorship to team in databases, tools, access, data prep techniques\nBasic Qualifications:Ability to apply knowledge of multidisciplinary business principles and practices to achieve successful outcomes in cross-functional projects and activitiesExposure and business-applicable experience in several Modeling & Machine Learning Techniques (regression, tree models, survival analysis, cluster analysis, forecasting, anomaly detection, association rules, etc.)Exposure and business-applicable experience in several data ETL (Teradata, Oracle, SQL, Python, Java, Ruby, Pig)Experience with Azure, AWS Databricks preferredRetail, customer loyalty, and eCommerce experience, preferred\nSalary Range: The salary for this position is between $79,000 – $82,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]\nWant to change the world? Let us know.\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk"}
{"text": "experience provided by the support teams.\n\nPrimary Duties & Responsibilities\n\n Creates, develops, and maintains reports, datasets, dataflows, ad hoc requests, dashboards, metrics, etc. for end-users using system tools and databases.  Analyzes and determines data needs. Accesses, extracts, and integrates data from diverse sources residing on multiple platforms and implement data models by combining, synthesizing and structuring data.  Conduct data analysis and capture, develop, and document data definitions, business rules, and data quality requirements.  Ensure data integrity, consistency, and reliability by implementing quality assurance practices; performs quality data audits and analysis. Identify areas of improvement in data collection processes or systems and make recommendations to correct deficiencies.  Produce actionable reports that show key performance indicators, identify areas of improvement into current operations, and display root cause analysis of problems  Deep expertise in at least one business area or domain, with a broad understanding of the business and domains surrounding the main focus  Takes & applies design direction. Applies data visualization best practices to work deliverables.  Seeks to understand business process, user tasks, and as necessary, captures refined process documents  Implement best practices, methodologies, standards and processes and share across teams. Occasionally contribute to the development there of.  Takes initiatives to design and develop deliverables based on interpretation of findings and business client needs on a wide range of analytical topics  Provides consultation to business clients and may participate in cross-functional teams to address business issues  Contributes to the growth & development of the organization through actively sharing result & insights across the teams and with Business Clients  Identifies & captures business requirements, develops KPI frameworks  Regularly applies new perspectives, creative problem solving, and inter-departmental connections to improve analytical capabilities  Embrace continuous learning, curiosity, and ambiguity. \n\n Bring your best! What this role needs: \n\n Bachelor's degree in Computer Science, MIS, Mathematics, Statistics, Business or related field.  At least 4-5 years of professional experience  At least 4 years experience working in analytics related field  At least 3 years of hands-on experience doing analytics work  Expertise in visualization tool Power BI and relational data modeling techniques.  Expertise in development and application of analytical tools such as SAS, SQL, MS Excel, SPSS, R or other tool  Understanding of Service Management (e.g., Incident, Problem and Change Management)  Experience in Java or Javascript Development  Familiar with Cloud concepts  Familiar with Version Control such as Git  Familiar with CICD pipeline process  Demonstrated ability to deliver results and recommendations in written, verbal and presentation form at an appropriate level for a variety of business audiences.  Creative and innovative in problem solving and formulating solutions.  Proven excellence in problem solving, research, quantitative analysis, and analytical working techniques. \n\n Our Benefits! \n\n Collaborative team first environment  Tons of room for career growth.  We offer highly competitive compensation, including annual bonus opportunities  Medical/Dental/Vision plans, 401(k), pension program  We provide tuition reimbursement, commuter plans, and paid time off  We provide extensive Professional Training Opportunities  We offer an excellent Work/Life Balance  Hackathons/Dedication to Innovation \n\nCompensation Range\n\nPay Range - Start:\n\n$75,180.00\n\nPay Range - End\n\n$139,620.00\n\n Northwestern Mutual pays on a geographic-specific salary structure and placement in the salary range for this position will be determined by a number of factors including the skills, education, training, credentials and experience of the candidate; the scope, complexity as well as the cost of labor in the market; and other conditions of employment. At Northwestern Mutual, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. Please note that the salary range listed in the posting is the standard pay structure. Positions in certain locations (such as California) may provide an increase on the standard pay structure based on the location. Please click here for additiona   l information relating to location-based pay structures. \n\n Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now! \n\n We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law. \n\n If you work or would be working in California, Colorado, New York City, Washington or outside of a Corporate location, please click here for information pertaining to compensation and benefits. \n\nFIND YOUR FUTURE\n\nWe’re excited about the potential people bring to Northwestern Mutual. You can grow your career here while enjoying first-class perks, benefits, and commitment to diversity and inclusion.\n\nFlexible work schedulesConcierge serviceComprehensive benefitsEmployee resource groups"}
{"text": "experience, education, geographic location, and other factors. Description: This role is within an organization responsible for developing and maintaining a high-performance Advertising Platform across various online properties, including streaming services. The Ad Platform Research team focuses on transforming advertising with data and AI, seeking a lead machine learning engineer to develop prediction and optimization engines for addressable ad platforms.\nKey responsibilities include driving innovation, developing scalable solutions, collaborating with teams, and mentoring. Preferred qualifications include experience in digital advertising, knowledge of ML operations, and proficiency in relevant technologies like PyTorch and TensorFlow.\nBasic Qualifications:MS or PhD in computer science or EE.4+ years of working experience on machine learning, and statistics in leading internet companies.Experience in the advertising domain is preferred.Solid understanding of ML technologies, mathematics, and statistics.Proficient with Java, Python, Scala, Spark, SQL, large scale ML/DL platforms and processing tech stack.\nPreferred Qualifications:Experience in digital video advertising or digital marketing domain.Experience with feature store, audience segmentation and MLOps.Experience with Pytorch, TensorFlow, Kubeflow, SageMaker or Databricks.\nIf you are interested in this role, then please click APPLY NOW. For other opportunities available at Akkodis, or any questions, please contact Amit Kumar Singh at Amit.Singh@akkodis.com.\nEqual Opportunity Employer/Veterans/Disabled\nBenefit offerings include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits, and a 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs which are direct hire to a client.\nTo read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy."}
{"text": "experiences, and achieve superior results. Our associates are innovators who thrive through collaboration and are dedicated to excellence. At the heart of it all are the customers we serve. We are dedicated to creating fashion that not only looks good but also makes our customers feel good.\n\nThe impact you can have\n\nDesign and build data/ML products to solve challenging business problems.Develop cutting edge ML models that: predict customer behavior, forecast future demand, etc.Perform exploratory analysis and communicate results to our business partners.Be a thought leader within the department and larger analytics function.Work closely with business leaders to identify and design analytical solutions.Research and apply next-generation machine learning techniques.\n\nYou’ll bring to the role\n\n3+ years working in relevant field performing data analysis and building machine learning models.Education: MSc or PhD in a STEM FieldSolid foundation in Math, Statistics, or Computer Science.Strong communications skills for translating freely from business need to analytical approach to business recommendation.Strong programming skills in Python or R and SQL.\n\nBenefits At KnitWell Group\n\nYou will be eligible to receive a merchandise discount at select KnitWell Group brands, subject to each brand’s discount policies. Support for your individual development plus career mobility within our family of brands A culture of giving back – local volunteer opportunities, annual donation and volunteer match to eligible nonprofit organizations, and philanthropic activities to support our communities* Medical, dental, vision insurance & 401(K) retirement* Employee Assistance Program (EAP)Time off – paid time off & holidays*The target salary range for this role is: $95,000-120,000*Any job offer will consider factors such your qualifications, relevant experience, and skills. Eligibility of certain benefits and associate programs are subject to employment type and role.\n\nThis position works remotely. Occasional travel to a company office may be required.\n\nApplicants to this position must be authorized to work for any employer in the US without sponsorship. We are not providing sponsorship for this position.\n\nLocation:\n\nCorporate ascena – Remote\n\nPosition Type\n\nRegular/Full time\n\n\n\nThe Company is committed to hiring and developing the most qualified people at all levels. It is our policy in all personnel actions to ensure that all associates and potential associates are evaluated on the basis of qualifications and ability without regard to sex (including pregnancy), race, color, national origin, religion, age, disability that can reasonably be accommodated without undue hardship, genetic information, military status, sexual orientation, gender identity, or any other protected classification under federal, state, or local law. We do not discriminate in any of our employment policies and practices. All associates are expected to follow these principles in all relationships with other associates, applicants, or others with whom we do business."}
{"text": "skills, attention to detail, and the ability to work independently.\nQualificationsBachelor's degree in a relevant field such as Data Science, Statistics, Mathematics, or Computer ScienceProficient in data analysis tools and programming languages such as SQL, Python, and RExperience with data visualization tools such as Tableau or Power BIKnowledge of statistical methods and techniquesStrong problem-solving and critical thinking skillsExcellent communication and presentation skillsAbility to work independently and collaborate with cross-functional teamsAttention to detail and accuracy in data analysis\nPlease note that Fonetronics is"}
{"text": "experienced developer with a DevOps mindset who can bring an application from inception to production ensuring maintainability, quality, security and performance. The successful candidate will be expected to understand how to build, test, deploy and monitor enterprise grade applications using best practices.\n\nAn accomplished communicator both verbal and writtenUnderstanding of agile and software development life cycle conceptWork as part of the development team to break down high level requirements into small, testable, releasable components Create effective automated testsBe able to work on several projects simultaneouslyExperience of working in a team-orientated, collaborative environmentAbility to work to tight deadlinesProvide support for critical applications running in a production environmentMentor colleagues as neededHave fun! – contribute towards a productive yet fun team environment\n\n\nMinimum Requirements:\n\nTypically requires 10+ years of experience of relevant experience\n\nCritical Skills:\n\nIn depth knowledge/experience with the following:\n\nPython / JavaAzure CloudCI/CD using tools such as GitHub ActionsAutomated testingDocker and KubernetesRest APIsAuthentication and Authorization frameworksDatabases (relational and non-relational)Software development security fundamentals\n\n\nAdditional Experience:\n\nAzure AI servicesGenerative AI and Large Language ModelsInfrastructure as code e.g. TerraformaLinux adminScripting e.g. Bash\n\n\nEducation:\n\nBachelor's degree in a related field (e.g., Computer Science, Information Technology, Data Science) or equivalent experience\n\n\nWork Environment/Physical Demands:\n\nGeneral Office Requirements\n\nAt McKesson, we care about the well-being of the patients and communities we serve, and that starts with caring for our people. That’s why we have a Total Rewards package that includes comprehensive benefits to support physical, mental, and financial well-being. Our Total Rewards offerings serve the different needs of our diverse employee population and ensure they are the healthiest versions of themselves. For more information regarding benefits at McKesson, please click here.\n\nAs part of Total Rewards, we are proud to offer a competitive compensation package at McKesson. This is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. In addition to base pay, other compensation, such as an annual bonus or long-term incentive opportunities may be offered.\n\nOur Base Pay Range for this position\n\n$138,000 - $230,000\n\nMcKesson is an Equal Opportunity/Affirmative Action employer. \n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.Qualified applicants will not be disqualified from consideration for employment based upon criminal history.\n\nMcKesson is committed to being an \n\nCurrent employees must apply through the internal career site.\n\nJoin us at McKesson!"}
{"text": "Qualifications\n\n - Currently enrolled in a Bachelor’s or Master’s degree in Software Development, Computer Science, Computer Engineering, or a related technical discipline\n- Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employment.\n\nPreferred Qualifications: \n- Fluency in SQL or other programming languages (Python, R etc) for data manipulation\n- Ability to thrive in a fast paced work environment \n- Ability to drive projects to completion with minimal guidance\n- Ability to communicate the results of analyses in a clear and effective manner\n\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2\n\nBy submitting an application for this role, you accept and agree to our global applicant privacy policy, which may be accessed here: https://careers.tiktok.com/legal/privacy. \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $45 - $45annually. We cover 100% premium coverage for Full-Time intern medical insurance after 90 days from the date of hire. Medical coverage only, no dental or vision coverage.Our time off and leave plans are: Paid holidays and paid sick leave. The sick leave entitlement is based on the time you join.We also provide mental and emotional health benefits through our Employee Assistance Program and provide reimbursements for your mobile phone expense. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "Requirements\n\nJOB TITLE:  Financial and Data Analyst – Regulatory Compliance\n\nREQUISITION ID:  REG0019\n\nDEPARTMENT: Regulatory Affairs - Kansas City Headquarters or Topeka General Office\n\nLOCATION: Kansas City, MO\n\nTopeka, KS\n\nPAY RANGE: Regulatory Analyst I: $49,200 - $61,500\n\nRegulatory Analyst II: $60,200 - $75,300\n\nSr Regulatory Analyst: $76,400 - $101,800\n\nLead Regulatory Analyst: $89,900 - $119,900\n\nScheduled Work Hours: Monday - Friday, 8:00 a.m. – 5:00 p.m. (Other hours as required)\n\nOur team is currently looking for a cerebral Analyst who loves processing, transforming, and organizing big data into meaningful information to identify trends, outliers, and impacts to make business recommendations and process improvement. This individual will focus on assembling revenues from customer billing, usage, and cost data, understanding utility tariffs and rates to establish rate structures and set pricing for electric usage for Regulatory compliance.\n\nIf you are an individual who enjoys working in a high performing environment and thrives on adding value, this is the team for you! Our Regulatory department has offices in Topeka and Kansas City, and we offer a hybrid work schedule where team members work from home M/TH/F.\n\nSummary Of Primary Duties And Responsibilities\n\n Responsible for contributing or leading multiple regulatory projects through all phases in a timely, cost effective and efficient manner through collaboration with various departments within Evergy. Projects assigned vary greatly in size and complexity. Required to research and keep abreast of regulatory trends and maintain a working knowledge of the electric utility industry. Identifies and maintains a broad knowledge of the issues of importance to the Company; assimilate and evaluate information from a variety of sources and recommend strategy, solutions, or actions based on the analysis of the information and knowledge of technical and business principles and practices. Provide guidance to Company personnel concerning regulatory matters. Support Evergy regulatory initiatives through information gathering and analysis, leading to appropriate recommendations. Makes appropriate recommendations for changes/additions/deletions to existing rates, rules, and regulations or other regulatory matters. Develop internal and external working relationships to collaborate and meet Company goals and objectives. Ensures timely processing of information to meet regulatory requirements and minimize any negative impact on the Company; identifies priority issues; influences behavior to protect the best interests of Company’s customers and stakeholders and enhances corporate image.\n\nEducation And Experience Requirements\n\nRequires a degree at least as advanced as a Bachelor's degree in a business-related field, engineering, accounting, economics, quantitative analysis, or science. Advanced degree preferred. Professional certification (i.e. CPA, PE, etc.) is preferred.\n\nRegulatory Analyst II\n\n A minimum of 2 years of experience in the regulated utility industry is preferred.\n\nRegulatory Analyst Senior\n\n A minimum of 4 years of experience in the regulated utility industry is preferred. Knowledge of electric utility systems, accounting statements and financial concepts as used in a regulated electric utility environment is preferred.\n\nRegulatory Analyst Lead\n\n A minimum of 6 years of experience in the regulated utility industry is preferred. Knowledge of electric utility systems, accounting statements and financial concepts as used in a regulated electric utility environment is preferred.\n\nSkills, Knowledge, And Abilities Required\n\n Must be adaptable and have solid problem solving skills and be capable of researching issues using multiple sources of data and appropriate technical tools available; analyzing and synthesizing data gathered; transforming data gathered into information that can be used to facilitate decision making; drawing appropriate conclusions based on information and; making sound recommendations that consider the proper balance among the various interests of internal operations, customers, regulators, shareholders, and competitors that may be impacted by the recommendation. Must possess good written and verbal communications skills, capable of: listening to gather and synthesize information; speaking in a clear and concise manner to convey information often technical in nature; preparing written material that provides study results, reports, memos, testimony and documentation of subjects often technical in nature. Areas of proficiency include some or all of the following:Utilization of technology and Data Analytics software and concepts to handle big data and perform advanced analysisRegulated utility operation, construction, or maintenanceRetail cost of service, rate design, and state (KS & MO) regulationRates administration including rules & regulationsUtility billing and Commission generated customer complaint resolutionDevelopment, implementation, or maintenance of regulatory systemsPossesses a high degree of work initiative and ability to manage multiple projects.Must be proficient in the use of Microsoft Office applications and demonstrate the ability to learn other software products as necessary.\n\nLicenses, Certifications, Bonding, and/or Testing Required: None\n\nWorking Conditions: Normal office working conditions.\n\nEvergy has a clear mission – To empower a better future. Evergy chooses to live this mission by valuing People First. We know to accomplish becoming the most innovative and reliable energy company, we need a team with a variety of backgrounds, perspectives and skills. So, our goal is to be a diverse workforce that is representative of the communities we serve. We are committed to celebrating diversity and building a team where everyone – regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status – has a voice and can bring their best every day. We know that the more diverse and inclusive we are, the better we will be. Evergy is proud to be an equal opportunity workplace and is an affirmative action employer.\n\nDisclaimer: The above information has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.\n\nWork Locations\n\nTopeka General Office - Flr 10\n\nJob\n\nRegulatory\n\nUnposting Date\n\nApr 30, 2024"}
{"text": "Requirements\n\n Description and Requirements \n\n Role Value Proposition: \n\nWe are a centralized, enterprise-oriented group of data scientists whose goal is to provide mathematical and statistical based-insight to major decision processes at MetLife. This organization includes a specialist group focused on researching, developing, and deploying analytical methods that will help to transform MetLife. Specifically, this role will be working with the MetLife pet insurance group to automate and streamline the claims process and deliver valuable insights.\n\n Key Experience and Technical Skills: \n\n Required: \n\n Master’s or Doctorate in Quantitative Field (Physics, Math, Statistics, Operations Research, Engineering, Actuarial Science, etc.)  Atleast 3 years of experience as a data scientist/quantitative researcher/algorithmist (≥2 years for doctorate level)  Strong engineering background with programming proficiency in Python (version 3.).  High proficiency in classical statistical and mathematical modeling methods, including (but not restricted to) Regression, Classification, Clustering, Neural Networks, and Optimization.  Experience in cloud application development.  Familiarity with DevOps tools, unit testing, and CI/CD workflows, including Git.  Good written and oral communication, with the ability to communicate effectively with stakeholders.  Ability to navigate complexity and ambiguity to get things done in a fast-paced environment.  Mentor and provide oversight to junior associates while serving as an example by sharing relevant best practices. \n\n\n Preferred: \n\n Knowledge of actuarial science, insurance, or risk management modeling.  Experience working in Microsoft Azure, especially model deployment with Azure Machine Learning.  Experience in Computer Vision/Optical Character Recognition.  Experience with NLP tasks like entity recognition, clustering, and topic modeling.  Experience with GenAI prompt engineering. \n\n\n At MetLife, we’re leading the global transformation of an industry we’ve long defined. United in purpose, diverse in perspective, we’re dedicated to making a difference in the lives of our customers. \n\n The wage range for applicants for this position is $105,000-  $152,300. This role is also eligible for annual short-term incentive compensation. MetLife offers a comprehensive benefits program, including healthcare benefits, life insurance, retirement benefits, parental leave, legal plan services, and paid time off. All incentives and benefits are subject to the applicable plan terms."}
{"text": "skills and attention to detail. Job Duties: · Participation in user interviews to understand technical and customer needs.· Developing front end website architecture based on Palantir Foundry.· Designing user interactions on web pages within Palantir Foundry Workshop.· Developing back-end code logic that leverages semantic object linking (ontologies) within Palantir Foundry Pipeline Builder, Code Workbook, and Ontology Manager.· Creating servers, databases, and datasets for functionality as needed.· Ensuring health of data connections and pipelines (utilizing filesystem, JDBC, SFTP, and webhook).· Ensuring conformance with security protocols and markings on sensitive data sets.· Ensuring responsiveness of web applications developed on low code/no code solutions.· Ensuring cross-platform optimization for mobile phones.· Seeing through projects from conception to finished product.· Meeting both technical and customer needs.· Staying abreast of developments in web applications and programming languages.· Lead other engineers to develop features on your projects.  Job Requirements:· Bachelor Degree in Computer Science, Management Information Systems, Engineering or related field and 4 years Required· Strong knowledge in programming languages and coding principles and procedures.· Strong knowledge in web development framework.· Strong attention to detail, facilitation, team building, collaboration, organization, and problem-solving skills.· Excellent verbal and written communication skills.· Ability to work methodically and analytically in a quantitative problem-solving environment.· Effective written and oral communication skills.· Demonstrated critical thinking skills.· Strong knowledge in Microsoft Office Suite (Word, Excel, and PPT).· Ability to obtain applicable certifications.· Palantir Foundry experience preferred.· Proficiency with fundamental front-end languages such as HTML, CSS, and JavaScript preferred.· Familiarity with JavaScript libraries such as Lodash, Math.js, Moment, Numeral, and es6-shim preferred· Proficiency with server-side languages for structured data processing; Python, PySpark, Java, Apache Spark, and SparkSQL preferred.· Familiarity with database technology such as MySQL, Oracle, MongoDB, and others preferred.· Familiarity with analytical tools for business intelligence and data science such as Power BI, Jupyter, and R Studio preferred.· Strong organizational and project management skills preferred.· Team leadership experience preferred\nRandyTechnical RecruiterAce Technologies Incrandy@acetechnologies.com"}
{"text": "Qualifications and Experience:\n\nBachelor’s degree in data science, Statistics, or related field, or an equivalent combination of education and experience.Working knowledge of Salesforce.Ability to leverage enterprise data for advanced reporting.Proficiency in combining various data sources for robust output.Strong knowledge of Annuity products and distribution structure.Influencing skills and change management abilities.4-6 years of experience in financial services.Strong organizational skills.Proven success in influencing across business units and management levels.Confidence and ability to make effective business decisions.Willingness to travel (less. than 10%)\n\nDrive. Discipline. Confidence. Focus. Commitment. Learn more about working at Athene.\n\nAthene is a Military Friendly Employer! Learn more about how we support our Veterans.\n\nAthene celebrates diversity, is committed to inclusion and is proud to be"}
{"text": "Skills & Experience\n\nBachelor's/University degree. 10+ years of experience in finance/project management. Experience and proficiency building data pipelines and performing analytics using KNIME (or similar software). Experience creating team SharePoint sites and maintaining content to make information and documents easily accessible. Proficiency with Visual Basic for Applications (VBA) for Microsoft Office. Proficiency with SQL and relational database management systems. Strong proficiency with Microsoft Excel. Significant experience building end-user tools with Microsoft Access. \n\nDesired Skills & Experience\n\nExperience in using Lynx UI, Optima Cognos Reporting Tool, risk management (Facility Management, Collateral) and extracting data from Data Globe (especially data schemas: DGSTREAM, DGFU, DGREF & DGLOBE). Good understanding on loan data hierarchy (Request/Credit Agreement/Facility/GFRN) in Lynx. \n\nWhat You Will Be Doing\n\nCreate and maintain centralized SharePoint site and associated content for overall Data Remediation Transformation Program. Develop and maintain automated workflow tools to facilitate regulatory remediation efforts. Support BAU reporting & analytics processes. Support transformation and/or risk and control agenda/priorities for the larger team. Analysis and report on remediation progress/metrics to key stakeholders. Design and implement governance processes for escalation where required. Partners with cross function peers to create, administer, track, and eventually close projects. Integrate subject matter and industry expertise. Proactively identify emerging risks and assist the control team with resolving control gaps and issues and helps to create corrective action plans, inclusive of root cause identification and resolution. Actively focus on process improvement, creating efficiencies, and ensuring proactive approach and partnership to audit management. Ensure timely project tracking, status reporting and escalation. Develop and maintain RAID logs and general issue management, with appropriate escalation. Establish strong relationships with stakeholders across the company at all levels, businesses, and locations. \n\nPosted By: Melissa Klein"}
{"text": "experienced in DBT, Snowflake, and Azure - this is the role for you. We are looking for a Lead Data Engineer that has previous experience working in large, collaborative teams and are open to a contract position until the end of the year with high likelihood of extensions. Apply now!\nTHE COMPANYWe are currently partnered with a leading healthcare payments company that is at the forefront of healthcare transaction transparency through platforms that bridge the gap between financial systems and providers, consumers, and insurers - are you ready for this opportunity?\nTHE ROLEAs a Lead Data Engineer, you will:Design high-level technical solutions for streaming and batch processingDevelop reusable components for analytics data productsCollaborate on product feature implementationDrive new data projects and architectureEstablish CI/CD frameworks and lead design and code reviewsAnalyze data for scalable solutionsMentor offshore Data Engineers and partner with product and engineering management\nYOUR SKILLS AND EXPERIENCEA successful Lead Data Engineer in this role will have the following skills and experience:Snowflake (Columnar MPP Cloud data warehouse), including SnowparkDBT (ETL tool)PythonDesigning and implementing Data WarehouseHealthcare knowledge and experience working within healthcare provider dataMDM exp (TAMR preferred, it is nice to have)\nNICE TO HAVEExperience with Azure/AWS cloud technologyProficiency in SQL objects (procedures, triggers, views, functions) in SQL Server, including SQL query optimizationsUnderstanding of T-SQL, indexes, stored procedures, triggers, functions, views, etcDesign and development of Azure/AWS Data Factory Pipelines preferredDesign and development of data marts in Snowflake preferredWorking knowledge of Azure/AWS Architecture, Data Lake, Data FactoryBusiness analysis experience to analyze data, write code, and drive solutionsKnowledge of Git, Azure DevOps, Agile, Jira, and Confluence\nHOW TO APPLYPlease express your interest in this position by sending your resume via the Apply link on this page."}
{"text": "skills, attention to detail, and experience working with data in Excel. The candidate must enjoy collaborative work, actively participate in the development of team presentations, and engage in review of other analyst findings. ResponsibilitiesThe Junior Analyst will be responsible for examining data from different sources with the goal of providing insights into NHLBI, its mission, business processes, and information systems. Responsibilities for this position include:Develop a strong understanding of the organization, functions, and data sources to be able to ensure analytical sources and methodologies are appropriately applied for the data need.Develop clear and well-structured analytical plans.Ensure data sources, assumptions, methodologies, and visualization approaches are consistent with prior work by the OPAE.Assess the validity of source data and subsequent findings.Produce high quality, reliable data analysis on a variety of functional areas.Explain the outcome/results by identifying trends and creating visualizations.Use best practices in data analysis and visualization.Exhibit results, conclusions, and recommendations to leadership, and customize presentations to align with various audiences.Document and communicate analysis results (briefings, reports, and/or backup analysis files) in a manner that clearly articulates the approach, results, and data-driven recommendations.Continually assess all current activities and proactively communicate potential issues and/or challenges.May support data scientists on various projects. Qualifications Minimum qualifications:Bachelor’s degree in data science or related fields.Minimum of 2 years of demonstrable experience in data analysis.Must have 2 years of experience in using Excel for data analysis and visualization andWillingness to learn basic data science tools and methodologies.Intermediate to advanced proficiency with industry-standard word processing, spreadsheet, and presentation software programs.Excellent verbal and written communication skills.Strong attention to detail.Collaborative team player.Proven problem solving and critical thinking skills.Must be able to obtain Public Trust Clearance.US work authorization (we participate in E-Verify). Preferred qualifications:Proficient in the use of basic data science tools and methodologies (python, SQL, machine learning).MS in data science or related fields.\nSalary and benefitsWe offer a competitive salary and a generous benefits package, including full health and dental, HSA and retirement accounts, short- and long-term disability insurance, life insurance, paid time off and 11 federal holidays. Location: Washington DC, Hybrid"}
{"text": "skills in a global environment. Finally, you will interact with other members of our United States Health and Benefits team and can make important contributions to process improvements and new analytical tools.\n\nThis position requires an analytical mind who is detail oriented with work product and outputs using Microsoft Office tools. The position also requires the ability to accurately execute written and verbal instructions.\n\nThe Role\n\nManage NQTL Operational Data Portion Of Parity Assessment, Including\n\nPrepare NQTL carrier operational data requests on behalf of each client/carrierCoordinate with Project Manager regarding sending requests, timing, status, and follow-upAttend internal and client kick off meeting with QTL/NQTL team Monitor carrier and vendor responsiveness to data requestsValidate completeness of response and report any issues or impact to timeline proactively to Project ManagerComplete initial review of carrier responses for parity projectsMap carrier responses to appropriate section of NQTL documentCite specific reference, criteria, and example(s) for each responseDraft gap analysis based on documents reviewed including master templates and client specific responsesCollaborate with health management senior reviewer and Project Manager to conduct follow up calls/emails with carriers/vendors as needed for clarification and submission of additional documentationSupport the overall completion of the NQTL operational data component of the reportSchedule meetings and coordinate efforts of colleaguesParticipation in weekly or bi-weekly project status calls as neededParticipate in MHPAEA office hours as neededMaintain materials/documentation in a manner consistent with WTW work processes and peer review standards (e.g., filing of drafts, deliverables, client and carrier documentation)\n\nQualifications\n\nThe Requirements\n\n2-5 years’ experience dealing with health and welfare plans, ideally gained in a benefit consulting/brokerage firm, Health Plan, Third Party Administrator, Regulatory Agency, Legal/Compliance Firm, or insurance companyUS Registered Nurse or LCSW with active licensed preferredHealth analytics experience preferredUnderstanding of health plan utilization/case management processes, network management, credentialing, quality management, pharmacy prior-authorization processes and health plan operations Basic understanding of Mental Health Parity and Mental Health Parity Non-Qualitative Treatment LimitsWell-organized and detail orientedStrong project management and critical thinking skillsSelf- starter attitude and ability to work individually and as part of a teamStrong written and verbal communication skillsSense of accountability and ownershipFlexibility and proven ability to identify and resolve issuesStrong analytical, creative, and integrative skillsAbility to ask the right questions and seek help where appropriate, from colleagues and clients.Strong client service orientation and ability to respond to all communications effectively and in a timely mannerExcellent Microsoft Office skills, particularly in Excel and Power Point\n\n\n\nCompensation And Benefits\n\nBase salary range and benefits information for this position are being included in accordance with requirements of various state/local pay transparency legislation. Please note that salaries may vary for different individuals in the same role based on several factors, including but not limited to location of the role, individual competencies, education/professional certifications, qualifications/experience, performance in the role and potential for revenue generation (Producer roles only).\n\nCompensation\n\nThe base salary compensation range being offered for this role is $80,000 - 100,000 USD.\n\nThis role is also eligible for an annual short-term incentive bonus.\n\nCompany Benefits\n\nWTW provides a competitive benefit package which includes the following (eligibility requirements apply):\n\nHealth and Welfare Benefits: Medical (including prescription coverage), Dental, Vision, Health Savings Account, Commuter Account, Health Care and Dependent Care Flexible Spending Accounts, Group Accident, Group Critical Illness, Life Insurance, AD&D, Group Legal, Identify Theft Protection, Wellbeing Program and Work/Life Resources (including Employee Assistance Program)Leave Benefits: Paid Holidays, Annual Paid Time Off (includes paid state/local paid leave where required), Short-Term Disability, Long-Term Disability, Other Leaves (e.g., Bereavement, FMLA, ADA, Jury Duty, Military Leave, and Parental and Adoption Leave), Paid Time Off (Washington State only)Retirement Benefits: Contributory Pension Plan and Savings Plan (401k). All Level 38 and more senior roles may also be eligible for non-qualified Deferred Compensation and Deferred Savings Plans."}
{"text": "experienced Data Engineer to join our team in designing, developing, and optimizing data pipelines and ETL processes in the journey of Raymond James to develop a leading Wealth Management Platform. In this role, you will collaborate closely with business owners of multiple product areas, Enterprise data teams, and application development teams, leveraging your expertise in SQL, Oracle, stored procedures, SSIS, and Integration Services/ODI to ensure the seamless ingestion, integration, transformation, and orchestration of data. You will write complex stored procedures to curate data from central data stores and surface it in meaningful ways to business leaders and external partners to support daily operations and business intelligence. Your experience with complex ETL pipelines will be instrumental in creating a scalable, high-performance data environment. This role will follow a hybrid workstyle in the following location: Saint Petersburg, FL.\n\nEssential Duties and requirements and data integration needs, translate them into effective data solutions, and deliver top-quality outcomes. Implement and manage end-to-end data pipelines, ensuring data accuracy, reliability, data quality, performance, and timeliness. Collaborate with Data engineering and Development teams to design, develop, test, and maintain robust and scalable ELT/ETL pipelines using SQL scripts, stored procedures, and other tools and services. Work independently to prioritize and execute approved tasks to meet deadlines and budget constraints. Organize, manage, and track multiple detailed tasks and assignments with frequently changing priorities in a fast-paced work environment. Work closely with change management team to maintain a well-organized and documented repository of codes, scripts, and configurations using Azure. Proactively identify opportunities to automate tasks and develop reusable frameworks. Provide and maintain detailed documentation of all solutions. Collaborate with other technical teams to troubleshoot incidents as they occur. Establish and maintain effective working relationships at all levels of the organization, including negotiating resources. Perform other duties and responsibilities as assigned.\n\nQualifications:\n\nKnowledge of:\n\n Financial Services Industry knowledge is a plus. SSIS or ODI experience is essential.\n\nSkill in:\n\n Must have relevant experience in various database platforms, ETL solutions/products, ETL architecture. Expert-level experience with Oracle (or similar DB platforms), ETL architecture, and development. Expert level experience in Performance Optimization of ETL and Database (Oracle – SQL, PLSQL or similar) Expert-level experience with efficient Data Integration patterns/technologies. Experience with Oracle Data Integrator (ODI) or SQL Server Integration Services (SSIS). Experience with Tableau, Qlik Sense, Thought Spot, or other BI visualization tools. Strong coding and problem-solving skills, and attention to detail in data quality and accuracy. Proficiency in SQL programming and stored procedures for efficient data manipulation and transformation. Experience working with large-scale, high-volume data environments. Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases. Ability to navigate internal documentation to find appropriate resources to achieve success.\n\nEducation/Previous Experience\n\n TYPICALLY requires a Bachelor's degree; 3-5 years of relevant experience. May have one or more technical or business-related certifications.\n\nAt Raymond James our associates use five guiding behaviors (Develop, Collaborate, Decide, Deliver, Improve) to deliver on the firm's core values of client-first, integrity, independence and a conservative, long-term view.\n\nWe expect our associates at all levels to:\n\n Grow professionally and inspire others to do the same Work with and through others to achieve desired outcomes Make prompt, pragmatic choices and act with the client in mind Take ownership and hold themselves and others accountable for delivering results that matter Contribute to the continuous evolution of the firm\n\nAt Raymond James – as part of our people-first culture, we honor, value, and respect the uniqueness, experiences, and backgrounds of all of our Associates. When associates bring their best authentic selves, our organization, clients, and communities thrive. The Company is \n\nJob :\n\nTechnology\n\nPrimary Location :\n\nUS-FL-St. Petersburg-Saint Petersburg\n\nOrganization :\n\nTechnology\n\nSchedule :\n\nFull-time\n\nShift :\n\nDay Job\n\nTravel :\n\nYes, 5 % of the Time\n\nEligible for Discretionary Bonus : \n\nYes"}
{"text": "experienced and results-driven Data Engineering Manager to lead our data engineering team. As the Data Engineering Manager, you will play a pivotal role in architecting, building, and managing our data infrastructure and analytics platform. This is a hybrid role based in Charlotte, NC, offering the opportunity to work with a talented and diverse team.\nrequirements, prioritize initiatives, and align data strategy with business objectives.Work closely with business users to gather requirements, define data dictionaries, and ensure data quality and integrity in reporting solutions.Incorporate AI/ML capabilities into the data pipeline, leveraging machine learning algorithms and techniques to enhance data analysis and insights generation.\nQualifications:Bachelor's degree in Computer Science, Engineering, or related field; advanced degree preferred.Minimum of 7+ years of experience in data engineering or related roles, with a track record of success in building and managing data infrastructure and analytics platforms.Strong experience with modern data technologies and platforms. Experience building in an Azure-based platform, so previous experience with Azure Synapse and Data Factory is a requirement. Proven experience managing teams of data engineers, both onshore and offshore, with a focus on driving collaboration, innovation, and results.Excellent communication and interpersonal skills, with the ability to effectively engage with executives, business users, and technical teams.Experience working in the finance/insurance industry or InsurTech space is a plus.Familiarity with AI/ML concepts and techniques, with the ability to integrate machine learning capabilities into the data pipeline.\nWhat's in it for you? If you're a strategic leader with a passion for data engineering and a track record of success in building and managing data infrastructure, we invite you to apply for the Data Engineering Manager role. You will be rewarded a competitive salary, bonus & other benefits."}
{"text": "requirements and recommend solutions/improvements that enable the department to operate more efficiently and improve performance. This position will help oversee how changes are implemented regarding data, to help identify and troubleshoot issues during the build process."}
{"text": "skills to help establish routine reporting, conduct root cause analysis, and continuously improve data quality and processes.\nExperience in data analysis, problem-solving, or data scienceProficiency in Excel required, with experience in Tableau, SQL, or SAS preferred.Open to using various technologiesA mix of technical skills and the ability to learn supply chain domain knowledgeStrong communication and storytelling skillsEntrepreneurial mindset with flexibility to work in a dynamic environment\nSoft Skills Needed:Problem solving - Ability to creatively solve problems through data analysis.Curiosity - A curious nature and willingness to learn. Carter prioritizes this over experience.Entrepreneurial mindset - Comfort with ambiguity and willingness to work scrappy in a dynamic environment.Critical thinking - Ability to think critically about data and uncover insights.Communication - Comfort communicating findings to cross-functional teams.Adaptability - Openness to different perspectives and willingness to be influenced by new ideas.Go-getter attitude - Self-starter mentality who is comfortable wearing multiple hats.\nQualities of Successful Candidates:Carter is seeking a problem-solver first and foremost, not a supply chain expert. He prioritizes soft skills over industry experience.We are looking for a self-starter who is eager to take ownership of this role.This is an opportunity for hands-on experience working directly with a senior leader to help transform data and processes.The ideal candidate will be a creative problem-solver who thrives in an ambiguous environment.The data environment is dynamic and ambiguous with limited resources currently. Candidates should be comfortable with uncertainty."}
{"text": "experience:\n\nGS-15:\n\nData Science Strategy and Design  \n\nServes as an authority for scientific data analysis using advanced statistical techniques via the application of computer programs and/or appropriate algorithms to inform the program’s cybersecurity modernization strategies for automation and data driven authority to operate, ongoing continuous monitoring of security controls, and risk management.Determines appropriate data science products and/or services and collaborates with internal and external customers to define project scopes, requirements, and deliverables for a full array of data science functions to include: defining data requirements, implementing databases, analyzing data, developing data standards, building AI/ML models, etc. Develops, modifies, and/or provides input to project plans.\n\n\nApplied Data Science\n\nDevelops, administers, controls, coordinates, and executes assigned data science requirements, which requires technical expertise across the data life cycle (e.g., data collection, ingestion, storage, modeling, access, integration, analysis, and decision support). Uses analytic and statistical software to programmatically prepare data for analysis and clean imperfect data including structured, semi-structured, and unstructured sources such as vulnerability scans, configuration scans, the results from manual and automated control testing, and system security plans and other cybersecurity data and documentation.\n\n\nCloud Data Security Support\n\nSupports emerging IT and IT cybersecurity initiatives including but not limited to cloud computing, DevSecOps (i.e., development, security, and operations), continuous integration and continuous delivery, vulnerability management, and safe integration of emerging technology, ensuring related data needs are appropriately accounted for in the program's strategy.Maintains current knowledge and skill in cloud security, web application security, network architecture, and application development to conduct data science functions within the context of program operations.\n\n\nCustomer Communications and Reporting     \n\nTranslates complex business logic, analytic findings, and data limitations into concise, plain language reports or other materials such as visualizations and dashboards.Designs presentations and interpretations of analytical outputs tailored to specific audiences including the use of interactivity and narrative storytelling with data where appropriate. Collaborates with teammates, internal and external data consumers, and stakeholders in a reproducible and organized manner.\n\n\nRequirements\n\n Conditions of Employment\n\nUS Citizenship or National (Residents of American Samoa and Swains Island)Meet all eligibility requirements within 30 days of the closing date.Register with Selective Service if you are a male born after 12/31/1959\n\n\nIf selected, you must meet the following conditions:\n\nCurrent or Former Political Appointees: The Office of Personnel Management (OPM) must authorize employment offers made to current or former political appointees. If you are currently, or have been within the last 5 years, a political Schedule A, Schedule C or NonCareer SES employee in the Executive Branch, you must disclose this information to the HR Office. Failure to disclose this information could result in disciplinary action including removal from Federal Service..Undergo and pass a background investigation (Tier 4 investigation level).Have your identity and work status eligibility verified if you are not a GSA employee. We will use the Department of Homeland Security’s e-Verify system for this. Any discrepancies must be resolved as a condition of continued employment.\n\n\nQualifications\n\nFor each job on your resume, provide:\n\nthe exact dates you held each job (from month/year to month/year)number of hours per week you worked (if part time). \n\n\nIf you have volunteered your service through a National Service program (e.g., Peace Corps, Americorps), we encourage you to apply and include this experience on your resume.\n\nFor a brief video on creating a Federal resume, click here .\n\nThe GS-15 salary range starts at $143,736 per year.\n\nIf you are a new federal employee, your starting salary will likely be set at the Step 1 of the grade for which you are selected.\n\nAll candidates for Data Scientist positions must meet one of the following basic qualification requirements:\n\nBasic Requirement:\n\n Degree: Mathematics, statistics, computer science, data science or field directly related to the position. The degree must be in a major field of study (at least at the baccalaureate level) that is appropriate for the position.\n\n\nOR\n\n Combination of education and experience: Courses equivalent to a major field of study (30 semester hours) as shown above, plus additional education or appropriate experience.\n\n\nSpecialized Experience: In addition to meeting the basic requirements above, applicants must demonstrate that they have at least one year of specialized experience equivalent to the GS-14 level in Federal service. Specialized experience is defined as:\n\nImplementing and integrating appropriate technology, architecture, and tooling to support data science activities, including artificial intelligence/machine learning capabilities;Identifying data requirements and standards to support emerging IT and IT cybersecurity initiatives (e.g. cloud computing, DevSecOps, continuous integration and continuous delivery);Developing models that can identify quality, anomalies, and concerning trends in structured/semistructured/unstructured data to provide near real time feedback; andDeveloping tooling, models, and visualizations using general-purpose programming languages (such as Python) and/or tools optimized for statistical and data analysis (such as R).\n\n\nThis position has a positive education requirement: Applicants must submit a copy of their college or university transcript(s) and certificates by the closing date of announcements to verify qualifications. If selected, an official transcript will be required prior to appointment.\n\nAdditional Information\n\nBargaining Unit Status: This position is ineligible for the bargaining unit.\n\nRelocation-related expenses are not approved and will be your responsibility.\n\nOn a case-by-case basis, the following incentives may be approved:\n\n Recruitment incentive if you are new to the federal government Relocation incentive if you are a current federal employee Credit toward vacation leave if you are new to the federal government\n\n\nAdditional vacancies may be filled through this announcement in this or other GSA organizations within the same commuting area as needed; through other means; or not at all.\n\nGSA is committed to diversity, equity, inclusion and accessibility that goes beyond our compliance with \n\nValuing and embracing diversity, promoting equity, inclusion and accessibility, and expecting cultural competence; andFostering a work environment where all employees, customers and stakeholders feel respected and valued.\n\n\nOur commitment is:\n\nReflected in our policies, procedures and work environment;Recognized by our employees, customers and stakeholders; andDrives our efforts to recruit, attract, retain and engage the diverse workforce needed to achieve our mission."}
{"text": "experience in data platforms involving the implementation of end-to-end data pipelinesHands-on exp with the AZURE cloud data platformImplementation exp with column-oriented database technologies (Big Query, Redshift, Vertica), NoSQL database technologies (DynamoDB, BigTable, CosmosDB, Cassandra), and traditional database systems (i.e. SQL Server, Oracle, MySQL)Exp in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Spark, Spark Streaming, Google Cloud DataFlow, Azure Data Factory, etc.Exp in data modeling, warehouse design, and fact/dimension implementations.Bachelor's/ Masters degree in Computer Science, Engineering, or related field.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”\nSet Yourself Apart With:Exposure to a wide range of reporting and visualization tools, Python, DBTCertifications for any of the cloud services like AWS, GCP, or AzureExp working with code repositories and continuous integrationUnderstanding of development and project methodologiesWillingness to travel to office/client site when required (This is a Hybrid role with 3 days/week)\nAnnual Pay Ranges are listed below:\nSenior Associate Data Engineering L1: 95,000- 128,000 USDSenior Associate Data Engineering L2: 108,000- 145,000 USDThe range shown represents a grouping of relevant ranges currently used at Publicis Sapient. The actual range for this position may differ, depending on location and the specific skillset required for the work.\nBenefits of Working Here:Flexible vacation policyUnlimited PTO's15 company paid holidays annuallyWork Your World programGenerous parental leave and new parent transition programTuition reimbursementCorporate gift matching program401 (k)\nAll your information will be kept confidential according to"}
{"text": "Qualifications\n\n - Bachelor degree in Mathematics, Statistics, Computer Science, or Analytics \n- At least 3 years of Data Science experience\n- Causal Inference, Experimentation, Product Analytics, Machine Learning, and Statistics experience\n- SQL\n- Python or R\n\nPreferred Qualifications:\n- User Growth Experience\n- Advanced Degree (MS, PhD.) in Mathematics, Statistics, Analytics, etc \n- Business oriented. Have a strong business sense to proactively help UG Product and Operations identify key business challenges using data-driven insights.\n- Have strong curiosity and self-driving force, like to accept challenges, Aim for the Highest.\n- Have excellent communication skills, an open mind, and positive critical thinking\n- Solid technical & knowledge of A/B testing methodologies, can consistently explore and find the best practice\n- Insightful data sense and rigorous logical mindset, capable of providing systematic approaches to solve business problems;\n- End-to-end ownership: embrace the ownership mindset\n- Have a strong ability to work under pressure, have the courage to overcome difficulties, and accept challenges.\n\n\n\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\n\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $176355 - $329333 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "RequirementsWe are in search of a candidate with exceptional proficiency in Google Sheets.The ideal candidate will have an outstanding ability to manipulate, analyze, and extract business logic from existing reports, implement it in the new ones, and manage data within Google Sheets.A basic understanding of the finance and data domain is also required, as this knowledge will be essential in handling the data.Additionally, the candidate should possess a basic understanding of SQL for tasks related to data validation and metrics calculations.The role demands strong analytical skills, an eye for detail, and a commitment to delivering high-quality results. Compensation: $37.50 - $50.00 per hour"}
{"text": "Qualifications / Skills:5+ years of industry experience collecting data and building data pipelines.Degree in Computer Science or related fieldExpert knowledge of databases and SQLMastery of PythonExperience building data pipelines from end to end:Understanding business use cases and requirements for different internal teamsPrototyping initial collection and leveraging existing tools and/or creating new toolsBuilding and deploying enterprise-grade data pipelinesMaintenance of such pipelines with a focus on performance and data qualityExperience working with structured, semi-structured, and unstructured data.Experience with Azure Dev Ops or other cloud provider’s technology stack,Experience with code versioning and repository software.Experience being an active member of highly functional Agile teams.Ability to think critically and creatively in a dynamic environment, while picking up new tools and domain knowledge along the wayA positive attitude and a growth mindsetExcellent programming skills coupled with an understanding of software design patterns and good engineering practices.\nBonus Qualifications:Experience with Spark Python webapp development skills (Streamlit/Flask/Django/Dash)Experience using property, geospatial, and image data.Experience solving financial and risk domain problems."}
{"text": "Experience guiding strategic direction of workgroups, setting policy, while also having the skills to be an individual contributorSupport and train junior team members on tools and technical tasks to guide their learning while supporting the organizational analytic needs\n\n\nWho You Are\n\nMaster or PhD in Data Science (preferred) or in a quantitative field (Computer Science, Engineering, Statistics, Mathematics, Physics, Operation Research etc.)5+ years experience with a Master's or 3+ years professional experience with a PhDMust have hands-on experience in building models using classic statistical modeling techniques such as Logistic regression or advanced machine learning techniquesExperience in large data processing and handling is a plus - familiarity with big data platforms and applications such as Hadoop, Pig, Hive, Spark, AWS.Experience in data querying languages such as SQL, scripting/analytical languages such as Python/R.Deep understanding of machine learning/statistical algorithms such as XGBoostDemonstrated ability to frame business problems into mathematical programming problems, apply thought leadership and tools from other industries or academics to engineer a solution and deliver business insights.\n\n\nPreferred\n\nFinancial industry backgroundsKnowledge of the YouTube or Content Creator industry\n\n\nPersonal Attributes\n\nAbility to understand and analyze data and communicate findings to stakeholdersHigh level of comfort with quantitative conceptsStrong attention to detailBest-in-class analytical/critical/creative thinking skills Innovative and flexible approach to collaboration and developmentAbility to thrive in entrepreneurial environments with multiple projects and evolving priorities.Work independently/remotely with little oversightForge relationships at all levels of the company\n\n\nWhy Spotter\n\nMedical insurance covered up to 100%Dental & vision insurance401(k) matchingStock optionsAutonomy and upward mobilityDiverse, equitable, and inclusive culture, where your voice matters.\n\n\nSpotter is \n\nEqual access to programs, services and employment is available to all persons. Those applicants requiring reasonable accommodations as part of the application and/or interview process should notify a representative of the Human Resources Department."}
{"text": "Qualifications\n\nYou Have:\n\nMinimum of 3 years of hands-on data analysis experience in full-time professional, data-heavy, and machine learning focused roleStrong engineering and coding skills, with the ability to write production code. Proficiency in Python required, Java and/or other languages optionalExperience with Google Cloud Platform, Amazon Web Services or other cloud computing platformsExperience developing and deploying machine learning and statistical modelsStrong quantitative intuition and data visualization skills for ad-hoc and exploratory analysisThe versatility to communicate clearly with both technical and non-technical audiencesExperience with tree based models and gradient boosting is helpful but not required\n\n\nAdditional Information\n\nBlock takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.\n\nZone A: USD $163,600 - USD $245,400\n\nZone B: USD $155,400 - USD $233,200\n\nZone C: USD $147,300 - USD $220,900\n\nZone D: USD $139,000 - USD $208,600\n\nTo find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.\n\nFull-time employee benefits include the following:\n\nHealthcare coverage (Medical, Vision and Dental insurance)Health Savings Account and Flexible Spending AccountRetirement Plans including company match Employee Stock Purchase ProgramWellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance Paid parental and caregiving leavePaid time off (including 12 paid holidays)Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) Learning and Development resourcesPaid Life insurance, AD&D, and disability benefits \n\n\nThese benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.\n\nWe’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.\n\nWe believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible. Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our I+D page.\n\nAdditionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.\n\nWe’ve noticed a rise in recruiting impersonations across the industry, where individuals are sending fake job offer emails. Contact from any of our recruiters or employees will always come from an email address ending with @block.xyz, @squareup.com, @tidal.com, or @afterpay.com, @clearpay.co.uk.\n\nBlock, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.\n\nWhile there is no specific deadline to apply for this role, on average, U.S. open roles are posted for 70 days before being filled by a successful candidate."}
{"text": "experience.\n\nWe are looking for a highly energetic and collaborative Senior Data Engineer with experience leading enterprise data projects around Business and IT operations. The ideal candidate should be an expert in leading projects in developing and testing data pipelines, data analytics efforts, proactive issue identification and resolution and alerting mechanism using traditional, new and emerging technologies. Excellent written and verbal communication skills and ability to liaise with technologists to executives is key to be successful in this role. \n• Assembling large to complex sets of data that meet non-functional and functional business requirements• Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes• Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using GCP/Azure and SQL technologies• Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition• Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues• Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues• Strong background in data warehouse design• Overseeing the integration of new technologies and initiatives into data standards and structures• Strong Knowledge in Spark, PySpark, SQL, PL/SQL (Procedures, Function, Triggers, Packages and fixing the problems.)• Experience in Cloud platform(GCP/Azure) data migration – Source/Sink mapping, Build pipelines, work flow implementation, ETL and data validation processing• Strong verbal and written communication skills to effectively share findings with shareholders• Experience in Data Analytics, optimization, machine learning techniques or Python is added advantage• Good understanding of web-based application development tech stacks like Java, AngularJs, NodeJs is a plus•Key Responsibilities• 20% Requirements and design• 60% coding & testing and 10% review coding done by developers, analyze and help to solve problems• 5% deployments and release planning• 5% customer relations You bring:• Bachelor’s degree in Computer Science, Computer Engineering or a software related discipline. A Master’s degree in a related field is an added plus• 6 + years of experience in Data Warehouse and Hadoop/Big Data• 3+ years of experience in strategic data planning, standards, procedures, and governance• 4+ years of hands-on experience in Python or Scala• 4+ years of experience in writing and tuning SQLs, Spark queries• 3+ years of experience working as a member of an Agile team• Experience with Kubernetes and containers is a plus• Experience in understanding and managing Hadoop Log Files.• Experience in understanding Hadoop multiple data processing engines such as interactive SQL, real time streaming, data science and batch processing to handle data stored in a single platform in Yarn.• Experience in Data Analysis, Data Cleaning (Scrubbing), Data Validation and Verification, Data Conversion, Data Migrations and Data Mining.• Experience in all the phases of Data warehouse life cycle involving Requirement Analysis, Design, Coding, Testing, and Deployment., ETL Flow• Experience in architecting, designing, installation, configuration and management of Apache Hadoop Clusters• Experience in analyzing data in HDFS through Map Reduce, Hive and Pig• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.• Strong analytic skills related to working with unstructured datasets• Experience in Migrating Big Data Workloads• Experience with data pipeline and workflow management tools: Airflow• Experience with scripting languages: Python, Scala, etc.• Cloud Administration For this role, we value:• The ability to adapt quickly to a fast-paced environment• Excellent written and oral communication skills• A critical thinker that challenges assumptions and seeks new ideas• Proactive sharing of accomplishments, knowledge, lessons, and updates across the organization• Experience designing, building, testing and releasing software solutions in a complex, large organization• Demonstrated functional and technical leadership• Demonstrated analytical and problem-solving skills (ability to identify, formulate, and solve engineering problems) Overall Experience level:8-12 years in IT with min 6+ years of Data Engineering and Analyst experience."}
{"text": "experience Life at Visa.\n\nJob Description\n\nAbout the Team:\n\nVISA is the leader in the payment industry and has been for a long time, but we are also quickly transitioning into a technology company that is fostering an environment for applying the newest technology to solve exciting problems in this area. For a payment system to work well, the risk techniques, performance, and scalability are critical. These techniques and systems benefit from big data, data mining, artificial intelligence, machine learning, cloud computing, & many other advance technologies. At VISA, we have all of these. If you want to be on the cutting edge of the payment space, learn fast, and make a big impact, then the Artificial Intelligence Platform team may be an ideal place for you!\n\nOur team needs a Senior Data Engineer with proven knowledge of web application and web service development who will focus on creating new capabilities for the AI Platform while maturing our code base and development processes. You are a dedicated developer who can work and collaborate in a dynamic environment as a valued member of our Agile Scrum teams. You should have strong problem-solving abilities and be passionate about coding, testing and debugging skills. You know how to fill product backlog and deliver production-ready code. You must be willing to go beyond the routine and be prepared to do a little bit of everything.\n\nAs an integral part of the development team, you will sometimes explore new requirements and design, and at times refactor existing functionality for performance and maintainability. But the goal is always working on ways to make us more efficient and provide better solutions to our end customers. Flexibility and willingness to take on new tasks as needed are important to success. If this sounds exciting to you, we would love to discuss and tell you more about our work!\n\nEssential Functions:\n\nCollaborate with project team members (Product Managers, Architects, Analysts, Software Engineers, Project Managers, etc.) to ensure development and implementation of new data driven business solutionsDrive development effort End-to-End for on-time delivery of high quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standardsResponsibilities span all phases of solution development including collaborating with senior technical staff and PM to identify, document, plan contingency, track and manage risks and issues until all are resolvedPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner. \n\nThis is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.\n\nQualifications\n\nBasic Qualifications:\n\n 2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work\n\nExperience\n\nPreferred Qualifications:\n\n 3 or more years of work experience with a Bachelor’s Degree or more than 2 years of work\n\nexperience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n\n Exposure to leading-edge areas such as Machine Learning, Big Data, Distributed Systems, and/or Site Reliability Engineering.  Experience in at least one of the following: Golang, Java, or C/C++ Experience implementing solutions for low-latency, distributed services using open standard\n\ntechnologies.\n\n Familiarity with web service standards and related patterns (REST, gRPC).  Familiarity with fraud detection is a plus. \n\nAdditional Information\n\nWork Hours: Varies upon the needs of the department.\n\nTravel Requirements: This position requires travel 5-10% of the time.\n\nMental/Physical Requirements: This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.\n\nVisa is an \n\nVisa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of Article 49 of the San Francisco Police Code.\n\nU.S. APPLICANTS ONLY: The estimated salary range for a new hire into this position is 113,000.00 to 159,550.00 USD per year, which may include potential sales incentive payments (if applicable). Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. In addition, this position may be eligible for bonus and equity. Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program."}
{"text": "experience Must have experience with SAPProgramming Knowledge is a huge plus Documentation Skills is a must - data load & accuracy\n requirements and address data-related issues.\n Play a critical role in effectively managing and leveraging master data assets to support business processes, decision-making, and strategic initiatives.Title: Master Data Analyst Location: Houston, TexasClient Industry: Oil and Gas\n \nAbout Korn Ferry \nKorn Ferry unleashes potential in people, teams, and organizations. We work with our clients to design optimal organization structures, roles, and responsibilities. We help them hire the right people and advise them on how to reward and motivate their workforce while developing professionals as they navigate and advance their careers. To learn more, please visit Korn Ferry at www.Kornferry.com"}
{"text": "experience where customer success continues to motivate what is next.\n\nNetradyne is committed to building a world-class team of technologists and industry experts to deliver products that improve safety, increase productivity, and optimize collaboration within organizations. With growth exceeding 4x year over year, our solution is quickly being recognized as a significant disruptive technology – that has put ‘legacy’ providers in a “spin” cycle trying to catch up. Our team is growing, and we need forward-thinking, uncompromising, competitive team members to continue to facilitate our growth.\n\nDeep Learning Research Engineer\n\nWe are looking for a highly independent and self-driven Senior Research Engineer who is passionate about pushing the boundaries of deep learning research, to join our fast-growing technology team. This person should be able to work autonomously, think creatively, and explore new ideas and approaches to tackle complex problems in the field. You will have an opportunity to work with very large-scale real-world driving data. Netradyne analyzes over 100 million miles of driving data every month, covering over 1.25 million miles of US roads. This role provides a unique opportunity to work with cutting-edge technology and tackle complex problems in the field of deep learning using vast real-world datasets. The Deep Learning Research Engineer will have the chance to make a significant impact on road safety and advance the field of deep learning research. If you are driven by curiosity and have a passion for innovation, we encourage you to apply.\n\nResponsibilities\n\nDevelop and implement deep learning algorithms to extract valuable insights from large-scale real-world vision data.Design and commercialize algorithms characterizing driving behavior.Innovate and develop proof-of-concept solutions showcasing novel capabilities.\n\n\nRequirements\n\nPh.D. in Computer Science, Electrical Engineering, or a related field with publications in top conferences (CVPR/NeurIPs/ICML/ICLR).Strong background in deep learning, machine learning, and computer vision.Excellent programming skills – Python.Proficiency in PyTorch or TensorFlow.Experience with training large models with huge datasets.Ability to take abstract product concepts and turn them into reality.Location: San Diego, CA - Hybrid\n\n\nDesired Skills\n\nExperience with image, video, and time-series data.Experience with road scene understanding (objects, lanes, interactions, signs, etc.).Experience with person/driver scene understanding (pose, distracted, eye status etc.).Experience with Predictive analytics.\n\n\nOther Essential Abilities and Skills: \n\nStrong analytical and problem-solving skills.Excellent verbal and written communication skills.Energetic or passionate about AI.Ability to work independently and as part of a team.\n\n\nEconomic Package Includes:\n\nSalary $145,000- $180,000Company Paid Health Care, Dental, and Vision CoverageIncluding Coverage for your partner and dependentsThree Health Care Plan OptionsFSA and HSA OptionsGenerous PTO and Sick Leave401(K) Disability and Life Insurance Benefits$50 phone stipend per pay period\n\nSan Diego Pay Range\n\n$145,000—$180,000 USD\n\nWe are committed to an inclusive and diverse team. Netradyne is an equal-opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status, or any legally protected status.\n\nIf there is a match between your experiences/skills and the Company's needs, we will contact you directly.\n\nNetradyne is an equal-opportunity employer.\n\nApplicants only - Recruiting agencies do not contact.\n\nCalifornia Consumer Privacy Act Notice\n\nThis notice applies if you are a resident of California (“California Consumer”) and have provided Personal Information to Netradyne that is subject to the California Consumer Privacy Act (“CCPA”). We typically collect Personal Information in the capacity of a service provider to our clients, who are responsible for providing notice to their employees and contractors and complying with CCPA requirements.\n\nDuring the past 12 months, we have collected the following categories of Personal Information: (a) identifiers; (b) biometric information (see our Biometric Data Privacy Policy for more information); (c) Internet or other electronic network activity information; (d) geolocation data; (e) Audio, electronic, visual, thermal, olfactory, or similar information; (f) professional or employment-related information (from job applicants and from clients regarding their employees and contractors); and (g) education information (from job applicants). We will not discriminate against any person that exercises any rights under the CCPA.\n\nWe have collected this Personal Information for the business purposes and commercial purposes described in this Policy, including to provide the Services to our clients, process job applications, and for marketing and promotion.\n\nThe sources of such Personal Information are you, our clients and our service providers. We have shared this information this only with our clients (if you are an employee or contractor of them) or our service providers.\n\nIf you are a California Consumer, you have the following rights under the CCPA:\n\nYou have the right to request:The categories and specific pieces of your Personal Information that we’ve collected;The categories of sources from which we collected your Personal Information;The business or commercial purposes for which we collected or sold your Personal Information; andThe categories of third parties with which we shared your Personal Information.You can submit a request to us for the following additional information:The categories of third parties to whom we’ve sold Personal Information, and the category or categories of Personal Information sold to each; andThe categories of third parties to whom we’ve disclosed Personal Information, and the category or categories of Personal Information disclosed to each.You can request that we delete the Personal Information we have collected about you, except for situations when that information is necessary for us to: provide you with a product or service that you requested; perform a contract we entered into with you; maintain the functionality or security of our systems; comply with or exercise rights provided by the law; or use the information internally in ways that are compatible with the context in which you provided the information to us, or that are reasonably aligned with your expectations based on your relationship with us.You have the right to request that we not sell your Personal Information. However, we do not offer this opt-out as we do not sell your Personal Information as that term is defined under the CCPA.\n\nYou can make a request under the CCPA by e-mailing us at privacy@netradyne.com We may request additional information from you to verify your identify. You may also designate an authorized agent to submit a request on your behalf. To do so, we will require either (1) a valid power of attorney, or (2) signed written permission from you. In the event your authorized agent is relying on signed written permission, we may also need to verify your identity and/or contact you directly to confirm permission to proceed with the request.\n\nAs noted above, if your request concerns Personal Information collected in our capacity as a service provider to a client, we are not responsible for responding to the request and may send the request to the client for a response.\n\nGoverning law\n\nThis Services are provided in the United States, and are located and targeted to persons in the United States and our policies are directed at compliance with those laws. If you are uncertain whether this Policy conflicts with the applicable local privacy laws where you are located, you should not submit your Personal Information to Netradyne."}
{"text": "requirements and design/ maintain/ optimize data pipeline to ingest, transform, and load structured and unstructured data from various sources into the data warehouse or data lake.Design and implement data models and schemas to support analytical and reporting requirements.Collaborate with data scientists and analysts to define and structure data for effective analysis and reporting.Develop and maintain ETL (Extract, Transform, Load) processes.Administer, optimize, and manage databases, data warehouses, and data lakes to ensure performance, reliability, and scalability.Enforce data governance policies, standards, and best practices to maintain data quality, privacy, and security.Create and maintain comprehensive documentation for data architecture, processes, and systems.Troubleshoot and resolve data-related problems and optimize system performance.Partner with IT support team on production processes, continuous improvement, and production deployments.\n\nYOU MUST HAVE\n\nBachelor’s degree from an accredited institution in a technical discipline such as the sciences, technology, engineering or mathematicsTwo or more years of relevant experience in Data Engineering, ETL Development, Database Administration.Experience in Azure Databricks, CI/CD & Dev Ops ProcessExpert in scripting and querying languages, such as Python, SQL, PySparkExperience with both Structured and Unstructured dataSFDC business/ technical knowledgeKnowledge of Agile development methodology\n\nWE VALUE\n\nWorking with at least one NoSQL system (HBase, Cassandra, MongoDB)Knowledge of databases, data warehouse platforms (Snowflake) and Cloud based tools.Experience in using data integration tools for ETL processes.Knowledge of Data Modelling techniques including schema design for both rational and NoSQL databasesUnderstanding of Hadoop's ecosystem (including HDFS) and Spark for processing and analyzing large-scale datasets.Demonstrated experience in cutting-edge packages such as SciKit, TensorFlow, Pytorch, GPT, PySpark, Bit bucket etc.Ability to develop and communicate technical vision for projects and initiatives that can be understood by customers and management.Proven mentoring ability to drive results and technical growth in peers.Effective communication skills (verbal, written, and presentation) for interacting with customers and peers.Demonstrated application of statistics, statistical modeling, and statistical process control.\n\nAdditional Information\n\nJOB ID: HRD228162Category: EngineeringLocation: 855 S Mint St,Charlotte,North Carolina,28202,United StatesExempt\n\nEngineering (EMEA)\n\nHoneywell is"}
{"text": "requirements.  May assist in proposal development. \n\nRequired Skills\n\n Bachelor's degree  4 years of experience in positions of increasing responsibility. This work should include working with large datasets (e.g., data mining), and conducting a variety of analytics (including but not limited to techniques, such as statistical analysis, clustering, segmentation, machine learning, natural language processing, and GIS). The experience should include a strong emphasis on programming.  Knowledge in at least one of the following programming languages: R, Python, SAS, Stata.  Strong foundation in areas of statistics, machine learning, and research methods.  Working knowledge of different types of data that can be collected, e.g., social media, administrative, webpages, survey, and/or sensor data.  Strong problem-solving skills.  Ability to organize and prioritize work assignments to meet project needs.  Strong written and verbal communication, including strong technical writing skills.  Able to explain technology, techniques, and approaches to expert and non-expert audiences. \n\nDesired Expertise And Skills\n\n Expertise conducting data science work in at least one of NORC’s research domains.  Experience using and developing tools for reproducibility, e.g., R Markdown, Quarto, Git, package development.  Expertise in R or Python  Knowledge in multiple of the following programming languages: R, Python, Stata, SAS, SQL  Experience in at least one of the following areas: natural language processing, GIS, and data visualization (especially interactive data visualization) \n\nSalary And Benefits\n\nThe pay range for this position is $88,000 – $157,000.\n\nBenefits\n\nThis position is classified as regular. Regular staff are eligible for NORC’s comprehensive benefits program. Benefits include, but are not limited to:\n\n Generously subsidized health insurance, effective on the first day of employment  Dental and vision insurance  A defined contribution retirement program, along with a separate voluntary 403(b) retirement program  Group life insurance, long-term and short-term disability insurance  Benefits that promote work/life balance, including generous paid time off, holidays; paid parental leave, tuition assistance, and an Employee Assistance Program (EAP). \n\nNORC’s Approach to Equity and Transparency\n\nPay and benefits transparency helps to reduce wage gaps. As part of our commitment to pay equity and salary transparency, NORC includes a salary range for each job opening along with information about eligible benefit offerings. At NORC, we take a comprehensive approach to setting salary ranges and reviewing raises and promotions, which is overseen by a formal Salary Review Committee (SRC).\n\nWhat We Do\n\nNORC at the University of Chicago is an objective, non-partisan research institution that delivers reliable data and rigorous analysis to guide critical programmatic, business, and policy decisions. Since 1941, our teams have conducted groundbreaking studies, created and applied innovative methods and tools, and advanced principles of scientific integrity and collaboration. Today, government, corporate, and nonprofit clients around the world partner with us to transform increasingly complex information into useful knowledge.\n\nWho We Are\n\nFor over 75 years, NORC has evolved in many ways, moving the needle with research methods, technical applications and groundbreaking research findings. But our tradition of excellence, passion for innovation, and commitment to collegiality have remained constant components of who we are as a brand, and who each of us is as a member of the NORC team. With world-class benefits, a business casual environment, and an emphasis on continuous learning, NORC is a place where people join for the stellar research and analysis work for which we’re known, and stay for the relationships they form with their colleagues who take pride in the impact their work is making on a global scale.\n\n\n\nNORC is an affirmative action, equal opportunity employer that values and actively seeks diversity in the workforce. NORC evaluates qualified applicants without regard to race, color, religion, sex, national origin, disability, veteran status, sexual orientation, gender identity, and other legally- protected characteristics."}
{"text": "requirements.Implement vendor-developed models for consumer and commercial credit loss or prepayment.Monitor performance of quantitative models and support independent model validation efforts in accordance with the model risk management policy.Establish and document model implementation controls that satisfy model risk management, audit, and regulatory requirements.\n\nQualifications for the Jr. Data Scientist include: \n\n3+ years of experience in quantitative modeling, development, or implementation.Working experience in data manipulation and advanced data analysis.Experience with SAS, R, Python, and proficiency working with large datasets is required.Applied experience with Logistic Regression, Linear Regression, Survival Analysis, Time Series Analysis, Decision Trees, and Cluster Analysis.Experience in at least one (1) of the following areas is preferred; real estate products, auto, credit card, student loan, or commercial loan.\n\nCompensation for the Jr. Data Scientist include:\n\nHourly Pay: $40 - $45/hr -- Salary Range: $90,000-$110,000 + Bonus Package **depending on experience** Full benefits: PTO, Paid Holidays, Cigna Healthcare, Dental, Vision, 401k with ADPTS\n\nKeywords:\n\nData, data science, R, sas, python, datasets, logistic regression, linear regression, survival analysis, time series analysis, decision trees, cluster analysis, real estate, auto, credit card, consumer lending, consumer banking, modeling, model implementation, model risk, documentation, prepayment risk, financial services,"}
{"text": "requirements and ensure data qualityEnsure the security and privacy of sensitive data by implementing appropriate access controlsMonitor and optimize data pipeline performance to ensure timely and accurate data deliveryDocument data pipeline processes, data dictionaries, and data storage solutions\nRequirements:Bachelor's degree in Computer Science, Computer Engineering, or a related technical fieldMinimum of five years of professional experience working as a Data Engineer or Software DeveloperStrong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similarProficient in at least one scripting language such as Python, JavaScript, or RUnderstanding of data modeling, data integration and data quality processesFamiliarity with cloud platforms such as AWS, Azure, or Google Cloud PlatformStrong analytical and problem solving skillsFull Stack Software Development experience in a professional setting is highly desired, but not required\nThis is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!"}
{"text": "SKILLS:Google Pub/Sub - yearsBigQuery - yearsGoogle Dataform – yearsData ingestion to Big Query experience - yearsGoogle Cloud Storage - yearsCloud Composer - yearsFusion Cloud - yearsGitHub - yearsSQL - years"}
{"text": "Qualifications:• 10+ years of experience • Experience and proficiency building data pipelines and performing analytics using KNIME (or similar software)• Experience creating team SharePoint sites and maintaining content to make information and documents easily accessible• Proficiency with Visual Basic for Applications (VBA) for Microsoft Office• Proficiency with SQL and relational database management systems• Strong proficiency with Microsoft Excel• Significant experience building end-user tools with Microsoft Access• Experience in using Lynx UI, Optima Cognos Reporting Tool, (Facility Management, Collateral) and extracting data from Data Globe (especially data schemas: DGSTREAM, DGFU, DGREF & DGLOBE)Good understanding on Loan data hierarchy (Request/Credit Agreement/Facility/GFRN) in Lynx."}
{"text": "experience in data science focused on data for client studies, modelling, EDA, data wrangling, ETL, and/or ML/AIFluency in Python, R, and/or other computing languagesFamiliarity with SQL and relational databasesExperience with a variety of Business Intelligence tools including Tableau and PowerBIProven and dynamic leadership capabilities; training relevant staff on technical data skillsProven ability to work directly with clients in complex analyses and presenting deliverables to non-technical personnelAbility to travel as necessary to meet project and client requirementsDemonstrated ability to work autonomously and be self-directedProven ability to work within a collaborative team environment, excellent communication skills, and coordinate activities between program components\n\n\nAt Jacobs, we’re challenging today to reinvent tomorrow by solving the world’s most critical problems for thriving cities, resilient environments, mission-critical outcomes, operational advancement, scientific discovery and cutting-edge manufacturing, turning abstract ideas into realities that transform the world for good. With $15 billion in revenue and a talent force of more than 60,000, Jacobs provides a full spectrum of professional services including consulting, technical, scientific and project delivery for the government and private sector."}
{"text": "skills - SQL, data analysis, support Data team members with build-out tasksIdentify impacts to data conversion and/or maintenance within cross-team discussions (priority areas: MFG/SCM/FIN) and coordinate mitigation/follow-upLead the design and implementation of an SAP project deliverables including functional and technical specifications, testing, training and go-live supportAdvise on best practices in architecting, designing & supporting S/4 PP, MM, EWM, and ECC SCM applicationsAnalyze, recommend, plan, design, develop, and/or implement solutions to meet strategic, usability, performance, reliability, control, and security requirementsMust be self-motivated with capability to work independentlyAbility to lead, organize, and balance multiple projects/prioritiesIdentify design gaps in proposed solutions and advise of solutionsExperience with ECC/S4 field mappingExperience conducting Data Migrations and Conversions\nRequirements:  o Broad knowledge of SAP ECC/S4 functionality and common standard table structuresStrong technical skills - SQL, data analysis, support Data team members with build-out tasksIdentify impacts to data conversion and/or maintenance within cross-team discussions (priority areas: MFG/SCM/FIN) and coordinate mitigation/follow-upLead the design and implementation of an SAP project deliverables including functional and technical specifications, testing, training and go-live supportAdvise on best practices in architecting, designing & supporting S/4 PP, MM, EWM, and ECC SCM applicationsAnalyze, recommend, plan, design, develop, and/or implement solutions to meet strategic, usability, performance, reliability, control, and security requirementsMust be self-motivated with capability to work independentlyAbility to lead, organize, and balance multiple projects/prioritiesIdentify design gaps in proposed solutions and advise of solutionsExperience with ECC/S4 field mappingExperience conducting Data Migrations and Conversions"}
{"text": "Skills / Experience:\nRequired: Proficiency with Python, pyTorch, Linux, Docker, Kubernetes, Jupyter. Expertise in Deep Learning, Transformers, Natural Language Processing, Large Language Models\nPreferred: Experience with genomics data, molecular genetics. Distributed computing tools like Ray, Dask, Spark."}
{"text": "Qualifications: Bachelor's degree in Computer Science, Information Technology, or a related field.10+ years of experience in data warehouse architecture and development.Proven expertise in Microsoft Azure Data Services (ADLS, Synapse Analytics, Data Factory).Strong understanding of data warehousing concepts, data modeling, ETL/ELT processes, and big data platforms.Experience with data integration techniques, self-service data preparation, and DevOps tools (Azure DevOps, Jenkins, etc.).Excellent communication and presentation skills to collaborate effectively with technical and non-technical stakeholders.Strong analytical skills and a passion for learning new technologies.Ability to work independently and as part of a team, prioritizing workload effectively."}
{"text": "experience in machine learning, distributed microservices, and full stack systems  Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake  Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community  Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment  Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance \n\nBasic Qualifications: \n\n Bachelor’s Degree  At least 4 years of experience in application development (Internship experience does not apply)  At least 1 year of experience in big data technologies \n\nPreferred Qualifications: \n\n 5+ years of experience in application development including Python, SQL, Scala, or Java  2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)  3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)  2+ year experience working on real-time data and streaming applications  2+ years of experience with NoSQL implementation (Mongo, Cassandra)  2+ years of data warehousing experience (Redshift or Snowflake)  3+ years of experience with UNIX/Linux including basic commands and shell scripting  2+ years of experience with Agile engineering practices \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nNew York City (Hybrid On-Site): $165,100 - $188,500 for Senior Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "experiences, revenue generation, ad targeting, and other business outcomes.Conduct data processing and analysis to uncover hidden patterns, correlations, and insights.Design and implement A/B testing frameworks to test model quality and effectiveness.Collaborate with engineering and product development teams to integrate data science solutions into our products and services.Stay up-to-date with the latest technologies and techniques in data science, machine learning, and artificial intelligence.\nTechnical Requirements:Strong proficiency in programming languages such as Python or R for data analysis and modeling.Extensive experience with machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests, etc.).Experience with data visualization tools (e.g., Matplotlib, Seaborn, Tableau).Familiarity with big data frameworks and tools (e.g., Hadoop, Spark).Proficient in using query languages such as SQL.Experience with cloud computing platforms (AWS, Azure, or Google Cloud) is a plus.Understanding of software development practices and tools, including version control (Git).\nExperience:3+ years of experience in a Data Scientist or similar role.Demonstrated success in developing and deploying data models, algorithms, and predictive analytics solutions.Experience working with large, complex datasets and solving analytical problems using quantitative approaches.\nWho You Are:Analytically minded with a passion for uncovering insights through data analysis.Creative problem solver who is eager to tackle complex challenges.Excellent communicator capable of explaining complex technical concepts to non-technical stakeholders.Self-motivated and able to work independently in a remote environment.A collaborative team player who thrives in a dynamic, fast-paced setting.\nJoin Us:At RTeams, you'll be part of an innovative company that values the transformative power of data. Enjoy the flexibility of remote work across the US, with standard working hours that support work-life balance. Here, we believe in empowering our team members to innovate, explore, and make a significant impact."}
{"text": "experience. The team’s mission is to build cutting edge software applications and data models that generate proprietary investment insights and provide the investment team with tools that augment the investment decision making process.\nAbout the Role:Develop features and machine learning models that augment the Firm’s investment decision making processWork collaboratively with machine learning engineers and software engineers to build, deploy, monitor, and maintain machine learning modelsWork collaboratively with team members to promote technical rigor and adopt best practicesCollaborate with data scientists, engineers, and other stakeholders in translating project requirements into technical specificationsYou will help shape the future of software engineering at Valor by bringing your ideas on improving and automating what we do and how we do it\nWe’re excited about candidates that have:B.S. and/or M.S. in Computer Science, Applied Mathematics, Statistics, or related field, especially with coursework in machine learning2+ years of machine learning, data science, and/or statistical modeling experience, with significant contributions that you can talk toExceptional coding skills in Python and SQL, to include common Python libraries like Pandas, Scikit-Learn, PyTorch, and/or TensorFlowExperience with any of the following:Time-series modelingGraph-based modelingSupervised learning, especially boosted tree algorithms such as XGBoost and LightGBMNatural Language Processing (incl. LLMs)\nAdditionally, experience with any of the following is a bonus:Experience with deploying and monitoring machine learning modelsExperience with Docker and GPU-based infrastructureExperience with modern cloud platforms (AWS, Azure, or GCP)Modern data pipeline experienceBig Data processing (Spark, PySpark, Scala, Dask)Passion for machine learning while being mission-driven, hard-working, humble, intellectually curious, and most importantly, great team playersBias for execution and delivery. You know that what matters is delivering software that works every timeAbility to assist in system design and the generation of key technical assumptions while encouraging solutions that respect existing infrastructureWillingness to be resourceful, flexible, and adaptable; no task is too big or too small\nOur Tech Stack:Frontend: React with Hooks, Material UIBackend: Python, Fast APITooling: Google Cloud PlatformData: PostgreSQL, Firestore, BigQuery, Elastic Search, Prefect, Kafka, Scala, Spark, dbt"}
{"text": "experience with artificial intelligence, NLP, language models and advanced analytics is matched by a passion to build great products, lead innovation, be a mentor and guide to other Engineering team members. In the past you have been part of a startup or corporate innovation team working in fast-moving environments. You can point to numerous examples which have demonstrated your ability to creativity solve technical challenges.\n\nA Pioneer in the Fintech, Intralinks is a 27-year-old company. 1/3 of the world’s M&A runs on our Virtual Data Room product, $1 of every $2 dollars of private equity is raised through our Fund raising and reporting solutions.\n\nWhy You Will Love It Here! \n\nFlexibility: Hybrid Work Model & a Business Casual Dress Code, including jeansYour Future: 401k Matching Program, Professional Development ReimbursementWork/Life Balance: Flexible Personal/Vacation Time Off, Sick Leave, Paid HolidaysYour Wellbeing: Medical, Dental, Vision, Employee Assistance Program, Parental LeaveDiversity & Inclusion: Committed to Welcoming, Celebrating and Thriving on DiversityTraining: Hands-On, Team-Customized, including SS&C UniversityExtra Perks: Discounts on fitness clubs, travel and more!\n\nWhat You Will Get To Do:\n\nAs a Data Scientist, you will be working with the largest repository of corporate, board-level business information in the world. You will work with Product Managers, Business Analysts, Data Analysts, User Experience Designers, ML Engineers, and Senior Executives to gather requirements and apply data science methodologies to solve complex business problems.\n\nYou should have deep expertise in analyzing large, complex data sets from multiple domains, then translating this analysis to models which can run at scale in a SaaS business. You will be a part of an established global team focused on Analytics, Search and Artificial Intelligence with researchers in developers in Waltham, MA, Bucharest, Romania and Hyderabad, India all focused on the development of solutions for Investment Bankers, Private Equity, and other industries.\n\nWork with the AI team in building a world-class software, functioning as a thought leader in ensuring team development efforts resulting in successful delivery of AI systems.Collaborate with cross functional agile teams of software engineers, data engineers, ML engineers, Product Managers and others in building new product featuresManage and execute entire data projects from start to finish including cross-functional project management, data gathering and manipulation, analysis and modeling, and communication of insights and recommendations.Demonstrate a high degree of originality and creativity when developing solutions to solve problems like entity recognition, document classification etc. utilizing methods such as statistical analysis, natural language understanding and optimization, and deep learning.Work independently to manage multiple projects at once while ensuring deadlines are met and data output is accurate and appropriate for the business. Must also be able to deal with ambiguity and make independent decisions about what data and approach is best for the task at hand.Think strategically about data as a core enterprise asset and assist in all phases of the advanced analytic development processThe scope of work includes Forecast, Prediction Models, Outlier Reporting, Risk Analysis, Document classification, Data Extraction, Adhoc analysis.Implementation of Supervised and Unsupervised model development techniques\n\nWhat You Will Bring:\n\nAdvanced NLP Development: Design, develop, and optimize the Natural Language Processing (NLP) models using state-of-the-art techniques, with a focus on understanding complex language structures, semantics, and context.Large Language Model (LLM) Expertise: Leverage your expertise in working with large language models. Stay up to date with the latest advancements in LLMs and implement strategies for fine-tuning, transfer learning, and adapting these models to specific domains.Quantization and Model Optimization: Implement advanced quantization techniques to optimize deep learning models for efficient deployment on resource-constrained environments, ensuring minimal loss in performance while reducing memory and computational demands.Natural Language Generation (NLG): Utilize your deep knowledge of NLG techniques to develop systems that can generate coherent and contextually relevant human-like text, catering to various applications such as content creation, conversational agents, and data summarization.AI Model Deployment: Take charge of deploying AI models into production environments, collaborating closely with DevOps and software engineering teams to ensure seamless integration, scalability, and real-time performance of NLP models in various applications.Deep Learning Research: Stay at the forefront of deep learning research and methodologies, applying innovative techniques to address challenges in NLP tasks, such as named entity recognition, sentiment analysis, language translation, and more.PyTorch Champion: Serve as the PyTorch expert within the team, driving the development process using PyTorch's flexible framework for designing, training, and deploying complex neural network architectures.Algorithmic Development: Research, experiment, and develop novel algorithms that push the boundaries of NLP tasks, incorporating techniques like attention mechanisms, transformer architectures, and reinforcement learning to achieve state-of-the-art results.Collaborative Problem Solving: Collaborate with cross-functional teams including data scientists, machine learning engineers, and domain experts to identify business challenges and propose NLP-based solutions that have a tangible impact.\n\nA Plus:\n\nInnovative Model Architecture: Design novel NLP architectures that integrate cutting-edge techniques such as cross-modal attention, graph neural networks, and unsupervised pre-training to solve complex multimodal language understanding tasks.NLG Elevation: Elevate NLG capabilities by developing advanced systems that not only generate human-like text but also adapt tone, style, and domain-specific nuances to produce contextually relevant and emotionally resonant narratives.Transformer Customization: Customize transformer-based architectures to specific domains and tasks, leveraging techniques like knowledge distillation, architecture pruning, and dynamic attention mechanisms to enhance efficiency and performance.A minimum of 2 years of developing and deploying AI/NLP/LLM modelsPlease note that applications without this experience will not be considered. Some of this experience needs to be with NLP and deep learning technologies.Masters or Ph.D. with experience in Machine Learning/Statistics/Data ScienceExperience with traditional as well as modern machine learning/statistical techniques, including NLP algorithms, LLMs, and quantization strategies.Strong implementation experience with high-level languages, such as Python, R or similar scripting languages.Familiarity with Linux/Unix/Shell environments.Strong hands-on skills in sourcing, cleaning, manipulating and analyzing large volumes of data.Strong written and oral communication skills.Intense intellectual curiosity – strong desire to always be learningAnalytical, creative, and innovative approach to solving open-ended problemsSolid understanding of software engineering practices and version control systems.Excellent problem-solving skills and a passion for staying updated with the latest advancements in the field.Experience with product development is a plus.Experience with Financial Services is desired but not required. Much of our data relates to Investment Banking and M&A.\n\nThank you for your interest in SS&C! To further explore this opportunity, please apply through our careers page on the corporate website at www.ssctech.com/careers.\n\nUnless explicitly requested or approached by SS&C Technologies, Inc. or any of its affiliated companies, the company will not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services. SS&C offers excellent benefits including health, dental, 401k plan, tuition and professional development reimbursement plan. SS&C Technologies is an"}
{"text": "experience using ETL and platforms like Snowflake. If you are a Senior data engineer who thrives in a transforming organization where an impact can be made apply today! This role is remote, but preference will be given to local candidates. This role does not support C2C or sponsorship at this time.\nJob Description:Managing the data availability, data integrity, and data migration needsManages and continually improves the technology used between campuses and software systems with regard to data files and integration needs.Provides support for any data storage and/or retrieval issues, as well as develops and maintains relevant reports for the department.This role will be responsible for how the organization plans, specifies, enables, creates, acquires, maintains, uses, archives, retrieves, controls and purges data.This position is also expected to be able to create databases, stored procedures, user-defined functions, and create data transformation processes via ETL tools such as Informatica, Microsoft SQL Server Integrated Services (SSIS), etc.Experience with Salesforce is a requirement for this role.Job skills and become more effective members of the team. Provide guidance on best practices for data management and integrations, as well as help identify opportunities for process improvement.Develop and maintain relevant reports for the department and provide insights into the performance and effectiveness of data management and integration processes. Work closely with personnel and partner institutions to understand their reporting needs and ensure that the reports are accurate and timely.Other duties as assigned.Additional Requirements:Operation Essential Personnel - This position may be asked to work during an official closure, but who is otherwise not required to report to the physical campus during a closure and whose absence does not present a risk to the safety, resources, and well-being of the organizationQualifications:Bachelor's degree with 5 years of experienceExperience with data migrations, integrations and reporting tools. Personnel management experience. Must have working experience with the Salesforce platform and Snowflake.Data Cloud experience.Degree Substitution: Client is committed to creating an inclusive space where our employees feel valued for their skills and uniqueness. If a candidate does not meet the minimum qualifications as listed, but has significant experience within the major job responsibilities, we welcome their application.Preferred Qualifications:Master's degree with 7 years of experienceSpecific Experience: Experience with Tableau, Microsoft SQL Reporting Services and BOOMi are preferred.Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.Questions? Call your local office at 1.888.490.3195. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals.© 2021 Robert Half."}
{"text": "Qualifications\n\nBachelor's degree in Data Analytics or equivalentMust be proficient in Excel, Visual Basic preferredMust be proficient in Power BI and M/Power Query/DAXMachine learning experience preferredBackground and credit check required\n\nCore Competencies \n\nStrive to do the right thing by displaying trust and integrity.Embody the principles of servant leadership, even in a non-people management role, by putting the needs of others first, valuing diverse perspectives by sincerely appreciating and considering others’ opinions and ideas and demonstrating a positive and humble attitude.Demonstrated ability to work independently and on a team; ability to lead, execute and/or delegate as needed, while also collaborating with others to get the job done.Establish and maintain effective working relationships at every level of the organization; invest in building relationships with the Field Operations and Field Support Center team members.Help champion an inclusive working environment by:Empowering others to bring their full selves to the workplace.Celebrating, welcoming, and valuing the different backgrounds and experiences that make up our workforce.Recognizing that all team members are valued, regardless of race, background, tenure, or title.Ability to self-manage, show initiative, be proactive, and drive results.Communicate professionally, both verbally and in writing to coworkers and customers\nPhysical Requirements\n\nMust be able to remain in stationary position in an office environment: 80%Will frequently move about inside the office to access files, office machinery, etc.Must be able to operate basic office machinery.Must be able to lift and move any files weighing up to 12 pounds for audits or etc.Must be able to communicate with team and management and be able to exchange accurate information in these situations.\n\nRequired Cognitive Skills\n\nMust be able to problem solve and prioritize tasks.Must be able to manage stress depending on deadlines and ongoing projects. Must be able to multitask.Must be able to receive and analyze information.Must be able to quickly communicate solutions if problems occur.Must be able to demonstrate a high degree of sound judgement and initiative.\n\nBenefits & Perks\n\nMedical, Dental, Vision, Disability & Life InsuranceWellness Benefits401(k) Retirement PlanEmployee Stock Purchase ProgramPaid Holidays & Vacation DaysProfessional Growth OpportunitiesDevelopment & Training Programs\n\nThis job description is subject to change at any time.\n\nEQUAL OPPORTUNITY EMPLOYER\n\nLaunch your career with a national building materials distributor and discover opportunities for growth and advancement. We value our team members and believe them to be our greatest assets. As such, we invest in training and strive to provide a work-life balance.\n\nApply Now"}
{"text": "requirements and translating them into automated solutions, to include workflow technical design and implementation. Business Analyst will serve as the technical liaison with the software programming team for implementing workflow solutions and project monitoring, to include ongoing workflow maintenance.\n\nResponsibilities\n\nServes as team member on assigned teamsProvides requested drilldowns, follow -ups and recommendations that may be requested by the team and presents in report formatProvides technical support and analytical support on DSRIP projects, operational planning and performance monitoringCreates custom and standard reports for DSIRP program managementAnalyzes data and performs drilldowns when indicated\n\nRequirements\n\nAssociate Degree preferred2-3 years of experience preferred\n\nWork Type\n\nFull Time\n\n\n\nhttps://www.\n\nWe endeavor to make this site accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at (844) 257-6925."}
{"text": "requirements identification, requirements validation, testing, and troubleshooting.Create and maintain standard operating procedures (SOPs) and documentation for mission equipment and systems.\n\nJob Requirements\n\nActive Top-Secret Clearance with the willingness to take an agency CI polygraph when requested.High School Diploma (HS) and 13+ years, Associates Degree (AS) and 10+ years, Bachelors’ Degree (BS) and 8+ years or Masters’ Degree (MS) and 5+ years of relevant experience.Effective communications skills and able to effectively brief audiences.Ability to organize work so assignments are completed in a timely manner.Ability to perform analysis and quickly resolve issues.Ability to multi-task.Demonstrated experience working with management with minimal guidance or direction.Skilled with the Microsoft Office software suite.\n\nFounded in 1975, AMERICAN SYSTEMS is one of the largest employee-owned companies in the United States. We are a government services contractor focused on delivering Strategic Solutions to complex national priority programs with 100+ locations worldwide. Through our focus on quality, strong cultural beliefs, and innovation we deliver excellence every day.\n\nCompany Awards\n\nForbes National Best Midsize CompaniesEnergage National Best WorkplacesWashington Post Best Workplaces\n\nVeteran Hiring Awards\n\nGOLD AWARD by U.S. Department of Labor Hire Vets MedallionBEST FOR VETS EMPLOYER by Military TimesTOP 10 MILITARY FRIENDLY EMPLOYER by MilitaryFriendly.com\n\nAMERICAN SYSTEMS is committed to pay transparency for our applicants and employee-owners. The salary range for this position is $90,000 - $105,000. Actual compensation will be determined based on several factors including relevant experience and permitted by law. AMERICAN SYSTEMS provides for the welfare of its employees and their dependents through a comprehensive benefits program by offering healthcare benefits, paid leave, retirement plans (including ESOP and 401k), insurance programs, and education and training assistance."}
{"text": "Qualifications/Formal EducationRequired: Bachelor’s Degree (or foreign equivalent) or in lieu of a degree, at least 12 years of experience in the field of Information Technology or Business (work experience or a combination of education and work experience in the field of Information Technology or Business).Preferred:Master’s degree preferred.Six Sigma, Lean trainingKnowledge and Experience Required5+ years of business or IT experience3+ years of experience with large implementations and business systemsUnderstanding of data domains and data sourcesPreferred:Experience with data technologies and tools such as Snowflake and Oracle Cloud ERPExperience with Scrum methodologies\nThis position works from the office three days a week and remotely two days a week."}
{"text": "experience)\nSupplier Call Notes:SQL – 4 or more years, program management skills, ability to work with non-technical stakeholdersVisualization experience – TableauOrchestration platform – Azkaban/Airflow – 2yrsApplied Data Science background – nice to havePython based tools – linear models, tree models, clustering, pandasGroup supports CS teamsExperience with Sales or Customer Success analytics in the pastAlternate Job Titles: BI Analyst/BI Developer/Data Analyst"}
{"text": "Skills :• Configure AEP to get the data set needed and then use spark (AWS glue ) to load data in the data lake• should be strong in SQL• Need good communication skills.• Preferred to have knowledge in python and redshift"}
{"text": "experience, improve efficiency, and reduce cost. As an example, we manage catalog data imported from hundreds of retailers, and we build product and knowledge graphs on top of the catalog data to support a wide range of applications including search and ads.\n\nWe are looking for talented Ph.D. students to have an internship in our fast moving team. You will have the opportunity to work on a very large scope of problems in search, ads, personalization, recommendation, fulfillment, product and knowledge graph, pricing, etc.\n\nAbout The Job\n\nBased on your passion and background, you may choose to work in a few different areas:\n\nQuery understanding - Using cutting-edge NLP technologies to understand the intent of user queries.Search relevance and ranking - Improving search relevance by incorporating signals from various sources.Ads quality, pCTR, etc. - Improving ads revenue and ROAS.Knowledge graphs - Working on graph data management and knowledge discovery, and creating a natural language interface for data access.Fraud detection and prevention - Using cost sensitive learning to reduce loss.Pricing - Estimating willingness-to-pay, and optimizing revenue and user experience.Logistics - Optimization in a variety of situations, including supply/demand prediction, last mile delivery, in-store optimization, etc.\n\nAbout You\n\nMinimum Qualifications:\n\nPh.D. student in computer science, mathematics, statistics, economics, or related areas.Strong programming (Python, C++) and algorithmic skills.Good communication skills. Curious, willing to learn, self-motivated, hands-on.\n\nPreferred Qualifications\n\nPh.D. student at a top tier university in the United States and/or CanadaPrior internship/work experience in the machine learning space\n\nInstacart provides highly market-competitive compensation and benefits in each location where our employees work. This role is remote and the base pay range for a successful candidate is dependent on their permanent work location. Please review our Flex First remote work policy here.\n\nOffers may vary based on many factors, such as candidate experience and skills required for the role. Please read more about our benefits offerings here.\n\nFor US based candidates, the base pay ranges for a successful candidate are listed below.\n\nCA, NY, CT, NJ\n\n$50—$50 USD\n\nWA\n\n$47.50—$47.50 USD\n\nOR, DE, ME, MA, MD, NH, RI, VT, DC, PA, VA, CO, TX, IL, HI\n\n$45—$45 USD\n\nAll other states\n\n$42—$42 USD"}
{"text": "Requirements Documents (BRDs) and technical design Documents (TRDs).Use appropriate tools to collect, correlate and analyze data.Record and maintain technical data for use in developing operating and instruction manuals.Develop simple to complex ETL mappings in Informatica and document all business rules applied in ETL logic to ensure the development is in-line with Functional/Technical specification documents or any other requirements documentation.Utilize AWS services to implement end to end data pipelines to derive insights.Utilize Informatica MDM hub (Siperian) on 9.x and 10.x versions to make any design & architecture changes including configuring & fine-tuning fuzzy logic Informatica MDM to meet the changing business needs and implementing new processes & projects.Conduct data warehouse/BI/Analytics/ETL applications development and testing using ETL tools like Informatica Powercenter.Create technical documentations such as technical specification documents, technical design documents, Data flow diagrams, process diagrams and process illustrations.Implement batch and continuous data ingestion pipelines using AWS SQS and Python connectors.Collaborate with various departments, architects, project managers and technical managers to provide estimates, develop overall implementation solution plan and serve as a lead to implement solutions.Implement concepts such as Streams, Tasks, Clustering, Data purge, semistructured (XML, JSON) and unstructured data handling and streaming data loads.Assist in the development of standards and procedures.Apply and execute standard information systems theories, concepts, and techniques.Utilize Analysis, Design, Development, Testing, Data Analysis, Data Governance, Reporting, Impact Analysis, Applications Maintenance and cloud technologies.Identifies the business benefits of alternative strategies.Ensures compliance between business strategies and technology directions.May prepare testing plans to confirm that requirements and system design are accurate and complete and user conduct trainings.Identify process disconnects and translate them into improvement opportunities with cost savings or avoidance, productivity improvements, or revenue generating business benefits.Develop business relationships and integrate activities with other IT areas to ensure successful implementation and support of project efforts.Write SQL queries to analyze the data thoroughly and present results of analysis to larger group.Perform complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshooting.Analyze departmental processes and needs and make recommendations that are most effective means to satisfy those needs.Develop data ingestion, data processing and raw data pipelines for different data sources to AWS.Partner effectively with all teams to ensure all business requirements and SLAs are met, and data quality is maintained.Communicate business needs and drivers to development groups to assure the implementation phase can fulfill the business need.Establish organizational objectives and delegates assignments.\n\nQualifications\n\nMust possess a Bachelor’s degree or foreign academic equivalent in Computer Science, Applied Computer Science, Computer Engineering, Information Technology or a highly related field of study with 5 years of related experience.\n\nIn the alternative, employer will accept a Master’s degree in the aforementioned fields plus 2 years of related experience.\n\nEach educational alternative with at least two (2) years of experience in the following:\n\ndata warehouse/BI/Analytics/ETL applications development and testing using ETL tools like Informatica Powercenter;implementing batch and continuous data ingestion pipelines using AWS SQS and Python connectors;Streams, Tasks, Clustering, Data purge, semistructured (XML, JSON) and unstructured data handling and streaming data loads; (iv)Analysis, Design, Development, Testing, Data Analysis, Data Governance, Reporting, Impact Analysis, Applications Maintenance and cloud technologies; (v)complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshooting; &developing data ingestion, data processing and raw data pipelines for different data sources to AWS.\n\nEmployer will accept any suitable combination of education, training or experience related to the job opportunity.\n\nApply online at https://careers.abbvie.com/en. Refer to Req ID: REF24104Y.\n\nAdditional Information\n\nWe offer a comprehensive package of benefits including paid time off (vacation, holidays, sick), medical/dental/vision insurance and 401(k) to eligible employees. This job is eligible to participate in our short-term and long-term incentive programs.\n\nAbbVie is committed to operating with integrity, driving innovation, transforming lives, serving our community and embracing diversity and inclusion. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a protected veteran, or any other legally protected group status."}
{"text": "Qualifications)\n\n 3+ years of analytical experience 3+ years of statistical software experience with SAS, SQL, and R 1+ years of pharmacy claims & enrollment data experience\n\nHow To Stand Out (Preferred Qualifications)\n\n Experience with data visualization tools such as Tableau or Power BI Healthcare informatics and/or medical claims experience Ability to communicate and present data to stakeholders Intermediate level Microsoft Office Suite knowledge (Word, Excel, Outlook)\n\n#HealthcareServices #CareerOpportunity #CompetitivePay #RemoteWork #ProfessionalGrowth\n\nAt Talentify, we prioritize candidate privacy and champion equal-opportunity employment. Central to our mission is our partnership with companies that share this commitment. We aim to foster a fair, transparent, and secure hiring environment for all. If you encounter any employer not adhering to these principles, please bring it to our attention immediately.\n\nTalentify is not the EOR (Employer of Record) for this position. Our role in this specific opportunity is to connect outstanding candidates with a top-tier employer.\n\nTalentify helps candidates around the world to discover and stay focused on the jobs they want until they can complete a full application in the hiring company career page/ATS."}
{"text": "requirements into technical solutions, including data model designs and API integrations.Troubleshoot and resolve data-related issues, providing support and guidance to ensure continuity and efficiency of data operations.Proactively identify opportunities for data infrastructure improvements and innovations to support business growth and efficiency.Contribute to the development and implementation of data governance and quality frameworks to maintain high standards of data integrity and reliability.Perform other duties as assigned, such as ancillary automation tasks.\nNote: This job description in no way states or implies that these are the only duties to be performed by the employee(s) incumbent in this position. Employees will be required to follow any other job-related instructions and to perform any other job-related duties requested by any person authorized to give instructions or assignments. All duties and responsibilities are essential functions and requirements and are subject to possible modification to reasonably accommodate individuals with disabilities.\n\nQUALIFICATIONS\nTECHNICAL SKILLS & ABILITIESProficient in ETL process design, development, and optimization.Strong understanding of data modeling, warehousing, and management concepts.Knowledgeable in scripting and automation tools, with proficiency in languages such as Python, SQL, and Shell scripting.Familiarity with cloud services and infrastructure (Azure & AWS) for data processing and storage.Ability to work independently and in a team, with excellent problem-solving and troubleshooting skills.Experience with version control systems, such as Git, for managing changes to codebase and documentation.Ability to read and understand technical manuals, procedural documentation, and OEM guides.Able to work independently and communicate clearly.Ability to define problems, collect data, establish facts, and draw valid conclusions.Ability to effectively prioritize and execute tasks in a high-pressure environment.\n\nCORE COMPETENCIESAdaptability: Quickly adjust to new information, changing conditions, or unexpected obstacles.Technical Proficiency: Maintains an advanced level of technical knowledge and skills in the field of data engineering.Innovation: Actively seeks new ways to grow and be challenged using both formal and informal development channels.Documentation & Data Integrity: Consistently adheres to established procedures for data input, organization, maintenance, and timeliness into designated systems/databases in order to ensure data accuracy and reliability.Goal Setting: Holds team members responsible for SMART goals that align with departmental business objectives enabling teams to work towards a common goal.Critical Thinking: Evaluates information by questioning assumptions and reflects on all options and risks in order to make informed decisions.Problem Solving: Consistently applies sound problem solving in order to ensure smooth flow of goods and services.Strategic Planning: Contributes to strategic implementation of daily processes, considering timelines, risks, and resources in order to accomplish goals.Monitoring: Tracks, reviews and reflects on relevant data in order to effectively evaluate progress towards goals.\n\nEDUCATION & EXPERIENCEBachelor’s degree in Computer Science, Engineering, Information Technology, or related field preferred.3+ years of experience in a data engineering role, with a proven track record of developing and managing ETL processes, data pipelines, and integrations.Experience with data processing frameworks and tools (e.g., Apache Spark, Hadoop, Airflow, Databricks).Experience with productivity software such as Microsoft Outlook, Word, Excel, etc.\n\nREQUIREMENTSMust be at least 21 years of age.Must be able to travel 25% of the time.\n\nThe physical demands described here are representative of those that must be met by an associate to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\nProlonged periods of standing, sitting at a desk, and working on a computerAbility to get in and out of vehicle and walk up and down stairs during your shift.Must be able to stand, sit for prolonged periods of time, bend, kneel, squat, and twist.\n\nBENEFITS & COMPENSATIONAll employees are provided competitive compensation, paid training, and employee discounts on our products and services.We offer a range of benefits packages based on employee eligibility*, including:Paid Vacation Time, Paid Sick Leave, Paid Holidays, Parental Leave.Health, Dental, and Vision Insurance.Employee Assistance Program.401k with generous employer match.Life Insurance.\n*Additional details about compensation and benefits eligibility for this role will be provided during the hiring process.\n\nCOMPANYWe are setting the industry standard to influence and inspire through our innovative methods. We merge together cannabis culture with cutting-edge technology to deliver the best possible customer and employee experience(s).\nThe company operates a fully integrated cannabis business with a footprint that covers the entire breadth of the state of California; the world's largest cannabis market. We control our supply chain from cultivation to retail, which enables us to deliver clean and tested cannabis products at unmatched prices.\nOur mission is to provide the one-stop-shop cannabis experience by offering exceptional customer experience service and diversified products. We strive to build long-term customer loyalty.\n\nWe provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training."}
{"text": "requirements.Prepares and presents results of analysis along with improvements and/or recommendations to the business at all levels of management.Coordinates with global sourcing team and peers to aggregate data align reporting.Maintain data integrity of databases and make changes as required to enhance accuracy, usefulness and access.Acts as a Subject Matter Expert (SME) for key systems/processes in subject teams and day-to-day functions.Develops scenario planning tools/models (exit/maintain/grow). Prepares forecasts and analyzes trends in general business conditions.Request for Proposal (RFP) activities – inviting suppliers to participate in RFP, loading RFP into Sourcing tool, collecting RFP responses, conducting qualitative and quantitative analyses.Assists Sourcing Leads in maintaining pipeline, reports on savings targets.\nQualifications:Bachelors Degree is required.Minimum of 4 years of relevant procurement analyst experience.Advanced Excel skills are required.C.P.M., C.P.S.M., or N.A.C.M. certifications or eligibility preferred.Strong vendor management and contract experience.Ability to act as a Subject Matter Expert (SME) for key systems and processes.Proficiency in developing scenario planning tools/models and preparing forecasts.Strong attention to detail and accuracy in data management.Excellent communication skills, both written and verbal."}
{"text": "experience where you can also make an impact on your community. While safety is a serious business, we are a supportive team that is optimizing the remote experience to create strong and fulfilling relationships even when we are physically apart. Our group of hard-working employees thrive in a positive and inclusive environment, where a bias towards action is rewarded.\n\nWe have raised over $380M in venture capital from investors including Tiger Global, Andreessen Horowitz, Matrix Partners, Meritech Capital Partners, and Initialized Capital. Now surpassing a $3B valuation, Flock is scaling intentionally and seeking the best and brightest to help us meet our goal of reducing crime in the United States by 25% in the next three years.\n\nThe Opportunity\n\nAs a Senior Data Analyst on the ML team, you will be responsible for extracting insights aggregated from various data sources, developing dashboards to identify trends and patterns that highlight model performance issues, performing analysis to determine the root-cause of ML and product anomalies, and presenting your findings to stakeholders to guide business and technical decisions. You will work closely with ML and Data Engineering, and other engineering teams to direct and validate technical and business decisions using data. Your work will directly impact the success of our ML initiatives, ensuring they are grounded in solid data analysis and capable of delivering tangible business value.\n\nThe Skillset\n\n3+ years of experience as Data Analyst / Data ScientistAbility to develop robust monitoring and alerting systems to detect regressions in model behaviorStrong proficiency in SQLStrong proficiency using Data Warehouses and other large data systems (Spark, Trino, etc.)Strong proficiency in developing dashboards (Tableau, SuperSet, Sigma, etc.)Experience in statistical analysisExperience using Python as a scripting languageBasic git knowledgeBasic bash knowledgeKnowledge in ML concepts is desirable\n\nFeeling uneasy that you haven’t ticked every box? That’s okay, we’ve felt that way too. Studies have shown women and minorities are less likely to apply unless they meet all qualifications. We encourage you to break the status quo and apply to roles that would make you excited to come to work every day.\n\n90 Days at Flock\n\nWe are a results-oriented culture and believe job descriptions are a thing of the past. We prescribe to 90 day plans and believe that good days, lead to good weeks, which lead to good months. This serves as a preview of the 90 day plan you will receive if you were to be hired as a Senior Data Analyst at Flock Safety.\n\nThe First 30 Days\n\nUnderstand the teams, data sources, and existing tools used to work with data. Work with ML and Data Engineering to improve monitoring and alerting. \n\nThe First 60 Days \n\nWork to help develop the metrics and data necessary to quantify changes made to the systems from cross-functional projects. \n\n90 Days & Beyond\n\nUnderstand the data that ML and related projects are most involved inGain competency in developing monitoring and analysis to deliver value from that dataCollaborate well with the team and the rest of the FlockAnalyze known anomalies to understand the reason for the specific behavior\n\nThe Interview Process \n\nWe want our interview process to be a true reflection of our culture: transparent and collaborative. Throughout the interview process, your recruiter will guide you through the next steps and ensure you feel prepared every step of the way.\n\n[Pick and choose what stages you will have in your interview process]\n\nOur First Chat: During this first conversation, you’ll meet with a recruiter to chat through your background, what you could bring to Flock, what you are looking for in your next role, and who we are. The Hiring Manager Interview: You will meet with your potential future boss to really dive into the role, the team, expectations, and what success means at Flock. This is your chance to really nerd out with someone in your field. The Technical Assessment: Our technical assessments seek to test the hard skills required to do the job. Engineers may find themselves in coding interviews or architecture discussions, sales roles may present mock discovery calls, and leadership roles may craft 90 day plans. Your recruiter will inform you of which assessment you will be assigned and ensure you are fully prepared for your big day. The Executive Review: A chance to meet an executive in your function and view Flock from a different lens. Be prepared to ask well-thought-out questions about the company, culture, and more. \n\nThe Perks\n\n💰Salary & Equity: In this role, you’ll receive a starting salary of $110,000-$145,000 as well as stock options\n\n🌴Use what you need PTO: We seriously mean it, plus 11 company holidays and your birthday off!\n\n⚕️Fully-paid health benefits plan for employees: including Medical, Dental, and Vision and an HSA match.\n\n👪Family Leave: All employees receive 12 weeks of 100% paid parental leave. Birthing parents are eligible for an additional 6-8 weeks of physical recovery time.\n\n🍼Fertility & Family Benefits: We have partnered with Maven, a complete digital health benefit for starting and raising a family. We will reimburse $10,000 a year for adoption, surrogacy, or infertility.\n\n🧠Mental Health: All employees receive an annual subscription to Headspace\n\n💖Caregiver Support: We have partnered with Cariloop to provide our employees with caregiver support\n\n💸Carta Tax Advisor: Employees receive 1:1 sessions with Equity Tax Advisors who can address individual grants, model tax scenarios, and answer general questions.\n\n💻WFH Stipend: $150 per month to cover the costs of working from home.\n\n📚L&D Stipend: $250 per year to use on Audible, Calm, Masterclass, Duolingo, Grammarly and so much more.\n\n🏠Home Office Stipend: A one-time $750 to help you create your dream office.\n\n🏢Coworking Space: If you’re not local to our ATL HQ, we’ll provide $250 a month to get you set up with an All Access Membership to WeWork (or a local coworking space in your area).\n\n🐾Pet Insurance: We’ve partnered with Pumpkin to provide insurance for our employee’s fur babies.\n\nFlock is \n\nIf you need assistance or an accommodation due to a disability, please email us at careers@flocksafety.com. This information will be treated as confidential and used only to determine an appropriate accommodation for the interview process."}
{"text": "Skills : Look for someone who has experience in the Healthcare domain and worked with Healthcare DataShould have good experience in Azure Stack (Data Lake/Blob Storage, Power BI Services, Azure Data Factory (or equivalent), Data bricks) and strong on SQL Server \nQUALIFICATION REQUIREMENTS• Bachelor’s degree (BS/BA) in Information Systems, Software Engineering, Computer Science, Data Engineering, or related field required. Master’s degree (MS/MA) preferred.• Experience with ETL/ELT, taking data from various data sources and formats and ingesting into a cloud-native data warehouse required.• Experience with Azure Stack (Data Lake/Blob Storage, PowerBI Services, Azure Data Factory (or equivalent), Databrick) and production level experience with on-premises Microsoft SQL Server required.• Experience with one of the following: Python, R, and/or Scala as well as standard analytic libraries/packages (e.g., pandas, Numpy, dplyr, data table, stringr, Slick, and/or Kafka) and related distribution frameworks required.• Strong verbal and written communication skills required.• Familiarity with agile and lean concepts that drive towards MVPs and iterative learning to generate the desired business and technology outcomes required.• Experience with DataRobot, Domino Data Labs, Salesforce MC, Veeva CRM preferred.• Familiarity with modern data stack components like Snowflake, dbt, Stitch, Tableau, and Airflow• Familiarity with statistical concepts and analytic modeling (e.g., regression analyses, hypothesis testing, and ML based modeling) preferred.• Experience with software engineering best practices like version control with Git and CI/CD preferred.• Experience with US healthcare and healthcare data, as well as familiarity with HIPAA guidelines, and best practices for handling and storing PHI and PII preferred.• Experience with healthcare marketing analytics, healthcare data (claims), and common medical coding sets (ICD, HCPCs, NPIs) preferred.\nif your interested please share me resume to syed@thoughtwavesoft.com or else call me 630-326-4525 Asap"}
{"text": "experience with security and meet our compliance goals.\n\nThis position requires deep customer focus and the ability to build relationships and feedback loops to continuously improve products based on evolving customer needs and security goals. You will obsess about our data and think around the corner to transform data into new insights. You will have ownership over your design solutions from conception through implementation, working closely with engineering teams to ensure the high standards of your designs are maintained. You will guide the team to build and maintain data infrastructure, develop and improve analytics products, simplify data access and unlock self-service insights.\n\nWhat you bring\n\nYou have an “anything is possible” mindset when faced with highly ambiguous and complex problems. You are proud of your work, skills, judgment, and expertise – but you are simultaneously eager to listen to feedback from others. You have a background in complex data products or developing deep understanding in technical or scientific domains, geek out over customer experience, and believe that no experience is perfect unless it delights customers in unexpected ways. You have demonstrated design acumen and believe in the power of strategic design to differentiate experience and increase customer trust. You are an influencer who doesn’t hold your ideas as precious and who iterates quickly, based on data, and delivers recommendations and analysis that tell the story of the customer experience your team is delivering.\n\nWhy you want to be on the Security Issue Management team\n\nWe are technology professionals, who are genuinely passionate about continuously improving the security of Amazon systems on behalf of the Amazon customer. We also take pride in reducing the effort required for service teams across Amazon to build and maintain their systems securely. We are on a mission, have fun working together, and have a culture of learning and career growth.\n\nKey job responsibilities\n\n Hire and develop a team of high-performing DEs and BIEs to create and support datasets, metrics and KPIs to meet our business goals. Work with PMs, PMTs, SDMs, and Tech team members to to build relevant data products and solutions. Architecture design and implementation of next generation data pipelines and BI solutions Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc. Build and deliver high quality data architecture and pipelines to support business analyst, data scientists, and customer reporting needs. Interface with other technology teams to extract, transform, and load data from a wide variety of data sources Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\n\nA day in the life\n\nYou will own and contribute to studies that evaluate the customer experience of executing use cases on security issue management services, as well as technology/industry trends and the voice of the customer. You will join an expanding team, leverage your technical and domain skills, and apply a variety of methodologies to drive these studies. You will collaborate with product teams and senior leaders to provide insights and drive improvements that help us achieve our vision to be Earth's Most Customer-Centric Company.\n\nAbout The Team\n\nWe are a team of builders that develop products, services, ideas, and various ways of leveraging data to influence product and service offerings to raise the bar in security. Security issue management system is SDO’s centralized tool with the goal of becoming the only place SDO builders need to go to understand and manage their security issues and to drive action on the highest security risk areas of the business as well as for security teams to drive campaigns efficiently and effectively while simultaneously reducing the effort teams spend on security related tasks.\n\nDiverse Experiences\n\nAmazon Security values diverse experiences. Even if you do not meet all of the qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn’t followed a traditional path, or includes alternative experiences, don’t let it stop you from applying.\n\nWhy Amazon Security\n\nAt Amazon, security is central to maintaining customer trust and delivering delightful customer experiences. Our organization is responsible for creating and maintaining a high bar for security across all of Amazon’s products and services. We offer talented security professionals the chance to accelerate their careers with opportunities to build experience in a wide variety of areas including cloud, devices, retail, entertainment, healthcare, operations, and physical stores.\n\nWork/Life Balance\n\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why flexible work hours and arrangements are part of our culture. When we feel supported in the workplace and at home, there’s nothing we can’t achieve.\n\nInclusive Team Culture\n\nIn Amazon Security, it’s in our nature to learn and be curious. Ongoing DEI events and learning experiences inspire us to continue learning and to embrace our uniqueness. Addressing the toughest security challenges requires that we seek out and celebrate a diversity of ideas, perspectives, and voices.\n\nTraining and Career growth\n\nWe’re continuously raising our performance bar as we strive to become Earth’s Best Employer. That’s why you’ll find endless knowledge-sharing, training, and other career-advancing resources here to help you develop into a better-rounded professional.\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nAustin, TX, USA | Seattle, WA, USA\n\nBasic Qualifications\n\n 5+ years of data engineering experience 2+ years of processing data with a massively parallel technology (such as Redshift, Teradata, Netezza, Spark or Hadoop based big data solution) experience 2+ years of developing and operating large-scale data structures for business intelligence analytics (using ETL/ELT processes) experience Experience communicating to senior management and customers verbally and in writing Experience leading and influencing the data or BI strategy of your team or organization\n\nPreferred Qualifications\n\n Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with AWS Tools and Technologies (Redshift, S3, EC2) Knowledge of Data modelling, advanced SQL with Oracle, MySQL, and Columnar Databases at an advanced level Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Experience and demonstrated industry leadership in the fields of database or data warehousing, data sciences and big data processing\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $136,000/year in our lowest geographic market up to $264,500/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2604587"}
{"text": "Requirements are:Experience in Banking and Financial ServicesSQL DevelopmentSAS DevelopmentExperience in an Agile Environment.\nPlease apply below for immediate consideration and to set up an initial call."}
{"text": "Qualifications:\n\n A minimum of 12 years of experience in data strategy, data management, or a related field, with at least 5 years in a leadership role. Proven experience in developing and implementing data strategies and driving data-driven transformation. Data Modeling experience is a must. Understanding of Relational and Dimensional Modeling, Normalization, Key Structures, Indexing, Partitioning, etc. Experience with ERWIN, ER Studio, or other data modelling tool required Proficiency with SQL query writing in a modern relational database environment, and data analysis and reverse engineering skills are a must Knowledge of Python, AWS tools and services, and modern data architecture concepts (such as Microservices Architecture) Excellent leadership and team management skills, with the ability to inspire and motivate teams. Exceptional communication and collaboration skills, with the ability to articulate the value of data in driving business outcomes to both technical and non-technical stakeholders. Bachelor's or Master’s degree in Computer Science, Data Science, Information Systems or related field. Advanced degree preferred.\n\nThe Vice President of Data Strategy plays a crucial role in transforming the company into a data-driven organization. The ideal candidate will be a strategic thinker, a strong leader, and a passionate advocate for the power of data.\n\nFor US-based roles only: the anticipated hiring base salary range for this position is [[$162,200]] - [[$235,100]], depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody’s also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.\n\nMoody’s is \n\nFor San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.\n\nThis position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.\n\nClick here to view our full \n\nCandidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody’s Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.\n\nFor more information on the Securities Trading Program, please refer to the STP Quick Reference guide on ComplianceNet\n\nPlease note: STP categories are assigned by the hiring teams and are subject to change over the course of an employee’s tenure with Moody’s."}
{"text": "experience in using, manipulating, and extracting insights from healthcare data with a particular focus on using machine learning with claims data. The applicant will be driven by curiosity, collaborating with a cross-functional team of Product Managers, Software Engineers, and Data Analysts.\n\nResponsibilities\n\nApply data science, machine learning, and healthcare domain expertise to advance and oversee Lucina’s pregnancy identification and risk-scoring algorithms.Analyze healthcare data to study patterns of care and patient conditions which correlate to specific outcomes.Collaborate on clinical committee research and development work.Complete ad hoc analyses and reports from internal or external customers prioritized by management throughout the year.\n\nQualifications\n\nDegree or practical experience in Applied Math, Statistics, Engineering, Information Management with 3 or more years of data analytics experience, Masters degree a plus.Experience manipulating and analyzing healthcare data (payer’s claims experience a plus)Strong communication skills with ability to describe complex ideas to key stakeholdersA proven track record applying Data Science to healthcare data. Technical skills include, but are not limited to, regression models, classification techniques, decision trees, clustering, pattern recognition, supervised and unsupervised learning, the bias-variance tradeoff, and neural networksExperience with clinical design patterns and causal modeling techniques (e.g., GLM methods, propensity score matching, etc.) a plusProficiency in Python and core data science libraries such as Pandas, SciKit-Learn, NumPy, Matplotlib, SciPy, and Keras.Advanced SQL and experience working with relational databases and data warehousesExperience with two or more of the following: SQL Server, SQL Data Warehouse Appliance, Netezza, Hadoop, Spark, Snowflake, Tableau, Power BI, or similar data technologies.AWS, Azure, GCP or similar cloud experience a plusFamiliarity with CI/CD and Git (commits, pushes, branching, and pull requests) a plusExperience with Databricks or PySpark a plus"}
{"text": "skills and healthcare industry knowledge to provide insights that support decision-making and operational improvements within family medicine, specialty, and urgent care settings. This role involves analyzing healthcare data, developing, and maintaining insightful dashboards, and communicating findings to senior leadership, thereby directly influencing the optimization and effective management of healthcare services.\n\nWhat You Will Do\n\nAnalyze complex healthcare data sets to identify trends, patterns, and insights that can drive business decisions and improve patient care. Utilize statistical techniques to validate data and findings.Develop and maintain robust dashboards and reports using SQL and PowerBI. Ensure these tools provide actionable insights and support data-driven decision-making processes.Work closely with senior leadership to understand business objectives and provide data analyses that support strategic initiatives. Offer recommendations based on data findings to enhance business and clinical operations.Translate complex data findings into clear, compelling stories that engage stakeholders and support organizational goals. Present data visually and narratively to explain the significance behind the numbers.Collaborate with various departments to ensure the accurate collection, analysis, and interpretation of data. Provide training and support to team members on data analytics tools and best practices.Implement and maintain quality control measures to ensure data accuracy and integrity. Regularly review and update data analysis methodologies to adhere to industry standards and best practices.Role models the principles of a Just Culture and Vail Health ValuesPerform other duties as assigned. Must be HIPAA compliant\n\n\nThis description is not intended and should not be construed to be an exhaustive list of all responsibilities, skills and efforts or work conditions associated with the job. It is intended to be an accurate reflection of the general nature and level of the job.\n\nExperience\n\nWhat you will need:\n\n5 years of experience in data analysis, including the use of statistical methods to collect and analyze data and generate quality business reports required1 year of healthcare experience in family medicine, ambulatory services, and urgent care preferred Previous experience working the data directly from an EMR such as Cerner, Allscripts, and Epic preferred\n\n\nLicense(s)\n\nN/A\n\n\nCertification(s)\n\nN/A\n\n\nComputer / Typing\n\nMust possess, or be able to obtain within 90 days, the computers skills necessary to complete online learning requirements for job-specific competencies, access online forms and policies, complete online benefits enrollment, etc.\n\n\nMust have working knowledge of the English language, including reading, writing, and speaking English.\n\nEducation\n\nBachelor's or Master's degree in Data Science, Statistics, Healthcare Administration, or related field preferred.\n\n\nApplication Close Date: April 23, 2024\n\nBenefits At Vail Health (Full And Part Time) Include\n\nCompetitive Wages & Family Benefits: Competitive wagesParental leave (4 weeks paid)Housing programsChildcare reimbursement Comprehensive Health Benefits: MedicalDental VisionEducational Programs: Tuition Assistance Existing Student Loan RepaymentSpecialty Certification ReimbursementAnnual Supplemental Educational FundsPaid Time Off: Up to five weeks in your first year of employment and continues to grow each year.Retirement & Supplemental Insurance: 403(b) Retirement plan with immediate matching Life insuranceShort and long-term disabilityRecreation Benefits, Wellness & More: Up to $1,000 annual wellbeing reimbursementRecreation discountsPet insurance\nPay is based upon relevant education and experience per year.\n\nYearly Pay\n\n$87,581.31—$131,350.54 USD\n\nSome roles may be based outside of our Colorado office (remote-only positions). Roles based outside of our primary office can sit in any of the following states: AZ, CO, CT, DC, FL, GA, ID, IL, KS, MA, MD, MI, MN, NC, NJ, OH, OR, PA, SC, TN, TX, UT, VA, WA, and WI. Please only apply if you are able to live and work primarily in one of the states listed above. State locations and specifics are subject to change as our hiring requirements shift.\n\nRelocation and housing assistance may be available."}
{"text": "Qualifications:Master's or Ph.D. in Computer Science, Statistics, Mathematics, or a related field.Minimum of 5 years of experience in a data science role, with a focus on payments fraud detection and prevention.Proficiency in programming languages such as Python, R, or Scala, and experience with data manipulation and analysis libraries (e.g., pandas, NumPy).Strong understanding of machine learning techniques, including supervised and unsupervised learning algorithms.Experience with big data technologies such as Hadoop, Spark, or Hive.Excellent problem-solving skills and the ability to translate business requirements into technical solutions.Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.Prior experience working in the financial technology industry is a plus.\nBenefits:Opportunity to work with some incredibly successful leaders in the FinTech space.Equity at a Unicorn company.Fully remote.Full health & dental coverage.\nDoes this sound like it might be a good fit for you? Apply below today and we can set up some time to speak."}
{"text": "QualificationsBachelor's or Master's degree in Statistics or Applied Mathematics or equivalent experience 1 - 2 years' Data Analysis experience Proficient in SQL"}
{"text": "experience for this role and the offer will be commensurate with that.\n\nThe Company\n\nAs a member of Munich Re's US operations, we offer the financial strength and stability that comes with being part of the world's preeminent insurance and reinsurance brand. Our risk experts work together to assemble the right mix of products and services to help our clients stay competitive – from traditional reinsurance coverages, to niche and specialty reinsurance and insurance products.\n\nMunich Re Facultative and Corporate (MR F&C) was created to bundle Munich Re’s facultative and corporate business into one global unit. By aligning our single risk business in one unified team we better position Munich Re to provide a holistic suite of products and services that meet the growing demands of our clients and broker partners.\n\nThe Opportunity\n\nFuture focused and always one step ahead\n\nWork closely with various Facultative & Corporate Underwriting Departments, as well as other internal stakeholders.\n\nIn this position you will: \n\nCreate new Power BI Dashboards, and update & maintain existing dashboardsAutomate processes using Python or other toolsDevelop data pipelines/ automate data extraction from various data sourcesAssist in production requests as neededManipulate data for analytics purposes\n\n\nQualifications: \n\nBe enrolled in a Master’s program. Preferred majors include: Information Systems, Statistics, Computer Science, or MathematicsPrevious work experienceGood analytical and problem solving skillsComputer skills required: Python, visualization tools (such as Power BI), ExcelProficient oral and written communication skills.Be able to work 32-40 hours/week in a hybrid working environment Position duration is 1 year\n\n\nAt Munich Re, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the customers we serve and the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services.\n\nWe are"}
{"text": "skills in a growth-oriented environment. If you're eager to drive change and grow professionally, Telegraph offers a dynamic and supportive setting to do just that. As our Data Scientist, you will be responsible for the following:\nPredictive Model Development/Improvement: Spearhead the development and refinement of predictive models to accurately forecast Estimated Times of Arrival (ETA) for freight, leveraging advanced machine learning techniques.Ad-hoc Data Analyses: Conduct comprehensive ad-hoc data analyses to uncover insights, support decision-making processes, and address complex business challenges.Data Infrastructure Development: Take the lead in expanding our data infrastructure, including building out data marts with dbt and integrating new data sources to enrich our analytical capabilities.Data Visualization: Create intuitive and insightful data visualizations to communicate findings and empower stakeholders with actionable intelligence.Optimization Problems: Employ optimization techniques and algorithms to solve potential optimization challenges, particularly with telematics data, enhancing operational efficiencies and outcomes.\nTo be successful at Telegraph, you must bring…Educational Background: Bachelor’s degree in Data Science, Computer Science, Statistics, or a related field.Professional Experience: At least 5 years of experience as a Data Scientist, preferably within the freight or logistics industry.Technical Expertise: Profound knowledge in machine learning, statistical modeling, and algorithm development.Programming Proficiency: Skilled in programming languages such as Python, R, and familiar with libraries like Pandas, NumPy, scikit-learn, TensorFlow, or PyTorch.Optimization Techniques: Experience with optimization techniques and their practical applications.SQL Mastery: Proficiency in SQL for data querying and analysis.Data Visualization Tools: Competence in using data visualization tools such as Tableau, Power BI, or D3.js.Cloud Computing Platforms: Familiarity with cloud computing platforms like AWS, GCP, enhancing our data processing and storage capabilities.Independent Work Ethic: Ability to work independently, managing multiple projects with diverse objectives simultaneously.Communication Skills: Excellent analytical, problem-solving, and communication skills, capable of presenting complex data in an understandable manner.Team Collaboration: Strong desire to collaborate in a fast-paced, team-oriented environment.Industry Knowledge: Knowledge of the freight or transportation industry is a significant plus.\nReasons it pays to work at Telegraph (in addition to the pay itself!)...Remote Friendly: Distributed team focused on culture + results v. proximity. Availability to collaborate on U.S. timezone–friendly hours is a must, with a high preference for CST.Offsites: Quarterly company get-togethers with a hackathon theme.PTO: Unlimited - and a culture that empowers you to take it.Benefits: Health, Vision, Dental + 401k.Parental Leave: 12 weeks of paid time.Co-working stipend: Every employee is offered monthly access to a local co-working facility.Equity: Above market equity and compensation packages.Sabbatical: 1 month, paid, beginning after 4 years of employment.\nDon’t Meet Every Single Requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every. single. qualification. At Telegraph, we are dedicated to building a diverse, inclusive and just workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with 100% of the qualifications listed, we encourage you to still apply. Our hiring philosophy looks to weed people INTO our process, not weed them out! Who knows, you may be just the right candidate for a future role with Telegraph. Just go for it... submit your resume!"}
{"text": "experience of data analysis or equivalent experience (university included)Bachelor’s degree in Computer Science, Information Security, Data Analytics, or a related fieldExperience with Python for data wrangling, analysis, and visualization, leveraging libraries such as Pandas and NumPyExperience with PowerBI, Tableau, or another comparable data visualization tool to build interactive dashboardsFamiliarity with FAIR methodology and basic fundamentals of probability and statisticsFamiliarity with the NIST Cybersecurity Framework is a plusMust be able to effectively communicate to various types of audiencesAbility to think critically, solve problems, make decisions and build trust across the organizationStrong logic and reason along with problem solving skills.Ability to work independently.\n\nPlayer Benefits\n\nWe treat our team right\n\nFrom our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:\n\nAn exciting and fun environment committed to driving real growthOpportunities to build really cool products that fans loveMentorship and professional development resources to help you refine your gameBe well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another \n\nFanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to"}
{"text": "skills across application domains.\n\nQualifications\n\nMinimum Qualifications:\n\nBS/BA and 5+ years of relevant work experience -OR-MS/MA and 3+ years of relevant work experience -OR-PhD with 1+ year of relevant experience\n\nPreferred Qualifications\n\nExperience training machine learning models in frameworks like PyTorchExperience applying machine learning and artificial intelligence to image processing specific applications. Additional domain application experience is preferred - geospatial intelligence, computer vision, few-shot learning, adversarial machine learning, social computing, etc.7+ years of experience with natural language processing5+ years in machine learning or applied science/research in academia or industry5+ years of experience with general purpose programming language (Python, Scala, Java, C, C++, etc.)Ability to obtain an TS/SCI clearance\n\nHazardous Working Conditions/Environment\n\nNot applicable.\n\nAdditional Information\n\nThis position requires the ability to obtain and maintain a federal security clearance.\n\nRequirements\n\nU.S. CitizenshipBackground Investigation: Applicants selected will be subject to a Federal background investigation and must meet eligibility requirements for access to classified matter in accordance with 10 CFR 710, Appendix B.Drug Testing: All Security Clearance positions are Testing Designated Positions, which means that the candidate selected is subject to pre-employment and random drug testing. In addition, applicants must be able to demonstrate non-use of illegal drugs, including marijuana, for the 12 consecutive months preceding completion of the requisite Questionnaire for National Security Positions (QNSP). \n\nNote: Applicants will be considered ineligible for security clearance processing by the U.S. Department of Energy until non-use of illegal drugs, including marijuana, for 12 months can be demonstrated.\n\nTesting Designated Position\n\nThis position is a Testing Designated Position (TDP). The candidate selected for this position will be subject to pre-employment and random drug testing for illegal drugs, including marijuana, consistent with the Controlled Substances Act and the PNNL Workplace Substance Abuse Program.\n\nAbout PNNL\n\nPacific Northwest National Laboratory (PNNL) is a world-class research institution powered by a highly educated, diverse workforce committed to the values of Integrity, Creativity, Collaboration, Impact, and Courage. Every year, scores of dynamic, driven people come to PNNL to work with renowned researchers on meaningful science, innovations and outcomes for the U.S. Department of Energy and other sponsors; here is your chance to be one of them!\n\nAt PNNL, you will find an exciting research environment and excellent benefits including health insurance, flexible work schedules and telework options. PNNL is located in eastern Washington State—the dry side of Washington known for its stellar outdoor recreation and affordable cost of living. The Lab’s campus is only a 45-minute flight (or ~3-hour drive) from Seattle or Portland, and is serviced by the convenient PSC airport, connected to 8 major hubs.\n\nCommitment to Excellence, Diversity, Equity, Inclusion, and \n\nOur laboratory is committed to a diverse and inclusive work environment dedicated to solving critical challenges in fundamental sciences, national security, and energy resiliency. We are proud to be an \n\nPacific Northwest National Laboratory considers all applicants for employment without regard to race, religion, color, sex (including pregnancy, sexual orientation, and gender identity), national origin, age, disability, genetic information (including family medical history), protected veteran status, and any other status or characteristic protected by federal, state, and/or local laws.\n\nWe are committed to providing reasonable accommodations for individuals with disabilities and disabled veterans in our job application procedures and in employment. If you need assistance or an accommodation due to a disability, contact us at careers@pnnl.gov.\n\nDrug Free Workplace\n\nPNNL is committed to a drug-free workplace supported by Workplace Substance Abuse Program (WSAP) and complies with federal laws prohibiting the possession and use of illegal drugs.\n\nIf you are offered employment at PNNL, you must pass a drug test prior to commencing employment. PNNL complies with federal law regarding illegal drug use. Under federal law, marijuana remains an illegal drug. If you test positive for any illegal controlled substance, including marijuana, your offer of employment will be withdrawn.\n\nHSPD-12 PIV Credential Requirement\n\nIn accordance with Homeland Security Presidential Directive 12 (HSPD-12) and Department of Energy (DOE) Order 473.1A, new employees are required to obtain and maintain a HSPD-12 Personal Identity Verification (PIV) Credential. To obtain this credential, new employees must successfully complete and pass a Federal Tier 1 background check investigation. This investigation includes a declaration of illegal drug activities, including use, supply, possession, or manufacture within the last year. This includes marijuana and cannabis derivatives, which are still considered illegal under federal law, regardless of state laws.\n\nMandatory Requirements\n\nPlease be aware that the Department of Energy (DOE) prohibits DOE employees and contractors from having any affiliation with the foreign government of a country DOE has identified as a “country of risk” without explicit approval by DOE and Battelle. If you are offered a position at PNNL and currently have any affiliation with the government of one of these countries, you will be required to disclose this information and recuse yourself of that affiliation or receive approval from DOE and Battelle prior to your first day of employment.\n\nRockstar Rewards\n\nEmployees and their families are offered medical insurance, dental insurance, vision insurance, health savings account, flexible spending accounts, basic life insurance, disability insurance*, employee assistance program, business travel insurance, tuition assistance, supplemental parental bonding leave**, surrogacy and adoption assistance, and fertility support. Employees are automatically enrolled in our company funded pension plan* and may enroll in our 401k savings plan. Employees may accrue up to 120 vacation hours per year and may receive ten paid holidays per year.\n\n Research Associates excluded.Once eligibility requirements are met.\n\nClick Here For Rockstar Rewards\n\nMinimum Salary\n\nUSD $134,500.00/Yr.\n\nMaximum Salary\n\nUSD $219,500.00/Yr."}
{"text": "skills in the creation and interpretation of quantitative analyses and predictive models and assist the Director in the development of Analysis plans.Demonstrate ability to independently design rigorous clinical, financial, and quality analyses grounded in data science.Contribute to the write-ups, including relevant portions of manuscripts, abstracts, posters, and slide presentations. Author and present studies at scientific conferences and other appropriate venues on behalf of the study team, as needed.\n\nQualificationsEducation & background\nPh.D./MD with training or equivalent terminal degree in health economics, data science, statistics, computer science, or related field.Demonstrated expertise in relevant applied analytical methods in healthcare (payor/provider).At least 5 years of experience in developing, implementing, and overseeing models related to health services/ outcomes research and medical information programs or related work experience.A comparable combination of education and experience will be considered in lieu of the above-stated qualifications.Demonstrate prior independent application of data science methods specifically to healthcare industry data at the expert level.Ability to leverage cutting-edge data science experience from other industries (e.g. population segmentation, risk analysis, optimization analysis, real-time analytics) to advance healthcare analytics will be strongly considered in lieu of healthcare experience.\n Advanced Analytics SkillsetA high level of proficiency in clinical and scientific research methodologies to generate research questions, and query complex clinical data to conduct descriptive and predictive analysis that create new insights to address companies’ business needs.Expert ability to extract and manipulate data utilizing SQL from large, complex data sets without supervision.Expert in using SAS or R or other statistical packages to conduct statistical analysis and modeling.Ability to independently select and apply multiple advanced data mining, statistical analysis, and predictive modeling methods.Experience with observational study designs, including fluency in methods for confounding control (e.g. propensity scores), tactics for missing data (e.g. imputation), and other statistical methods relevant to the study of large real-world data sets.Knowledge of hands-on experience with data visualization tools (e.g. Tableau, Power BI, etc.)\nCommunication & Stakeholder InteractionEffective data analysis and interpretation skills with the ability to draw and present quantitative conclusions leveraging graphs, and other visualizations to enable rapid understanding of clinical data to deliver business insights.Ability to evaluate, interpret & synthesize scientific data, and to verbally present and critically discuss clinical trials and published data in internal and external meetings.Present updates (written and/or verbal) on business results using insights from dashboards, reports, and/or ad-hoc analysis.Exceptional interpersonal skills, and entrepreneurial orientation characterized by pragmatism, independence, self-determination, and an agile, flexible behavior style.Excellent communicator with the ability to prepare and deliver clear scientific and business communication materials (documents, presentations) for internal and external facing activities.Ability to influence senior executives through effective executive communication of data science methods and study results.\nBusinessDemonstrated understanding of the differences between business requirements, scientific rigor, and technical constraints with the ability to distill complex issues and ideas down to simple comprehensible terms.Demonstrated understanding of financial metrics and cost efficiencies that have a positive business impact.\nProject ManagementExcellent time management, organizational, and prioritization skills with the ability to balance multiple priorities with experience in project management including proposal or grant preparation, developing project plans and Gantt charts, spending plans, and managing work execution including earned value.Quick learner with the ability to gain a deep understanding of company processes, cross-service interactions, and interdependencies.Self-driven, scientifically curious individual who thrives in a high pace, and rapidly evolving business environment that supports entrepreneurs and founders.\nPreferredExperience with Agile principles and development methodology.Broad knowledge of advanced analytics, research infrastructure, discovery platforms, and the application of artificial intelligence/machine learning for health."}
{"text": "SKILLS:1. Work experience in a Human Services agency ideally related to human services programs including Electronic Benefits Transfer (EBT) including SNAP and TANF benefits.2. Experience with Quick Base platform and SQL.\n3. Strong proficiency in data science tools such as R or Python. Experience with data visualization tools such as Tableau or Power BI\n 4. Ability to transform issuance and notices files.\n \nResponsibilities\n1. Data analysis and modelling, including Designing and developing machine learning and predictive models and algorithms.\nPerforming exploratory data analysis to identify patterns and trends.Developing and maintaining database and data systems to support business needs.Interpreting and communicating data analysis results to stakeholders.Collaborating with other teams to develop and implement data-driven solutions.2. Data management and governance, including Ensuring compliance with data privacy regulations and company data governance policies.\nDeveloping and implementing data access and security controls.Identifying and resolving data quality issues.Managing data migration and integration projects.3. Provide subject matter expertise on data-related topics, including Providing guidance and support to other teams on data-related issues.\nDeveloping and delivering training and education materials related to data analysis and modelling.Employing data science tools to improve data collection and accuracy.Identifying opportunities to utilize data science tools to streamline business processes and increase efficiency.Using data science tools to improve technical reports and the effectiveness of reporting databases and platforms.4. Collaborate with stakeholders to identify and prioritize data-related initiatives, including Partnering with business leaders to understand their data needs and goals.\nContributing to the development of data strategies and roadmaps.Providing recommendations for data-related investments and initiatives.Leading or participating in cross-functional teams to execute data-related initiatives.Qualifications:\nMinimum of 5 years of experience in data analysis/science and modelling, with a focus on machine learning and/or predictive analytics.Strong knowledge of data science tools, including data quality, governance, and security.Strong proficiency in data science tools such as R or Python. Experience with data visualization tools such as Tableau or Power BI Experience with SQL and other database technologies.Ability to work independently and manage multiple projects simultaneously."}
{"text": "requirements Provide technical support to assist clients and partners during and after product implementation Engage in professional development opportunities to remain up to date on best practices surrounding data strategy to support Gen AI products \n\nMust-haves:\n\n3+ years of relevant work experience Understanding of complex data flows, identification of data processing bottlenecks and designing and implementing solutions Ability to assess business rules, collaborate with stakeholders and perform source-to-target data mapping, design and review Proficient in C#, Python, SQL Experience working with Azure Functions Experience working with Power BI and other Microsoft Power Platform products Experience in software development in a production environment. Experience in cloud computing and data storage Experience processing large sets of structured, semi-structured, and unstructured data (cleansing, storage, retrieval) Experience supporting Web Applications is preferred Proven ability to balance and manage multiple, competing priorities Collaborative interpersonal skills and ability to work within cross-functional teams A Bachelor's degree is preferred \n\nPerks are available through our 3rd Party Employer of Record (Available upon completion of waiting period for eligible engagements)\n\nHealth Benefits: Medical, Dental, Vision, 401k, FSA, Commuter Benefit Program\n\nPlease note: In order to create a safe, productive work environment, our client is requiring all contractors who plan to be onsite to be fully vaccinated according to the CDC guidelines. Prior to coming into our offices, contractors will be required to attest that they are fully vaccinated."}
{"text": "experience in deploying real-time AI/ML models using Google Cloud PlatforStrong programming skills in Python and PySpark.· Proficiency with SQL and relational databases, data warehouses, and BigQuery.· Experience in scaling marketing-related AI/ML solutions such as cross/upsell, recommended systems, and category propensity.· Experience in deploying and managing Large scale Machine Learning Models is a plus· Expertise with classical ML algorithm like K-NN, LSH, logistic regression, linear regression, SVM, Random forest and clustering.· Good understanding of ML & DL algorithms and frameworks (Scikit-learn,Spacy, Tensorflow/Keras/ PyTorch)· Experience in deep learning Algorithms like MLP, CNN, RNN, LSTMs and GANs, Transformers and LLMs.· Excellent programming skills in Python· Expertise in Google Cloud and operationalization of models using MLOPs.· Experience in scheduling jobs for automated training and inference of AI/ML models using airflow or any other workflow orchestration platform.· Proficiency in collecting data from different data sources, data cleaning, preprocessing, and feature engineering.· Understanding of regression, classification, and unsupervised ML algorithms.· Experience in mentoring junior associates in scaling AI/ML models.· Excellent problem-solving and analytical skills.· Strong written and verbal communication skills, with the ability to present and explain complex concepts to both technical and non-technical audiences."}
{"text": "experience and make a real impact for our members.\n\nWe believe in our core values of honesty, integrity, loyalty and service. They’re what guides everything we do – from how we treat our members to how we treat each other. Come be a part of what makes us so special!\n\nThe Opportunity\n\nWe offer a flexible work environment that requires an individual to be in the office 4 days per week. This position can be based in one of the following locations: San Antonio, TX; Phoenix, AZ; Colorado Springs, CO; Plano, TX or Tampa, FL.\n\nRelocation assistance is not available for this position.\n\nThis candidate selected for this position will be working on the D&S Data Science team applying artificial intelligence and machine learning solutions to support a variety of business applications from automating key business processes, to improved routing of phone calls, to better understanding our members needs and the service we deliver. This position will work with a broad range of business partners from product lines to contact center and everything in between.\n\nTranslates business problems into applied statistical, machine learning, simulation, and optimization solutions to advise actionable business insights and drive business value through automation, revenue generation, and expense and risk reduction. In collaboration with engineering partners, delivers solutions at scale, and enables customer-facing applications. Leverages database, cloud, and programming knowledge to build analytical modeling solutions using statistical and machine learning techniques. Collaborates with other data scientists to improve USAA’s tooling, growing the company’s library of internal packages and applications. Works with model risk management to validate the results and stability of models before being pushed to production at scale.\n\nWhat You’ll Do\n\nCaptures, interprets, and manipulates structured and unstructured data to enable analytical solutions for the business.Selects the appropriate modeling technique and/or technology with consideration to data limitations, application, and business needs.Develops and deploys models within the Model Development Control (MDC) and Model Risk Management (MRM) framework.Composes technical documents for knowledge persistence, risk management, and technical review audiences. Consults with peers for mentorship, as needed.Translates business request(s) into specific analytical questions, executing on the analysis and/or modeling, and communicating outcomes to non-technical business colleagues.Consults with Data Engineering, IT, the business, and other internal stakeholders to deploy analytical solutions that are aligned with the customer’s vision and specifications and consistent with modeling best practices and model risk management standards.Seeks opportunities and materials to learn new techniques, technologies, and methodologies.Ensures risks associated with business activities are optimally identified, measured, monitored, and controlled in accordance with risk and compliance policies and procedures.\n\nWhat You Have\n\nBachelor’s degree in mathematics, computer science, statistics, economics, finance, actuarial sciences, science and engineering, or other similar quantitative discipline; OR 4 years of experience in statistics, mathematics, quantitative analytics, or related experience (in addition to the minimum years of experience required) may be substituted in lieu of degree.2 years of experience in predictive analytics or data analysis OR advanced degree (e.g., Master’s, PhD) in mathematics, computer science, statistics, economics, finance, actuarial sciences, science and engineering, or other similar quantitative discipline.Experience in training and validating statistical, physical, machine learning, and other advanced analytics models.Experience in one or more dynamic scripted language (such as Python, R, etc.) for performing statistical analyses and/or building and scoring AI/ML models.Ability to write code that is easy to follow, well detailed, and commented where necessary to explain logic (high code transparency).Experience in querying and preprocessing data from structured and/or unstructured databases using query languages such as SQL, HQL, NoSQL, etc.Experience in working with structured, semi-structured, and unstructured data files such as delimited numeric data files, JSON/XML files, and/or text documents, images, etc.Familiarity with performing ad-hoc analytics using descriptive, diagnostic, and inferential statistics.Experience with the concepts and technologies associated with classical supervised modeling for prediction such as linear/logistic regression, discriminant analysis, support vector machines, decision trees, forest models, etc.Experience with the concepts and technologies associated with unsupervised modeling such as k-means clustering, hierarchical/agglomerative clustering, neighbors algorithms, DBSCAN, etc.Ability to communicate analytical and modeling results to non-technical business partners.\n\nWhat Sets You Apart\n\nKnowledge or experience with Natural Language Processing (NLP).Intermediate experience using Python.\n\nThe above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.\n\nWhat We Offer\n\nCompensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location. The salary range for this position is: $89,990 - $161,990.\n\nEmployees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.\n\nBenefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.\n\nFor more details on our outstanding benefits, please visit our benefits page on USAAjobs.com.\n\nApplications for this position are accepted on an ongoing basis, this posting will remain open until the position is filled. Thus, interested candidates are encouraged to apply the same day they view this posting. \n\nUSAA is"}
{"text": "experience with Data Modeling in Big Data environment and have worked on massive structured/unstructured datasets beforeBig Data stack (Hadoop, Hive, Spark, Kafka, Airflow/OOZIE, BigQuery/Presto/Impala etc.)Minimum 5 years of Full Stack expertise inone of the following stacks and comfortable exploring othersMERN stack: JavaScript - MongoDB - Express - ReactJS - Node.js (Preferred)MEAN stack: JavaScript - MongoDB - Express - AngularJS - Node.jsLAMP stack: JavaScript - Linux - Apache - MySQL – PHPLEMP stack: JavaScript - Linux - Nginx - MySQL – PHPDjango stack: JavaScript - Python - Django – MySQLRuby on Rails: JavaScript - Ruby - SQLite – RailsExperience working in AWS/GCP/Azure cloud environmentsServer programming using NodeJS, Python, PHP, ASPDatabase programming using SQL, SQLite, Hive/Hadoop, or MongoDBExperienced in APIs and Micro services development and managementProficient in object-oriented language – Python/ScalapreferredExpert in networking concepts and security protocolsSolid understanding of containerized platforms (Docker, Kubernetes)Experience using GIT, JIRA, and Confluence tools\n\n\nJob ResponsibilitiesDesign, develop, test, deploy, maintain and enhance our desktop-based data web applications from front-end to back-end.Triage product and system issues and debug/resolve by analyzing the source of issues and impact on service operations and qualityCollaborate with product managers, data engineers, data analysts, and marketing tech vendors to prioritize engineering deadlines and deliverablesWork with data engineers and data scientists to design and deploy complex data models, fueling the applicationDeploy applications on cloud infrastructure, ensuring scalability, performance, and securityIntegrate application with in-house big data system and third-party marketing systems using APIs and other solutionsWrite clean, efficient, and reusable code and contribute to existing documentation and adapt content based on product/program updates and user feedbackStay updated on emerging web application and data modeling technologies and integrate with the product where applicableWork with product managers, data scientists, and engineers to understand business goals and data science and marketing tech stack of the company\n\nCore Qualifications:Curious, ownership, and outcome mindset with the ability to pivot as per business requirementsBachelor’s degree in Computer Science, Engineering, or related fieldYou have interest to grow your knowledge in the area of Data Science and willing to lend a hand in “Machine Learning” application development when requiredComfortable with Agile Principles/Scrum/KanbanExperience developing products for marketing and sales teams in Omni-channel organizations, small or large"}
{"text": "experience in a Data analytics role Clinical experience (PT, informatics, healthcare, quality, safety, etc.)Strong SQL experience Strong Excel skills Simple formulas etc. Microsoft Office Suite Experience using Tableau and Power BI Building dashboards and visualizationsExperience with reporting to CMS or other regulatory agencyAbility to perform data miningnical database information to include, but not limited to, the enterprise data warehouse (EDW). Plusses:Epic certifications Health Data Analyst (CHDA), or Professional in Healthcare Quality (CPHQ), or Professional in Patient Safety (CPPS) certificationExcellent communication and experience working with C-Suite level  Day to Day: The role of Strategic Clinical Reporting Specialist is pivotal in leveraging clinical expertise and familiarity with electronic medical records to enhance the creation and upkeep of reports focused on quality and safety. The specialist will act as a vital asset to healthcare facilities, guiding them towards meeting their strategic objectives for quality and safety. With proficiency in contemporary reporting software, the specialist will oversee report generation, problem-solving, educational efforts, and updates to the system. Additionally, this role encompasses providing support to clinical users in the domain of Clinical Quality & Safety, necessitating active engagement with various corporate sectors, including IT, to facilitate the seamless execution of prioritized quality and safety strategies. - Serve as a clinical information specialist for Microsoft products, with a focus on Power Business Intelligence and Power Automate.- Offer expertise in Epic System and act as a subject matter expert for clinical reporting modules.- Utilize quality measurement tools to provide strategic insights and promote transformative clinical outcomes.- Apply clinical knowledge to develop, test, troubleshoot, and maintain reports containing clinical data.- Analyze outcomes related to quality and safety, identify performance gaps, and pinpoint improvement opportunities.- Lead report requests and modifications to enhance clinical and market quality and safety.- Work collaboratively with IT and Clinical Analytics teams to ensure data accuracy and quality for clinical reporting.- Adopt a holistic, comprehensive, and team-based approach to efficiently extract accurate healthcare quality data using electronic health record and analytic tools.- Provide ongoing support and facilitate user support calls for healthcare facilities.- Keep abreast of data warehouse content and learn to utilize new database tools as they are introduced.- Support new facilities with clinical quality and safety reporting and assist in transitioning other clinical reporting functions."}
{"text": "requirements, identifying relevant data points for analysis, scrutinizing data to derive insights, and presenting findings through visualization. Additionally, the analyst will provide support for verification and user acceptance testing, as well as conduct training sessions to facilitate the adoption of new solutions. Their overarching goal is to redefine how data is utilized, promoting informed decision-making across the organization.\n\nRelationships\n\nReports to a Sr Manager – Enterprise Analytics.\n\nInteracts with internal stakeholders across various business units as well as external suppliers and partners.\n\nEssential Functions\n\n Have a strong data and analytics background, experienced in building dashboards and analytics solutions for functional and cross-functional teams, as well as proficient in data modeling and report visualizations  Collaborate with business stakeholders to address complex business challenges  Responsible for identifying, analyzing, and interpreting trends or patterns in complex data sets  Able to independently manage multiple projects with competing priorities  Leverage data, descriptive and predictive analytics, and data visualizations to monitor and improve business processes through actionable insights that drive operational excellence  Excellent problem solving, critical thinking, and communication skills with the ability to understand, communicate, and present technical and complicated analyses/solutions in a clear and simple manner to stakeholders  Serve as a technical authority in analytic tools and best practices providing support in training, mentoring, and enhancing the skills of other team members  Should have excellent communication skills, written and verbal, high attention to detail, and the ability to work effectively in cross-functional, matrixed teams  Acts as a primary liaison between the business function and vendor to provide updates, ensure alignment and monitors vendor activities  Should have an expert understanding of the software development lifecycle, while working with hybrid teams including dev teams and third-party vendors \n\nPhysical Requirements\n\n0-10% overnight travel required. 10% travel including some international.\n\nQualifications\n\n Education Level: Bachelor’s degree in business, information systems, computer science, or related field preferred  Experience Level: 8+ years related experience in delivering BI and analytics solutions  Specific or technical job skills: Experience in BI solution design, data management and database development techniques  Experience in Dashboarding and Visualization Technologies (Qlik Sense, Tableau, Power BI etc.) is a must  Working knowledge of data modeling, data warehousing, and ability to access relational and multi-dimensional databases (e.g. SQL, Snowflake, Redshift)  Working knowledge of augmented analytics tools like Tellius/Thoughtspot  Ability to work independently  Demonstrated problem solver with an ability to provide technical solutions to a wide range of complex problems  Excellent technical writing, document organization, technical document review and analytical thinking skills are required  Excellent interpersonal, negotiation, written and oral communication skills  Expert knowledge of systems analysis and design techniques  Knowledge of all phases of the project and system development methodologies \n\nPreferred Skills\n\n Knowledge of Pharmaceutical Data like iQVIA, KOMODO, Veeva, Customer or Product Master Systems is a plus  Knowledge of Snowflake Database is a plus  Experience in handling onsite/offshore team is a plus  Experiences in agile development and testing preferred \n\nWe commit to an inclusive recruitment process and equality of opportunity for all our job applicants.\n\nAt Novo Nordisk we recognize that it is no longer good enough to aspire to be the best company in the world. We need to aspire to be the best company for the world and we know that this is only possible with talented employees with diverse perspectives, backgrounds and cultures. We are therefore committed to creating an inclusive culture that celebrates the diversity of our employees, the patients we serve and communities we operate in. Together, we’re life changing.\n\nNovo Nordisk is \n\nIf you are interested in applying to Novo Nordisk and need special assistance or an accommodation to apply, please call us at 1-855-411-5290. This contact is for accommodation requests only and cannot be used to inquire about the status of applications."}
{"text": "requirements.Utilize software to create metrics and develop actionable recommendations.Identify, manage, and implement process improvements related to reporting delivery.Collaborate with stakeholders to identify reporting needs.Work closely with technical support, database administrators, software developers, and other business groups.Assist in data discovery for cleanup activities with users and project teams.Train new and existing staff on query development and usage, including creating training materials.\nBasic Qualifications:Bachelor's degree in Information Technology or Computer Science.9 years of progressively responsible programming experience or equivalent combination of training and experience.Minimum of 7 years of experience in QA, data, and reporting optimization.Expertise in SQL, SSMS, SSRM, and Excel.\nAPPLY TODAY!\n*Solü Technology Partners provides equal employment opportunities ("}
{"text": "Hi ProfessionalHope you're doing well, Please go through the below job, let me know if you’re interested to apply, kindly share your most updated resume. GCP Data Engineer FULLY REMOTE 6+ Months  MUST HAVE:GCPAirflow or Cloud ComposerKafkaBigQuery"}
{"text": "requirements.Optimizing the existing GenAI models for performance improvement, scalability, and efficiencyDevelop and maintain the AI Pipeline that includes data processing, feature extraction, model training and evaluation.Collaboration with software engineering and operations teams to ensure seamless integration and deployment of AI models.Develop the documentation like technical specification, user guides, technical architecture, etc.SkillsBachelor’s or master’s degree in computer science, Engineering, or a related fieldMinimum 5 years of experience in Data Science and Machine LearningIn-depth knowledge of machine learning, deep learning, and generative AI techniquesKnowledge and experience of development and implementing Generative AI modelsProficiency in programming languages such as Python, R, and frameworks like TensorFlow, PyTorch or KerasExperience with natural language processing (NLP) techniques and tools, such as SpaCy, NLTK, or Hugging Face.Strong understanding of frameworks such as BERT, GPT, or Transformer modelsFamiliarity with computer vision techniques for image recognition, object detection, or image generationFamiliar with cloud-based platforms and services, such as AWS, GCP, or Azure.Expertise in data engineering, including data curation, cleaning, and preprocessingKnowledge of trusted AI practices, ensuring fairness, transparency, and accountability in AI models and systemsExcellent problem-solving and analytical skills, with the ability to translate business requirements into technical solutionsStrong communication and interpersonal skills, with the ability to collaborate effectively with stakeholders at various levelsTrack record of driving innovation and staying updated with the latest AI research and advancements\n Thanks and Regards,Vikash KumarUS Technical Recruiter InfoTech Spectrum Inc2060, Walsh Ave, #120, Santa Clara, CA 95050Direct : 551-273-2078Email : vikash.kumar@infotechspectrum.com Linkedin : https://www.linkedin.com/in/vikash-sharma-613467216/Web: www.infotechspectrum.com\nA Minority Business Enterprise, Certified by NMSDC"}
{"text": "Requirements: \nMinimum 3+ years in a data science function working in an equities trading environmentExpertise in Python, with a strong command of data manipulation and analysis libraries specifically, Pandas and NumPyProficient in Linux environments with shell-scripting capabilitiesProficient in managing and optimizing databases (Postgres in particular is a plus)\nOpen on compensation, hybrid work model\nIf interested please get in touch by applying or at Volkan.ozbicer@radleyjames.com with your CV to discuss further."}
{"text": "skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers.  Responsibilitiesknowledge of and experience applying multivariate statistical methods such as GLM, analysis of quasi-experimental research designs, SARIMAX, longitudinal analysis, classification, dimension reduction, clustering, hierarchical linear (random effects) modeling, etc.managing and analyzing structured and unstructured data using tools such as Python. Experience with machine learning, text mining/NLP, or modeling high-dimensional data Experience with Workforce Planning, talent acquisition and best methodologies to model their components and metrics:impact, demand planning, internal movement, attrition forecast, organization structure modeling, etc. Proficiency in reading and writing SQL queries 2+ years of experience consulting, including working with stakeholders to understand and clarify their people insight needs, and communicating analyses to technical and non-technical audiencesconceiving, developing, and implementing empirical research and experiments Experience designing and collaborating to build tools that communicate practical analytics to stakeholders in simple and easy to understand visuals and terms.Knowledge of best practices in talent & organization management areas such as employee selection, employee engagement, performance management, diversity, organizational structures, or retention\nRequired Skills: TABLEAU,HR METRICS,EMPLOYEE DATA MANAGEMENT,DATA VISUALIZATION,BAYESIAN STATISTICS,\nSince 2002 Maxonic has been at the forefront of connecting candidate strengths to client challenges. Our award winning, dedicated team of recruiting professionals are specialized by technology, are great listeners, and will seek to find a position that meets the long-term career needs of our candidates. We take pride in the over 10,000 candidates that we have placed, and the repeat business that we earn from our satisfied clients.\nInterested in Applying?\nPlease apply with your most current resume. Feel free to contact Neha Dhiman (neha.d@maxonic.com / (408) 400-2309) for more details"}
{"text": "Requirements NOTE: Candidates already possessing a Top Secret clearance preferred. Requirements * High School diploma or GED, Undergraduate degree preferred Ability to grasp and understand the organization and functions of the customer Meticulous data entry skills Excellent communication skills; oral and written Competence to review, interpret, and evaluate complex legal and non-legal documents Attention to detail and the ability to read and follow directions is extremely important Strong organizational and prioritization skills Experience with the Microsoft Office suite of applications (Excel, PowerPoint, Word) and other common software applications, to include databases, intermediate skills preferred Proven commitment and competence to provide excellent customer service; positive and flexible Ability to work in a team environment and maintain a professional dispositionThis position requires U.S. Citizenship and a 7 (or 10) year minimum background investigation ** NOTE: The 20% pay differential is dependent upon the customer's order for services and requires an Active Top-Secret security clearance. Agency Overview The mission of the Federal Bureau of Investigation (FBI) is to protect the American people and uphold the Constitution of the United States. FBI investigates a broad range of criminal violations, integrating the use of asset forfeiture into its overall strategy to eliminate targeted criminal enterprises. The FBI has successfully used asset forfeiture in White Collar Crime, Organized Crime, Drug, Violent Crime and Terrorism investigations. Benefits Overview At EnProVera, we recognize the diverse needs of our employees and strive to provide an excellent package to help meet those needs. Comprehensive benefits are offered with greater choice and flexibility to support your health, work-life balance, and professional growth. A package providing employee only coverage can be built around our basic plans at $0 employee cost for: Medical, Dental, Vision, Term Life Insurance, Accidental Death -amp; Dismemberment Insurance, Short-Term Disability, and Employee Assistance Program."}
{"text": "Experienced (relevant combo of work and education)\n\nEducation Desired\n\nBachelor of Computer Engineering\n\nTravel Percentage\n\n1 - 5%\n\nJob Description\n\nMachine Learning Data Engineer Specialist\n\nFIS technology processes more than $40 Trillion per year and enables 95% of the world’s leading banks. Our Fraud Intelligence team is on the cutting edge of data science and machine learning technology that detects and prevents fraud on a global scale. As a Machine Learning Data Engineer, you will tackle challenges ranging from identity theft , to credit card fraud, to money laundering, and more. The technology you build will protect individuals, businesses and financial institutions from fraudsters ranging from individuals up to multinational organized crime rings.\n\nThe fraud prevention space is fast-paced and rapidly changing . You will work cross-discipline with data scientists, analytics, product, and more. Our ideal candidate not only brings technical skills to the table but has the appetite to dig into deeply complex problems, while learning new skills along the way. We are leading the way and leveraging our wealth of data to create best-in-class solutions.\n\nNote~ This position is based in the greater Seattle/Bellevue, WA area. We plan to bring the team together regularly for design, ideation, and connection building.\n\nJob Description\n\nWe are looking for talented Machine Learning Engineer s to join our team. The ideal candidate will have experience in data management, building and deploying machine learning models, and managing the build pipelines.\n\nResponsibilities\n\n D esign, build, and manag e the data pipelines and infrastructure that collect, store, and process large volumes of transactional and customer data from various sources.  Develop , deploy , and scale machine learning models and applications in production and lower environments  E nsure data quality , security and availability for the data, notebooks, models, experiments and applications.  Integrate ML models with the SaaS platform and other services and tools, such as the model registry, feature store, data lake, and event streams.  Collaborate with data scientists to develop and test machine learning models.  Monitor and optimize machine learning models in production.  Govern the data in the pipeline.  Stay up-to-date with the latest developments in machine learning and data management.  Assist in setting roadmap direction of Fraud Intelligence.  Trains and mentors team members and clients. \n\nRequirements\n\n Bachelor’s or Master’s degree in Computer Science , Mathematics, Engineering or a related field.  10+ years of experience in machine learning engineering.  E xperience with data management and data pipelines.  Experience with building and deploying machine learning models.  Experience with managing build pipelines.  Strong programming skills in Python and Java .  Strong problem-solving skills.  Excellent communication and collaboration skills.  Experience with financial services data sources.  Experience with AWS, Snowflake, Databricks is required .  Experience with MLflow and Feast or other Feature Stores is helpful.  Typically requires ten or more years of experience . \n\nIf you are interested in joining this exciting new team, please visit the FIS careers page for more information .\n\n#Platform\n\nFIS is committed to providing its employees with an exciting career opportunity and competitive compensation. The pay range for this full-time position is $133,520.00 - $224,300.00 and reflects the minimum and maximum target for new hire salaries for this position based on the posted role, level, and location. Within the range, actual individual starting pay is determined additional factors, including job-related skills, experience, and relevant education or training. Any changes in work location will also impact actual individual starting pay. Please consult with your recruiter about the specific salary range for your preferred location during the hiring process.\n\nPrivacy Statement\n\nFIS is committed to protecting the privacy and security of all personal information that we process in order to provide services to our clients. For specific information on how FIS protects personal information online, please see the Online Privacy Notice .\n\n\n\nFIS is \n\nFor positions located in the US, the following conditions apply. If you are made a conditional offer of employment, you will be required to undergo a drug test. ADA Disclaimer~ In developing this job description care was taken to include all competencies needed to successfully perform in this position. However, for Americans with Disabilities Act (ADA) purposes, the essential functions of the job may or may not have been described for purposes of ADA reasonable accommodation. All reasonable accommodation requests will be reviewed and evaluated on a case-by-case basis.\n\nSourcing Model\n\nRecruitment at FIS works primarily on a direct sourcing model; a relatively small portion of our hiring is through recruitment agencies. FIS does not accept resumes from recruitment agencies which are not on the preferred supplier list and is not responsible for any related fees for resumes submitted to job postings, our employees, or any other part of our company.\n\n#pridepass"}
{"text": "experience of Walmart's associates.\n\nAt Walmart, we are seeking a talented and experienced Data Scientist to join our team. As a Senior Data Scientist, you will play a crucial role in our People Analytics department, utilizing your expertise in building causal inference machine learning models to drive data-driven decision-making.\n\nResponsibilities\n\nDevelop and implement advanced statistical models and machine learning algorithms to analyze large datasets and extract valuable insights.Collaborate with cross-functional teams and Center of Excellence (COE) partners across our Global People function to identify business problems and develop analytical plans to address them.Conduct rigorous statistical analysis and hypothesis testing to uncover patterns and trends in data.Apply Bayesian inference techniques to enhance the accuracy and reliability of predictive models.Utilize your strong problem-solving skills to scope problems and identify quick wins for our customers.Evaluate programs and quantify the return on investment (ROI) to drive data-driven decision-making.Collaborate primarily with the Global People centers of excellence and support our People partners in the business.\n\nRequirements\n\nBachelor's or Master's degree in Data Science, Statistics, Computer Science, or a related field.Minimum of 3 years of experience as a Data Scientist, such as in retail or e-commerce.Strong proficiency in building and productionizing and maintaining causal inference machine learning models.Familiarity with Bayesian inference is a strong plus.Experience using Directed Acyclic Graphs (DAGs) for causal modeling to avoid common pitfalls in causal inference, such as confounding variables and reverse causality.Proficiency in programming languages such as Python or R.Experience with SQL and handling very large datasetsExperience with data visualization tools such as Tableau or Power BI.Excellent problem-solving and analytical skills.Strong communication and interpersonal skills.People Analytics experience a plus but not required\n\nCulture At Walmart\n\nAt Walmart, we value diversity, inclusion, and collaboration. We foster an environment that encourages innovation and continuous learning. Our team members are passionate about making a positive impact on the lives of our customers and communities. We believe in empowering our employees and providing them with opportunities for growth and development.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years' experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years' experience in an analytics related field. Option 3 - 5 years' experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData science, machine learning, optimization models, Master's degree in Machine Learning, Computer Science, Information Technology, Operations Research, Statistics, Applied Mathematics, Econometrics, Successful completion of one or more assessments in Python, Spark, Scala, or R, Using open source frameworks (for example, scikit learn, tensorflow, torch)\n\nPrimary Location...\n\n508 SW 8TH ST, BENTONVILLE, AR 72712, United States of America"}
{"text": "skills and be responsible for leading advanced reporting development, performing data analytics, and managing projects within the BI domain. This role requires a strong foundation in BI tools and technologies, along with basic knowledge in machine learning and AI to drive data-driven decision-making processes. The BI Lead will collaborate closely with cross-functional teams to understand business needs and deliver actionable insights.\nWHAT YOU WILL BE DOING: Gather BI Requirements: Identifying gaps in BI and proposing targeted solutions (and BRD) to enhance the insights on end-to-end AI performance, reliability, and customer experience. Work with Back-end analyst to build the end-to-end reports. Phone and Chat Conversational Text Analysis: Collecting, organizing, and analyzing extensive datasets encompassing phone and chat interactions, including utterances, text, user feedback, metrics, system behavior, and user behaviors.Data Visualization: Using data visualization tools to represent complex data sets in intuitive and visually appealing ways, enabling stakeholders to grasp insights quickly.Trend Monitoring: Monitoring both emerging trends, outliner, and change management in both AI system behaviors and user behaviors, ensuring alignment with business objectives. Performance Evaluation: Evaluating the effectiveness and efficiency of AI models through benchmarking against predefined success criteria, ensuring continuous optimization and enhancement.Insight Generation: Extracting actionable insights from data analysis to guide opportunity identification and decision-making processes.Experiment Analysis: involves the systematic examination of experimental data to draw conclusions, evaluate hypotheses, and inform decision-making processesReporting and Communication: Presenting findings, recommendations, and insights to stakeholders, including both technical and non-technical audiences, through comprehensive reports, presentations, and various communication channels.Ad Hoc Analysis: Conducting ad hoc analysis to answer specific business questions or address immediate needs from stakeholders.Collaboration: Collaborating with cross-functional teams, including data scientists, engineers, planners, product managers, and business stakeholders, to drive optimization and BI solutions forward, fostering synergy and innovation.Ethical Considerations: Ensuring that AI technologies and applications adhere to ethical guidelines and principles, such as fairness, transparency, and accountability.\nMINIMUM QUALIFICATIONS: Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.Minimum of 5 years of experience in BI and data analytics, with a proven track record of leading BI projects.Strong knowledge of BI tools (e.g., Power BI, Tableau, Qlik) and databases (SQL, NoSQL).Experience with data modeling, ETL processes, and data warehousing concepts.Proficient in data analysis and visualization techniques.Basic understanding of machine learning algorithms and AI concepts.Excellent project management skills, with the ability to manage multiple projects simultaneously.Strong problem-solving abilities and analytical thinking.Effective communication and interpersonal skills, with the ability to interact with both technical and non-technical stakeholders, as well as offshore delivery team.  Desired Skills:Certification in BI tools or project management (e.g., PMP, Scrum Master).Experience with cloud data technologies (Azure, AWS, Google Cloud).Knowledge of programming languages (Python, R) for data analysis.\nWHAT WE HAVE TO OFFER:Because we know how important our people are to the success of our clients, it’s a priority to make sure we stay committed to our employees and making Beyondsoft a great place to work. We take pride in offering competitive compensation and benefits along with a company culture that embodies continuous learning, growth, and training with a dedicated focus on employee satisfaction and work/life balance. \nA competitive pay range of $65-70 /hr (depending on experience).15 days per year of Paid Time Off (PTO).9 paid holidays per year (which includes 1 personal floating holiday). 401(k) retirement plan with company match. Eligible employees (and their families) are offered the following company-sponsored benefits: Medical, dental, and vision insurance, health savings account (HSA), short-term and long-term disability, employee assistance plan (EAP), and basic life and AD&D insurance. Eligible employees (and their families) are offered the following voluntary employee-funded benefits: Health care flexible spending account, dependent care flexible spending account, commuter benefits, voluntary accident & critical injury coverage, voluntary long-term care coverage, and voluntary life and AD&D insurance.  Beyondsoft provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type with regards to race, color, religion, age, sex, national origin, disability status, genetics, veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, and the full employee lifecycle up through and including termination."}
{"text": "requirements into analytical frameworks.Dashboard Development: Design and maintain dashboards using Power Query in Excel, good in analytics in generating metrics & measures and ensuring accurate and real-time data representation. \nRequired QualificationsProfessional Experience: 3-6 years as a business analyst, with mandatory experience in the CPG sector and should have worked on brand dataTechnical Proficiency: Advanced skills in Excel and Power Query;Communication Skills: Exceptional ability to communicate complex data insights to non-technical stakeholders.Location: Position based in Springdale. Preferred AttributesProven experience in data-driven decision-making processes.Ability to handle multiple projects simultaneously, with a focus on deadlines and results."}
{"text": "Qualifications:\n\nBachelor's degree in Science with 9 years of experience or Master's Degree and 7 years of experience or PhD with 4 years of experienceHigh interest in problem solving and analyses using dataExperience with programming and development language syntaxBasic knowledge of probability and statisticsExperience creating and presenting analysis resultsActive DoD Secret Security Clearance Secret\n\nPreferred Qualifications\n\nExperience developing scripts in PythonExperience working with SQL and databasesExperience with Cisco network performance analysisExperience with Power BI and Tableau\n\nSalary Range: $139,700 - $209,500\n\nThe above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.\n\nEmployees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.\n\nThe application period for the job is estimated to be 20 days from the job posting date. However, this timeline may be shortened or extended depending on business needs and the availability of qualified candidates.\n\nNorthrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete"}
{"text": "experience across the globe! We have major campus work locations in the United States, Canada, Australia, India, France, Israel, and the United Kingdom. We believe in true innovation and collaboration from customer focused experiences to internal, day to day partnerships.\nTop skills: SQL, Tableau, Communication\nJob DescriptionJoin our Customer Success & Growth Team as a Data Analyst and help shape the future of Intuit's products and services. As a key member of our Data Analytics department, you'll play a crucial role in driving business strategies and optimizing performance through data-driven insights. Collaborating with cross-functional teams, you'll have the opportunity to influence decisions, improve customer experiences, and contribute to Intuit's continued success.\nDuties and ResponsibilitiesDrive business results: You will identify and help craft the most important KPIs to monitor the effectiveness of our operations and drive automated availability of those metrics. Amidst a sea of data, you will distill the data into key storylines that create a shared understanding of opportunities and influence teams to act.Amplify reporting insights: You will demonstrate your reporting craft by reimagining the existing reporting suite of dashboards using data visualization best practices, including uniform visuals, metric definitions, and accessible data dictionaries, to provide a comprehensive view of KPI performance.Improve operational performance: You will identify areas of operational and experiential opportunity using data-driven insights and root cause analysis, providing recommendations to subject-matter experts and partnering with teams to optimize to improve customer experiences and enable more efficient business processes.Automate data capabilities: You will leverage advanced modeling techniques and self-serve tools to develop new metrics, data pipelines, and expanded capabilities to automate processes and support business decisions, establishing consistent sources of truth and enabling faster customer speed to benefit.5+ years of experience working in the analytics field Ability to tell stories with data, educate effectively, and instill confidence, motivating stakeholders to act on recommendationsOutstanding communications skills with both technical and non-technical colleaguesExperience as a business partner for senior leaders; comfortable distilling complex data into a simple storyline Excited to mentor other team members and developing talent Highly proficient in SQL, Tableau, and ExcelExperience with programming languages including R or Python preferred Excellent problem-solving skills and end to end quantitative thinkingAbility to manage multiple projects simultaneously to meet objectives and key deadlinesProactive and inquisitive learner... seeks out and capitalizes on opportunities for change that enhance the business, rather than reacting to circumstances\nRequired Experience/SkillsBachelor's degree in Computer Science, Statistics, Mathematics, or related field.5+ years of experience in data analysis or a related role.Proficiency in SQL, Python, or other programming languages.Experience with data visualization tools such as Tableau.Strong analytical and problem-solving skills.Excellent communication and collaboration skills.Ability to work independently and prioritize tasks in a fast-paced environment.\nNice-to-HavesExperience in the financial technology industry.Knowledge of machine learning techniques.Familiarity with cloud platforms\nPay & Benefits SummaryTarget hourly rate: $59-69.57/hr.Reimbursement for travel (manager-approved expenses)Health benefitsTime off allowance (certain states applicable)Corporate outings and events.Holiday Pay\nBusiness Data Analyst | SQL | Tableau | Communication | Dashboard Creation | Data Pipelines | KPI Reporting | Data Analytics"}
{"text": "skills requirements: • Business Data Analysis with Wealth Management experience (10+ years). • Strong in AWS and SQL queries and Python. \nWealth Management Domain knowledge required: • Prime broker-dealer business, alternative investments, retirement funds, portfolio management • Experience working with ledger book tools like Beta, and Fund Master would be a plus. • Trade placing and execution on behalf of clients. Tools like Client worksheet balance, which advisors use to execute trades on behalf of the clients • Client portfolio construction, Client portfolio rebalancing as per market conditions, etc."}
{"text": "experience working in product analytics or data scienceTrack record of initiating, and executing large / complex deep dive projectsTrack record of developing data insights that generates business impactExpert level SQL, Python skillsExcellent data visualization skills and data storytelling skillsExperience in product data science in a dynamic, innovative, and fast-paced high tech environmentSolid understanding of statisticsPractical experience in AB testing and causal inferenceStrong understanding of ML algorithms\n\nWhat We Offer\n\nMarket competitive and pay equity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual lifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming and fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including company holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully stocked kitchens, and geo-specific commuter benefits\n\nBase pay for the successful applicant will depend on a variety of job-related factors, which may include education, training, experience, location, business needs, or market demands. The expected salary range for this role is based on the location where the work will be performed and is aligned to one of 3 compensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For other locations not listed, compensation can be discussed with your recruiter during the interview process.\n\nZone 1 (Menlo Park, CA; New York, NY; Bellevue, WA; Washington, DC)\n\n$161,500—$190,000 USD\n\nZone 2 (Denver, CO; Westlake, TX; Chicago, IL)\n\n$141,950—$167,000 USD\n\nZone 3 (Lake Mary, FL)\n\n$125,800—$148,000 USD\n\nClick Here To Learn More About Robinhood’s Benefits.\n\nWe’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing finance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you feel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people invigorated by our mission, values, and drive to change the world, not just those who simply check off all the boxes.\n\nRobinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants and employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and skills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone. Additionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy rights. To review Robinhood's Privacy Policy please review the specific policy applicable to the country where you are applying."}
{"text": "experience is important.\n\nSpecific Duties, Job Functions:\n\nDesign, Develop and Optimize AWS services with AWS Cost Explorer, Performance Insights, and Well-Architected Tools.Deliver data ingestion identified by Product owners.Design data ingestions, including source data characteristics, destination data store requirements, and performance and scalability needs.Participate in and lead code reviews focused on ensuring the code's correctness, efficiency, robustness, and readability.Propose and advocate for development standards (patterns, processes, tools, etc.)Manage implantation partner's resources and provide work instructions.\n\nBasic Qualifications\n\nBS in a technical discipline and 2+ years of building enterprise data platforms\n\nOR\n\nHS diploma and 5+ years of building enterprise data platformsExperience building data solutions using AWS services such as Appflow, Airflow, Redshift etc and Databricks.Experience building developer documentation with tools like JIRA, Confluence and GitHub etcProficiency in Python OR proficiency with a similar language and familiarity with PythonProficiency in SQLExperience deploying applications using Terraform or a similar tool.Experience building CI/CD pipelines using GitHub Actions or a similar toolProficiency in managing code as part of a team using GitHubFamiliarity with agile development methodologiesExperience building data-driven unit test suites for data platformsExperience modelling highly dimensional data\n\nPreferred Qualifications\n\nExperience in the biotech industry, or another highly regulated industryExperience with Starburst, ThoughtSpot, Tableau, or SpotFireExperience with Domain Driven Design for data modelling\n\nThe salary range for this position is: $90,695.00 - $117,370.00. Gilead considers a variety of factors when determining base compensation, including experience, qualifications, and geographic location. These considerations mean actual compensation will vary. This position may also be eligible for a discretionary annual bonus, discretionary stock-based long-term incentives (eligibility may vary based on role), paid time off, and a benefits package. Benefits include company-sponsored medical, dental, vision, and life insurance plans*.\n\nFor Additional Benefits Information, Visit:\n\nhttps://www.gilead.com/careers/compensation-benefits-and-wellbeing\n\n Eligible employees may participate in benefit plans, subject to the terms and conditions of the applicable plans.\n\nFor Jobs In The United States:\n\nAs \n\nFor more information about \n\nNOTICE: EMPLOYEE POLYGRAPH PROTECTION ACT\n\nYOUR RIGHTS UNDER THE FAMILY AND MEDICAL LEAVE ACT\n\nPAY TRANSPARENCY NONDISCRIMINATION PROVISION\n\nOur environment respects individual differences and recognizes each employee as an integral member of our company. Our workforce reflects these values and celebrates the individuals who make up our growing team.\n\nGilead provides a work environment free of harassment and prohibited conduct. We promote and support individual differences and diversity of thoughts and opinion.\n\nFor Current Gilead Employees And Contractors:\n\nPlease log onto your Internal Career Site to apply for this job."}
{"text": "experience. Through the use of modern technologies centered on data and analytics, we provide customers with powerful tools that are grounded in value, transparency and simplicity to improve cash flow management efficiency.\n\nOur Team\n\nThe Cloud Data Engineering team is a global team responsible for engineering and governance of public cloud database and storage platforms, and data integration solutions to support our Payment Orchestration and Execution Platform. Working in close partnership with application teams building the new system, the team is responsible for identifying application requirements and delivering resilient, secure, scalable solutions to fit their needs.\n\nWe are seeking highly collaborative, creative, and intellectually curious engineers who are passionate about forming and implementing cutting-edge cloud computing capabilities. Candidates should be comfortable working in a fast-paced DevOps environment.\n\nResponsibilities And Qualifications\n\n Partner with colleagues across engineering and risk teams to define, communicate, and promote data storage and data integration best practices and governance for public cloud application deployment.  Automate the provisioning of data services using Terraform.  Design and develop central Terraform modules to simplify the adoption of standard data services by application teams, such as databases, data streaming and analytics services.  Design, develop and maintain platform support for those same data services, including observability, resiliency, and availability.  Possess strong verbal and written communication skills and ability to present, persuade and influence peers, vendors, and executives.  Energetic, self-directed, and self-motivated, able to build and sustain long-term relationships across a multitude of stakeholders in a fast paced, multi-directional work environment.  Exceptional analytical skills, able to apply expertise to drive complex, technical and highly commercial solutions.  Experience supporting complex production application environments. \n\nBasic Qualifications\n\n Proficiency in designing, developing, and testing software in one or both of Python and Java; open to using multiple languages.  Experience with version control, continuous integration, deployment, and configuration management tools in a DevOps environment.  Ability to reason about performance, security, resiliency, and process interactions in complex distributed systems.  Experience meeting demands for high availability and scale.  Ability to communicate technical concepts effectively, both written and orally, as well as the interpersonal skills required to collaborate effectively with colleagues across diverse technology teams.  Ability to rapidly and effectively understand and translate requirements into technical solutions. \n\nPreferred Qualifications\n\n Hands on experience with open-source or commercial data streaming/ETL tools such as Apache Flink, Amazon Kinesis or Apache Kafka.  Hands on experience architecting, designing, and developing applications in an Amazon Web Services, Google Cloud Platform, or Microsoft Azure cloud environment.  Hands on experience with relational and NoSQL databases such as PostgreSQL, MongoDB, Redis or Amazon DynamoDB.  Experience using infrastructure as code tools (e.g. Terraform).  Experience using CICD (preferably Gitlab)."}
{"text": "Skills & Abilities (KSAs) [Proficiency Levels are from 5-Expert to 1-Novice]: 3-5 years’ experience in data engineering, including designing and implementing data pipelines and ETL processes. Proficiency with data management platforms such as SAS Viya, Alteryx, or others. (Proficiency level – 4) Proficiency in programming languages such as Python, SQL, or Java. (Proficiency level – 4) Strong analytical and problem-solving skills, with the ability to analyze complex datasets and extract actionable insights. (Proficiency level – 4) Knowledge of relational database design and data modeling. (Proficiency level – 4) Ability to establish and maintain effective working relationships with others. (Proficiency level – 3) Ability to work independently. (Proficiency level – 3) Ability to determine work priorities and ensure proper completion of work assignments. (Proficiency level – 3) Ability to communicate effectively, both verbally and in writing. (Proficiency level – 3) \nPreferred Knowledge, Skills & Abilities (KSAs): Familiarity with environmental science, water quality, or related fields. Experience with implementing data warehouses, data lakes, or data lakehouses. Experience with cloud computing platforms such as Azure.Experience with business intelligence tools such as Qlik Sense.\nEducation: Bachelor’s or master’s degree in Data Science, Computer Science, Information Systems or other Information Technology major, or equivalent work experience."}
{"text": "experience with kubernetes operating knowledge.Working with data pipelines and experience with Spark and FlinkExcellent communication skillsNice to have:Programming experience in Scala, Java, and PythonKnowledge on Machine Learning (Client)\nJob description:The client seeks to improve products by using data as the voice of our customers. We are looking for engineers to collaborate with users of our infrastructure and architect new pipelines to improve the user onboarding experience. As part of this group, you will work with petabytes of data daily using diverse technologies like Spark, Flink, Kafka, Hadoop, and others. You will be expected to effectively partner with upstream engineering teams and downstream analytical & product consumers. Experience:10+ YOE, with 5+ years of experience designing and implementing batch or real-time data pipelinesHands-on experience on batch processing (Spark, Presto, Hive) or streaming (Flink, Beam, Spark Streaming)Experience in AWS and knowledge in its ecosystem. Experience in scaling and operating kubernetes.Excellent communication skills is a must, experience working with customers directly to explain how they would use the infrastructure to build complex data pipelinesProven ability to work in an agile environment, flexible to adapt to changesAble to work independently, research on possible solutions to unblock customerProgramming experience in Scala, Java, or PythonFast learner and experience with other common big data open source technologies is a big plusKnowledge on machine learning (Client) is a nice-to-have"}
{"text": "requirements, gather data, and develop solutions that meet business objectives.Develop and maintain scalable and efficient codebase for training, testing, and deploying machine learning models in production environments.Conduct research and experiment with new techniques and algorithms to improve model performance and accuracy.Work closely with software engineers and DevOps teams to integrate machine learning models into existing systems and infrastructure.Collaborate with stakeholders to understand feedback and iterate on machine learning models to continuously improve performance and user experience.Mentor junior team members and provide technical guidance and support on machine learning best practices and methodologies.Stay up-to-date with the latest advancements in machine learning and AI technologies, and proactively identify opportunities for applying them to solve business problems.\nQualifications:\nUS Citizenship required for this roleBachelor's or Master's degree in Computer Science, Engineering, Mathematics, or a related field.10+ years of hands-on experience in developing and deploying machine learning models and algorithms, with a focus on Generative AI.Proficiency in programming languages such as Python, with experience in machine learning libraries such as OpenAI, TensorFlow, PyTorch, or Keras.Strong understanding of machine learning concepts and algorithms, including deep learning frameworks, reinforcement learning, and natural language processing.Experience with cloud platforms such as AWS, Azure, or Google Cloud for deploying machine learning models at scale.Solid understanding of software engineering principles and best practices, including version control, testing, and deployment pipelines.Excellent problem-solving skills and ability to think creatively to develop innovative solutions to complex problems.Strong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.Proven track record of delivering high-quality machine learning solutions on time and within budget in a fast-paced and dynamic environment."}
{"text": "requirements to technology solutions. You will serve as mentor for junior engineers within the team. You will be responsible for design and implementation of technical solutions to achieve business goals for the product offering.\n\nThe Skills You Bring\n\nMust have 5+ years of experience in Java application development using Spring Boot.2+ years of experience using AWS services S3, EC2, Lambda, CFT5+ years of experience in database design and development using Oracle / PostgreSQL, NoSQL databases (DynamoDB, AeroSpike).2+ years of UI development using Angular, jQuery, JavaScript, HTML 5Strong knowledge in Java, J2EE, Spring MVC, Spring Core, Angular.Strong understanding of CI/CD tools such as Jenkins, Artifactory, Deploying applications in DevOps environmentDeep understanding of API design, including versioning, API documentation (Swagger)Solid Understanding of developing highly scalable distributed systems using Open-source technologies.Solid Understanding of E2E ALM tools like JIRA, gitStash, FishEye, Crucible, Maven, Jenkins, uDeploy.Comfortable with Code Quality/Coverage tools (Sonar or equivalent)Working knowledge of public/private cloud capabilities including compute, storage and scaling will be desirable. \n\n\nThe Value You Deliver\n\nUse your experience to help design and implement efficient technology solutions.Be able to adjust priority of items based on business value.Be able to recognize technology trends in the cloud space and assist in adopting fresh solutions as offered by cloud service providers.Formulating and driving the tactical and strategic technology direction of the HealthCare business for some of the firm’s most strategic workplace clients.Proposing and delivering strategic efficiency and scale technology solutionsMaintaining an atmosphere of collaboration, and approachability every day \n\n\nHow Your Work Impacts The Organization\n\nFidelity Workplace Solutions delivers benefits solutions for many of the US largest businesses. Your work will deliver technology to support the Personalized recommendations in Workplace Investing, which will allow for a superior customer experience, innovation in product and service offering, and increased sales and retention.\n\nAt Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working”. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, all business days, every four weeks. These requirements are subject to change.\n\nCertifications\n\nCompany Overview\n\nFidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients’ money.\n\nJoin Us\n\nAt Fidelity, you’ll find endless opportunities to build a meaningful career that positively impacts peoples’ lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees’ Choice Award, we have been recognized by our employees as a top 10 Best Place to Work in 2024. And you don’t need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you’ve always imagined.\n\nFidelity's working model blends the best of working offsite with maximizing time together in person to meet associate and business needs. Currently, most hybrid roles require associates to work onsite all business days of one assigned week per four-week period (beginning in September 2024, the requirement will be two full assigned weeks).\n\nAt Fidelity, we value honesty, integrity, and the safety of our associates and customers within a heavily regulated industry. Certain roles may require candidates to go through a preliminary credit check during the screening process. Candidates who are presented with a Fidelity offer will need to go through a background investigation, detailed in this document, and may be asked to provide additional documentation as requested. This investigation includes but is not limited to a criminal, civil litigations and regulatory review, employment, education, and credit review (role dependent). These investigations will account for 7 years or more of history, depending on the role. Where permitted by federal or state law, Fidelity will also conduct a pre-employment drug screen, which will review for the following substances: Amphetamines, THC (marijuana), cocaine, opiates, phencyclidine.\n\nWe invite you to Find Your Fidelity at fidelitycareers.com.\n\nFidelity Investments is \n\nFidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 3."}
{"text": "Cloudious LLC is one of the fastest emerging IT Solutions and Services company headquartered in San Jose, CA with their global offices in Canada, EMEA & APAC.\nWe are currently hiring a seasoned Sr. Data Engineer who comes with a strong consulting mindset\nSr. Data EngineerDallas, TX (Onsite)12+ Months ContractNeed 9+ Years Mandatory\nPython , SparkSQL , BigQuery , Spark , Cloud SQL , BigQuery ML , Risk Management , Spark in Scala"}
{"text": "requirements for new reporting requests.Performs other reporting and analysis as requested.Performs other duties as assigned. Privacy and Data Security requirements.Understand that compliance with these responsibilities is critical to BI operations, security, and compliance requirements.\nMinimum RequirementsBachelor’s Degree in Accounting, Finance or related field required.At least three years of experience in an accounting or finance related position.Valid state drivers’ license and the ability to safely operate a motor vehicle to travel to field offices. US citizenship required.Ability to receive client’s approval to work on contract required.Must live in the US 3 of the last 5 years (military and study abroad included).Expert knowledge of MS Excel software, including Vlookups and pivot tables.Working knowledge and understanding of Generally Accepted Accounting Principles with the ability to apply to business transactions.Analytical ability.Attention to detail.Effective communication skills with internal and external contacts at all levels.Good interpersonal skills.Ability to handle multiple tasks simultaneously, prioritize appropriately and meet deadlines.Sense of urgency with the ability to respond quickly to internal and external customer requests.Ability to work with computers and the necessary software typically used by the department. Working Conditions: Encountered on a regular basis as part of the work this job performs.Typical office environment.Some travel is required.BI Incorporated"}
{"text": "experience in querying the high volumes of data and doing the data analysis must inform how you design solutions that align with the business objectives.Advanced proficiency with SQL and programming languages such as Python and Spark is a must.Experience with HadoopExperience with data integration from different sources into Big Data systems is preferable.Experience optimizing data pipelines, architecture, and datasets.Knowledge of cloud technologies like AWS is preferable but not required.Experience quality testing and coding. These solutions will deploy across products that are important to our customers and the business. They must be high-quality and functional.A willingness to collaborate. Our best work is done when we work together - either with non-technical or technical leads. You should be interested in learning from others regardless of their role in the organization.You have worked previously with an Agile team or understand these concepts. You expect to participate in daily standup meetings, you’ll complete your projects or stories during our sprints, and you’ll be ready to meet frequent deployment deadlines.\n\nThe starting pay range for this position is:\n\n$104,400.00 - $139,200.00\n\nAdditionally, you will be eligible to participate in our incentive program based upon the achievement of organization, team and personal performance.\n\nRemarkable benefits:\n\n Health coverage for medical, dental, vision 401(K) saving plan with company match AND Pension Tuition assistance PTO for community volunteer programs Wellness program Employee discounts\n\nAuto Club Enterprises is the largest federation of AAA clubs in the nation. We have 14,000 employees in 21 states helping 17 million members. The strength of our organization is our employees. Bringing together and supporting different cultures, backgrounds, personalities, and strengths creates a team capable of delivering legendary, lifetime service to our members. When we embrace our diversity – we win. All of Us! With our national brand recognition, long-standing reputation since 1902, and constantly growing membership, we are seeking career-minded, service-driven professionals to join our team.\n\n“Through dedicated employees we proudly deliver legendary service and beneficial products that provide members peace of mind and value.”\n\nAAA is \n\nThe Automobile Club of Southern California will consider for employment all qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable federal, state and local laws, including the City of Los Angeles’ Fair Chance Initiative for Hiring Ordinance."}
{"text": "skills in SQL and data analysis, and extensive experience in data migrations and conversions. The successful candidate will be instrumental in leading the design and implementation of SAP project deliverables, ensuring high-quality data integration, migration, and system efficiency.\nKey Requirements:Proven experience in SAP ECC/S4 data migration and system implementations.Strong command of SQL and proven data analysis capabilities.Familiarity with SAP standard table structures and the ability to efficiently handle data build-out tasks.Demonstrated leadership in managing comprehensive SAP projects from design through to go-live.Excellent problem-solving skills and the ability to work independently.Effective communication and interpersonal skills to lead and collaborate with diverse teams.Resumes must be concise, not exceeding 7 pages.Must be COVID vaccinated, as per company policy, even if working remotely."}
{"text": "experience in bash and cloud concepts such as (EC2, EMR, Glue, ECS, Lambda, IAM, Security Groups, S3, etc.)Utilize programming languages like Python, Java and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as SnowflakeCollaborate with Product owner and Tech lead to solve Business user questions on the data pipelines team supports and resolve infrastructure issues.\nKey Requirements and Technology Experience: \nSkills: Lead Data Engineer- Python, Spark/PySpark , AWS- EMR, Glue.5+ years experience using programming languages like Python, Java5+ years Distributed data/computing tools (MapReduce, Hive, Spark, EMR, Kafka)3+ years experience in AWS tech stack (EC2, EMR, Glue, ECS, Lambda, IAM, Security Groups, S3, etc.)3+ years AgileFlexible in experimenting with and learning new technologies\nOur client is a leading Financial Industry, and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration. \nPyramid Consulting, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws."}
{"text": "skills in order to help us expand our global features. This individual will be integral in ensuring we are able to deliver high quality data to our customers.\n\nPlease note that this contract term up to 9 months.\n\nMajor Responsibilities\n\nAnalyze and improve data quality of multilingual text classifiers.Work with linguistics and engineering teams to build out new parsers across languages.Translate various taxonomies such as Skills, Titles, and Occupations.\n\nSkills/Abilities\n\nCompetency in reading and writing JapaneseUnderstanding of syntax and structural analysis of languagesMicrosoft Excel experience (including vlookups, data cleanup, and functions)Knowledge of query languages such as SQLStrong knowledge of rule writing using RegExKnowledge of text analysis or machine learning principlesExperience with data analysis using tools such as Excel or Python\n\nEducation And Experience\n\nBachelor’s degree in Linguistics, Data Analytics, NLP or similar (preferred)Knowledge of other languages. \n\nLightcast is a global leader in labor market insights with headquarters in Moscow (ID) with offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities. Lightcast is proud to be an equal opportunity workplace and is committed to"}
{"text": "experience in data analysis.Must have 3+ years with Alteryx used professionally, “Alteryx core certified” candidates will get a prioritized interview slots. Experience supporting full Agile and Waterfall software development lifecycles (including understanding business processes, gathering user requirements, design, testing, deployment and training).Advanced SQL knowledge and experience.Advanced experience in Jira, Confluence, Excel, Tableau and VBA preferred\nThank youMahesh SanaEA Team INC."}
{"text": "experience as an SME in complex enterprise-level projects, 5+ years of experience analyzing info and statistical data to prepare reports and studies for professional use, and experience working with education and workforce data.\nIf you’re interested, I'll gladly provide more details about the role and further discuss your qualifications.\nThanks,Stephen M HrutkaPrincipal Consultantwww.hruckus.com\nExecutive Summary: HRUCKUS is looking to hire a Data Analyst resource to provide data analysis and management support. The Data Analyst must have at least 10 years of overall experience.\nPosition Description: The role of the Data Analyst is to provide data analysis support for the Office of Education Through Employment Pathways, which is located within the Office of the Deputy Mayor for Education. This is a highly skilled position requiring familiarity with educational data and policies.\nThe position will require the resources to produce data analysis, focusing on education and workforce-related data sets, and to produce public-facing write-ups of the data analysis that share key data insights in accessible language for the public. The Data Analyst shall have knowledge and expertise with R and Tableau. The role will involve working closely across education and workforce agency teams. The ETEP Data Analyst should possess solid written and verbal communication skills and be able to address both business and technical audiences.\nDuties:Analyzes information and statistical data to prepare reports and studies for use by professionalsCreates charts and graphics to present statistical analysis in an easily digestible format for a non-technical audienceCreate public-facing written reports to present analytic findings to the public in an accessible languagePlans, organizes, and conducts research focused on education and workforce-related topics\nPosition Requirements:Master’s Degree in Data Analytics or related field or equivalent experienceExperience working in R requiredExperience in working with business stakeholders to support their data needsExperience presenting technical information to non-technical audiencesExperience working with education and workforce-related data preferred\nSkill | Required/ Desired | YearsExperience analyzing info and statistical data to prepare reports and studies for professional us | Required | 5Data analysis using R | Required | 5Experience working with education and workforce data | Required | 5MS Excel | Required | 6Strong communication skills - must be able to communicate (written/oral) complex data concepts to non-technical stakeholders-incl. public | Required | 5Experience planning, organizing, and conducting research in various areas | Required | 56-10 yrs. as SME in complex enterprise-level projects | Required | 6Master’s degree in Data analysis or related field or equivalent experience | Required"}
{"text": "experience for our TikTok users. \n\nE-commerce - Alliance \nThe E-commerce Alliance team aims to serve merchants and creators in the e-commerce platform to meet merchants' business indicators and improve creators' creative efficiency. By cooperating with merchants and creators, we aim to provide high-quality content and a personalized shopping experience for TikTok users, create efficient shopping tools at seller centers, and promote cooperation between merchants and creators.\n\nE-commerce - Search\nThe Search E-Commerce team is responsible for the search algorithm for TikTok's rapidly growing global e-commerce business. We use state-of-the-art large-scale machine learning technology, the cutting-edge NLP, CV and multi-modal technology to build the industry's top-class search engine to provide the best e-commerce search experience, for more than 1 billion monthly active TikTok users around the world. Our mission is to build a world where \"there is no hard-to-sell good-priced product in the world\".\n\nE-commerce - Search Growth \nThe Search Growth E-commerce team is at the forefront of developing the search recommendation algorithm for TikTok's rapidly expanding global e-commerce enterprise. Utilizing cutting-edge machine learning technology, advanced NLP, CV, recommendation, and multi-modal technology, we're shaping a pioneering engine within the industry. Our objective is to deliver the ultimate e-commerce search experience to over 1 billion active TikTok users worldwide. experience, and promote healthy ecological development \n\nQualifications\n\n Qualifications\n- Bachelor above degree in computer science or relevant areas.\n- 3+ years of experience with a solid foundation in data structure and algorithm design, and be proficient in using one of the programming languages such as Python, Java, C++, R, etc.;\n- Familiar with common machine/deep learning, causal inference, and operational optimization algorithms, including classification, regression, clustering methods, as well as mathematical programming and heuristic algorithms;\n- Familiar with at least one framework of TensorFlow / PyTorch / MXNet and its training and deployment details,as well as the training acceleration methods such as mixed precision training and distributed training;\n- Familiar with big data related frameworks and application, those who are familiar with MR or Spark are preferred\n\nPreferred Qualifications:\n- Experience in recommendation systems, online advertising, ranking, search, information retrieval, natural language processing, machine learning, large-scale data mining, or related fields.\n- Publications at KDD, NeurlPS, WWW, SIGIR, WSDM, ICML, IJCAI, AAAI, RECSYS and related conferences/journals, or experience in data mining/machine learning competitions such as Kaggle/KDD-cup etc.\n\nTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.\n\nTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://shorturl.at/cdpT2 \n\nJob Information:\n\n【For Pay Transparency】Compensation Description (annually) The base salary range for this position in the selected city is $137750 - $337250 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Our company benefits are designed to convey company culture and values, to create an efficient and inspiring work environment, and to support our employees to give their best in both work and life. We offer the following benefits to eligible employees: We cover 100% premium coverage for employee medical insurance, approximately 75% premium coverage for dependents and offer a Health Savings Account(HSA) with a company match. As well as Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life and AD&D insurance plans. In addition to Flexible Spending Account(FSA) Options like Health Care, Limited Purpose and Dependent Care. Our time off and leave plans are: 10 paid holidays per year plus 17 days of Paid Personal Time Off (PPTO) (prorated upon hire and increased by tenure) and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability. We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra. A 401K company match, gym and cellphone service reimbursements. The Company reserves the right to modify or change these benefits programs at any time, with or without notice."}
{"text": "skills, analytical abilities, written and verbal communication skills, and the ability to influence cross-functional teams. They are an expert with SQL, ETL, Tableau (or similar data visualization tools) and have an ability to quickly translate business requirements into technical solutions. The candidate is a self-starter and team player and able to think big while paying careful attention to detail.\n\nIf you are ready to drive consistently great customer outcomes and accelerate the growth of our business, come join the Oracle Cloud Infrastructure (OCI) organization.\n\nThe role offers a unique opportunity to manage and build new data storage, pipelining, and visualization solutions from the ground up. You should possess high attention to detail, have excellent communication skills, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred. Above all you should be passionate about working with large data sets and someone who loves to bring datasets together to answer business questions and drive change.\n\n3+ years of data engineering experienceProvide technical thought leadership for data pipeline, SQL, and warehouse architecture, flow, database optimization, and business intelligence functions.Experience in Data transformation, structures, & pipelines, SQL, and data Performance Optimization Applying your extensive knowledge of software architecture to manage software development tasks associated with developing, debugging, or designing software applications, operating systems, and databases according to provided design specifications.Build enhancements within an existing software architecture and envision future improvements to the architecture.Assist in the development of short, medium, and long-term plans to achieve strategic objectives.Regularly interact across functional areas with senior management or executives to ensure unit objectives are met.Exercises independent judgement in methods, techniques and evaluation criteria for obtaining results.Mentor team members enabling operational excellence across the organization.Understand the OCI ecosystem and the broader Oracle ecosystem on the Data Analytics and retrieval aspects\n\n\nResponsibilities\n\nKey job responsibilities\n\nInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using Oracle services and internal toolsSupport various components of the data pipelines, including ingestion, validation, cleansing and curationImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, Redshift, and OLAP technologies to support research needsInterface with researchers and business stakeholders, gathering requirements and support ad-hoc data access to large data setsBuild and deliver high quality data sets to support research scientists and customer reporting needsContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersBecome a subject matter expert on OCI and its data sources, and assist non-technical stakeholders with understanding and visualizing dataEducate the team on best practices for upholding data security and data privacy standardsMaintain clear and consistent documentation of relevant data sources and code frameworks\n\n\nQualifications\n\n Qualifications \n\nDisclaimer:\n\nCertain US customer or client-facing roles may be required to comply with applicable requirements, such as immunization and occupational health mandates.\n\nRange and benefit information provided in this posting are specific to the stated locations only\n\nUS: Hiring Range: from $74,800 - $178,200 per year. May be eligible for bonus and equity.\n\nOracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle’s differing products, industries and lines of business.\n\nCandidates are typically placed into the range based on the preceding factors as well as internal peer equity.\n\nOracle US offers a comprehensive benefits package which includes the following:\n\n Medical, dental, and vision insurance, including expert medical opinion Short term disability and long term disability Life insurance and AD&D Supplemental life insurance (Employee/Spouse/Child) Health care and dependent care Flexible Spending Accounts Pre-tax commuter and parking benefits 401(k) Savings and Investment Plan with company match Paid time off: Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position. Accrued Vacation is provided to all other employees eligible for vacation benefits. For employees working at least 35 hours per week, the vacation accrual rate is 13 days annually for the first three years of employment and 18 days annually for subsequent years of employment. Vacation accrual is prorated for employees working between 20 and 34 hours per week. Employees working fewer than 20 hours per week are not eligible for vacation. 11 paid holidays Paid sick leave: 72 hours of paid sick leave upon date of hire. Refreshes each calendar year. Unused balance will carry over each year up to a maximum cap of 112 hours. Paid parental leave Adoption assistance Employee Stock Purchase Plan Financial planning and group legal Voluntary benefits including auto, homeowner and pet insurance\n\n\nAbout Us\n\nAs a world leader in cloud solutions, Oracle uses tomorrow’s technology to tackle today’s problems. True innovation starts with diverse perspectives and various abilities and backgrounds.\n\nWhen everyone’s voice is heard, we’re inspired to go beyond what’s been done before. It’s why we’re committed to expanding our inclusive workforce that promotes diverse insights and perspectives.\n\nWe’ve partnered with industry-leaders in almost every sector—and continue to thrive after 40+ years of change by operating with integrity.\n\nOracle careers open the door to global opportunities where work-life balance flourishes. We offer a highly competitive suite of employee benefits designed on the principles of parity and consistency. We put our people first with flexible medical, life insurance and retirement options. We also encourage employees to give back to their communities through our volunteer programs.\n\nWe’re committed to including people with disabilities at all stages of the employment process. If you require accessibility assistance or accommodation for a disability at any point, let us know by calling +1 888 404 2494, option one.\n\nDisclaimer:\n\nOracle is an \n\n Which includes being a United States Affirmative Action Employer"}
{"text": "skills:· 8+ years of Strong ETL & Data warehousing concepts· Strong Attunity experience· DB2 and MongoDB database working experience.· AWS service utilization experience with services like EC2, EMR, S3, Step Functions Glue and Dynamo· Control M experience· Atlassian suite (Jira, Confluence and Bitbucket) experience· Experience using CI/CD pipeline jobs.Good to have skills:· Prefer Strong IICS experience (Informatica Intelligent CloudServices)\n\nHarika NittaUS IT RecruiterE-mail: hnitta@sageitinc.netDirect No:+1(945)732-4161Office: 972-996-0650 Ext 394"}
{"text": "requirements and metrics.\nProvide training and support to end-users on data quality best practices and tools.\nDevelop and maintain documentation related to data quality processes.\n\n Education Qualification: \n\nBachelor's degree in a related field such as Data Science, Computer Science, or Information Systems.\n\nRequired Skills: \n\nExperience working as a BA/Data Analyst in a Data warehouse/Data governance platform.\nStrong analytical and problem-solving skills.\nProficiency in SQL, data analysis, and data visualization tools. \nCritical thinking.\nAbility to understand and examine complex datasets.\nAbility to interpret Data quality results and metrics.\n\nDesired Skills:\n\nKnowledge of Data quality standards and processes.\nProven experience in a Data Quality Analyst or similar role.\nExperience with data quality tools such as Informatica, PowerCurve, or Collibra DQ is preferred.\nCertifications in data management or quality assurance (e.g.\nCertified Data Management Professional, Certified Quality Analysis)."}
{"text": "experienced Data Engineer to maintain and enhance current data environment while providing, maintaining, and distributing data to a global team.\n\nAs a Data Engineer, you will need to review technical design, develop and enhance data systems and pipelines supporting process optimization in business intelligence activities in PATOE, as well as integrate in-house data infrastructure with partner systems.\n\nYou should be an advanced in the architecture of data warehousing solutions, using multiple platforms / tech. In addition, you should have strong analytical skills and excel in the design, creation, management, and business use of large data sets, combining raw information from different sources. On top of that, you should have excellent communication skills and ability to adjust communication to different groups of stakeholders in order to be able to work with business analysts and engineers to determine how best to design the data management setup\n\nMain Responsibilities Include\n\n Designing, implementing, and supporting scalable systems to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day business decision making Work closely with business owners, product managers, Business Intelligence Engineers to explore new data sources and deliver the data Interface with other teams to extract, transform, and load data from a wide variety of data sources using AWS big data technologies Own end-to-end process from data analysis, data extraction, data ingestion, data cleaning and manipulation and delivering the data for reporting Build robust and scalable data integration (ETL) pipelines using SQL, Python, Spark and AWS services. Explore and learn the latest AWS technologies to provide new capabilities and increase efficiencies Create automated alarming and dashboards to monitor data integrity. Collaborate with TDT engineers and evaluate internal processes, approaches, and technical systems to establish highest standards and ensure safety for our associates and leaders\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nArlington, VA, USA | Bellevue, WA, USA | Chicago, IL, USA | Nashville, TN, USA | New York, NY, USA | Seattle, WA, USA\n\nBasic Qualifications\n\n 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL\n\nPreferred Qualifications\n\n Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2610050"}
{"text": "experience in Spark, Python, Scala, Hadoop, Java.Must have hands on experience in AWS ETL Glue, Lambda, DynamoDB.Must have hands on experience in PySpark on Jupyter notebook.Must have experience in CI/CD, AWS S3, AWS EC2, AWS IAM, AWS Data Lake, SQL.Building and managing public and private cloud infrastructure with AWS, EC2 and S3 resources.Participating in requirement analysis and planning the development.Support production environment.Designing automation processes as per the operational needs of an organization.Reviewing the code, design and providing expertise in the development and integration of systems.\nrequirements.Work in the Agile methodology (SCRUM).Collaborate with the team to work on developing new features.Participating in High level design session and collaborating with Business on the best practices and solutions that maximize the client needs."}
{"text": "skills, and become a part of our global community of talented, diverse, and knowledgeable colleagues.\n\nResponsibilities\n\nMachine Learning Development\n\nMaintains, as well as furthers, enhances existing machine learning modules for automotive applications including autonomous vehicles. Designs and implements new machine learning based approaches based on existing frameworks. Keeps up to speed with the state of the art of academic research and AI/ML technology in the Automotive industry. Applies industry and technology expertise to real business problems. Coordinates with automotive engineers and autonomous driving software experts. Transfers technologies and solutions to automotive OEM development divisions. \n\nData Engineering and Pipelines:\n\nUnderstand business context and wrangles large, complex datasets. Create repeatable, reusable code for data preprocessing, feature engineering, and model training. Build robust ML pipelines using Google Vertex AI, BigQuery and other GCP services. \n\nResponsible AI and Fairness:\n\nConsider ethical implications and fairness throughout the ML model development process. Collaborate with other roles (such as data engineers, product managers, and business analysts) to ensure long-term success. \n\nInfrastructure and MLOps:\n\nWork with infrastructure as code to manage cloud resources. Implement CI/CD pipelines for model deployment and monitoring. Monitor and improve ML solutions. Implement MLOps using Vertex AI pipelines on the GCP platform. \n\nProcess Documentation and Representation\n\nDevelops technical specifications and documentation. Represents the Customer in the technical community, such as at conferences. \n\nQualifications\n\n7 - 10 years of professional experience REQUIRED5+ years’ Deep Learning experience REQUIREDMaster’s Degree in Computer Science or equivalent. PhD Strongly Preferred. \n\nRequired Skills\n\nStrong communication skills must be able to describe and explain complex AI/ML concepts and models to business leaders. Desire and ability to work effectively within a group or team. Strong knowledge of different machine learning algorithms. Deep Learning: Proficiency in deep learning techniques and frameworksMachine Learning: Strong understanding of traditional machine learning algorithms and their applications. Computer Vision: Expertise in computer vision, including object detection, image segmentation, and image recognitionProficiency in NLP techniques, including sentiment analysis, text generation, and language understanding models. Experience with multimodal language modeling and applications. Neural Network Architectures: Deep understanding of various neural network architectures such as CNNs, RNNs, and Transformers. Reinforcement Learning: Familiarity with reinforcement learning algorithms and their applications in AI.\\Data Preprocessing: Skills in data cleaning, feature engineering, and data augmentation. Model Training And Tuning: Experience in training, fine-tuning, and optimizing AI models. Model Deployment: Knowledge of model deployment techniques, including containerization (Docker) and orchestration (Kubernetes). Understanding of Generative AI concepts and LLM Models tailored to a wide variety of automotive applications. Strong documentation skills for model architecture, code, and processes. \n\nDesired Skills\n\nAI Ethics: Awareness of ethical considerations in AI, including bias mitigation and fairness. Legal And Regulatory Knowledge: Understanding of AI-related legal and regulatory considerations, including data privacy and intellectual property. Data Management: Proficiency in data storage and management systems, including databases and data lakes. Cloud Computing: Familiarity with Google Cloud Platform. Experience with GCP, Vertex AI and BigQuery is a plus. \n\nThe salary range for this position takes into consideration a variety of factors, including but not limited to skill sets, level of experience, applicable office location, training, licensure and certifications, and other business and organizational needs. The new hire salary range displays the minimum and maximum salary targets for this position across all US locations, and the range has not been adjusted for any specific state differentials. It is not typical for a candidate to be hired at or near the top of the range for their role, and compensation decisions are dependent on the unique facts and circumstances regarding each candidate. A reasonable estimate of the current salary range for this position is $92,118 to $202,730. Please note that the salary range posted reflects the base salary only and does not include benefits or any potential equity or variable bonus programs. Information regarding the benefits available for this position are in our benefits overview .\n\nWho We Are\n\nPerficient is a leading global digital consultancy. We imagine, create, engineer, and run digital transformation solutions that help our clients exceed customers’ expectations, outpace competition, and grow their business. With unparalleled strategy, creative, and technology capabilities, our colleagues bring big thinking and innovative ideas, along with a practical approach to help our clients – the world’s largest enterprises and biggest brands succeed.\n\nWhat We Believe\n\nAt Perficient, we promise to challenge, champion, and celebrate our people. You will experience a unique and collaborative culture that values every voice. Join our team, and you’ll become part of something truly special.\n\nWe believe in developing a workforce that is as diverse and inclusive as the clients we work with. We’re committed to actively listening, learning, and acting to further advance our organization, our communities, and our future leaders… and we’re not done yet.\n\nPerficient, Inc. proudly provides equal employment opportunities (\n\nApplications will be accepted until the position is filled or the posting removed.\n\nDisability Accommodations:\n\nPerficient is committed to providing a barrier-free employment process with reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or accommodation due to a disability, please contact us.\n\nDisclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time. \n\nAbout Us\n\nPerficient is always looking for the best and brightest talent and we need you! We’re a quickly growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, experience work-life balance, and become a part of our global community of talented, diverse, and knowledgeable colleagues.\n\n \n\nSelect work authorization questions to ask when applicants apply\n\n1. Are you legally authorized to work in the United States?\n\n2. Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?"}
{"text": "requirements and objectives. Collect, cleanse, and validate data from various sources to ensure accuracy and consistency. Develop and implement data cleaning processes to identify and resolve errors, duplicates, and inconsistencies in datasets. Create and maintain data dictionaries, documentation, and metadata to facilitate data understanding and usage. Design and execute data transformation and normalization processes to prepare raw data for analysis. Design, standardize, and maintain data hierarchy for business functions within the team. Perform exploratory data analysis to identify trends, patterns, and outliers in the data. Develop and maintain automated data cleansing pipelines to streamline the data preparation process. Provide insights and recommendations to improve data quality, integrity, and usability. Stay updated on emerging trends, best practices, and technologies in data cleansing and data management. QualificationsQualifications: Bachelor’s degree required in computer science, Statistics, Mathematics, or related field. Proven experience (2 years) as a Data Analyst, Data Engineer, or similar role, with a focus on data cleansing and preparation. Competencies: Strong analytical and problem-solving skills with the ability to translate business requirements into technical solutions. Proficiency in Power Query (M Language, DAX) for data transformation and cleansing within Microsoft Excel and Power BI environments. Proficiency in SQL and data manipulation tools (e.g., Python and R). Experience with data visualization tools (e.g., Tableau, Power BI) is a plus. Experience with ERP systems, particularly JDE (JD Edwards), and familiarity with its data structures and modules for sales orders related tables. Experience working with large-scale datasets and data warehousing technologies (e.g., iSeries IBM). Attention to detail and a commitment to data accuracy and quality. Excellent communication and collaboration skills with the ability to work effectively in a team environment. Additional InformationWhy work for Cornerstone Building Brands?The US base salary range for this full-time position is $85,000 to $95,000 + medical, dental, vision benefits starting day 1 + 401k and PTO. Our salary ranges are determined by role, level, and location. Individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. (Full-time is defined as regularly working 30+ hours per week.)Our teams are at the heart of our purpose to positively contribute to the communities where we live, work and play. Full-time* team members receive** medical, dental and vision benefits starting day 1. Other benefits include PTO, paid holidays, FSA, life insurance, LTD, STD, 401k, EAP, discount programs, tuition reimbursement, training, and professional development. You can also join one of our Employee Resource Groups which help support our commitment to providing a diverse and inclusive work environment.*Full-time is defined as regularly working 30+ hours per week. **Union programs may vary depending on the collective bargaining agreement.All your information will be kept confidential according to"}
{"text": "skills in order to help us expand our global features. This individual will be integral in ensuring we are able to deliver high quality data to our customers.\n\nPlease note that this contract term up to 9 months.\n\nMajor Responsibilities\n\nAnalyze and improve data quality of multilingual text classifiers.Work with linguistics and engineering teams to build out new parsers across languages.Translate various taxonomies such as Skills, Titles, and Occupations.\n\nSkills/Abilities\n\nCompetency in reading and writing ChineseUnderstanding of syntax and structural analysis of languagesMicrosoft Excel experience (including vlookups, data cleanup, and functions)Knowledge of query languages such as SQLStrong knowledge of rule writing using RegExKnowledge of text analysis or machine learning principlesExperience with data analysis using tools such as Excel or Python\n\nEducation And Experience\n\nBachelor’s degree in Linguistics, Data Analytics, NLP or similar (preferred)Knowledge of other languages. \n\nLightcast is a global leader in labor market insights with headquarters in Moscow (ID) with offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities. Lightcast is proud to be an equal opportunity workplace and is committed to"}
{"text": "experienced and skilled VP of Data Engineering to join their team. The ideal candidate will deeply understand data architecture, cloud infrastructure and the ability to design and implement scalable, secure, and reliable data solutions.\n\nKey Responsibilities\n\nLead the design, development, and implementation of data infrastructure solutions in multiple public Cloud platforms and services (Azure, AWS, and GCP) using industry standards and best practicesTranslate business needs into data models supporting long-term solutions using SQL and non-SQL databases on cloud-based platforms. Create and maintain conceptual, logical, and physical data models and corresponding metadata using best practices to ensure high data quality and access. Identify data gaps and enforce strong practices to close any data quality issues promptly; establish a single version of truth for reference data that benefits consumers. Contribute to and implement a strategy for data management in private and public clouds, leveraging cloud-native tools and techniques. Establish and keep up with Data Non-Functional Requirements (NFR) to ensure that metadata, data mappings, data lineage, and other related items meet the policy requirements for being complete, accurate, and consistent. Manage data as a strategic asset and operationalize data governance, data quality, data integrity, and controls across the organization. Introduce and propagate modern engineering practices around data, including reusable/configurable data quality and data access control frameworks. Drive consistency, efficiency, and cost benefits through establishing and continuously improving data management practices and standards. Stay current on the latest trends and technologies in data and cloud infrastructure. \n\nSkills & Qualifications\n\n10+ years of experience in data architecture and cloud infrastructure8+ years of experience with reference data management, business information architecture, analytics, business process re-engineering, and Product Management7+ of experience demonstrating expert-level knowledge of cloud architecture patterns (microservices, event-driven, serverless, API first and API gateways, service mesh, CQRS, stateless design)5+ years of data mapping and data lineage (create or analyze)3+ years of technical leadership in a data and technical environment, including Data Engineering, Data modeling, Metadata management, etc. A master's or bachelor's degree in computer science, information systems, or a related fieldStrong SQL and Python knowledgeStrong knowledge of business operational processes, data, and technology platformsAbility to prioritize deliverables and manage multiple complex work streams simultaneouslyExperience with data warehousing, data lakes, and data pipelinesFundamental knowledge of database systems (relational and object stores), including scaling, sharing, and replicationDeep understanding of monitoring and logging in Cloud environments, including retention and cost optimization strategiesAbility to create high-quality documentation about data architectural decisions, design rationale, and implementation guidelines. Secure Development Lifecycle and Agile Development Methodology using DevSecOps and CI/CD concepts and practicesInfrastructure as code and Continuous integration and delivery/deploymentDemonstrated ability to work well in a cross-functional environment with both technical and non-technical team members. Understanding of energy markets a plusExcellent communication and interpersonal skills"}
{"text": "Experience : 7+Locations: Remote (San Francisco, CA) Once in a quarter they need to Visit San Francisco, CA\nDuration: Contract/Fulltime \nJob Description:\n4+ years of professional experience in Stream/Batch Processing systems at scale.Strong Programming skills in Java, Python.Experience in Public Cloud is a must. Experience with GCP and GCP managed services is a strong plus.i. Experience in Messaging/Stream Processing systems on Cloud such as Pub/Sub, Kafka, Kinesis, DataFlow, Flink etc, and/Orii. Experience in Batch Processing systems such as Hadoop, Pig, Hive, Spark. Experience with Dataproc is a strong plus.Knowledge of DevOps principles and tools (e.g. CI/CD, IaC/Terraform).Strong understanding of Containerization technologies (e.g., Docker, Kubernetes).Strong problem-solving and critical thinking skills.Strong written/verbal communication skills with the ability to thrive in a remote work environment(For Senior leads/architects) Ability to explore new areas/problems as well as design and architect scalable solutions in Stream/Batch Processing at scale. Ability to technically lead a team of engineers on a project/component."}
{"text": "experienced Principal Applied AI Engineer to join our dynamic team. The ideal candidate will have a strong background in computer science, with a specialization in transformers and large language models. This role demands a blend of expertise in research and practical application, as you will be responsible for developing and deploying AI/ML models that drive innovation in healthcare data processing.\nYour ResponsibilitiesDesigning, developing, and deploying advanced ML models and AI agents, prioritizing generative AI techniques, to automate the extraction, classification, and auditing of medical billing dataCollaborating closely with cross-functional teams to seamlessly integrate AI solutions into the existing infrastructure, focusing on operational efficiency, scalability, and cost optimizationConducting thorough testing and validation of models to ensure their accuracy, efficiency, and reliability meet the highest standardsKeeping abreast of the latest advancements in AI and machine learning, particularly in the realm of generative AI technologies, and assessing their suitability and potential impact on our objectivesContributing to the continual enhancement of our AI framework, ensuring it maintains its position at the forefront of technological innovationEvaluating new technologies for potential integration into our systems, aligning with our strategic objectives and business needsProviding mentorship to junior engineers, sharing insights, and fostering a culture of continuous learning and development within the teamCollaborating with stakeholders to gain a deep understanding of business needs and translating them into technical solutions that deliver tangible value\nWhat We’re Looking ForPhD or Master’s degree in Computer Science with a specialization in transformers, large language models, or a closely related field. Relevant industry experience will also be consideredProven experience with LLMs and Deep Learning systemsPreference for experience deploying AI or ML models in a production environmentPreferred experience with medical/claims dataSolid understanding of machine learning, deep learning, and generative AI technologiesProficiency in programming languages such as Python, along with frameworks/libraries like TensorFlow, PyTorch, etc.Demonstrated ability to stay abreast of the latest AI research and methodologiesStrong problem-solving skills and adaptability to thrive in a fast-paced, dynamic environmentExcellent communication and collaboration skills, capable of effectively conveying complex technical concepts to non-technical stakeholders\nOur CultureAt Alaffia, we fundamentally believe that the whole is more valuable than the sum of its individual parts. Further to that point, we believe a diverse team of individuals with various backgrounds, ideologies, and types of training generates the most value. If you want to work alongside driven people on a mission to make a major impact at the core of U.S. healthcare by implementing the latest in cutting-edge technologies, then we’d like to meet you!\nWhat Else Do You Get Working With Us?Company stock optionsEmployer-sponsored Medical, Dental, and Vision benefitsHybrid work environment - work from the office and homeFlexible, paid vacation policyWork in a flat organizational structure — direct access to Leadership*Please note: Alaffia Health does not provide employment sponsorships at this time."}
{"text": "skills in a dynamic and entrepreneurial operating environment. Duties include:Interpret and refine customer requirements/requests for informationQuantifying, Measuring and Analyzing Financial and Utilization Metrics of HealthcarePerform research and analysis of complex healthcare claims and pharmacy dataEffectively fulfill information needs using available analytical tools and techniques or through development of ad hoc queriesDevelop and present actionable insights to internal customers, including management, as well as external customersManage and meet competing deliverables in a dynamic and fast-paced environmentMaintain and develop enterprise reports in Tableau serverDevelop and execute queries and reports in Access, SQL, and VBAOther duties as assigned Skills/Qualifications:Ability to present complex information in an easy-to-understand mannerStrong desire to learn the design, development, and maintain of ongoing metrics, reports, analyses, etc.Ability to solve problems logically and systematically with attention to detailExceptional verbal and written communication skillsCollaborative working style with the ability to effectively interface with different organizations and personalitiesSelf-motivated with ability to work remotely is a must Education/Experience:Bachelor's degree in Business, Statistics, Computer Science, Mathematics or related fieldExperience in two or more of the following environments: Access, Excel, SQL, VBA, PythonExperience with quantitative analyticsExperience managing/working with large databases is a strong advantageExperience with Tableau Desktop/Server strongly preferredProject management experience is a plusKnowledge of PBM/healthcare industry strongly preferred"}
{"text": "experience – we hope you share our enthusiasm!\n\nQualifications\n\n BS/MS with quantitative focus (e.g. Economics, Computer Science, Mathematics, Physics, Statistics) or equivalent practical experience  5+ years of experience in data engineering, software engineering, or other related roles  3+ years experience operating databases (e.g. Redshift, MySQL, MongoDB) and advanced query authoring & tuning  3+ years of dimensional data modeling & schema design in data warehouses  3+ years of experience developing and operating large scale big data technologies (e.g. Apache Spark, Presto, HDFS, Apache Kafka, Apache Druid)  Experience with ETL tooling (e.g. Airflow)  Expertise in Python  Experience designing, implementing and maintaining production grade data pipelines  Knowledge of cloud-based production grade data architectures (e.g. AWS EC2, EMR, Glue, S3, Redshift)  An eye for automation and instrumentation in all data-related aspects  Work experience in an interdisciplinary / cross-functional field \n\n Preferred Qualifications \n\n Working experience in SaaS companies  Strong cross-functional and interpersonal skills with demonstrated ability to communicate technical content to general audiences  Entrepreneurial in nature - able to keep moving initiatives forward in ambiguous situations \n\nCisco values the perspectives and skills that emerge from employees with diverse backgrounds. That's why Cisco is expanding the boundaries of discovering top talent by not only focusing on candidates with educational degrees and experience but also placing more emphasis on unlocking potential. We believe that everyone has something to offer and that diverse teams are better equipped to solve problems, innovate, and create a positive impact.\n\nWe encourage you to apply even if you do not believe you meet every single qualification . Not all strong candidates will meet every single qualification. Research shows that people from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy. We urge you not to prematurely exclude yourself and to apply if you're interested in this work.\n\nCisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.\n\nUs – Compensation Range – Message To Applicants\n\n150,900 USD - 222,200 USD\n\n Message to applicants applying to work in the U.S.: \n\nWhen available, the salary range posted for this position reflects the projected hiring range for new hire, full-time salaries in U.S. locations, not including equity or benefits. For non-sales roles the hiring ranges reflect base salary only; employees are also eligible to receive annual bonuses. Hiring ranges for sales positions include base and incentive compensation target. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications, or training. Applicants may not be eligible for the full salary range based on their U.S. hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.\n\nU.S. employees have access to quality medical, dental and vision insurance, a 401(k) plan with a Cisco matching contribution, short and long-term disability coverage, basic life insurance and numerous wellbeing offerings. Employees receive up to twelve paid holidays per calendar year, which includes one floating holiday, plus a day off for their birthday. Employees accrue up to 20 days of Paid Time Off (PTO) each year and have access to paid time away to deal with critical or emergency issues without tapping into their PTO. We offer additional paid time to volunteer and give back to the community. Employees are also able to purchase company stock through our Employee Stock Purchase Program.\n\nEmployees on sales plans earn performance-based incentive pay on top of their base salary, which is split between quota and non-quota components. For quota-based incentive pay, Cisco pays at the standard rate of 1% of incentive target for each 1% revenue attainment against the quota up to 100%. Once performance exceeds 100% quota attainment, incentive rates may increase up to five times the standard rate with no cap on incentive compensation. For non-quota-based sales performance elements such as strategic sales objectives, Cisco may pay up to 125% of target. Cisco sales plans do not have a minimum threshold of performance for sales incentive compensation to be paid."}
{"text": "QUALIFICATIONS & SKILLS:  Required:Bachelor's degree in Computer Science or other technical field or equivalent work experience 8+ years of progressively responsible positions in Information Technology including 5+ years’ experience in Data Engineering3+ years of leadership experience in all aspects of Data Engineering3+ years’ management/supervisory experience in Data Engineering with accountability for enforcing talent management needs and performance standards. Demonstrated leadership and experience managing multidiscipline, high-performance work teams. Strong competency in project management and execution of multiple or large projects. Experience working with customers to develop solutions to complex business problems. Proven ability to communicate effectively with internal/external stakeholders to support business initiatives. Proven ability to function in an environment which requires flexibility, good judgment and intelligent decision making, often based on limited information and/or extreme conditions. Ability to formulate, implement and evaluate plans, programs and procedures applicable to customer relationship and demand management. Preferred: MBA At least 4 years of Data Engineering experience with Big Data Technologies: Databricks, Snowflake, Apache Spark, Hadoop, or KafkaAt least 3 years of microservices development experience: Python, Java or ScalaAt least 2 years of experience building data pipelines, CICD pipelines, and fit for purpose data storesAt least 1 year of experience in Cloud technologies: AWS, Azure, Google Cloud, OpenStack, Docker, Ansible, Chef or TerraformFinancial Services industry experience"}
{"text": "requirements, and objectives for Clinical initiatives  Technical SME for system activities for the clinical system(s), enhancements, and integration projects. Coordinates support activities across vendor(s)  Systems include but are not limited to eTMF, EDC, CTMS and Analytics  Interfaces with external vendors at all levels to manage the relationship and ensure the proper delivery of services  Document Data Transfer Agreements for Data Exchange between BioNTech and Data Providers (CRO, Partner Organizations)  Document Data Transformation logic and interact with development team to convert business logic into technical details \n\n What you have to offer: \n\n Bachelor’s or higher degree in a scientific discipline (e.g., computer science/information systems, engineering, mathematics, natural sciences, medical, or biomedical science)  Extensive experience/knowledge of technologies and trends including Visualizations /Advanced Analytics  Outstanding analytical skills and result orientation  Ability to understand complex functional requirements/requests from business users translate them into pragmatic and workable (technology) solutions  Familiar with Clinical Operations data, proven capabilities in supporting data analysis needs  Demonstrated success in supporting implementation projects of Clinical Operations solutions \n\n  Benefits for you  \n\nBioNTech  is committed to the wellbeing of our team members and offers a variety of benefits in support of our diverse employee base. We offer competitive remuneration packages which is determined by the specific role, location of employment and also the selected candidate’s qualifications and experience.\n\nClick here to find out more about what we do, the careers we offer and also the benefits we provide.\n\nNote: The availability, eligibility and design of the listed benefits may vary depending on the location. The final requirements for the individual use of our benefits are based on the company's internal policies and applicable law.\n\n  Have we kindled your pioneering spirit?  \n\nThis vacancy is to be hired at one of the following locations –   Cambridge Erie Street   . Apply now for your preferred job and location through our careers site .\n\nBioNTech does not tolerate discrimination, favoritism, or harassment based on gender, political opinion, religion or belief, nationality, ethnic or social origin, age, sexual orientation, marital status, disability, physical appearance, health status or any other aspect of personal status. We are committed to creating a diverse and inclusive environment and are proud to be"}
{"text": "experience in the Research domain, you will possess knowledge of diverse assay types such as IHC, flow cytometry, cytokine data, but specialize in genomics and transcriptomics. Your ultimate goal will be to place data at the fingertips of stakeholders and enable science to go faster. You will join an enthusiastic, agile, fast-paced and explorative global data engineering team.\n\nWe have a hybrid model that requires being onsite in Princeton, NJ 60% of the time.\n\nResponsibilities\n\nDesign, implement and manage ETL data pipelines that process and transform vast amounts of scientific data from public, internal and partner sources into various repositories on a cloud platform (AWS)Incorporate bioinformatic tools and libraries to the processing pipelines for omics assays such as DNASeq, RNASeq, or proteomicsEnhance end-to-end workflows with automation that rapidly accelerate data flow with pipeline management tools such as Step Functions, Airflow, or Databricks WorkflowsImplement and maintain bespoke databases for scientific data (RWE, in-house labs, CRO data) and consumption by analysis applications and AI productsInnovate and advise on the latest technologies and standard methodologies in Data Engineering and Data Management, including recent advancements with GenAI, and latest bioinformatics tools and techniques in RNA sequencing analysisManage relationships and project coordination with external parties such as Contract Research Organizations (CRO) and vendor consultants / contractorsDefine and contribute to data engineering practices for the group, establishing shareable templates and frameworks, determining best usage of specific cloud services and tools, and working with vendors to provision cutting edge tools and technologiesCollaborate with stakeholders to determine best-suited data enablement methods to optimize the interpretation of the data, including creating presentations and leading tutorials on data usage as appropriateApply value-balanced approaches to the development of the data ecosystem and pipeline initiativesProactively communicate data ecosystem and pipeline value propositions to partnering collaborators, specifically around data strategy and management practicesParticipate in GxP validation processes\n\nRequirements\n\nBS/MS in Computer Science, Bioinformatics, or a related field with 5+ years of software engineering experience (8+ years for senior role) or a PhD in Computer Science, Bioinformatics or a related field and 2+ years of software engineering experience (5+ years for senior role)Excellent skills and deep knowledge of ETL pipeline, automation and workflow managements tools such as Airflow, AWS Glue, Amazon Kinesis, AWS Step Functions, and CI/CD is a mustExcellent skills and deep knowledge in Python, Pythonic design and object-oriented programming is a must, including common Python libraries such as pandas. Experience with R a plusExcellent understanding of different bioinformatics tools and databases such as STAR, HISAT2, DESeq2, Seurat and how they’re used on different types of genomic and transcriptomic data such as single cell transcriptomicsSolid understanding of modern data architectures and their implementation offerings including Databricks’ Delta Tables, Athena, Glue, Iceberg, and their applications to Lakehouse and medallion architectureExperience working with clinical data and understanding of GxP compliance and validation processesProficiency with modern software development methodologies such as Agile, source control, project management and issue tracking with JIRAProficiency with container strategies using Docker, Fargate, and ECRProficiency with AWS cloud computing services such as Lambda functions, ECS, Batch and Elastic Load Balancer and other compute frameworks such as Spark, EMR, and Databricks\n\nFor US based candidates, the proposed salary band for this position is as follows:\n\n$114,375.00---$190,625.00\n\nThe actual salary offer will carefully consider a wide range of factors, including your skills, qualifications, experience, and location. Also, certain positions are eligible for additional forms of compensation, such as bonuses.\n\nAbout You\n\nYou are passionate about our purpose and genuinely care about our mission to transform the lives of patients through innovative cancer treatmentYou bring rigor and excellence to all that you do. You are a fierce believer in our rooted-in-science approach to problem-solvingYou are a generous collaborator who can work in teams with diverse backgroundsYou are determined to do and be your best and take pride in enabling the best work of others on the teamYou are not afraid to grapple with the unknown and be innovativeYou have experience working in a fast-growing, dynamic company (or a strong desire to)You work hard and are not afraid to have a little fun while you do so\n\nLocations\n\nGenmab leverages the effectiveness of an agile working environment, when possible, for the betterment of employee work-life balance. Our offices are designed as open, community-based spaces that work to connect employees while being immersed in our state-of-the-art laboratories. Whether you’re in one of our collaboratively designed office spaces or working remotely, we thrive on connecting with each other to innovate.\n\nAbout Genmab\n\nGenmab is an international biotechnology company with a core purpose guiding its unstoppable team to strive towards improving the lives of patients through innovative and differentiated antibody therapeutics. For more than 20 years, its passionate, innovative and collaborative team has invented next-generation antibody technology platforms and leveraged translational research and data sciences, which has resulted in a proprietary pipeline including bispecific T-cell engagers, next-generation immune checkpoint modulators, effector function enhanced antibodies and antibody-drug conjugates. To help develop and deliver novel antibody therapies to patients, Genmab has formed 20+ strategic partnerships with biotechnology and pharmaceutical companies. By 2030, Genmab’s vision is to transform the lives of people with cancer and other serious diseases with Knock-Your-Socks-Off (KYSO™) antibody medicines.\n\nEstablished in 1999, Genmab is headquartered in Copenhagen, Denmark with locations in Utrecht, the Netherlands, Princeton, New Jersey, U.S. and Tokyo, Japan.\n\nOur commitment to diversity, equity, and inclusion\n\nWe are committed to fostering workplace diversity at all levels of the company and we believe it is essential for our continued success. No applicant shall be discriminated against or treated unfairly because of their race, color, religion, sex (including pregnancy, gender identity, and sexual orientation), national origin, age, disability, or genetic information. Learn more about our commitments on our website.\n\nGenmab is committed to protecting your personal data and privacy. Please see our privacy policy for handling your data in connection with your application on our website https://www.genmab.com/privacy.\n\nPlease note that if you are applying for a position in the Netherlands, Genmab’s policy for all permanently budgeted hires in NL is initially to offer a fixed-term employment contract for a year, if the employee performs well and if the business conditions do not change, renewal for an indefinite term may be considered after the fixed-term employment contract."}
{"text": "Requirements NOTE: Applicants with an Active TS Clearance preferred Requirements * High School diploma or GED, Undergraduate degree preferred Ability to grasp and understand the organization and functions of the customer Meticulous data entry skills Excellent communication skills; oral and written Competence to review, interpret, and evaluate complex legal and non-legal documents Attention to detail and the ability to read and follow directions is extremely important Strong organizational and prioritization skills Experience with the Microsoft Office suite of applications (Excel, PowerPoint, Word) and other common software applications, to include databases, intermediate skills preferred Proven commitment and competence to provide excellent customer service; positive and flexible Ability to work in a team environment and maintain a professional dispositionThis position requires U.S. Citizenship and a 7 (or 10) year minimum background investigation ** NOTE: The 20% pay differential is dependent upon the customer's order for services and requires an Active Top-Secret security clearance. Agency Overview The mission of the Federal Bureau of Investigation (FBI) is to protect the American people and uphold the Constitution of the United States. FBI investigates a broad range of criminal violations, integrating the use of asset forfeiture into its overall strategy to eliminate targeted criminal enterprises. The FBI has successfully used asset forfeiture in White Collar Crime, Organized Crime, Drug, Violent Crime and Terrorism investigations. Benefits Overview At EnProVera, we recognize the diverse needs of our employees and strive to provide an excellent package to help meet those needs. Comprehensive benefits are offered with greater choice and flexibility to support your health, work-life balance, and professional growth. A package providing employee only coverage can be built around our basic plans at $0 employee cost for: Medical, Dental, Vision, Term Life Insurance, Accidental Death -amp; Dismemberment Insurance, Short-Term Disability, and Employee Assistance Program."}
{"text": "Skills: Total IT exp - 12+ years Python – 6+ Yrs of Exp – Pyspark –6+ Yrs of Exp – Pytorch –6+ Yrs of Exp – GCP –3 + Yrs of Exp – Web development – Prior experience 3+ Years Docker – 4+ Years KubeFlow - 4+ Years"}
{"text": "requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.\nJob DescriptionTo apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.\nApplies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take actionUses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problemUses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activitiesApplies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancementsSupports account start-up analysis and/or report implementation as neededDevelop standardized and ad hoc site and/or customer reportingStreamlines and/or automates internal and external reportingMay investigate and recommend new technologies and information systemsMay conduct feasibility analyses on various processes and equipment to increase efficiency of operationsPartners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutionsDevelops predictive models to help drive decision makingDesigns, develops, and implements data gathering and reporting methods and procedures for OperationsResponsible for tracking, planning, analysis, and forecasting of storage capacities, inventory levels, equipment and/or labor requirementsCoordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely mannerMay coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuitsResponsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes\nRequired Education and ExperienceUndergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required0-2 years of analytics experience, required\nOur Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.\nOur Organization is"}
{"text": "QUALIFICATIONS:Bachelor's degree or higher in computer science, engineering, mathematics, or related field.Strong programming skills in languages such as Python, Java, or Scala.Proven experience as an MLOps Engineer, specifically with Azure ML and related Azure technologies.Familiarity with containerization technologies such as Docker and orchestration tools like Kubernetes.Proficiency in automation tools like JIRA, Ansible, Jenkins, Docker compose, Artifactory, etc.Knowledge of DevOps practices and tools for continuous integration, continuous deployment (CI/CD), and infrastructure as code (IaC).Experience with version control systems such as Git and collaboration tools like GitLab or GitHub.Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment.Strong communication skills and ability to effectively communicate technical concepts to non-technical stakeholders.Certification in cloud computing (e.g., AWS Certified Machine Learning Specialty, Google Professional Machine Learning Engineer).Knowledge of software engineering best practices such as test-driven development (TDD) and code reviews.Experience with Rstudio/POSIT connect, RapidMiner."}
{"text": "skills to provide strategic insights and solutions to complex business problems. This role is crucial to our organization as it plays a significant role in decision-making processes, product development, and business strategy. As a Consulting Staff Data Scientist, you will work closely with various teams, including engineering, product, and business strategy, to drive data-driven decisions and implement effective solutions. This is an excellent opportunity for a seasoned data scientist who is looking to make a significant impact in a fast-paced, innovative technology environment.\n\n\n\nWhy join us?\n\n\n Conversion to FTE at 6 months Remote Position Competitive total rewards package Cutting Edge Technology and Upward Mobility\n\nJob Details\n\nResponsibilities\n\n Apply advanced data science techniques to analyze and interpret complex data sets Develop and implement machine learning models to solve business problems Utilize Bayesian statistical modelling to provide insights and predictions Collaborate with cross-functional teams to understand business needs and provide data-driven solutions Present findings and insights to stakeholders, providing actionable recommendations Stay up-to-date with the latest technology trends and advancements in data science Mentor junior data scientists and contribute to the overall growth of the data science team\n\nQualifications\n\n Ph.D. in Data Science, Statistics, Computer Science, or a related field Minimum of 3 years of experience as a Data Scientist in the technology industry Proven experience in Bayesian statistical modelling and machine learning model development Strong knowledge of data structures, data modelling, and software architecture Proficient in programming languages such as Python, R, or Java Exceptional analytical, problem-solving, and critical thinking skills Excellent communication skills with the ability to explain complex concepts to non-technical stakeholders Strong project management skills with the ability to lead projects from conception to completion Experience with big data technologies such as Hadoop, Spark, or similar is a plus\n\nThe ideal candidate will be a strategic thinker with a passion for leveraging data to drive business results. If you are a seasoned data scientist looking for a challenging and rewarding role, we would love to hear from you.\n\nInterested in hearing more? Easy Apply now by clicking the \"Easy Apply\" button.\n\nWant to learn more about this role and Jobot Consulting?\n\nClick our Jobot Consulting logo and follow our LinkedIn page!"}
{"text": "requirements. If you do not have experience as a Business Analyst or Product Owner, you will be put through a training & Internship program.Experience in Requirement Gathering, Agile methodology, writing user stories, and building and planning roadmaps.Experience in preparing functional and detailed system design documentsDemonstrate expertise with SDLC methodologyAbility to communicate effectively across multiple levels of the organization, including with leadership.Demonstrated leadership, initiative, analytical skills, and sound business acumen, including the ability to understand and analyze recommendationsExperience with all phases of testing (i.e., system, integration, user acceptance), including creating use cases, test conditions, and review of output.Must be able to adjust and work effectively in a dynamic, changing environmentOther:Master’s Degree.We sponsor H1B or related work visas for eligible candidates on F1/OPT/CPT.We offer health insurance 100% paid.We follow"}
{"text": "Skill set Required: Primary:Python, Scala, AWS servicesNoSQL storage databases such Cassandra and MongoDBApache Beam and Apache SparkAmazon Redshift, Google BigQuery, and Snowflake Secondary:Java, Go languageMicroservices frameworks such as Kubernetes and Terraform."}
{"text": "skills. They are expected to work with stakeholders across C2FO from engineering, product, data science, business intelligence and more to coordinate efforts on projects with wide-reaching impacts on the company. As a technical leader, they must ensure they are tracking the team’s efficiency and quality of deliverables and regularly adjusting processes and timelines to ensure high-quality delivery. In addition to managing data engineers and collaborating with stakeholders across the company, the Data Engineering Manager will also have the final say on technical decisions involving our data warehouse platform. As such, this leader must be highly knowledgeable about the current state of the art in the cloud data warehouse space, including the transition from ETL to ELT, management of cloud data infrastructure, and performance considerations of columnar, MPP data warehouses.\n\nRequirements\n\nBachelor’s degree in Computer Science or a closely related field.Five years’ progressive experience which must include experience in building and maintaining enterprise-scale ETL (or ELT) pipelines using tools such as SSIS, DBT, or Talend (or similar)Demonstrated knowledge of the following:Data warehousing best practices such as Kimball Methodology or more modern ELT approachesBest practices for maintaining a cloud-based, columnar, MPP database such as Redshift or Snowflake, andSSIS, DBT (Data Build Tool), AWS Redshift (or other MPP data store)\n\nCommitment to Diversity and Inclusion. As \n\n \n\nWe do not discriminate based on race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment decisions are based on qualifications, merit, and business needs."}
{"text": "Qualifications:MS or PhD in computer science or EE.4+ years of experience in machine learning and statistics, preferably in leading internet companies.Solid understanding of ML technologies, mathematics, and statistics.Proficiency in Java, Python, Scala, Spark, SQL, and large-scale ML/DL platforms.Passion for understanding the ad business and seeking innovation opportunities.Experience thriving in a fast-paced, data-driven, collaborative, and iterative environment.\nMainz Brady Group is a technology staffing firm with offices in California, Oregon and Washington. We specialize in Information Technology and Engineering placements on a Contract, Contract-to-hire and Direct Hire basis. Mainz Brady Group is the recipient of multiple annual Excellence Awards from the Techserve Alliance, the leading association for IT and engineering staffing firms in the U.S.\nMainz Brady Group is"}
{"text": "skills and seeks individuals who perform well in team environments. \nFinal Filing Date: 4/30/2024\nJob Control Number: 419467"}
{"text": "ExperienceHive, HDFSKubernetes, Docker Airflow\n\nResponsibilities\n\nFluency in working with SQL and analyzing and modeling complex dataExperience working with Python or similar programming languages like Scala or JavaExperience building ETL/ELT stream/batch pipelines on big data platforms such as Snowflake, Spark or othersCollaborate with peers across the entire range of development activities that includes distilling engineering designs from product requirements and data science, development of work plans, implementation, testing, productization, monitoring, and maintenanceStrong problem-solving skills in optimizing solutions for improved performance, scalability and reduced infrastructure costsUnderstanding of ad-tech terms and methodologies a plusExperience with data privacy and secure architectures. Experience with data cleanrooms a plus\n\nQualifications\n\n5+ years of Data EngineeringStrong knowledge of methodology and tools to increase data qualityHands on experience working with continuous integration tools such as JenkinsExperience with source control systems such as GitHubExpert knowledge of writing technical documentation/defects and issuesUnderstanding of ad-tech terms and methodologies a plusB.S. or equivalent in Computer Science, Math, or similarly technical field preferred. Advanced degree is a plus\n\nPerks\n\nUnlimited paid time off each year Company sponsored health, dental and vision benefits for you and your dependentsEmployee Advisory Groups / Proactive Social Groups401k PlanReferral BonusProgressive approach to paid parental leaveEpic personal and professional growth opportunities\n\nAbout\n\nWe believe every human on the planet should have the option of free access to the world’s information and content. In many cases this belief is powered by a three way value exchange between a publisher producing free content, a consumer consuming it and an advertiser paying the publisher for the chance to connect with its audience. The underpinning of this value exchange relies on having an independent auditing, measurement and optimization layer to power the transaction between the advertiser and publisher.\n\nToday the industry standard tools for advertising and media measurement and optimization are usually designed where increased personalization, higher advertising return on investment and increased publisher revenues often comes with negative trade off for consumer privacy or security risks of leaking private data. We envision a world where this doesn't have to be the case - a world where consumer privacy, security, and governance are incorporated into the fabric of the codebase while enabling the necessary business use-cases to effectively keep the world’s information and content free for everyone.\n\nVideoAmp’s mission is to create software and data solutions to enable advertisers to accurately measure and optimize their entire portfolio of linear TV, OTT, digital and walled garden investments while empowering publishers to effectively align and monetize their audiences with the advertiser’s desired outcome in a consumer-first privacy paradigm.\n\nMinimum base salary of $140,000 + Equity + Benefits. The actual compensation offer will be determined by a number of factors, including, but not limited to, applicant's qualifications, skills, and experience.\n\nCome and Join Us!"}
{"text": "requirements and the technical implementation of our Pimcore migration project. The ideal candidate will have a solid background in data analysis, particularly with expertise in SQL and navigating various data systems. This role will involve working closely with our Pimcore implementor and business stakeholders to ensure a smooth transition and optimization of our data landscape.\nKey requirements and optimize data structures within the Pimcore system.Work closely with the Pimcore implementor to translate business needs into technical requirements and vice versa.Independently drive data analysis projects, from initial scoping to final implementation, while keeping stakeholders informed of progress and outcomes.\nQualifications and Skills:Bachelor's degree in a relevant field such as Computer Science, Data Analytics, or Information Systems.Minimum of 3+ years of experience in a data analysis role, with a proven track record of successfully delivering data-driven solutions.Proficiency in SQL is essential, including the ability to write complex queries, create ERD documents, and navigate data warehouses.Experience with Pimcore is preferred but not required; however, a strong aptitude for learning new systems and technologies is essential.Excellent communication skills with the ability to effectively interact with technical and non-technical stakeholders.Strong problem-solving skills and the ability to work independently to drive projects forward.\nBrooksource provides equal employment opportunities ("}
{"text": "QUALIFICATIONS: \n\nEducation:\n\n12 years of related experience with a Bachelor’s degree; or 8 years and a Master’s degree; or a PhD with 5 years experience; or equivalent experience\n\nExperience:\n\nWork experience in biotech/pharmaceutical industry or medical research for a minimum of 8 years (or 4 years for a PhD with relevant training)Experience in clinical developmentExperience in ophthalmology and/or biologic/gene therapy a plus\n\nSkills:\n\nStrong SAS programming skills required with proficiency in SAS/BASE, SAS Macros, SAS/Stat and ODS (proficiency in SAS/SQL, SAS/GRAPH or SAS/ACCESS is a plus)Proficiency in R programming a plusProficiency in Microsoft Office Apps, such as WORD, EXCEL, and PowerPoint (familiar with the “Chart” features in EXCEL/PowerPoint a plus)Good understanding of standards specific to clinical trials such as CDISC, SDTM, and ADaM, MedDRA, WHODRUGExperience with all clinical phases (I, II, III, and IV) is desirableExperience with BLA/IND submissions is strongly desirableGood understanding of regulatory requirements for submission-related activities (e.g., CDISC, CDASH, eCTD) and CRT packages (e.g., XPTs Define/xml, reviewer’s guide, analysis metadata report, executable programs) is desirableAble to run the P21 checks is a plusKnowledge of applicable GCP/FDACHMP//ICH/HIPPA regulationsDisplays excellent organization and time management skills, excellent attention to detail, and ability to multi-task in a fast-paced environment with shifting priorities and/or conflicting deadlinesExcellent written and verbal communication skills and strong team player with demonstrated track record of success in cross-functional team environmentProven conceptual, analytical and strategic thinkingGood interpersonal and project management skillsProactively identifies risks, issues, and possible solutions\n\nBase salary compensation range:\n\nNational Range: $182,000/yr - $211,000/yr\n\nPlease note, the base salary compensation range and actual salary offered to the final candidate depends on various factors: candidate’s geographical location, relevant work experience, skills, and years of experience.\n\n4DMT provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, genetic information, marital status, status as a covered veteran, and any other category protected under applicable federal, state, provincial and local laws.\n\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities"}
{"text": "experience designing and building data sets and warehouses  Excellent ability to understand the needs of and collaborate with stakeholders in other functions, especially Analytics, and identify opportunities for process improvements across teams  Expertise in SQL for analytics/reporting/business intelligence and also for building SQL-based transforms inside an ETL pipeline  Experience designing, architecting, and maintaining a data warehouse and data marts that seamlessly stitches together data from production databases, clickstream data, and external APIs to serve multiple stakeholders  Familiarity building the above with a modern data stack based on a cloud-native data warehouse, in our case we use BigQuery, dbt, and Apache Airflow, but a similar stack is fine  Strong sense of ownership and pride in your work, from ideation and requirements-gathering to project completion and maintenance \n\nBonus points if you have \n\n Experience in the marketing domain and third party tools like Branch, Fivetran etc.  Experience building ETL data pipelines in a programming language, like Python or Scala  Experience using and/or configuring Business Intelligence tools (Looker, Tableau, Mode, et al)  Understanding of database internals and query optimization  Experience working with semi-structured or unstructured data in a data lake or similar  Experience working in data engineering or a similar discipline at a two-sided marketplace or similar B2C technology company  Experience mentoring and coaching data engineers and/or analysts \n\nThumbtack is a virtual-first company, meaning you can live and work from any one of our approved locations across the United States, Canada or the Philippines.* Learn more about our virtual-first working model here .\n\nFor candidates living in San Francisco / Bay Area, New York City, or Seattle metros, the expected salary range for the role is currently $252,000 - $308,000. Actual offered salaries will vary and will be based on various factors, such as calibrated job level, qualifications, skills, competencies, and proficiency for the role.\n\nFor candidates living in all other US locations, the expected salary range for this role is currently $214,200 - $262,200. Actual offered salaries will vary and will be based on various factors, such as calibrated job level, qualifications, skills, competencies, and proficiency for the role.\n\nBenefits & Perks\n\nVirtual-first working model coupled with in-person events20 company-wide holidays including a week-long end-of-year company shutdownLibraries (optional use collaboration & connection hubs) in San Francisco and Salt Lake City  WiFi reimbursements  Cell phone reimbursements (North America)  Employee Assistance Program for mental health and well-being \n\nLearn More About Us\n\n Life @ Thumbtack Blog  How Thumbtack is embracing virtual work  Follow us on LinkedIn  Meet the pros who inspire us \n\nThumbtack embraces diversity. We are proud to be an equal opportunity workplace and do not discriminate on the basis of sex, race, color, age, pregnancy, sexual orientation, gender identity or expression, religion, national origin, ancestry, citizenship, marital status, military or veteran status, genetic information, disability status, or any other characteristic protected by federal, provincial, state, or local law. We also will consider for employment qualified applicants with arrest and conviction records, consistent with applicable law.\n\nThumbtack is committed to working with and providing reasonable accommodation to individuals with disabilities. If you would like to request a reasonable accommodation for a medical condition or disability during any part of the application process, please contact: recruitingops@thumbtack.com .\n\nIf you are a California resident, please review information regarding your rights under California privacy laws contained in Thumbtack’s Privacy policy available at https://www.thumbtack.com/privacy/ ."}
{"text": "Qualifications\n Data Engineering, Data Modeling, and ETL (Extract Transform Load) skillsData Warehousing and Data Analytics skillsExperience with data-related tools and technologiesStrong problem-solving and analytical skillsExcellent written and verbal communication skillsAbility to work independently and remotelyExperience with cloud platforms (e.g., AWS, Azure) is a plusBachelor's degree in Computer Science, Information Systems, or related field"}
{"text": "skills to be able to work with business engineers and scientists and will have an immediate influence on day-to-day decision making.\n\nKey job responsibilities\n\n Develop data products, infrastructure and data pipelines leveraging AWS services (such as Redshift, EMR, Lambda etc.) and internal BDT tools (Datanet, Cradle, QuickSight) etc. Build machine learning operations (MLOps) to support automated LLM modeling and data refreshes using AWS tools (e.g., CloudFormation, ECR, SageMaker Model Registry, AWS Step Functions). Develop new data models and end to end data pipelines. Lead design reviews for the team. Provide technical and thoughtful leadership for the Business Intelligence team within CommOps org. Create and implement Data Governance strategy for mitigating privacy and security risks.\n\nAbout The Team\n\nAmazon Community Operations is helping Amazon become the most in-demand company locally, by anticipating our communities’ needs and building positive net impact anywhere we operate. CommOps improves the communities where we work and live by identifying and mitigating operational issues and driving positive social impacts. Amazon’s core competency in end-to-end logistics is one of the primary reasons that communities turn to us for help.\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nBellevue, WA, USA\n\nBasic Qualifications\n\n 5+ years of data engineering, database engineering, business intelligence or business analytics experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS\n\nPreferred Qualifications\n\n Experience mentoring team members on best practices Experience operating large data warehouses Experience with AWS tools (e.g., CloudFormation, ECR, SageMaker Model Registry, AWS Step Functions)\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $123,700/year in our lowest geographic market up to $240,500/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2618205"}
{"text": "experience in Azure Databricks to join our team. As a Senior Data Scientist at Techions, you will play a pivotal role in driving data-driven decision-making and developing advanced analytical solutions to address business challenges. You will work closely with cross-functional teams to extract insights from data, build predictive models, and deploy scalable solutions on the Azure Databricks platform.\nKey requirements, and success criteria.Utilize Azure Databricks for data preprocessing, feature engineering, model training, and deployment.Conduct exploratory data analysis to uncover hidden patterns and trends in large datasets.Develop data pipelines and workflows to automate repetitive tasks and streamline processes.Evaluate model performance and iterate on solutions to improve accuracy and efficiency.Stay current with emerging trends and technologies in data science and machine learning.\nRequirements:Bachelor's or Master's degree in Computer Science, Statistics, Mathematics, or related field.Proven experience as a Data Scientist, preferably in a senior or lead role.Strong proficiency in Python, R, or other programming languages commonly used in data science.Hands-on experience with Data science libraries like pandas, scikit-learn and jupyter notebooks.Hands-on experience with Azure Databricks for data processing, model training, and deployment.Solid understanding of machine learning algorithms, statistical techniques, and data visualization.Excellent problem-solving skills and the ability to think creatively to tackle complex problems.Strong communication skills with the ability to effectively convey technical concepts to non-technical stakeholders.\nPreferred Qualifications:Experience working in Agile development environments.Familiarity with cloud computing platforms such as Azure or AWS.Knowledge of big data technologies such as Hadoop, Spark, or Kafka.Join us at Techions and be part of a dynamic team that is shaping the future of technology. If you are passionate about data science and eager to make a meaningful impact, we want to hear from you!"}
{"text": "Experience: MS + 5yrs or PhD + 3yrs in quantitative field: Statistics, Applied Math, Computer Science, Physics or equivalent. 2+ yrs building personalized recommender systems, employing advanced techniques such as deep learning-based models, sequential recommendation algorithms, reinforcement learning frameworks, and/or dynamic re-ranking. Skilled in continuous model evaluation and adaptation to evolving user preferences and behavior patterns based on real-world feedback.Data-Centric Mindset: Be willing to explore the data and have it guide you to the best solution. Able to utilize a diverse range of advanced statistical and analytic techniques to inform development priorities and decision-making processes.Languages and Compute Frameworks: Able to write readable, testable, maintainable and extensible code in Python, SQL, and Spark. Bonus points for Ray.Production ML: Experience developing data preparation, model training and inference pipelines using cloud-managed tools like Dataproc, EMR, Airflow, vector databases (FAISS, Pinecone), or equivalent. ML Frameworks: Deep experience w/ PyTorch, XGBoost, SparkML, model registries (Hugging Face), LLM APIs, etc. Theory: Understanding of latest deep learning architectures and when to apply a given pre-trained model. Some experience with techniques like prompt engineering, fine tuning, reinforcement learning w/ human feedback, model distillation and model compression. Up-to-date on recent advances in the LLM space.Communication: Able to navigate large projects with multiple collaborators. Excellent cross-functional and verbal communication skills, enabling seamless communication with business partners and stakeholders.\n\nThe US base salary range for this position is $161,600.00 - $202,000.00 + bonus , equity and benefits.\n\nActual compensation offered will be based on factors such as the candidate’s work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process.\n\nWe want our employees and their families to thrive. In addition to comprehensive benefits we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.\n\nAbout Us\n\nZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams — all in one platform.\n\nZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.\n\nZoomInfo is proud to be"}
{"text": "experience as a Machine Learning Engineer, Data Scientist, or similar role.Strong background in machine learning, deep learning, and statistical modeling.Proficiency in programming languages such as Python, R, or Java.Experience with machine learning frameworks and libraries such as TensorFlow, PyTorch, scikit-learn, etc.Solid understanding of software engineering principles and best practices.Excellent problem-solving and analytical skills.Strong communication and collaboration skills.Ability to work effectively in a fast-paced and dynamic environment.\nThanks and Have a nice day, Raj KumarLorven Technologies, Inc. 101 Morgan Lane | Suite 209 | Plainsboro | NJ 08536Tel: 609-799-4202 X 247 | Fax: 609-799-4204Email: raj.k@lorventech.com | Web: www.lorventech.com Inc 5000 Fastest Growing Companies in AmericaTechServe Alliance Excellence AwardNJ Fast 50 Growing CompanySmart CEO Future 50 AwardsConsulting Magazine Fast 50 Growing Company in AmericaUSPAACC Fast 100 Asian American companies Excellence AwardSBE/MWBE Certified | E-Verified EmployerUnder Bill s.1618 Title III passed by the 105th U.S. Congress this mail cannot be considered Spam as long as we include contact information and a remove link for removal from our mailing list. To be removed from our mailing list reply with REMOVE in the subject heading and your email address in the body. Include complete address and/or domain/ aliases to be removed. If you still get the emails, please call us at the numbers given above."}
{"text": "Qualifications\n\nExperience & Education Proven experience in data science, statistics, computer science, or a related field. Formal education in a relevant discipline is preferred but not mandatory.Technical Skills Proficiency in data analysis and statistical software (e.g., Python, R, SQL) and familiarity with machine learning frameworks and libraries.Data Visualization Strong skills in data visualization tools and techniques to effectively communicate insights.Problem-Solving Ability to tackle complex problems with a strategic and analytical approach, turning data into actionable insights.Communication Excellent communication skills, with the ability to translate complex data into clear and compelling narratives fo\n\nThis is a volunteer opportunity provided by VolunteerMatch, in partnership with LinkedIn for Good."}
{"text": "QualificationsData Engineering, Data Modeling, and ETL (Extract Transform Load) skillsMonitor and support data pipelines and ETL workflowsData Warehousing and Data Analytics skillsExperience with Azure cloud services and toolsStrong problem-solving and analytical skillsProficiency in SQL and other programming languagesExperience with data integration and data migrationExcellent communication and collaboration skillsBachelor's degree in Computer Science, Engineering, or related field\nEnterprise Required SkillsPython, Big data, Data warehouse, ETL, Development, azure, Azure Data Factory, Azure Databricks, Azure SQL Server, Snowflake, data pipelines\nTop Skills Details1. 3+ years with ETL Development with Azure stack (Azure Data Factory, Azure Databricks, Azure Blob, Azure SQL).  2. 3+ years with Spark, SQL, and Python. This will show up with working with large sets of data in an enterprise environment.  3. Looking for Proactive individuals who have completed projects from start to completion and have an ability to work independently and once ramped up, require minimal handholding."}
{"text": "Qualifications - ExternalMinimum one (1) year programming experience.Minimum one (1) year statistical analysis and modeling experience.Bachelors degree in Mathematics, Statistics, Engineering, Social/Physical/Life Science, Business, or related field OR Minimum two (2) years experience in data analytics or a directly related field. Preferred Qualifications:One (1) year experience working with SQL.One (1) year machine learning experience.One (1) year experience working with artificial intelligence tools.One (1) year statistical modeling experience.One (1) year data simulation experience.One (1) year experience working with data visualization tools.One (1) year experience working with Open Source Tools (e g , R, Python).One (1) year experience working with Tableau.One (1) year experience working with business intelligence tools.One (1) year experience working with Excel.One (1) year experience working with SPSS.One (1) year experience working with statistical analysis software.One (1) year experience working with Access."}
{"text": "experience in software engineering with a specialization in ML/AI systems.Proven ability to develop and drive scalable solutions.Proficiency in Pytorch and experience in deploying solutions across cloud, on-premises, and hybrid environments.Experience in the biotech/pharma industry with knowledge of protein ML models.\nThis is an exciting opportunity to collaborate with teams including AI Researchers, Software Engineers and Protein Scientists to define and execute an ambitious product roadmap.\nWe offer a competitive salary and benefits package, as well as opportunities for growth and advancement within the company. If you are a motivated and talented Machine Learning Engineer with a passion for drug discovery, deep learning or the development of new medicines then I encourage you to apply.\nFor any further questions please message Aaron Walker on LinkedIn or email me via: awalker@barringtonjames.com"}
{"text": "Qualifications:Bachelor's degree in Biology, Computer Science, Statistics and Decision Science, or related field.Proficiency in R, Python, Linux, Shiny, and HTML.Strong skills in data processing, integration, scientific data analysis, machine learning, and data visualization.Experience in developing scientific applications using R, Shiny, and/or Python.Excellent problem-solving and communication skills.Ability to work independently and collaboratively in a team environment.\n\n\n\n\n\n\n\n\n\nPriceSenz is"}
{"text": "requirements including GCP and is able to perform these essential duties and responsibilities with some direct supervision.Working closely with external Data Management contractors, Clinical Development, Clinical Affairs, the Clinical Trial Managers, Sample Bank team, Legal department, Logistics team, and Marketing teams to ensure project tasks are managed to completion.Ensure standards for statistical analyses are followed via statistical program validation and quality control activities.Provide statistical input, develop statistical analysis plan for clinical study protocols.Manage data management project timelines and schedules for projects within and outside the department.Select, validate, manage database for clinical data tracking (EDC system)Responsible for monitoring assigned clinical and outcomes studies to ensure compliance with clinical study protocols, investigator agreements, and applicable corporate regulatory requirements.Other job duties as assigned, which may include:Assist in developing departmental SOPs.Preparation of statistical reports for regulatory submissions, including global, FDA, IRBs or internal customersPreparation for departmental and executive meetingsClinical data entry and QCCase Report Form (CRF) and query managementSelect, validate, manage electronic CRF (eCRF) software/database.Database training for new users; support for current usersSample data analysis for Product Development teams.Benchmark study result analysis with Product Development and Marketing teams.Carry out internal auditing of clinical databases.Any other duties deemed necessary by management.\n\nEducation, Experience, And Qualifications\n\nBachelor's Degree with 5 years’ in Molecular Biology / Biochemistry or related discipline, with experience in a regulated clinical research environmentMaster's Degree with 3 years’ or Ph.D. (preferred) in Molecular Biology / Biochemistry or related discipline, with experience in a regulated clinical research environmentPh.D. with 1 year, in Molecular Biology / Biochemistry or related discipline, with experience in a regulated clinical research environmentSuperior communication, presentation and writing skills.Highly organized with proven time management and prioritization skillsComputer programing skills for clinical data mining is desired.Experience in molecular and clinical data analysis.Experience in searching through and understanding scientific and regulatory literature.Experience in working independently in a fast-paced environment with rapidly changing priorities.Experience in documentation and record management for laboratory and clinical studies (GLP and GCP)Experience or knowledge of the requirements on working in a FDA regulated environment.\n\nTravel Requirements\n\n20% 10% international\n\nWhat We Offer\n\nReceive a competitive salary and benefits package as you grow your career at DiaSorin. Join our team and discover how your work can impact the lives of people all over the world.\n\nDiasorin is \n\nIn compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and Canada and to complete the required employment eligibility verification document form upon hire.\n\nDiasorin is committed to providing reasonable accommodations for qualified individuals with disabilities. If you are a US or Canada candidate and require assistance or accommodation during the application process, please contact the North America Talent Acquisition Team at hrtalent@diasorin.com or 1-800-328-1482 to request an accommodation.\n\nThe above job description is intended to describe the general content, identify the essential functions, and set forth the requirements for the performance of this job. It is not to be construed as an exhaustive statement of duties, responsibilities, or requirements.\n\nDiasorin reserves the right to modify or amend this job posting as needed to comply with local laws and regulations.\n\nPlease note that offers of employment at Diasorin may be contingent upon successful completion of a pre-employment background check and drug screen, subject to applicable laws and regulations.\n\nThis position is not eligible for partnership with a third-party search firm vendor without expressed, written consent from the Diasorin Human Resources Department."}
{"text": "Experience - 9+ years\nall visas acceptable\nMust have: python, spark9+Experience in the design and development of large-scale big-data engineering applications,2+ years of Python development experienceSpark experience5+ years implementing batch and real-time Data integration frameworks and/or applications,Proficient with DevOps, Continuous Integration, and Continuous Delivery (Jenkins, Stash).Experience and comfort executing projects in Agile environments (Kanban and Scrum)\n\nIf you are interested, let me know; it is a W2 contract position.\nFor immediate response sureshp@vmcsofttech.com\nThanks and Regards,US IT RecruiterSuresh. P. V.480-407-6916"}
{"text": "Qualifications:  2+ years as data engineer, software engineer, or data analyst. Battery Engineering / Electrical Engineering experience desired. Working knowledge and experience with big data.Strong working knowledge of Python, SQL, and Git. Basic knowledge of SQL databases, Spark, data warehousing, and shell scripting.Candidate must have solid competency in statistics and the ability to provide value-added analysis. Self-starter with entrepreneurial experience and ability to interact with other functions in matrix environment. Proven creativity to go beyond current tools to deliver best solution to the problem. Familiarity with database modeling and data warehousing principles.Experience in designing and building data models to improve accessibility, efficiency, and quality of data. Improve ELT efficiency and answering business critical questions with data.Experience building scalable data pipelines using Spark, etc. is a plus.Desirable for experience with Apple OS, such as iOS, MacOS, etc.\nJob Description:  Write ELT pipelines in SQL and Python. Utilize advanced technologies, for modeling enhancements.Test pipeline and transformations, and document data pipelines.Maintain data and software traceability through GitHub.Build a high-quality data transformation framework, implementing and operating data pipelines with an understanding of data and ML lifecycles.Understand end to end nature of data lifecycles to deliver high quality data and debug data concerns.Drive development of data products in collaboration with data scientists and analysts. Automate reporting where possible to make team more efficient.Be able to analyze factory, user, and failure data and use engineering understanding mechanisms to resolve battery problems. Work with diverse teams including data scientists, engineers, product managers and executivesDeliver high quality analytic insights from a data warehouse.Provide ad-hoc reporting as necessary (sometimes urgent escalation)Write programs for data filtering, organization, reporting. Write programs for uploading to and maintaining data in SQL database. Develop basic data management and selection programs on SQL. \nEducation:\nMS or Ph.D. in Computer Science, Software Engineering, Battery Engineering, Machine Learning, Statistics, Operations Research or related field. CSSBB, CQE desired."}
{"text": "experienced data scientist who thrives on innovation and craves the vibrancy of a startup environment.\nResponsibilitiesProven experience in applying advanced data science algorithms such as neural networks, SVM, random forests, gradient boosting machines, or deep learning.Demonstrable expertise in at least three classes of advanced algorithms.Prior experience with live recommender systems and their implementation.Proficiency in deep learning frameworks, preferably TensorFlow.Proven track record in implementing scalable, distributed, and highly available systems on Cloud Platform (AWS, Azure, or GCP).Strong machine learning and AI skills.Strong communication skills, adaptability, and a thirst for innovation.High autonomy, ownership, and leadership mentality are crucial as you will be a pivotal member shaping our organization's future.Strong skills in data processing with R, SQL, Python, and PySpark.\nNice to haveSolid understanding of the computational complexity involved in model training and inference, especially in the context of real-time and near real-time applications.Familiarity with the management and analysis of large-scale assets.A team player with a collaborative mindset who is eager to learn and apply new methods and tools.A sense of pride and ownership in your work, along with the ability to represent your team confidently to other departments."}
{"text": "experience and should be completely comfortable and up to date with the recent versions of Java.  The candidates must have current / very recent 1-2+ years development experience that includes: Must Have Skills:• AWS with EKS, EMR, S3• Python• Spark• Java (2+ years)• Hadoop, Parquet, json, csv• Airflow• Kafka• Linux• CI/CD Highly desired:• Dynamo• Flink• Oracle• Databricks• SQL"}
{"text": "experience neededVery strong experience in Kafka and Kafka data injection Strong exp in working with API.Strong exp in Python with AWS.Experience with Informatica IICS and Snowflake. Expertise in Snowflake's cloud data platform, including data loading, transformation, and querying using Snowflake SQL.Experience with SQL-based development, optimization, and tuning for large-scale data processing.Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes.hands-on experience in IICS or Informatica Power Center ETL development1+ years of hands-on experience in Linux and shell scripting.1+ years of experience working with git.1+ years of related industry experience in an enterprise environment.1+ years of hands-on experience in Python programming."}
{"text": "experience as a lead full stack Java developer with strong JSP and servlets and UI development along with some backend technologies experience Another primary skill is Team handling and responsible for Junior developer’s code reviews and onsite/offshore coordination experience is a must. \n \nPreferable local candidates\n \nRequired skills: We need resources with Java, JSP, Servlets, JavaScript, jQuery, HTML, CSS, MSSQL, SOAP, MVC frameworks Spring or Struts, Spring Boot, and Restful web services.\n\nThe position must have the following:\nMinimum of 14+ years of hands-on Java development experience.Strong experience on Application development & solution & Design.Strong experience in debugging and analytical skills.5 years of hands-on JavaScript experience.Extensive experience in delivering enterprise solutions with JSP, Servlets, Security and MVC.Strong experience with programming HTML/CSS technologiesGood understanding in XML, XSD, and XSLT.Strong experience in developing and consuming REST/SOAP web services.Expert-level knowledge of JDBC, backend SQL, database-stored procedures and JPA frameworks.Experience with Agile and JenkinsStrong experience with Junit and any mocking framework like mockito, jmockit etc.Good experience with SCM tools like Git, and TFSManagement or mentor experience with development teamsGood knowledge of a micro-service environmentKnowledge of web accessibility and Ajax\n\nNice To Have Requirements:\nExperience in the backbone is a plus.Experience in Splunk is a plus.Experience in cache frameworks like Redis, and Memcached is a plus.6 months of hands-on SCSS experience.\n\nBasic Qualifications:\nBachelors/Master s Degree in Computer Science or related field in a reputed institution5 years of professional experience in software development with most of them from a product companyProficient in Java Development.\nramarao@paramountsoft.net/770-299-3929"}
{"text": "requirements and prioritize projects. \n\nTeam Leadership And Development\n\n Lead a team of data engineers, analysts, and DBAs, providing guidance, coaching, and support.  Set clear objectives and performance expectations.  Foster a culture of collaboration, innovation, and continuous learning.  Maintain a high level of technical and business expertise in the group \n\nCollaboration With Business And IT Units\n\n Work closely with business units to understand their evolving needs and challenges in a fast-changing industry.  Collaborate with other IT units to integrate data systems and ensure data consistency and compatibility.  Prioritize project work based on business impact and strategic objectives. \n\nData Analysis And Reporting\n\n Oversee the development of dashboards, reports, and visualizations to provide insights into business needs.  Provide training and support to users for effective use of data tools and reports  Ensure production support for data-related issues and inquiries \n\nProject Management\n\n Manage projects related to data engineering and business intelligence initiatives  Define project scope, goals, and deliverables.  Monitor progress, manage resources, and mitigate risks to ensure successful project completion. \n\nRole\n\n What you need to be successful in this role: \n\n Bachelor's degree in Mathematics, Computer Science, Data Science or a related field.  8 to 15 years of experience in data engineering, business intelligence, or data science.  3 to 5 years of experience supervising technical staff.  5 to 8 years of experience programming in SQL, Python or R.  3 to 5 years of experience with data visualization tools (e.g., OAS, Tableau, Power BI).  Understanding of relational databases including Oracle and PostGres.  Understanding of data warehousing and data marts best practices.  Electricity Markets, including Day-Ahead, Real-Time, FTR, ARR, and Markets Monitoring desired.  From Holyoke, MA, ISO New England oversees the 24/7 operation of the power grid that covers the six-states of New England and administers the region’s $15+ billion “stock exchange” for the buying and selling of wholesale electricity. The power system is constantly evolving as new technologies emerge and energy policies evolve. There is a lot happening at our organization behind the scenes to make sure the grid continuously yields reliable electricity at competitive prices while addressing the clean energy transition here in New England. COME JOIN US in making an impact within the region!\n\nTo learn more about what we offer our employees visit:\n\n Mission, Vision, and Values \n\n Living in Western New England\n\n What we Offer\n\nDiversity and Inclusion \n\n Careers \n\nFollow Us On\n\n LinkedIn\n\nTwitter\n\nYouTube \n\n Equal Opportunity  : We are proud to be an \n\nDrug Free Environment\n\nWe maintain a drug-free workplace and perform pre-employment substance abuse testing.\n\nSocial Networking Notice\n\nISO New England reserves the right to review the candidate's postings on any social networking site accessible in the public domain as part of the candidate assessment process.\n\nApply Now"}
{"text": "Skills: Senior Technical Business/Data Analyst with good JIRA skills(GC/GC-EAD/USC)\nSkill: Senior Technical Business/Data Analyst with good JIRA skillsOverall 10+ years exp as Tech BA or Data analyst, 3 to 5 years of experience in direct business-facing roles.Senior Data Analyst with extensive SQL and PLSQL experience.Writing/Optimizing basic to advanced SQL queries as required.Able to troubleshoot complex Oracle Procedures, functions and packages.Extensive experience handling business owners, senior leaders.Good Agile and JIRA experience, knowledge of writing stories, EPICS, Scrum ceremonies.Knowing Unix Shell commands is good to have."}
{"text": "skills and experience to create positive, long-lasting change for our region; while maintaining a healthy balance between personal and professional endeavors. We offer competitive benefits, flexibility in schedule and partial telework, half-days on Fridays, a collaborative culture, and mission-centered work. To learn more about our vision and mission, please visit cfrichmond.org/About/Our-Story. TITLE: Data Analyst REPORTS TO: Chief Information OfficerCLASSIFICATION: Full-time, exempt; salaried-benefits eligible; 37.5 hours per week POSITION DESCRIPTION: At the Community Foundation for a greater Richmond, data is key to everything we do. As a Data Analyst, you will leverage analytic and technical skills to help us innovate, build and maintain well-managed solutions and capabilities for our customer focused businesses. On any given day you will be challenged with regional data, customer data and operational data. Creating indicators, metrics and reports that provided data and information to our business leaders is a critical part of this role. \nOur Data Analyst will be responsible for designing, developing and managing the data architecture, infrastructure and tools necessary for collecting, storing, processing and analyzing data. The primary focus is to create data sets and reporting that enable the organization to derive valuable insights from their data. PRIMARY DUTIES AND SKILLS AND EXPERIENCE:  Collecting, managing, and analyzing dataMining data and conducting basic analyses, using business intelligence and visualization tools like MS Power BI, MS Excel, and TableauManaging data (organizing, cleaning, and storing them in relational databases)Interpreting data, analyzing results using basic statistical techniquesDeveloping and implementing data analyses, data collection systems and other strategies that optimize efficiency and quality.Acquiring data from primary or secondary data sources and maintaining databasesAcquiring, analyzing, and presenting data to support decision makingInspecting, cleaning, transforming, and modeling data to support decision-makingData entry, governance, and validationProblem-solving skills: Strong analytical and problem-solving skills, ability to troubleshoot and debug complex software issues.Communication skills: Strong verbal and written communication skills, ability to explain technical concepts to non-technical stakeholders.Technical curiosity: A desire to stay up to date with new technologies and industry trends, ability to quickly learn new tools and technologies as needed.Collaborating with other team members to design and develop new capabilities to support business needs.\n\nPREFERRED QUALIFICATIONS:Education: A bachelor's or master's degree in computer science, software engineering, technology, engineering, mathematics, or a related fieldExperience in data analyticsExperience coding in Salesforce, Python, Microsoft SQLExperience working within process management and improvement methodologies – Agile, Lean etc.Experience working with Microsoft Azure data environments.Experience delivering Data Governance and Data Quality Management concepts and practices within the financial services industry. If you are interested, please visit https://www.cfrichmond.org/discover/cf/join-us and submit your cover letter including salary requirements and resume to https://www.cfengage.org/jobapplication. No phone calls or agencies, please. Don’t meet every single requirement? We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about this role, but your experience doesn’t align perfectly with every qualification, we encourage you to apply anyway. You may be the right candidate for this or other roles."}
{"text": "experienceTo apply please email/ jack.crowley@searchability.com\nWe are a leading innovator in the AI sector, working on making machines as advanced as possible!\nWHO ARE WE?We are a high tech AI company who are shaping the way machines learn and interact with humans - If you are looking to join an exciting company, get in touch!\nWHAT WILL YOU BE DOING?\nYou will be working on our flagship products - Assisting in the rapid deployment of products that appeal to the market we are in, with the challenge of then scaling it afterwards. This role will give you the chance to work closely with our CTO and be a vital part of our growing tech team.\nWE NEED YOU TO HAVE….Solid Python ExperienceWork professionally with PyTorchSolid C++ experience\nIT’S NICE TO HAVE….NLP/AI and ML experienceOpen Source experienceTraining deep neural networksWork with robots a plus\n\nTO BE CONSIDERED….Please either apply by clicking online or emailing me directly to jack.crowley@searchability.com. I can make myself available outside of normal working hours to suit from 7am until 10pm. If unavailable, please leave a message and either myself or one of my colleagues will respond. By applying for this role, you give express consent for us to process & submit (subject to required skills) your application to our client in conjunction with this vacancy only. Also feel free to follow me on Twitter @SearchableJack or connect with me on LinkedIn, just search Jack Crowley in Google! I look forward to hearing from you.\nMachine Learning, AI, ML, NLP, Deep Learning, Python, PyTorch"}
{"text": "Skills:1. Extensive knowledge of Data Management, Data Governance, Data quality activities, tools, and frameworks, with experience reporting on large amounts of data while understanding the importance of meeting deliverables.2. Experience implementing and using data management tools such as data quality, and business/technical metadata catalogs, with strong experience implementing master data management tools and processes.3. Demonstrated experience with master data management projects, preferably company or person disambiguation.4. Ability to create datasets from a variety of disparate sources to further data governance initiatives and processes.5. Demonstrated experience in performing data mining on large datasets to supplement data governance quality improvement initiatives.6. Strong experience of SQL and Python, relational and non-relational databases, database structures, and unstructured databases, and preferably graph and other NoSQL databases.7. Strong understanding of data quality frameworks within data lifecycle management.8. Demonstrated experience driving data quality initiatives and resolution.9. Demonstrated experience with process improvement, workflow, benchmarking and / or evaluation of business processes.10. Ability to write various documents such as functional requirements, data quality rules, and policy definitions.\nThe following benefits are offered for this position: medical, dental, & vision insurance, short-term disability, life and AD&D insurance, and a 401(k) retirement plan."}
{"text": "Experience in Production Operations or Well Engineering Strong scripting/programming skills (Python preferable)\n\nDesired: \n\n Strong time series surveillance background (eg. OSI PI, PI AF, Seeq) Strong scripting/programming skills (Python preferable) Strong communication and collaboration skills Working knowledge of machine learning application (eg. scikit-learn) Working knowledge of SQL and process historians Delivers positive results through realistic planning to accomplish goals Must be able to handle multiple concurrent tasks with an ability to prioritize and manage tasks effectively\n\n\n\nApex Systems is \n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\n4400 Cox Road\n\nSuite 200\n\nGlen Allen, Virginia 23060\n\nApex Systems is"}
{"text": "QualificationsBachelor's or Master's degree preferred EMS, Power Systems, Generation, Electrical Knowledge is preferred 1 - 2 years' Data Analysis experience"}
{"text": "skills to ensure data analytics objectives and requirements are met. IT Data Analyst works with IT and business stakeholders to identify processes and solutions to combine and normalize information from many disparate sources. This position improves data quality and consistency, supports program data requirements by analyzing process controls, creates common vocabulary, and performs root cause analysis.\nIn this position you will:Strong Ability to assess and analyze data with fluency in variety of enterprise systems like Cloud Database(s) & Technologies, on prem database like SQL Server, Db2, Teradata and HL7Experience in the complete data analytics project life cycle is required (requirements gathering, architecture, design, implementation, and support)Responsible for data mapping exercise for applications, data systemsPerform data analysis, using in-depth knowledge of databases, non-structured and healthcare dataResponsible for analyzing business requirements, designing, and developing quality and patient data registry applications or repositoriesWorks to think analytically and critically to lead data standardization and automating effortsExcellent communication skills to work with various product analyst/business users to understand data needsProvide advanced analysis and ad hoc operational data quality and data literacy reports as requested by stakeholders, business partners, and leadershipGood Data knowledge from a functional and technical sideAbility to understand clinical data in order to develop and design dataset models, data quality rules, and business requirement analysisAbility to prioritize and manage concurrent projects, demonstrate initiative, and work independently with minimal supervisionPresent data formally and informally and facilitate discussion regarding data outputsCreate documentation for work products and manage or meet target datesMust possess a strong understanding of current agile development methodologies\nRequirements:Nice to have - Working understanding of health industry data standards/normalization required for data interoperability and health information exchange (i.e. LOINC, SNOMED-CT, RxNorm, ICD-9, etc.3+ years of Experience in SQL database(s),1+ years of experience in Cloud technologies (Microsoft, Google or AWS). Preferred Azure and GCP platformExperience in Reporting tools like Looker, PBI \nWe are"}
{"text": "requirements, understand business needs, and translate them into technical solutions using Power BI and SQL.Perform data analysis and troubleshooting to identify trends, anomalies, and opportunities for improvement, and present findings to stakeholders.Participate in the design and implementation of data models, data pipelines, and data integration processes to ensure data quality, consistency, and integrity.Stay current with industry best practices, emerging technologies, and advancements in Power BI and SQL to continuously enhance the company's data analytics capabilities.Provide training, guidance, and support to end-users on utilizing Power BI dashboards and reports effectively to drive data-driven decision-making.Qualifications:Bachelor's degree in Computer Science, Information Systems, Engineering, or related field. Advanced degree preferred.Minimum of 3 years of hands-on experience in developing and maintaining Power BI dashboards and reports.Proficiency in writing complex SQL queries, stored procedures, and scripts to extract, transform, and analyze data from relational databases.Solid understanding of data visualization principles and best practices, with experience in designing intuitive and interactive dashboards.Strong analytical and problem-solving skills, with the ability to translate business requirements into technical solutions.Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.Experience with data modeling, data warehousing, and ETL processes is a plus.Relevant certifications in Power BI and SQL are desirable."}
{"text": "Requirements: US Citizen, GC Holders or Authorized to Work in the U.S.\n\nSr. Data Analyst will be responsible developing an understanding of the business and department processes in order to provide data analysis, highlight insights, and recommend solutions/improvements that enable the department to operate more efficiently and improve performance."}
{"text": "Qualifications:\n\n Bachelor’s degree  At least 4 years of experience programming with Python, Scala, or Java (Internship experience does not apply)  At least 3 years of experience designing and building data-intensive solutions using distributed computing  At least 2 years of on-the-job experience with an industry recognized ML frameworks (scikit-learn, PyTorch, Dask, Spark, or TensorFlow)  At least 1 year of experience productionizing, monitoring, and maintaining models \n\nPreferred Qualifications:\n\n 1+ years of experience building, scaling, and optimizing ML systems  1+ years of experience with data gathering and preparation for ML models  2+ years of experience with building models  2+ years of experience developing performant, resilient, and maintainable code  Experience developing and deploying ML solutions in a public cloud such as AWS, Azure, or Google Cloud Platform  Master's or doctoral degree in computer science, electrical engineering, mathematics, or a similar field  3+ years of experience with distributed file systems or multi-node database paradigms  Contributed to open source ML software  3+ years of experience building production-ready data pipelines that feed ML models  Experience designing, implementing, and scaling complex data pipelines for ML models and evaluating their performance \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nNew York City (Hybrid On-Site): $165,100 - $188,500 for Senior Machine Learning Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "requirements gathering, activities, and design phases. They are tasked with developing robust reporting capabilities, monitoring performance metrics, and implementing quality control measures to drive continuous improvement.Job Duties:Interpret data and derive actionable insights by applying statistical techniques to analyze results effectively.Develop and deploy databases, data collection systems, and advanced analytics strategies to enhance statistical efficiency and ensure data quality.Source data from primary and secondary sources, and maintain databases and data systems to ensure accessibility and reliability.Identify, analyze, and interpret trends and patterns within complex datasets to uncover valuable insights.Employ data cleaning techniques to filter and refine datasets, addressing any inconsistencies or errors to maintain data integrity.Optimize data processes by utilizing technical expertise in data modeling, database design and development, and data mining techniques.Proficiency in utilizing reporting packages such as Business Objects, SQL databases, and programming languages like XML and JavaScript for data analysis.Knowledge of statistical methods and experience using statistical packages such as Excel, SPSS, and SAS for analyzing datasets.Possess strong analytical skills, with the ability to collect, organize, and analyze significant amounts of data with meticulous attention to detail and accuracy.Skilled in query formulation, report writing, and presenting findings to stakeholders effectively.Job Qualifications:A bachelor's degree in Mathematics, Economics, Computer Science, Information Management, or Statistics provides a solid foundation in quantitative analysis and data management."}
{"text": "experience. Job Description:Python, Databricks, PySpark Domain expertise – Pharmacy experienceResponsible for product architecture, manage dependencies, feature prioritization, reviews and suggestion for solution Implementation. Responsible for Product Architecture of specific work streams and the domains within them·Responsible for product backlog prioritization Functional and Non-Functional-Responsible for Architecture output from each work stream·Manage dependencies within and outside of the work stream·Accountable for Product Quality·Ensure adoption of reusable components, flag issues and shortfalls"}
{"text": "Requirements: (NOT ACCEPTING CANDIDATES WITH VISA)Education:Bachelor’s degree in a relevant field, or equivalent experience in the Heavy-Duty vehicle or parts industry.Experience:Minimum of 2 years of experience in conducting web-based research, and/or 2+ years in the Heavy-Duty vehicle or parts industry.Proficiency in Microsoft Excel with intermediate-level skills.Strong research capabilities and resourcefulness, with a demonstrated ability to leverage internet search engines effectively.Detail-oriented mindset coupled with a proactive, self-starting attitude.Preferred Skills:Familiarity with the Automotive, Construction Equipment, Farm Equipment, or Trucking industries.Knowledge of parts schematics and drawings, enhancing the ability to interpret and analyze technical data.We offerBenefits:Competitive salary commensurate with experience.Comprehensive benefits package, including health insurance, retirement plans, and paid time off.Opportunities for career advancement and professional growth within a global organization.A dynamic and inclusive work environment that fosters creativity, collaboration, and innovation. Join Mann+Hummel and be part of a team that is shaping the future of filtration technology. Apply now to embark on a rewarding career in manufacturing engineering!"}
{"text": "experiences, perspectives, and backgrounds. We provide advisory services, strategies, and solutions to meet clients’ evolving needs amid dynamic market conditions and varying industries.\n\nWe strive to attract the most qualified, passionate candidates who specialize in investment banking, investment management, private wealth management, and a variety of other business functions.\n\nWe work tirelessly to create an inclusive culture and take pride in fostering employees’ professional and personal growth. We empower our people to bring their best thinking each day so we can deliver the tailored, thoughtful work and problem-solving abilities that our clients expect.\n\nEqually, we are proud of our long-term partnerships with the communities in which we live and work, a legacy we inherited from our founder.\n\nWe invite you to learn about how we are seeking excellence in everything we do and empowering our clients’ success with passion, creativity, and rigor. For more information, visit williamblair.com.\n\nThe Information Technology team has the mission of becoming a true business partner and we are searching for a Senior AI Engineer to help execute on this mission. The Senior AI Engineer will work on building end to end AI solutions leveraging proprietary and 3rd party data sources that drive business growth and achieve productivity gains. Ideal candidates love breaking down problems, building solutions, delivering actionable and data-driven insights, and working in a fast-paced, dynamic environment.\n\nResponsibilities Include But May Not Be Limited To\n\nDevelop and deploy robust data architectures (data lake, data warehouse, etc.) to handle large-scale datasets, ensuring data quality and integrity.Develop and implement Microservices architecture to facilitate the scalable and efficient management of our data services.Leverage and refine open-source generative AI models and use existing generative AI models to solve advanced data augmentation and analytics.Manage and optimize data processing workflows, ensuring timely and accurate data availability.Optimize data retrieval processes through database tuning, query optimization, and ensuring scalable infrastructures.Analyze structured and unstructured data to understand how our customers interact with our product and service offeringsPerform the design, analysis, and interpretation of projects from data requirement gathering to data processing, modeling, and recommendationsWork with data scientists, analysts, and business teams to understand data requirements and deliver scalable data solutions.Collaborate with IT, security, and compliance teams to ensure adherence to data management and protection standards.Manage and optimize cloud-based data solutions (preferably Azure: including Synapse, Azure Machine Learning, Databricks, ADF, and Azure Data Lake).Ensure robustness, scalability, and sustainability of data infrastructure in the cloud environment.Maintain comprehensive documentation of data models, pipelines, and ETL processes.\n\nQualifications\n\nMaster's degree in Computer Science, Engineering, Data Science, or a related field.5+ years of experience as a data engineer or machine learning engineer, with a proven track record in developing ETL processes, data pipeline architecture, and machine learning model development.Strong proficiency in Python for data processing and manipulation.Experience with SQL and Spark to handle data extraction, transformation, and loading of big data.Demonstrable expertise in designing and implementing efficient data models to support ETL processes and data analytics.Extensive experience managing and optimizing Azure cloud data technologies (Synapse, Databricks, ADF, or Azure Data Lake).Hands-on experience with API utilization, development, and management.Practical experience with event-driven architecture and real-time data processing.Ability to effectively communicate technical concepts to both technical and non-technical stakeholders.Experience with data analysis and statistical modeling using the Python ecosystem, with packages such as numpy, pandas, statsmodels, scikit-learn, etc.Experience working with various machine learning / deep learning algorithms and frameworks.Self-starter, comfortable with ambiguity, ability to initiate and drive projects with minimal oversight and guidance.A record of continuous learning and adaptation to stay updated with the latest in data engineering, machine learning, generative AI, cloud technologies, and data compliance standards.Certifications in Azure Data Engineering, Azure Machine Learning, Spark, or other relevant technologies.Proven track record of leveraging data to deliver business value and present data-driven insights to business audiences.Familiarity with PowerBI for developing interactive reports and data visualizations.Experience with LLMs and OpenAI APIs.Experience shipping code into production.Experience in the investment banking or financial sector.\n\nWilliam Blair is \n\nBe aware of hiring scams: William Blair has clear processes and guidelines with regards to recruiting. We do not request personal financial information in connection with an employment application nor does William Blair extend any employment offers without first conducting an interview through one of its registered offices. William Blair does not use instant messaging services such as WhatsApp, Telegram, or iMessage as part of the recruiting or interviewing process.\n\nNote to External Recruiters / Search Firms: William Blair does not accept unsolicited resumes and will not pay for any placement resulting from the receipt of an unsolicited resume. Any unsolicited resumes received will not be considered as a valid submission.\n\nDownload William Blair's Privacy Policies For Job Applicants\n\nCalifornia Consumer Privacy Act Privacy Notice (CCPA)General Data Protection Regulation Privacy Notice (GDPR)\n\nContact us should you have any questions or concerns."}
{"text": "requirements, prioritize tasks, and deliverintegrated solutions.Documentation and Best Practices: Document design decisions, implementation details, and bestpractices for data engineering processes, ensuring knowledge sharing and continuous improvementwithin the team.Qualifications:Bachelor's or Master's degree in Computer Science, Engineering, or related field.Proven experience as a Data Engineer, preferably with specialization in handling image data.Strong proficiency in cloud computing platforms (e.g., AWS, Azure, Google Cloud) and related services(e.g., S3, EC2, Lambda, Kubernetes).Experience with data engineering tools like DataBrick, Snowflake, Glue etc.Proficiency in programming languages commonly used in data engineering (e.g., Python, Scala, Java) andfamiliarity with relevant libraries and frameworks (e.g., Apache Spark, TensorFlow, OpenCV).Solid understanding of data modeling, schema design, and database technologies (e.g., SQL, NoSQL,data warehouses).Familiarity with DevOps practices, CI/CD pipelines, and containerization technologies (e.g., Docker,Kubernetes).Strong problem-solving skills, analytical thinking, and attention to detail.Excellent communication and collaboration skills, with the ability to work effectively in a cross-functionalteam environment."}
{"text": "experienceCollaborate with other solution and functional teams (e.g., commercial operations, professional services, clinical education, financial administration) to find practical and ambitious solutions to these gaps and aspirations.Identify critical success metrics with which to gauge the relative performance and progress of our managed service customers over time.\n\nYou're the right fit if: \nYou’ve acquired 7+ programming, data visualization, and healthcare informatics experience as well as knowledge of physiologic monitoring systems.Your skills include database design, modeling and dynamic visualization, Proficiency with R and/or Python libraries commonly used in data science, Python programming experience, hospital data flows such as CPOE, EMR, RIS, LIS and PACS. Experience in related data format standards such as HL7, DICOM, FHIR and IHE, healthcare terms and classifications (SNOMED CT, ICD10); high affinity with applying new IT platforms/dash boarding software tools for reporting and experience.You have a Master’s Degree in Computer Sciences, Biomedical Engineering, Bioinformatics, or a related field OR 10 years of work experience, preferred.You must be able to successfully perform the following minimum Physical, Cognitive and Environmental job requirements with or without accommodation for this position.You also need to have the ability to work with cross-functional teams, be self-motivated, committing to results and be flexible and quick-learning. You also should have excellent verbal and written communication skills, ability to manage complex projects along with demonstrated operational analytics and financial analysis capabilities.\n\nAbout Philips\n\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help improve the lives of others.\n\nLearn more about our business.Discover our rich and exciting history.Learn more about our purpose.Read more about our employee benefits.\n\nIf you’re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion here.\n\nAdditional Information\n\nUS work authorization is a precondition of employment. The company will not consider candidates who require sponsorship for a work-authorized visa, now or in the future.\n\nCompany relocation benefits will not be provided for this position. For this position, you must reside in or within commuting distance to the locations listed.\n\nThis requisition is expected to stay active for 45 days but may close earlier if a successful candidate is selected or business necessity dictates. Interested candidates are encouraged to apply as soon as possible to ensure consideration.\n\nPhilips is an Equal Employment and Opportunity Employer/Disabled/Veteran and maintains a drug-free workplace."}
{"text": "experience with agile engineering and problem-solving creativity. United by our core values and our purpose of helping people thrive in the brave pursuit of next, our 20,000+ people in 53 offices around the world combine experience across truly value\n\nJob Description\n\nPublicis Sapient is looking for a Senior Associate, Data Engineering (Azure) to be part of our team of top-notch technologists. You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future.\n\nYour Impact:Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our client's businessTranslate client's requirements to system design and develop a solution that delivers business valueLead, designed, develop, and deliver large-scale data systems, data processing, and data transformation projectsAutomate data platform operations and manage the post-production system and processesConduct technical feasibility assessments and provide project estimates for the design and development of the solutionMentor, help and grow junior team membersSet Yourself Apart With:Developer certifications in Azurecloud servicesUnderstanding of development and project methodologiesWillingness to travel\n\nQualifications\n\nYour Technical Skills & Experience:Demonstrable experience in data platforms involving implementation of end to end data pipelinesHands-on experience with at least one of the leading public cloud data platforms (Azure, AWS or Google Cloud)Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Glue ETL, Lambda, Spark, Spark Streaming, etc.Ability to handle module or track level responsibilities and contributing to tasks “hands-on”Experience in data modeling, warehouse design and fact/dimension implementationsExperience working with code repositories and continuous integrationData modeling, querying, and optimization for relational, NoSQL, timeseries, and graph databases and data warehouses and data lakesData processing programming using SQL, DBT, Python, and similar toolsLogical programming in Python, Spark, PySpark, Java, Javascript, and/or ScalaData ingest, validation, and enrichment pipeline design and implementationCloud-native data platform design with a focus on streaming and event-driven architecturesTest programming using automated testing frameworks, data validation and quality frameworks, and data lineage frameworksMetadata definition and management via data catalogs, service catalogs, and stewardship tools such as OpenMetadata, DataHub, Alation, AWS Glue Catalog, Google Data Catalog, and similarCode review and mentorshipBachelor’s degree in Computer Science, Engineering or related field.\n\nAdditional Information\n\nPay Range: $103,000-$154,000\n\nThe range shown represents a grouping of relevant ranges currently in use at Publicis Sapient. Actual range for this position may differ, depending on location and the specific skillset required for the work itself.\n\nBenefits of Working Here:Flexible vacation policy; time is not limited, allocated, or accrued16paid holidays throughout the yearGenerous parental leave and new parent transition programTuition reimbursementCorporate gift matching programAs part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to"}
{"text": "experience in building and maintaining data-driven applications to support the effective use of institutional data, working with technical consultants, and evaluating data tools.\n\nThe individual in this position will be highly detail-oriented, a skilled problem-solver and technical analyst, experienced at data extraction across multiple platforms, and possess the ability to communicate data analysis to multiple constituents with various levels of data acumen.\n\nJob Description\n\nPrimary Duties and Responsibilities\n\nManages a set of institutional surveys in collaboration with internal and national partners. Collects, cleans, maintains, and manages data to allow for longitudinal comparisons and comparisons with peer institutions. Designs and creates reports to deliver results to WashU colleagues.Builds automated, actionable and accurate reporting dashboards. Collects and interprets requirements and translates them into data visualizations. Recommends and implements automation of reporting; identifies and implements opportunities to streamline and improve processes.Conducts ad hoc research, analysis and presentation of institutional data to answer questions raised by WU senior management, including comparisons with peer institutions.Performs other duties as assigned.\n\nPreferred Qualifications\n\nDoctoral degree in applied statistics, biostatistics, social sciences, educational research, or related field. Experience in providing quantitative analysis to support senior management in a university environment.Strong quantitative, statistical and analytical skills; demonstrated ability to synthesize and transform complex data sets into user-friendly dissemination products (e.g., report, dashboards, briefs).Command of SPSS, SAS, or equivalent; experience with integrated databases and query languages.Understanding of appropriate statistical tests for various analysis situations (including, but not limited to correlation, ANOVA, t-test, chi-square).Demonstrated experience in designing and implementing data management protocols and processes and strong attention to detail and accuracy in editing, record-keeping, etc.Demonstrated knowledge of best practices for visualization and communicating results to variety of audiences.Advance skills in creating data visualizations using business intelligence software, ideally with experience in Tableau and/or Power BI.Experience working with data sets in various conditions/formats and which require building relationships across disparate data sources to create a comprehensive data model.Skilled in cleaning, standardizing, filtering, and transforming data to create datasets ready to be analyzed and displayed graphically.Understands what makes for a compelling visualization, can \"tell a story\" using data, and can draw insights from large datasets.Successful candidates will have to experience with the Microsoft Office suite and the ability to collaborate effectively with others on the platform.A broad understanding of the issues of higher education, research methods, and statistical techniques.Ability to work independently and collaboratively, handle multiple projects, and meet deadlines.Strong interpersonal, written, verbal, and presentation skills.\n\nRequired Qualifications\n\nBachelor’s degree and four years’ related work experience or Master’s degree in math, statistics, economics computer science, psychology, social work or related field plus two years’ related work experience.\n\nGrade\n\nG13\n\nSalary Range\n\n$64,700.00 - $110,500.00 / Annually\n\nThe salary range reflects base salaries paid for positions in a given job grade across the University. Individual rates within the range will be determined by factors including one's qualifications and performance, equity with others in the department, market rates for positions within the same grade and department budget.\n\nQuestions\n\nFor frequently asked questions about the application process, please refer to our External Applicant FAQ.\n\nAccommodation\n\nIf you are unable to use our online application system and would like an accommodation, please email CandidateQuestions@wustl.edu or call the dedicated accommodation inquiry number at 314-935-1149 and leave a voicemail with the nature of your request.\n\nPre-Employment Screening\n\nAll external candidates receiving an offer for employment will be required to submit to pre-employment screening for this position. The screenings will include criminal background check and, as applicable for the position, other background checks, drug screen, an employment and education or licensure/certification verification, physical examination, certain vaccinations and/or governmental registry checks. All offers are contingent upon successful completion of required screening.\n\nBenefits Statement\n\nPersonal\n\nUp to 22 days of vacation, 10 recognized holidays, and sick time.Competitive health insurance packages with priority appointments and lower copays/coinsurance.Want to Live Near Your Work and/or improve your commute? Take advantage of our free Metro transit U-Pass for eligible employees. We also offer a forgivable home loan of up to $12,500 for closing costs and a down payment for homes in eligible neighborhoods.WashU provides eligible employees with a defined contribution (403(b)) Retirement Savings Plan, which combines employee contributions and university contributions starting at 7%.\n\nWellness\n\nWellness challenges, annual health screenings, mental health resources, mindfulness programs and courses, employee assistance program (EAP), financial resources, access to dietitians, and more!\n\nFamily\n\nWe offer 4 weeks of caregiver leave to bond with your new child. Family care resources are also available for your continued childcare needs. Need adult care? We’ve got you covered.WashU covers the cost of tuition for you and your family, including dependent undergraduate-level college tuition up to 100% at WashU and 40% elsewhere after seven years with us.\n\nFor policies, detailed benefits, and eligibility, please visit: https://hr.wustl.edu/benefits/\n\n\n\nWashington University in St. Louis is committed to the principles and practices of \n\nDiversity Statement\n\nWashington University is dedicated to building a diverse community of individuals who are committed to contributing to an inclusive environment – fostering respect for all and welcoming individuals from diverse backgrounds, experiences and perspectives. Individuals with a commitment to these values are encouraged to apply."}
{"text": "experienceAnnual Bonus + Annual Merit Increase EligibilityComprehensive health benefits package3+ weeks of paid time off accrued during your first year401(K) plan with company match up to 7%Professional development opportunities and tuition reimbursementPaid time off to volunteer & company-sponsored volunteer events throughout the yearOther benefits include a free AAA Premier Membership, Health & Wellness Program, Health Concierge Service, Life Insurance and Short Term/Long Term Disability\nWhat You'll Do:Develop opportunities to optimize marketing campaigns by partnering with key business stakeholders to design effective targeting and segmentation strategies that maximize ROI in the following channels: Direct Mail, Email, Telemarketing, Text, Social, and Web.Design and build processes to execute and automate campaign strategies.Create data mining architectures/models/protocols, statistical reporting, and data analysis methodologies to identify trends in large data sets.Capture and mine data to identify customers most likely to respond to various direct marketing messages.Perform statistical analysis/modelling to generate lists of customers for targeted direct marketing campaigns.Automate existing marketing campaigns by converting existing SQL queries into GCP BigQuery and SQL Server stored procedures.Identify data gaps and new data sources to improve marketing effectiveness and to maximize the intended marketing audience.Work with SQL/Adobe Campaign Developer/Administrator/IT Team to integrate and test new data sources.Use data management tools to standardize customer contact information to improve delivery rates, campaign performance and analysis opportunities.Use data transfer tools to encrypt, compress, and secure data files that are shared and received with 3rd party providers. Update and team process and procedures.Enhance existing campaign management business processes to support efficient campaign management and marketing across multiple business lines.Other duties as assigned. Minimum Qualifications:Bachelor’s Degree in Computational and Data Science, Data Analytics, Economics, or Math; or BS degree in other field in combination with additional experience; equivalent education, experience and certifications will be considered.Comprehensive knowledge of 2016 Microsoft Office Suite.2+ years of experience working with relational data.2+ years of direct marketing experience.2+ years of experience using Adobe Campaign; SAS eGuide 4.1, 4.3 and 5.1 experience a plus.3+ years of experience programming in SQL. Google Cloud Platform (GCP) BigQuery experience combined with Microsoft SQL Server or commensurate experience preferred.2+ years of experience using Microsoft Power BI.Ability to work independently in fast paced environment with competing priorities, excellent oral and written communication skills and the ability to interpret end-user needs. To the qualified candidate, we can offer: Medical, Dental, Vision and Prescription coverageOther benefits include a free AAA Premier Membership, Health & Wellness Program, Health Concierge Service, and Short Term/Long Term Disability8 Paid HolidaysHybrid SchedulePaid time off to volunteerProfessional development opportunities and tuition reimbursementTuition reimbursement401(K) plan with company match up to 7%\n AAA Club Alliance (ACA) is \nOur investment in Diversity, Equity, and Inclusion:\nAt ACA, we are committed to cultivating a welcoming and inclusive workplace of team members with diverse backgrounds and experiences to enable us to meet our goals and support our values while serving our Members and customers. We strive to attract and retain candidates with a passion for their work and we encourage all qualified individuals, regardless of race, color, gender, identity, veteran status, sexual orientation, physical ability or national origin, to apply."}
{"text": "Experience:\n\nAssociate Degree in a technical field such as computer science, computer engineering or related field required.2 -3 years of experience required.Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.\n\nSkills:\n\nVBA ConceptsSQL BasicData Visualization Concepts\n\nCompensation:\n\nThe pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on a number of factors, including but not limited to, a candidate’s qualifications, skills, competencies, experience, location and end client requirements).\n\nBenefits and Ancillaries:\n\nMedical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee."}
{"text": "Experience\n\nData Analyst II\n\nEducation:\n\nBachelor’s Degree from an accredited college or university in Management and Information Systems, Computer Science, Statistics, Mathematics, or a related field.\n\nAND\n\nExperience:\n\nThree (3) years of experience related to the above described duties.\n\nData Analyst III\n\nEducation:\n\nBachelor’s Degree from an accredited college or university in Management and Information Systems, Computer Science, Statistics, Mathematics, or a related field.\n\nAND\n\nExperience:\n\nFour (4) years of experience related to the above described duties\n\nPreferred qualification: Master's Degree from an accredited four-year college or university in a related field.\n\nSubstitution Statement: Related experience may be substituted for education, on a basis set forth and approved by the Department of Human Resources.\n\nInterview Requirements: Any candidate who is called for an interview must notify the Department of Equal Opportunity/Regulatory Compliance in writing of any reasonable accommodation needed prior to the date of the interview.\n\nSalary/Wage Information\n\nTo learn more about our pay structure and view our salary ranges, click here to visit the Compensation page of our Human Resources website. This link is provided for general pay information. Hourly rate or salary may vary depending on qualifications, experience, and departmental budget. Note: Unclassified positions (faculty, executives, researchers and coaches) do not have established salary ranges.\n\n\n\nThe University of Mississippi provides equal opportunity in any employment practice, education program, or education activity to all qualified persons. The University complies with all applicable laws regarding equal opportunity and affirmative action and does not unlawfully discriminate against any employee or applicant for employment based upon race, color, gender, sex, pregnancy, sexual orientation, gender identity or expression, religion, citizenship, national origin, age, disability, veteran status, or genetic information.\n\nBackground Check Statement\n\nThe University of Mississippi is committed to providing a safe campus community. UM conducts background investigations for applicants being considered for employment. Background investigations include a criminal history record check, and when appropriate, a financial (credit) report or driving history check."}
{"text": "Qualifications:\nFluency in English (native or bilingual)Proficient in at least one programming language (Python, JavaScript, HTML, C++, C# and SQL)Excellent writing and grammar skillsA bachelor's degree (completed or in progress)"}
{"text": "Qualifications:A minimum of 3 years’ experience in a similar role.Strong knowledge of structured and non-structured database systems and data mining.Experience building data solutions for Software as a Service (SaaS) offerings.Excellent organizational and analytical abilities.Outstanding problem solver.Good written and verbal communication skills.Willingness to explore and learn new concepts, tools, and processes.\n\nCompetencies - Skills/Knowledge/Abilities:Conceptual, Logical, and Physical Data ModelingVariety of database expertise around MS SQL Server, PostgreSQL, DynamoDB, Mongo or Cassandradata dictionary, repository solutionsSecure Development PracticesCollaboration tools including JIRA, Confluence and/or Microsoft TeamsExperience with Batch/Real-time Data processingData Analytics and Business Intelligence experienceIn-depth knowledge of the full software development lifecycle; with exposure to agile or iterative approaches to delivery preferred.Strong verbal and written communication skillAbility to quickly learn modern technologies and business functions.Strong analytical skills to determine effective approaches to business solutions.Demonstrated effective performance in a team-oriented environment.\n\nAgain, we are NOT partnering with 3rd parties on this role and candidates need to be a US Citizen or Permanent Resident for this contract-to-hire opportunity, thank you😊"}
{"text": "experience. Our solutions help our customers solve difficult problems in the areas of Anti-Money Laundering/Counter Terrorist Financing, Identity Authentication & Verification, Fraud and Credit Risk mitigation and Customer Data Management. You can learn more about LexisNexis Risk at the link below, risk.lexisnexis.com\n\nAbout our Team:\n\nThe Marketing Analytics team provides marketing data analytics support to all the businesses with the Business Services vertical of LexisNexis Risk Solutions.\n\nAbout the Role:\n\nAs a Senior Marketing Data Engineer, you will play a critical role in driving data-driven marketing initiatives. You will be responsible for designing, implementing, and maintaining scalable data pipelines, ensuring high-quality data ingestion, transformation, and integration. Your expertise in data engineering, marketing analytics, and data management will contribute to the organization's marketing strategies and decision-making processes.\n\nYou will be responsible for: \n\nPerforming data pipeline development; designing, developing and maintaining robust, scalable and efficient data pipelines for marketing data ingestion and processing.Developing and maintaining marketing-specific databases, ensuring seamless integration with other systems.Marketing Database Development: designing and optimizing marketing databases to store and retrieve large volumes of data efficiently.Implementing database schemas, tables, and indexes tailored to the specific needs of marketing campaigns and analytics.Implementing data governance policies and best practices to ensure data security, privacy, and compliance with relevant regulations (e.g., GDPR, CCPA).Collaborating with legal and compliance teams to handle sensitive data responsibly.Performing all other duties as assigned.\n\nQualifications: \n\nHave Bachelor's or Master's in Computer Science, Information Technology, or a related field.Have 3+ experience in data engineering, with a focus on marketing data.Have expertise in data modeling, ETL processes, and data warehousing concepts.Have familiarity with cloud platforms like AWS, Azure, or Databricks. Have knowledge of database systems (SQL, NoSQL) and data integration tools.Have proficiency in programming languages such as Python, Java, or Scala.Have experience with big data technologies like Hadoop, Spark, or similar.Have excellent problem-solving and communication skills.\n\nLearn more about the LexisNexis Risk team and how we work here"}
{"text": "requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other data sources.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Improve, optimize and identify opportunities for efficient software development processes.Help achieve milestones as per sprint plan and prioritize to manage ad-hoc requests in parallel with ongoing sprints. Required Qualifications:5+ years of hands-on experience in building Data pipeline (ETL/ELT) in a cloud platformGCP knowledge strongly preferred - other cloud experience such as AWS. AZURE is ok5+ years of hands-on experience of building and operationalizing data processing systemsStrong Python scripting experience is very important requirement2+ years’ experience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, Kafka2+ years’ experience working with data platforms (Data warehouse, Data Lake, ODS)2+ years’ experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)Must have working experience with the clinical dataPreferred Qualifications:GCP (google cloud platform) experience3+ years of experience working on healthcare / clinical dataData analysis / Data mapping skillsPythonCloud Data flow/Data proc/FunctionWhistle map SDKGoogle Health care API/ FHIR store"}
{"text": "experienced Senior Data Engineer / Developer (see skills below) to join our team. The candidate should have demonstrated skills with WhereScape RED automation tools and the ability to design and implement fully operational solutions on Snowflake Data Warehouse. Additionally, the ideal candidate will have a strong background in delivering enterprise data warehouses, data lakes, with experience in designing and engineering end-to-end data analytics solutions. The duration of the engagement is about 12 months. This is a remote position; however, they may have to travel on site or to other courts or conferences on rare occasions. \nSenior Data Engineer/Developer\nKnowledge, Skills, and Abilities:\n· Proficiency in WhereScape RED for data warehouse automation, including designing, building, and managing data warehouses.\n· Expertise in Snowflake's cloud data platform, including data loading, transformation, and querying using Snowflake SQL.\n· Experience with SQL-based development, optimization, and tuning for large-scale data processing.\n· Strong understanding of dimensional modeling concepts and experience in designing and implementing data models for analytics and reporting purposes.\n· Ability to optimize data pipelines and queries for performance and scalability.\n· Familiarity with Snowflake's features such as virtual warehouses, data sharing, and data governance capabilities.\n· Knowledge of WhereScape scripting language (WSL) for customizing and extending automation processes.\n· Experience with data integration tools and techniques to ingest data from various sources into Snowflake.\n· Understanding of data governance principles and experience implementing data governance frameworks within Snowflake.\n· Ability to implement data quality checks and ensure data integrity within the data warehouse environment.\n· Strong SQL skills for data manipulation, optimization, and performance tuning.\n· Experience with data visualization tools such as Power BI."}
{"text": "experienced Data Engineer seeking new opportunities? Look no further! Our direct client, a prominent financial services company, is actively seeking talented individuals like you to join their team. Check out the job details below and apply today to take the next step in your career!\nMust have Skills:4+ years of work experience in Data Platform Administration/Engineering, or relatedHands on experience with Amazon Web Services (AWS) based solutions such as Lambda, Dynamo dB, Snowflake and S3.Knowledge of Data Warehouse technology (Unix/Teradata/Ab Initio/Python/Spark/Snowflake/No SQL).Experience in migrating ETL processes (not just data) from relational warehouse Databases to AWS based solutions. Experience in building & utilizing tools and frameworks within the Big Data ecosystem including Kafka, Spark, and NoSQL.Deep knowledge and very strong in SQL and Relational Databases.Knowledge of Data Warehouse technology (Unix/Teradata/Ab Initio).Willingness to continuously learn & share learnings with others.Ability to work in a fast-paced, rapidly changing environment.Very strong verbal & written communication skills.Experience within the Financial industry.Experience with programming languages like Java or Python services and build highly available environment using proper design patterns.Proven experience in software development methodologies.Strong analytical skills and ability to multi-task.Experience as part of an Agile engineering or development team.Strong experience working with a relational database and NoSQL database.Strong experience with CI/CD pipelines with Jenkins or similar; Git/GitHub; ArtifactoryExperience with Test Driven Development (TDD).Experience writing unit and service level tests to ensure adequate code coverage.Proven skills in high availability and scalability design, as well as performance monitoring.Experience developing and implementing API service architecture.Experience in working in a cloud environment such as AWS, GCP or Azure.Experience provisioning infrastructure with Terraform and Cloud Formation.Understanding of messaging systems like MQ, Rabbit MQ, Kafka, or Kinesis.Ability to adapt communication for effectiveness with business partners and other technical teams. Preferred Skills:Strong experience working with a relational database and NoSQL database.Experience with Test Driven Development (TDD).Experience provisioning infrastructure with Terraform and Cloud Formation.ETL/ELT Tools (AbInitio, DataStage, Informatica)Cloud Tools and Databases (AWS, Snowflake)Other programming languages (Unix scripting, Python, etc.)Leverage CI/CD framework for data integration, Open SourceBasic understanding of key infrastructure concepts (data centers as well as cloud hosting platform) to support business data needs.Experience optimizing SQL both relational and NoSQL"}
{"text": "experience, education, geographic location, and other factors.\nExperience5+ years of experience in large and complex IT projects, preferably in the Human Capital space5+ years of experience with supporting Data Integration, Interoperability, and Data Migrations5+ years of experience using common data models and AI tools that support built-in data governanceExperience applying data quality standardsProven ability to learn and adopt new technologiesExperience designing and implementing the data architecture and other data-related activitiesExperience leading data strategy to support creation and improvement of data architecture, data usage, and data governance\n\nIf you are interested in this role, then please click APPLY NOW. For other opportunities available at Akkodis, or any questions, please contact Preeti Ghanghas at 972-433-0648 or preeti.ghanghas@akkodisgroup.com.\n\nEqual Opportunity Employer/Veterans/Disabled \nBenefit offerings include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits, and a 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pays upon meeting eligibility criteria. \nTo read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy/"}
{"text": "Qualifications\n Analytical Skills, Data Analytics, and StatisticsStrong communication skills, both written and verbalExperience in data modeling and visualizationProficiency in programming languages such as SQL, Python, or RDetail-oriented with a strong attention to accuracyAbility to work independently and collaborate with teamsExperience with data management and cleaningBachelor's degree in a related field such as Data Science, Mathematics, or Computer Science"}
{"text": "experience: 10+ yearsNeed a Sr Data Engineer who has 5+ years of experience in Azure native services with good exposure to ADF, Synapse, ADLS Gen2, Strong SQL skills, spark. Experience in analyzing/reverse engineering SSIS packages to re-platform solution on AzureGood communication skills and ability to guide offshore team members."}
{"text": "experienceETL pipelinesAzure data factorySSIS experienceReporting data in the data martTeam Size: 2 other senior data engineers\n OVERVIEW:\n\nThe Senior Data Engineer is primarily responsible for designing, building, and maintaining the infrastructure that supports data storage, flow, and retrieval. The Senior Data Engineer works with large data sets and develops data pipelines that move data from source systems to data warehouses and processing systems.\n\nAll activities must be in compliance with \n\nFunctions:\n\nWork with business users, developers, and other stakeholders to determine and document the requirements of the data warehouse and data martsDesign and develop data pipelines for transferring data between several data sources in an efficient mannerCombine raw information from various sources and explore ways to enhance data quality and reliabilitUtilize analytical tools & programs, and collaborate with database developers and database administrators, working on several projectManage data flow in a large-scale production environment with multiple workloads with different requirementEnhance the automation of data flow pipelines and processes to reduce toil and improve performancDevelop and manage the data archiving processes based on different retention policieBuild and operate data pipelines including ensuring uptime through monitoring, alerting, and proactive maintenance tasksAssist the rest of the data operation team on migration of data sources to PaaS and other modern data platformsDesign and implement solutions to complex systemsTake part in triaging and troubleshooting issues impacting our services\n\nMinimum Requirements\n\n5+ years of experience as a SQL Server DBA or Database Developer5+ years of experience developing ETLs using SQL Server based tools like SSIS and ADF5+ years of exposure with modern cloud tools and servicesFamiliarity with developing large-scale ETL pipelines performing incremental loadExperience in migrating IaaS data sources and ETLs to PaaS servicesIn-depth knowledge and experience in database and ETL performance tuning, monitoring, and alertingParticipate in 24x7x365 on-call rotation and provide support during off-hours for production incidentsExperience or willing to learn and use Azure DevOps pipelines for continuous deployments (CI/CD)"}
{"text": "experience.Solving problems efficiently, creatively, and completely despite constraints in time or resources.Understanding how critical it is we maintain a high bar of data security and privacy.\n\n\nWe’re excited about you because you:\n\nHave the ability to adapt and apply evolving data technologies to business needs (which means the list of bullets below will change over time!).Have developed software using programming languages like Python, Scala, Java, Go, Ruby, etc.Have sufficient familiarity to understand SQL queries in the context of data pipelines (i.e. dbt).Have experience with distributed data tools (i.e. Spark, Flink, Kafka) on large datasets.Have worked with cloud-data warehouses (i.e. Snowflake, BigQuery, Redshift) or other warehousing solutions.Have an understanding of underlying infrastructure needed to serve production services (i.e. Kubernetes, AWS, GCP, Azure).\n\n\nAbout Strava\n\nStrava is Swedish for “strive,” which epitomizes who we are and what we do. We’re a passionate and committed team, unified by our mission to connect athletes to what motivates them and help them find their personal best. And with billions of activity uploads from all over the world, we have a humbling and audacious vision: to be the record of the world’s athletic activities and the technology that makes every effort count.\n\nStrava builds software that makes the best part of our athletes’ days even better. And just as we’re deeply committed to unlocking their potential, we’re dedicated to providing a world-class, inclusive workplace where our employees can grow and thrive, too. We’re backed by Sequoia Capital, Madrone Partners and Jackson Square Ventures, and we’re expanding in order to exceed the needs of our growing community of global athletes. Our culture reflects our community – we are continuously striving to hire and engage diverse teammates from all backgrounds, experiences and perspectives because we know we are a stronger team together.\n\nDespite challenges in the world around us, we are continuing to grow camaraderie and positivity within our culture and we are unified in our commitment to becoming an antiracist company. We are differentiated by our truly people-first approach, our compassionate leadership, and our belief that we can bring joy and inspiration to athletes’ lives — now more than ever. All to say, it’s a great time to join Strava!\n\nStrava is \n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nCalifornia Consumer Protection Act Applicant Notice"}
{"text": "experience and internal equity with other employees within the same job classification. This position is not eligible for overtime compensation.\n\nJob Type\n\nUnclassified\n\nDepartment\n\nTransportation\n\nAbout The Position\n\nDo you have a passion for management and continuous improvement? Bring that passion to the Maricopa County Department of Transportation! In this role, under limited supervision, you will manage and assist with analyzing crash data under the direction of the Assistant Transportation Systems Management & Operations Division Manager. Will you be the one that enables us to deliver exceptional results? Apply today!\n\nAbout Us\n\nWe value your time.  The time with your family.  The time you spend on the road.  And the time you spend making a difference. At the Maricopa County Department of Transportation (MCDOT), we provide connections that improve people's lives. By combining innovative technologies with the vast talent of our employees, we plan for future needs and operate a transportation system with the community in mind. Are you interested in contributing to the community in which you work, live, and play? Then join our team and help us build connections.\n\nProud to Offer \n\nPerform work with a greater purposeTuition reimbursementExceptional work-life balanceOpportunities for growth and development within Maricopa CountyLow-cost, high-value healthcare for you and your qualifying dependentsPaid vacation, sick time, and parental leaveExtensive wellness program, including healthcare premium discountsEmployee discounts for goods and servicesMaricopa County participates in the Arizona State Retirement System. This defined retirement benefit requires a 12.29% monthly contribution rate and includes a 100% employer match on Day 1Learn more at Work With Us | Maricopa County, AZ\n\nWe Require\n\nTwo years of professional analytical experience including reviewing/analyzing historical data, presenting data collected and providing recommendations, etc.Bachelor’s degree in business administration, public administration, or a closely related field A combination of post-secondary education and/or job-related experience may substitute for the minimum qualifications on a year-for-year basis\n\nWe Value\n\nExperience in organizational management and utilizing performance measures to drive resultsExperience with statistical analysis and other quantitative and qualitative research methods Skilled in assessing business practices and creating process improvementsExperience using databases, performing complex calculations, and utilizing spreadsheetsAble to work independently and provide process innovation\n\nJob Contributions \n\nDevelop and analyze performance measures to meet Transportation Systems Management & Operations Division (TSMO) safety goals for MCDOT. This will include identifying crash trends and setting targets for reducing crashesDevelop crash history reports from the ADOT crash database, and look for crash trends in conjunction with reviewing police reports. Review crash heat maps and conduct linear regression analysis. Provide benefit-cost analysis based on crash analysis and application of relevant Federal Highway Administration (FHWA) countermeasures for Highway Safety Improvement Plan (HSIP) funding applicationsWork with the TSMO Division transportation systems engineering team to prioritize and recommend Transportation Improvement Plan (TIP) and Maintenance Improvement Plan (MIP) Projects for MCDOT.Compile technical information and prepare the annual MCDOT Safety ReportAssist TSMO Division leadership by providing data-driven analysis for roadside safety audits. Manage standard operating procedures for Traffic Systems Engineering Team.Manage records retention for TSMO Division safety reports to ensure compliance with records retention policies\n\nWorking Conditions \n\nThis position requires the ability to interact with a diverse population in written and oral formats in a typical office setting where it is possible to be seated up to 100% of the time working at a desk, on a computer, and using the telephoneMust be able to move objects, i.e., lifting floor to waist up to 30 pounds, pushing or pulling up to 50 pounds a distance of 100 feet or less, and bending to search for filesYou must have the ability to navigate multiple computer screens at one time, filing with the ability to hold files back while placing a file, and carrying files from one person to another throughout the department\n\nSelection Procedure\n\nOnly the most qualified candidates will be consideredConsideration will only be given to candidates who submit online applicationsCandidates will be contacted primarily through email and their Workday online application profileMust pass a pre-employment background and/or fingerprint investigation as required by statute or policy, including drug and alcohol testing requirements for positions designated as safety-sensitive\n\nMaricopa County is \n\nApply Now!"}
{"text": "Experience of Delta Lake, DWH, Data Integration, Cloud, Design and Data Modelling.• Proficient in developing programs in Python and SQL• Experience with Data warehouse Dimensional data modeling.• Working with event based/streaming technologies to ingest and process data.• Working with structured, semi structured and unstructured data.• Optimize Databricks jobs for performance and scalability to handle big data workloads. • Monitor and troubleshoot Databricks jobs, identify and resolve issues or bottlenecks. • Implement best practices for data management, security, and governance within the Databricks environment. Experience designing and developing Enterprise Data Warehouse solutions.• Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process.• Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.\nQualifications:\n• 5+ years Python coding experience.• 5+ years - SQL Server based development of large datasets• 5+ years with Experience with developing and deploying ETL pipelines using Databricks Pyspark.• Experience in any cloud data warehouse like Synapse, Big Query, Redshift, Snowflake.• Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.• Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills.• Experience with Cloud based data architectures, messaging, and analytics.• Cloud certification(s).• Any experience with Airflow is a Plus."}
{"text": "Qualifications\n\nBachelor's degree or higher with quantitative focus in Econometrics, Statistics, Operations Research, Computer Science or related field (e.g. Mathematics).Instead of a degree, a minimum of three years of relevant experience in statistical/quantitative modeling and/or Machine Learning tools (R, Python, etc.) and in using various database tools (e.g. Hadoop, SQL) processing large volumes of structured and unstructured data.\n\nPreferred Skills\n\nCompetency in Python (or similar) development, debugging and toolchainApplied experience in Statistical Modeling and Machine LearningDemonstrated experience with communicating results and managing a project from start to finishAbility to use various database tools (e.g. SQL) to process large volumes of structured and unstructured dataFamiliarity with Linux, AWS and other deployment platforms\n\nCompensation\n\n$70,100 - $113,200/year depending on position level and experienceGainshare bonus up to 30% of your eligible earnings based on company performance\n\nBenefits\n\n401(k) with dollar-for-dollar company match up to 6%Medical, dental & vision, including free preventative careWellness & mental health programsHealth care flexible spending accounts, health savings accounts, & life insurancePaid time off, including volunteer time offPaid & unpaid sick leave where applicable, as well as short & long-term disabilityParental & family leave; military leave & payDiverse, inclusive & welcoming culture with Employee Resource GroupsCareer development & tuition assistanceOnsite gym & healthcare at large locations\n\nEnergage recognizes Progressive as a 2023 Top Workplace for: Innovation, Purposes & Values, Work-Life Flexibility, Compensation & Benefits, and Leadership.\n\nEqual Opportunity Employer\n\nSponsorship for work authorization for this position is available for candidates who already possess an H-1B- visa.\n\nFor ideas about how you might be able to protect yourself from job scams, visit our scam-awareness page at https://www.progressive.com/careers/how-we-hire/faq/job-scams/\n\nJob\n\nBusiness Analysis\n\nPrimary Location\n\nUnited States\n\nSchedule\n\nFull-time\n\nEmployee Status\n\nRegular\n\nWork From Home\n\nYes"}
{"text": "qualifications:\n\nMaster's degree in Statistics, Data Science, Mathematics, Physics, Economics, Operations Research, Engineering, or a related quantitative field.5 years of work experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or 3 years of work experience with a PhD degree.\n\nPreferred qualifications:\n\n8 years of work experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or 6 years of work experience with a PhD degree\n\nAbout The Job\n\nGoogle is and always will be an engineering company. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on millions, if not billions, of users. At Google, data scientists not only revolutionize search, they routinely work on massive scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, Social to Local, Google engineers are changing the world one technological achievement after another. As a Data Scientist, you will evaluate and improve Google's products. You will collaborate with a multi-disciplinary team of engineers and analysts on a wide range of problems. This position will bring scientific rigor and statistical methods to the challenges of product creation, development and improvement with an appreciation for the behaviors of the end user.\n\nThe US base salary range for this full-time position is $150,000-$223,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\n\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .\n\nResponsibilities\n\nCollaborate with stakeholders in cross-projects and team settings to identify and clarify business or product questions to answer. Provide feedback to translate and refine business questions into tractable analysis, evaluation metrics, or mathematical models.Use custom data infrastructure or existing data models as appropriate, using specialized knowledge. Design and evaluate models to mathematically express and solve defined problems with limited precedent.Gather information, business goals, priorities, and organizational context around the questions to answer, as well as the existing and upcoming data infrastructure.Own the process of gathering, extracting, and compiling data across sources via relevant tools (e.g., SQL, R, Python). Independently format, re-structure, and/or validate data to ensure quality, and review the dataset to ensure it is ready for analysis.\n\n\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to"}
{"text": "requirements. The ideal candidate should have strong programming skills, experience with data integration tools, and a deep understanding of data engineering principles.\n Qualifications\n Bachelor's degree or higher in Computer Science, Data Science, or a related fieldExtensive experience in data engineering, including data ingestion, data transformation, and data modelingProficiency in programming languages such as Python, Java, or ScalaExperience with data integration tools and technologies, such as Apache Kafka, Apache NiFi, or InformaticaStrong SQL skills and familiarity with relational and non-relational databasesKnowledge of big data technologies, such as Hadoop, Spark, or HiveExperience with cloud platforms, such as AWS or AzureUnderstanding of data governance and data quality best practicesAbility to work collaboratively in a cross-functional team environmentExcellent problem-solving and analytical skills\n Note: This role is open to W2 candidates only."}
{"text": "Experience with Marketing APIs, Big Query or other Cloud Data Warehouse tools (AWS, Snowflake, Databricks, etc.), ETL Tools (Rivery, Looker, Supermetrics) and/or Python are all pluses.You have a bachelor’s degree in a relevant field and have spent a minimum of 1 year working with marketing data and/or paid advertising campaigns, preferably in a fast-paced agency environment or team structure.\nWhat You’ll Do:Reporting: With your expertise in Advanced Excel, SQL, Google Data Studio/Visualization Tools, you’ll be maintaining customized dashboards in Google Sheets and Excel on a daily/weekly cadence, generating pivot tables and other visualizations that summarize thousands of lines of data. You should have experience in translating performance media metrics into coherent, digestible reports for client ad campaigns and be comfortable communicating via Google Slides or PowerPoint decks.Strategy: You’ll be responsible for learning our clients’ businesses and aligning their media strategy with business goals as well as understanding advertising trends within a single project and across multiple projects to help guide media buying strategy. With a background in paid media and/or digital advertising, you understand concepts like LTV, ROAS and Incremental Revenue and are able to be engaged in discussions with clients to gain trust and shareholder buy-in on media strategy and results.\nAt Gupta Media, we work hard every day to be better at all that we do — and that includes fostering a diverse and inclusive community that encourages and respects a broad range of backgrounds, experiences and ideas. Join us and help us find the next great idea.\nGupta Media is an office-first (with flexibility) organization and we are excited to have our team working together again"}
{"text": "experience: from patients finding clinics and making appointments, to checking in, to clinical documentation, and to the final bill paid by the patient. Our team is committed to changing healthcare for the better by innovating and revolutionizing on-demand healthcare for millions of patients across the country.\n\nExperity offers the following:\n\nBenefits – Comprehensive coverage starts first day of employment and includes Medical, Dental/Orthodontia, and Vision.Ownership - All Team Members are eligible for synthetic ownership in Experity upon one year of employment with real financial rewards when the company is successful!Employee Assistance Program - This robust program includes counseling, legal resolution, financial education, pet adoption assistance, identity theft and fraud resolution, and so much more.Flexibility – Experity is committed to helping team members face the demands of juggling work, family and life-related issues by offering flexible work scheduling to manage your work-life balance.Paid Time Off (PTO) - Experity offers a generous PTO plan and increases with milestones to ensure our Team Members have time to recharge, relax, and spend time with loved ones.Career Development – Experity maintains a learning program foundation for the company that allows Team Members to explore their potential and achieve their career goals.Team Building – We bring our Team Members together when we can to strengthen the team, build relationships, and have fun! We even have a family company picnic and a holiday party.Total Compensation - Competitive pay, quarterly bonuses and a 401(k) retirement plan with an employer match to help you save for your future and ensure that you can retire with financial security.\n\nHybrid workforce:\n\nExperity offers Team Members the opportunity to work remotely or in an office. While this position allows remote work, we require Team Members to live within a commutable distance from one of our locations to ensure you are available to come into the office as needed.\n\nJob Summary: \n\nWe are seeking a highly skilled and data-driven Go-to-Market (GTM) Data Analyst to join our team. The ideal candidate will be adept at aggregating and analyzing data from diverse sources, extracting valuable insights to inform strategic decisions, and proficient in building dynamic dashboards in Salesforce and other BI tools. Your expertise in SQL and data analytics will support our go-to-market strategy, optimize our sales funnel, and contribute to our overall success.\n\nExperience: \n\nBachelor’s or Master’s degree in Data Science, Computer Science, Information Technology, or a related field.Proven experience as a Data Analyst or similar role, with a strong focus on go-to-market strategies.Expertise in SQL and experience with database management.Proficiency in Salesforce and other BI tools (e.g., Tableau, Power BI).Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Excellent communication and presentation skills, capable of conveying complex data insights in a clear and persuasive manner.Adept at working in fast-paced environments and managing multiple projects simultaneously.Familiarity with sales and marketing metrics, and how they impact business decisions.\n\nBudgeted salary range:\n\n$66,900 to $91,000\n\nTeam Member Competencies:\n\nUnderstands role on the team and works to achieve goals to the best of your ability.Working within a team means there will be varying opinions and ideas. Active listening and thoughtfully responding to what your team member says.Take responsibility for your mistakes and look for solutions. Understand how your actions impact team.Provides assistance, information, or other support to others to build or maintain relationships.Maintaining a positive attitude. Tackle challenges as they come, and don’t let setbacks get you down.Gives honest and constructive feedback to other team members.When recognizing a problem, take action to solve it.Demonstrates and supports the organization's core values.\n\nEvery team member exhibits our core values:\n\nTeam FirstLift Others UpShare OpenlySet and Crush GoalsDelight the Client\n\nOur urgent care solutions include:\n\nElectronic Medical Records (EMR): Software that healthcare providers use to input patient data, such as medical history, diagnoses, treatment plans, medications, and test results.Patient Engagement (PE): Software that shows patients the wait times at various clinics, allows patients to reserve a spot in line if there's a wait, and book the appointment.Practice Management (PM): Software that the clinic front desk staff uses to register the patient once they arrive for their appointment.Billing and Revenue Cycle Management (RCM): Software that manages coding, billing and payer contracts for clinics so they don’t have to.Teleradiology: Board certified radiologist providing accurate and timely reads of results from X-rays, CT scans, MRIs, and ultrasounds, for our urgent care clients.Consulting: Consulting services for urgent care clinics to assist with opening, expanding and enhancing client's businesses"}
{"text": "skills, emphasizing cross-functional collaboration and comprehensive project planning, oversight, and execution. The software developer will play a crucial role in shaping and influencing the operational outcomes of the business through the implementation of innovative solutions.\nRoles and requirements and translate them into technical specifications.Deploy applications to AWS and manage cloud infrastructure as needed.Qualifications and requirements:Bachelor's degree in computer science, engineering, or a related field.Strong proficiency in the Java programming language and related frameworks such as Spring, Spring Boot, and Hibernate.Experience with cloud platforms, particularly AWS services such as EC2, S3, Lambda, and RDS.Familiarity with microservices architecture and containerization technologies like Docker.Solid understanding of the software development lifecycle (SDLC) and Agile methodologies.Excellent problem-solving skills and attention to detail.Ability to work independently as well as in a collaborative team environment.Effective communication skills, both verbal and written.Experience with continuous integration and deployment (CI/CD) tools is a plus.Other Benefits:Health Care Plan (Medical, Dental, and Vision).Visa SponsorshipOn-the-job Technical supportE- verified2 weeks of paid time off (vacation or sickness).Short-term and long-term disabilities.Training and development.Wellness Resources.Paid Time Off (PTO)\nCandidates who are missing the required skills might be provided an option to enhance their skills so that they can also apply for the role and make a career in the IT industry.***Freshers can also apply***"}
{"text": "skillset in data analysis, statistical modeling, and data visualization.Collaborate with marketing teams, IT, and other departments to gather data requirements and share insights.Clearly communicate findings and recommendations to both technical and non-technical stakeholders.Occasional travel for training, meetings, or trade shows may be required\nAdditional duties and Experience:Bachelor’s degree required5+ years of relevant work experience requiredIntermediate to advanced level of experience with Google Analytics, Tag Manager requiredIntermediate to advanced level of experience with SQL requiredIntermediate level of experience using Front-End Data Visualization & Analytical Tools is a must\n Specialized Skills:Fundamental understanding of major functions in a global organizationStrong business acumen (in one or more verticals) is preferredData literacy is a mustStrong analytics and data analysis skills is preferredStrong visualization skills is preferredUX design expertise is a plusExperience in a Life Sciences – Med Device company is a plusData science/Advanced analytical skills is a plus"}
{"text": "Requirements:- Expertise in data wrangling and manipulation in Python and SQL- Solid understanding of machine learning and statistical analysis- Excellent business acumen and ability to understand and solve complex business problems- Strong coding skills, comfortable with Object-Oriented Programming- Strong communication skills, with the ability to present complex data in a clear and concise manner- Good project management skills, with a proven track record of delivering projects on time and within scope- Bachelor's degree in Computer Science, Statistics, or a related field\nPerks and benefits:All Zestys experience:The opportunity to join a mission-focused companyPeople – the best part of ZestRobust medical, dental and vision insurance plansAnnual bonus plan participation401(k) with generous matchEmployee Awards and Recognition11 company holidaysWinter break (office closed between Christmas and New Year's Day)Unlimited vacation timeEmployee Resource GroupsGenerous family leave policy (12 week maternity leave / 6 week paternity leave)Phone, internet, wellness, and professional development allowancesEmployee gatherings, including Town Hall meetings\nAdditionally, our Burbank, CA area, hybrid model Zestys enjoy:Beautiful, modern, dog-friendly office with lounge areas, video games, and gigantic jigsaw puzzlesDaily catered lunches from LA’s best restaurants and a fully stocked kitchenComplimentary manicures, pedicures, and mindfulness sessionsCompany happy hours, social events, outings, and much more!\nAbout Zest AI:Creating a diverse and inclusive culture where all are welcomed, valued, and empowered to achieve our full potential is important to who we are and where we’re headed in the future. We know that unique backgrounds, experiences, and perspectives help us think bigger, spark innovation, and succeed together. Zest is committed to diversity, equity, and inclusion and encourages professionals from underrepresented groups in technology and financial services to apply.\nOur core values are Communication, Collaboration, Bias for Action, Client-centricity, and Heart. Learn more at Zest.ai, follow us on LinkedIn (linkedin.com/company/zest-ai/) or Twitter @Zest_AI, or check out our Insights blog (https://www.zest.ai/cms/insights)."}
{"text": "skills in data science, statistics, and computer science, particularly as they apply to the analysis and management of complex biomedical and clinical data.\n\nJob Posting Addendum\n\nSeattle Children’s Innovative Technologies Lab at Seattle Children’s Research Institute, led by Dr. Frederick Shic, seeks data scientists interested in advancing next-generation technologies (mobile applications, emphasizing remote eye tracking) that assess and predict infant development. The primary emphases of this work are on (1) AI/ML-driven characterization of developmental/cognitive abilities in infants; and (2) robust prediction of developmental outcomes, especially as related to autism spectrum conditions. Additional areas of focus include: biomarker discovery and refinement; human-centered design; eye-tracking methods; and behavioral imaging through computer vision.\n\nThe data scientist will work directly with Dr. Frederick Shic, a computer scientist by training with primary expertise in eye tracking, early development, and autism research; and will be supported by a multi-disciplinary team experienced with child development, clinical research, statistics, and computer programming/software development. This position involves (1) developing mobile app-based systems and methods that link attention to child development; (2) adaptation and evaluation of eye-tracking methods, with a focus on comparing webcam-based versus laboratory-based eye tracking; and (3) advancement of additional research topics relevant to predicting child outcomes.\n\nExperience in any of the following areas is of interest: Mobile Device Development/Applications, Computer Vision, Human-Centered Design, Visualization, Bioinformatics, Machine Learning, Probabilistic Modeling, Statistics, Computer Science, and Optimization. Strong programming/analytical background is necessary for all candidates.\n\nOur research offices are located in downtown Seattle at Seattle Children’s Research Institute’s Building Cure. We are affiliated with the Center for Child Health, Behavior and Development at Seattle Children’s, the Seattle Children’s Autism Center, and Departments of Pediatrics, Computer Science & Engineering, and Psychology at the University of Washington.\n\nFor more information, please see our website Seattle Children’s Innovative Technologies Lab (seattlechildrens.org) or contact Dr. Shic directly at fshic@uw.edu.\n\nRequirements\n\nRequired Education/Experience:\n\nBachelor's Degree in Science, Computer Science, Statistics, or comparable area of research.At least two (2) years of experience participating in data science and analytics related projects or research.\n\nRequired Credentials\n\nN/A.\n\nPreferred\n\nProficiency in the field of Biomedical/Health Informatics, including extensive familiarity with biological data, molecular biological databases, and clinical databases.Experience with Microsoft applications and tools, including in-depth technical and/or applications knowledge in assigned platforms.Relevant programming and data management experience, including experience with R, SQL, and Python.\n\nMin to Max Hourly Salary: $36.75 - $55.12 /hr Min to Max Annual Salary: $76,440.00 - $114,649.60 /yr Salary Information:\n\nThis compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.\n\nDisclaimer For Out Of State Applicants\n\nThis compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors.\n\nBenefits Information\n\nSeattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.\n\nAbout Us\n\nHope. Care. Cure. These three simple words capture what we do at Seattle Children’s – to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE\n\nAs one of the nation's top five pediatric research centers, Seattle Children's Research Institute is dedicated to providing hope, care, and cures to help every child live the healthiest and most fulfilling life possible.\n\nOur investigators are involved in hundreds of projects that cover every phase of research, from studying how diseases work to improving investigational therapies. They have pioneered groundbreaking cystic fibrosis treatments and cutting-edge cancer therapies that help a child's immune system defeat cancer, and made other major contributions to pediatric medicine.\n\nResearchers work in close collaboration with one another, their colleagues at partner institutions including the University of Washington and Fred Hutch and our healthcare providers at Seattle Children's Hospital, one of U.S. News & World Report’s top children's hospitals. This collaboration is one of our key strengths, allowing our faculty to draw on a variety of disciplines and techniques as they pursue solutions to some of medicine's most complex problems.\n\nWe are committed to not only treating disease but to eliminating it. Help us achieve our vision of being a worldwide leader in pediatric research aimed to improve the health and well-being of children. If you are interested in a challenging career aimed at groundbreaking research, Seattle Children's Research Institute is the place for you.\n\nOur Commitment To Diversity\n\nOur community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. \n\nThe people who work at Seattle Children’s are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels. \n\nSeattle Children’s is proud to be an Equal Opportunity Workplace and Affirmative Action Employer."}
{"text": "skills and resources to retool processes.Communicates with other areas regarding outcomes and reporting.\nSkillsGood organizational, customer service, communications, and analytical skills.Ability to use complex mathematical calculations and understand mathematical and statistical concepts. Knowledge of relevant computer support systems.Software and Tools: Microsoft Office. Ability to acquire programming skills across various software platforms. Preferred Skills and Abilities: Negotiation or persuasion skills.Ability to acquire knowledge of ICD9/CPT4 coding. Preferred Software and Other Tools: SAS or DB2, or another relational database. Work Environment: Typical office environment. Some travel between buildings and out of town.Experience with “Lean Management” and/or “Six Sigma” conceptsRequired Technologies: Office products (MS Word/MS Excel/Teams) MS Access Day To Day:Education:Required Education: Bachelor's degree in Statistics, Computer Science, Mathematics, Business, Healthcare, or another related field. or 2-year degree in Computer Science, Business, or a related field and 2 years of reporting and data analysis work experience OR 4 years of reporting and data analysis experience. Required Work Experience: See Education. Preferred Work Experience: 2 years of related research and analysis experience.\nTeamThe team has 11 members, each of whom is a diverse individuals who strive to exceed customer expectations. Within the greater team is a smaller team of 3 individuals who compose the “plan” team. This person would be a part of this sub-team. They work as a close-knit group and embrace a team atmosphere. They enjoy having fun while getting the work done.\nThis person will work with the 3R move team, the network vendor CPDI, staff management team SRM, and workstation support teams to integrate workstreams to provide workstation support services for I/S new hires. Will also help create departmental documentation for multiple workstation support teams. Will be asked to do some analysis of data and work with reporting teams. Soft Skills; Good communication verbal/written, good organization, good analysis, customer service, cross-team facilitation.\nEqual Opportunity EmployerRevolution Technologies, LLC is"}
{"text": "Experience in the biotech industry is advantageous. Requirements: Ø Expertise in deep learning techniques, with a focus on Generative AI and Large Language Models (LLMs).Ø Proficiency in Python programming and familiarity with libraries such as TensorFlow, PyTorch, or Keras.Ø Knowledge of cloud computing platforms, particularly AWS.Ø Strong analytical and problem-solving skills.Ø Excellent communication and collaboration abilities.Ø Experience in the biotech industry is a plus. Educational Qualifications: PhD in Computer Science or Machine Learning."}
{"text": "experience who possesses a strong technical skill set, particularly in working with Salesforce and Marketo datasets. The ideal candidate will be proficient in data integration tools like Fivetran/Mulesoft, cloud-based data platforms such as Snowflake, and have experience with AWS services. This individual will play a key role in designing and optimizing databases, performing data analysis and validation, and supporting various data-related initiatives across the organization.\n\nTake our Values in Action Self-Assessment to see how our values align!\n\nYour Role:\n\nDesign, develop, and maintain data pipelines to ingest, transform, and load datasets (e.g. Salesforce, Marketo) into our data warehouse.Work closely with cross-functional teams to gather requirements, define data models, and implement solutions that meet business needs.Collaborate with Business Intelligence analyst and business stakeholders to understand data requirements and translate them into technical specifications.Perform basic data analysis to identify trends, patterns, and anomalies, and provide insights to support decision-making processes.Conduct data validation and ensure data accuracy, completeness, and integrity through quality assurance checks and validation processes.Optimize database performance and scalability by fine-tuning queries, indexing strategies, and schema design.Monitor and troubleshoot data pipeline issues, resolve data integration errors, and ensure data pipelines are running smoothly.Stay current with emerging technologies, best practices, and trends in data engineering and cloud computing.\n\n\nYour Experience:\n\nBachelor's degree in Computer Science, Information Technology, or a related field.2-5 years of experience as a data engineer or in a similar role, preferably in a fast-paced environment.Hands-on experience working with Salesforce and Marketo datasets.Proficiency in data integration tools like Fivetran/Mulesoft and cloud-based data platforms such as Snowflake.Familiarity with AWS services, such as S3, Redshift, Glue, and Athena.Strong database and schema design skills, with knowledge of relational and non-relational databases.Strong attention to detail and a commitment to data quality and integrity.Effective communication skills with the ability to collaborate with cross-functional teams and communicate technical concepts to non-technical stakeholders.This role is an in-office, full-time, and exempt position. We are a work from office culture with lots of flexibility.\n\n\nCompensation:\n\nPay: $110,000 - $140,000 per yearBenefits: Our benefits package includes, but is not limited to, health care benefits (medical/dental/vision), retirement benefits, paid time off (PTO), holiday flex-days, summer flex-days, company-paid holidays, parental leave, transportation benefits, unlimited volunteer time off, professional, and personal wellbeing stipend, and other fringe benefits. Everyone in the organization has a clear path to ownership.\n\n\nOur Company\n\nOur #OneTeam feels a sense of connection, pride, and ownership with our mission, vision, values, and objectives. We are committed to building a business where 100% of our team members genuinely love where they work and are supported by their colleagues. We do this through building authentic relationships with one another, supporting each other to continuously learn and grow, working hard while having lots of fun, and giving back to our community.\n\nWe are a team founded on equity and respect, and we’re on a mission to help our clients, teammates, and global community thrive. Brighton Jones is committed to building a team comprised of diverse backgrounds, ideas, and experiences. We actively foster a compassionate and inclusive culture, one in which all members of the greater Brighton Jones family feel safe to express their unique identities and empowered to reach their full potential.\n\nTo the right individual, we offer very competitive compensation, benefits package, and rewarding career opportunities. If you’ve been reading this job description thinking to yourself, this is the place for me, include in your cover letter why you’re excited to join our growing #OneTeam and be sure to describe how the opportunity aligns with your career objectives.\n\nBrighton Jones provides equal employment opportunities (\n\nWhat To Expect In The Hiring Process\n\n Values in Action Self-Assessment - at Brighton Jones we don’t just talk about our values, we live by them! We’ve operationalized our values into behaviors and part of the application process includes completing a brief self-assessment on our Values in Action. Initial Interview - this 30-min chat with a recruiter or hiring manager gives us a chance to learn about your background and goals and share more about the role and company. Role Alignment - next you’ll meet with the hiring manager and a peer (virtual or in-person) to dive deeper into the role, responsibilities, and your transferrable experiences. Full Loop - you’ll meet the team in two, back-to-back interviews with team members you’re likely to work with. During these conversations, we’ll be listening for examples of your technical skills as well as how core values show up in your experience. There may also be a writing sample, role play, hiring, or technical assessment depending on the position. Personal Reference Calls - in this final step in the hiring process, you will be asked to arrange 1-2 personal reference calls with past managers of our choosing. What’s Next - interviewing is time consuming and stressful, and we appreciate you taking the time to get to know us. Whatever the final decision, we’ll let you know our decision as quickly as we can. If this role isn’t a good fit, we invite you to stay connected and apply again."}
{"text": "QualificationsRequirementsPh.D. in Computer Vision, Neuroscience, Computer Science, physics, EE or related fieldCandidate must be highly motivated, ambitious, and creative\nAdditional InformationThe Rockefeller University does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy, gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service or other non-merit factor. All qualified applicants will receive consideration for employment without regard to the characteristics listed above.\nThe salary of the finalist selected for this role will be set based on various factors, including but not limited to organizational budgets, qualifications, experience, education, licenses, specialty, and training. The hiring range provided represents The Rockefeller University's good faith and reasonable estimate of the range of possible compensation at the time of posting."}
{"text": "Qualifications:0-2 years relevant experienceAdvanced knowledge of MS Office Suite, including proficiency in Excel and Access.Consistently demonstrates clear and concise written and verbal communication skills.Demonstrated organization skills with an excellent attention to detail.Ability to focus on high quality work.\nEducation:Bachelor’s/University degree or equivalent experiencePlease share with me your updated resume if you are interested in applying for this role.\nDexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals.Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more.Dexian is"}
{"text": "SKILLS and EXPERIENCE:3-5+ years of experience domain knowledge with either support of core Banking application experience, Mortgage Servicing or Loan Originations or personal or auto loans within Finance Industry environmentAble to interact with the VP or C-level Business Executives and higher to gather requirements and collaborate with IT; working effectively and independently as well as be collaborative team-oriented team player.Ideally supported Mortgage servicing systems such as Black Knight’s MSP, Sagent, Finastra’s Fusion Servicing Director, Interlinq Loan Servicing (ILS) or other loan servicing platform OR support of other core banking or originations platformSome experience with the following core technologies: T-SQL; SQL Server 2016 or higher; Visual Studio 2017 or higher; SQL Server Data Tools; Team Foundation ServerWorking knowledge of T-SQL programming and scripting, as well as optimization techniques· 3 years of experience with a strong focus on SQL Relational databases, application and data integration (ETL), Data extractions, cleansing and integration.Some Report development experienceWorking knowledge of integrating applications using APIsStrong analytical, problem solving, collaboration and technical skillsAble to work well under deadlines in a changing environment and perform multiple tasks effectively and concurrently.Organized and detail-orientedBachelor’s degree or equivalence experience, ideally in Information Systems or Computer Science.\nESSENTIAL DUTIES AND requirements to support the delivery of effective data solutions.Assist in the design, develop and deploy solutions to support integration with enterprise applications, SaaS applications and other vendor data.Manage Mortgage Servicing and Customer Portal: Configure, monitor, maintain and upgrade the system. Execute and monitor End of Day and End of Month processing. Board newly acquired loans into the system. Apply batch updates and data extracts via SQL server.Apply system updates from vendor.Perform Report & ETL Development: Create, update, and maintain Crystal Reports and SSRS ReportsCreate, update, and maintain SSIS packages.Create, update, and maintain ETL packages for system integration. Maintain ETL process jobs and respond to critical data load issues during off hours.Create and maintain documentation of processes, databases, applications, and procedures as per department policy.\n\nAbout CGS Business Solutions:CGS specializes in IT business solutions, staffing and consulting services. With a strong focus in IT Applications, Network Infrastructure, Information Security, and Engineering. CGS is an INC 5000 company and is honored to be selected as one of the Best IT Recruitment Firms in California. After five consecutive Fastest Growing Company titles, CGS continues to break into new markets across the USA. Companies are counting on CGS to attract and help retain these resource pools in order to gain a competitive advantage the rapidly changing business environments."}
{"text": "experienceFinancial data experienceSomeone who is comfortable working with ambiguity. - This just means that the work environment will require this person to do research to solve problems and not need their hand held.Must currently be located in one of the following areas for hybrid work environment:Chicago, ILSeattle, WAAustin, TXEmeryville & Glendale, CADenver, COAtlanta, GANJMaryland\nrequirements.Performs timely remediation of identified account code and budget discrepancies.Works with key stakeholders on training of cost accuracy tools.Performs site maintenance as required.Works closely with team members, suppliers, and partners to understand process-related issuesand develop effective solutions.Mentors less experienced specialists as required.\nSkills:Advanced knowledge of Excel and the Microsoft Office suite.Working knowledge of hosting and downstream systems including, but not limited to Coupa,Workday, Procore, and Verisae.Ability to identify and correct moderately complex data integrity issues.Ability to prepare moderately complex data and related reports.Ability to research and solve moderately difficult problems involving data integrity, data reportingand data management systems.Ability to perform business math.Able to effectively present information and respond to questions in on-on-one interaction, meeting/ group participation / presentation situations involving Team Members, store and facilityleadership, regional leadership, and national / global leadership.Models’ excellent customer service skills.Work priorities are set according to pre-determined deadlines.Applies knowledge and experience to organize and complete assignments.Solves problems using well-understood methods and technologies, refers complex or sensitiveissues to higher levels.Works on assignments that are semi-routine or moderately complex in nature, recognizing the needfor occasional deviation from standard practice.Follows standard data and situational analysis practices and procedures.Evaluates and resolves issues using knowledge of what information to collect and where to find it.Understands the WFM organizational structure, associated hierarchy, and approval levels.Understands the types and timing of various departmental deadlines and events.Understands how the specialist role contributes to and facilitates Team Member, team, leader, andbusiness success.\nEducation & Experience:High school diploma and 2-4 years’ relevant experience.Associate degree, technical college or some college course work preferred."}
{"text": "experience with all aspects of the software development lifecycle, from design to deployment. Demonstrate understanding of the full life data lifecycle and the role that high-quality data plays across applications, machine learning, business analytics, and reporting. Lead and take ownership of assigned technical projects in a fast-paced environment. \nWhat you need to succeed (minimum qualifications)3-5+ years of experienceFamiliar with best practices for data ingestion and data designDevelop initial queries for profiling data, validating analysis, testing assumptions, driving data quality assessment specifications, and define a path to deploymentIdentify necessary business rules for extracting data along with functional or technical risks related to data sources (e.g. data latency, frequency, etc.)Knowledge of working with queries/applications, including performance tuning, utilizing indexes, and materialized views to improve query performanceContinuously improve quality, efficiency, and scalability of data pipelinesTrack record of advancing new technologies to improve data quality and reliabilityGood understanding of writing test cases to ensure data quality, reliability and high level of confidenceExperience working with database technologies and data development such as Python, PLSQL, etc.Development experience building and maintaining ETL pipelines\nWhat will give you a competitive edge (preferred qualifications)Bachelor's degree in Computer Science, Mathematics, Science, Industrial Engineering or related quantitative fieldAirline industry experienceAt least some post-degree professional experience\nBenefits and Perks to Help You Keep ClimbingOur culture is rooted in a shared dedication to living our values – Care, Integrity, Resilience, Servant Leadership, and Teamwork – every day, in everything we do. At our company, our people are our success. At the heart of what we offer is our focus on Sharing Success with our employees. Exploring a career at our company gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:Competitive salary, industry-leading proﬁt sharing program, and performance incentives 401(k) with generous company contributions up to 9% Paid time off including vacation, holidays, paid personal time, maternity and parental leaveComprehensive health beneﬁts including medical, dental, vision, short/long term disability and life beneﬁtsFamily care assistance through fertility support, surrogacy and adoption assistance, lactation support, subsidized back-up care, and programs that help with loved ones in all stagesHolistic Wellbeing programs to support physical, emotional, social, and financial health, including access to an employee assistance program offering support for you and anyone in your household, free financial coaching, and extensive resources supporting mental health Domestic and International space-available flight privileges for employees and eligible family membersCareer development programs to achieve your long-term career goals World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprintBusiness Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategiesRecognition rewards and awards through the platform Unstoppable TogetherAccess to over 500 discounts, specialty savings and voluntary benefits through perks such as car and hotel rentals and auto, home, and pet insurance, legal services, and childcare"}
{"text": "requirements. Use system reports and analyses to identify potentially problematic data, make corrections, and determine root cause for data problems from input errors or inadequate field edits, and suggest possible solutions. Develop reports, charts, graphs and tables for use by investigators and for publication and presentation. Analyze data processes in documentation. Collaborate with faculty and research staff on data collection and analysis methods. Provide documentation based on audit and reporting criteria to investigators and research staff. Communicate with government officials, grant agencies and industry representatives.  - Other duties may also be assigned\n\n\nDesired Qualifications\n\nBachelor's degree in computational and engineering sciences (e.g., computer science, computational biology, electrical engineering, biomedical engineering) or other related fields. Experience with signal processing of medical imaging data. Experience with or demonstrated interest in neuroimaging techniques. Experience with Neuroimaging (FSL, SPM, AFNI, or equivalent) software. Experience using high-performance computer clusters and bash/shell scripting. Experience in data science, statistics, optimization, machine learning, and/or deep learning. Experience with machine learning frameworks (e.g. PyTorch, Tensorflow, etc.) Experience with Statistical software (R, SAS, SPSS, or equivalent), and other common programming languages in neuroimaging (python, MATLAB). Prior experience with R is highly recommended for data analysis. \n\n\nEducation & Experience (required)\n\n Bachelor's degree or a combination of education and relevant experience. Experience in a quantitative discipline such as economics, finance, statistics or engineering. \n\n\nKnowledge, Skills And Abilities (required)\n\nSubstantial experience with MS Office and analytical programs Strong writing and analytical skills in machine learning. Ability to prioritize workload. \n\n\nPHYSICAL REQUIREMENTS*:\n\n Sitting in place at computer for long periods of time with extensive keyboarding/dexterity. Occasionally use a telephone. Rarely writing by hand.  - Consistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of his or her job.\n\n\nWorking Conditions\n\nSome work may be performed in a laboratory or field setting. This position is based in Stanford’s Research Park and has the option of a telecommuting/hybrid schedule subject to operational needs. This position is 75% FTE. \n\n\nThe expected pay range for this position is $48,360 to $72,750 per annum. Stanford University provides pay ranges representing its good faith estimate of what the university reasonably expects to pay for a position. The pay offered to a selected candidate will be determined based on factors such as (but not limited to) the scope and responsibilities of the position, the qualifications of the selected candidate, departmental budget availability, internal equity, geographic location and external market pay for comparable jobs.\n\n - Stanford is an"}
{"text": "experiences and perspectives each Klaviyo (we call ourselves Klaviyos) brings to our workplace each and every day. We believe everyone deserves a fair shot at success and appreciate the experiences each person brings beyond the traditional job requirements. If you’re a close but not exact match with the description, we hope you’ll still consider applying. Want to learn more about life at Klaviyo? Visit careers.klaviyo.com to see how we empower creators to own their own destiny.\n\nAbout The Team\n\nOur team is the dedicated data science resource for areas outside of R&D, such as Customer Success, Sales, and Finance, so you’ll have the chance to make an impact in a wide variety of settings and explore a wide variety of data science solutions.\n\nListen to our data science team podcast at https://medium.com/klaviyo-data-science and learn more about our technical culture at https://klaviyo.tech\n\nAbout The Role\n\nAs a Senior Data Scientist, you will expand the data science team’s impact on the internal operations and developer-facing features of Klaviyo and help raise the bar of excellence for the team. Klaviyo offers a high-growth environment across the board, and data science’s partnership with internal operations and developer experience is a shining example—the work you do will save hundreds of hours of manual work across multiple teams, directly contribute to moving key performance metrics like churn and net revenue retention, make it easier for Klaviyo customers to find the help they need when they need it, and help more businesses grow more quickly on Klaviyo by finding and using the right third-party apps.\n\nThe ideal candidate has a strong background in data science, statistics, and machine learning. We’re looking for someone who can build, train, and deploy models, work directly with technical and non-technical stakeholders to scope projects and adjust to learnings on the fly, and learn about the areas they work in deeply enough to become an expert but quickly enough to deliver timely solutions. The right candidate will have both broad and deep knowledge: you should have a solid fundamental grasp on a large number of data science modeling and problem-solving techniques, and you should have a deep understanding and expertise in some areas as well. We’re especially interested in candidates who have experience working directly with business stakeholders in fields such as customer support, and in candidates who have experience with recommender systems, but that experience is not required.\n\nAs a Senior Data Scientist, you should have experience writing production-ready code and building models and solutions that are used in practice to drive better business outcomes. We’re focused on shipping early and often. We prefer iterative solutions that are incrementally better to a long-debated “perfect” solution. You’ll also have a strong role in mentoring other members of the team. You’ll review their math and statistics work as well as their code to ensure high quality, and you’ll share your knowledge with the team to help others learn.\n\nThis role is based in Boston, MA and requires a weekly hybrid in-office component. \n\nHow You’ll Have An Impact\n\nWork directly with stakeholders across the business to understand business problems and scope data science projects that will solve those problemsBuild models and ship automated tools that directly move key business metrics, save time spent on manual tasks, and ultimately accelerate Klaviyo’s growthChoose the right solution for the problem at hand rather than relying on “one-size-fits-all” approachesMentor team members through statistical and code reviews, helping them learn best practices and level up their own skills\n\n30 days\n\nYou will have finished on-boarding including engineering and DS specific on-boarding. You will have met multiple members of the Data Science team and members of our partner teams in Customer Education, Customer Success, and Developer Experience, as well as starting to familiarize yourself with the data in those areas. You will have shipped their first small project (e.g. a small feature enhancement to existing tooling in these areas).\n\n60 days\n\nYou will have begun work on your first larger mission-critical project (e.g. building a net new user-facing tool). To do this, you’ll understand the current state of that product area, work with the product manager and engineering manager for the stakeholder team to refine the idea into a well-scoped project, and develop an opinion about what is needed to tackle that project, including the data needed, the success metrics, the technical approach, the back-testing and/or live experiments in production needed to evaluate success, etc. To share knowledge, you will have participated in at least 1 review for a project that a teammate is running.\n\n90 days\n\nYou will be actively working on, and likely delivering, your first mission-critical project. You will have identified opportunities and obstacles to progress, with plans to address them. You will be actively working with teammates across Data Science to accelerate progress. In partnership with your manager, you will be regularly in conversation with Support, Customer Education, and Engineering stakeholders to make sure work remains aligned with company priorities.\n\nUp to 1 year\n\nYou will have taken ownership of a large initiative within the team’s roadmap, e.g. a set of user-facing tools and improvements that support a common theme. You’re regarded as a strong technical contributor on the team and have influenced the work of other data scientists via code reviews and reviews of data science approaches. You continue to deliver new and innovative solutions to the internal operations and developer experience problems Klaviyo faces and find new ways to improve Klaviyo’s operations.\n\nWhat We’re Looking For\n\n3+ years professional industry experience as a data scientist with fundamental understanding and deep experience with a variety of statistical and/or machine learning algorithms (e.g. regression models, tree-based methods, large language models, clustering, neural networks) as well as ways to evaluate and validate the performance of those modelsBachelor’s or advanced degree in statistics, applied mathematics, computer science or other relevant quantitative discipline, or equivalent industry experience.Experience working with stakeholders directly during the course of a project and managing stakeholder relationshipsAbility to write clean, intelligible, production-ready code in Python, including experience with data science tools and packagesEnjoys the high-risk research aspect of data science, i.e. capable and excited to learn from null or negative results and iterate to a better solution\n\nNice To Have\n\nDemonstrated a measurable and sizeable impact with your data science workStrong experience analyzing data and making rigorous statements about what can or cannot be concludedExperience designing, implementing, and analyzing experiments\n\nThe pay range for this role is listed below. Sales roles are also eligible for variable compensation and hourly non-exempt roles are eligible for overtime in accordance with applicable law. This role is eligible for benefits, including: medical, dental and vision coverage, health savings accounts, flexible spending accounts, 401(k), flexible paid time off and company-paid holidays and a culture of learning that includes a learning allowance and access to a professional coaching service for all employees.\n\nBase Pay Range For US Locations:\n\n$156,800—$235,200 USD\n\nGet to Know Klaviyo\n\nWe’re Klaviyo (pronounced clay-vee-oh). We empower creators to own their destiny by making first-party data accessible and actionable like never before. We see limitless potential for the technology we’re developing to nurture personalized experiences in ecommerce and beyond. To reach our goals, we need our own crew of remarkable creators—ambitious and collaborative teammates who stay focused on our north star: delighting our customers. If you’re ready to do the best work of your career, where you’ll be welcomed as your whole self from day one and supported with generous benefits, we hope you’ll join us.\n\nKlaviyo is committed to a policy of equal opportunity and non-discrimination. We do not discriminate on the basis of race, ethnicity, citizenship, national origin, color, religion or religious creed, age, sex (including pregnancy), gender identity, sexual orientation, physical or mental disability, veteran or active military status, marital status, criminal record, genetics, retaliation, sexual harassment or any other characteristic protected by applicable law.\n\nIMPORTANT NOTICE: Our company takes the security and privacy of job applicants very seriously. We will never ask for payment, bank details, or personal financial information as part of the application process. All our legitimate job postings can be found on our official career site. Please be cautious of job offers that come from non-company email addresses (@klaviyo.com), instant messaging platforms, or unsolicited calls. \n\nYou can find our Job Applicant Privacy Notice here."}
{"text": "requirements, and translate them into actionable insights and solutions.Analyze and interpret data to extract valuable insights and drive decision-making processes.Lead the evaluation and validation of Generative AI models, ensuring their accuracy, reliability, and performance meet business requirements.Collaborate with cross-functional teams, including engineering, product management, and business development, to deliver high-quality solutions on time and within budget.Communicate findings, results, and recommendations to technical and non-technical stakeholders through presentations, reports, and documentation.\nQualifications :\nUS Citizenship required for this role.\nMaster's or degree in Computer Science, Data Science, Statistics, or a related field.10+ years of hands-on experience in building and deploying Generative AI models using OpenAI and Python.Proven track record of using predictive analytics, forecasting, modeling and data science methodologies in delivering high-impact projects on schedule and within budget.Nice to have knowledge in machine learning, deep learning, and natural language processing techniques, with a focus on Generative AI applications.Strong programming skills in Python, SQL, with experience in frameworks such as TensorFlow, OpenAI, PyTorch, or Keras.Proficiency in data manipulation, analysis, and visualization using libraries such as pandas, NumPy, and Matplotlib.Solid understanding of SQL and relational databases, with experience in querying, extracting, and manipulating data from databases.Solid understanding of software engineering principles and best practices, including version control, testing, and deployment.Experience with data warehousing and ETL processes, including designing and optimizing database schemas.Excellent communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams and communicate complex technical concepts to non-technical stakeholders.Strong problem-solving skills and a passion for exploring new ideas and technologies in Generative AI.Ability to thrive in a fast-paced and dynamic environment, with a strong sense of accountability and ownership."}
{"text": "experience in machine learning, distributed microservices, and full stack systems  Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake  Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community  Research cloud cost abnormalities and provide insights into its financial impact and solutions for supporting needed changes for correction  Work with lines of businesses to implement savings opportunities within their cloud footprints and applications.  Provide technical leadership and guidance around architectural best practices that help elevate Cost Optimization as a pillar of the Well-Architected Framework  Influence and help achieve our enterprise cost efficiency strategy \n\nBasic Qualifications: \n\n Bachelor’s Degree  At least 6 years of experience in application development (Internship experience does not apply)  At least 2 years of experience in big data technologies  At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud) \n\nPreferred Qualifications:\n\n 7+ years of experience in application development including Python, SQL, Scala, or Java  4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)  4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)  4+ year experience working on real-time data and streaming applications  4+ years of experience with NoSQL implementation (Mongo, Cassandra)  4+ years of data warehousing experience (Redshift or Snowflake)  4+ years of experience with UNIX/Linux including basic commands and shell scripting  2+ years of experience with Agile engineering practices \n\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nNew York City (Hybrid On-Site): $201,400 - $229,900 for Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is \n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."}
{"text": "experienced SQL experts to be part of the artificial intelligence (AI) revolution.\n\nIf you have 2+ years of recent work experience in SQL, this may be the perfect opportunity for you.\n\nJoin our team in training AI models to excel at coding generation! We're seeking talented data engineers to work remotely on exciting projects. As part of this opportunity, you'll contribute to training generative artificial intelligence models, helping them become proficient data analysts.\n\nResponsibilities\n\nWe have partnered with organizations to train AI large language models, helping cutting-edge generative AI models write better SQL code. Projects typically include discrete, highly variable problems that involve engaging with these models as they learn.\n\nYour responsibilities might include:\n\nSolving complex multi-faceted SQL problems including multiple CTEs, partitioning/window functions, and advanced joinsOptimizing SQL queries to maximize efficiency and readability.Code review SQL queries to ensure accuracy against specific customer requirements.Lead training sessions to build cross team SQL knowledge and share advanced SQL concepts.Assess the SQL level of new team members and identify promising new candidates to join our quickly growing team.Serve as the team Subject Matter Expert on all things SQL.\n\nNo previous experience with AI is necessary! You will receive detailed instructions on what is expected of you after you complete the application and verification process.\n\nQualifications:\n\nRequired qualifications: \n\nBachelor's degree in Data Science, Computer Science, or a related field AND 2+ years of experience in a data analysis or data science roleComplete fluency in the English languageAbility to articulate complex scientific concepts in a clear and engaging mannerExcellent attention to detail and ability to maintain consistency in writingSolid understanding of grammar, punctuation, and style guidelinesExpert proficiency in working with SQL 2+ years of recent work experience in SQL with a primary focus on building advanced dashboards\n\nWhy work on Outlier?\n\nCutting-Edge Projects: Work on challenging projects that push the boundaries of AI coding abilitiesFlexibility: Set your own hours and work remotely from anywhereWeekly payouts: Get paid conveniently on a weekly basisCollaborative environment: Join a team of talented professionals who share your passion for AI and programming\n\nPay: $55 per hour\n\nPLEASE NOTE : We collect, retain and use personal data for our professional business purposes, including notifying you of job opportunities that may be of interest and sharing with our affiliates. We limit the personal data we collect to that which we believe is appropriate and necessary to manage applicants’ needs, provide our services, and comply with applicable laws. Any information we collect in connection with your application will be treated in accordance with our internal policies and programs designed to protect personal data."}
{"text": "Requirements:B.S. in Data Science, Computer Science, Statistics, or a related field.3+ years of experience with BI tools (e.g., Domo, Power BI, Tableau, MicroStrategy, Zoho).3+ years of experience creating reports, visualizations, dashboards, and exports.Proficiency in processing data using Excel or Google Sheets.Understanding of data analysis best practices.Exceptional attention to detail. Preferred:Experience with the Domo Data Warehouse/Business Intelligence system.Experience with healthcare-related or digital health applications.\nBenefits:Mission-driven, gratifying work in an entrepreneurial environmentCompetitive compensationAbility to work remotelyFlexible work schedule\nJob Type: Full-time\nJob Location: Remote work; company located in the Greater Chicagoland Area\nThis is a U.S.-based, remote work position, and verification of U.S. work authorization is required. Background investigation and drug screening are also required, as allowed by law.\nAll applicants will receive consideration for employment without regard to race, color, religion,sex, sexual orientation, gender identity, national origin, disability-protected veteran status, orany other characteristics protected by law."}
{"text": "requirements of the business;Develop custom data models and algorithms to apply to data sets;Assess the effectiveness and accuracy of new data sources and data gathering techniques Discover opportunities for data acquisition;Develop data set processes for data modeling, mining and production;Oversee the data team and in collaboration with IT leadership provide day to day guidance and direction to achieve organizational goals in the timelines set;Employ a variety of languages and tools to marry systems together;Recommend ways to improve data reliability, efficiency and quality;Leverage large volumes of data from internal and external sources to answer business demands;Introduce automation through effective metadata management and using innovative and modern tools and techniques. Partially or completely automate the most common and repeatable data preparation and integration tasks;Propose appropriate data ingestion, preparation, integration and operationalization techniques in addressing data requirements;Lead the development of data governance policies and best practices for consumers and users of data we provision;Coordinate with different functional teams to implement models and monitor outcomes;Develop processes and tools to monitor and analyze model performance and data accuracy.\nRequirements: Proficiencies:Experienced in designing, building and managing data pipelines for data structures;Expertise with advanced analytics tools for Object-oriented/object function scripting. Includes languages such as C#, Python and others;Expert in SQL, PL/SQL, SSIS and SSAS;Knowledge and/or certifications on upcoming NoSQL/Hadoop-oriented databases like MongoDB, Cassandra, and others for non-relational databases;Strong experience in working with large, heterogeneous data sets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies;Experienced working with popular data discovery, analytics, and BI software tools such as Tableau, Power BI and others for semantic-layer-based data discovery;Experienced working with data governance/data quality and data security teams;Experienced employing MicroSoft MDM (Master Data Management) and MDS;Ability to troubleshoot complicated issues across multiple systems and driving solutions;Effectively convey technical concepts to non-technical individuals;Demonstrate a high level of Data Security Awareness;Financial data and or experience with the legal industry data is a plus;Experienced performing ETL using Alteryx/Intapp Integrate/Boomi is a plus.\nQualifications:A bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field or equivalent work experience;At least 8 years' experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks;At least 5 years' experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative;Excellent verbal and written communication skills;Excellent problem solving and analytical skills; Must be highly effective within a collaborative environment;Must be able to independently resolve issues and efficiently self-direct work activities based on the ability to capture, organize, and analyze information."}
{"text": "skills to explore, discover, and predict patterns contained within data sets for a wide range of government clients. This includes the derivation of clear narratives that help our clients understand their data and how those insights address their research questions. Responsibilities * Demonstrate the\n\nQuantico, VA, United States Servicenow Developer with Security Clearance Quantico VA 22134 United States Employee 2024-04-04 8340_7384915 ClearanceJobs No IT - Software No https://click.appcast.io/track/hl41sq4?cs=i8d&jg=20ym&bid=q948nGl57dNhZO5Va2AQmA==Job Description Founded in 2007, Markon Solutions is a nationally recognized consulting firm headquartered in Falls Church, Virginia. With employees in seven states and overseas, we support the intelligence community, defense and civilian sectors, as well as commercial clients. Markon is committed to advancing clients through project management and business improvement services offered in a premiere, highly personal environment. Join our winning team! We empower our employees to make decisions and encourage them to take on leadership roles. New ideas from employees are also encouraged. Markon is a place where you can truly demonstrate your capabilities to your full potential. We are consistently named a Washington Post Top Workplace and Best Place to Work in Virginia because we put our employees first. Markon offers an exceptional benefits package that ranks in the top tier of our industry. Markon Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. Description: Markon is seeking qualified candidates to support one of our Department of Defense clients in the Quantico, VA area for a critical IT role. requirements * ServiceNow portal implementation and customization; develops widgets and capabilities, leverages user interface policies, actions, and scripts * Implements ServiceNow users, groups, roles, ACL management * Develops applications and modules using APIs and customized notifications, approvals, and inbound email actions * Engineer ServiceNow implementation to include workflows, business rule customization and scripting * Perform analysis of current infrastructure and business processes * Work with current and identify future customers to ensure their mission needs will be met with minimal impact during transition * Focus on integrating services defined in the MCIEE Blueprint and DCI road map * Ensure requirements are captured and work with teammates to recommend best practices for a smooth cloud transition while maintaining high availability * Create and operate multiple cloud infrastructure environments based on best practices and maximizing vendor strengths * Participate in software development initiatives to support service management enhancements using ServiceNow workflows. * Promotes best practice implementations of a service management system in large-scale environments * Other duties as required Job Requirements Required: * This position requires an active DoD TS//SCI Clearance * Current DoD 8570 IAT Level II Certification (Security+ CE or equivalent) * Must be capable of performing analysis of current business processes and services provided * Knowledge of best practice approach of transition to cloud technologies or hybrid approach * Develop transition plan that keeps current stakeholders informed during the process * Design, Implement, and Automate environment creation * Document procedures, plans, testing, diagrams, maintenance, training, SLAs/OLAs * Must be capable of training peers with administration/migration processes * Must be capable of training customers of new processes/procedures resulting from ServiceNow transition * Excellent communication skills and interpersonal skills * Ability to work well independently and in a collaborative team environment Desired: * Familiar with Marine Corps processes, procedures, and interoperability requirements * Familiar with similar transitions and can inform lessons learned as needed * Department of Defense Architecture Framework (DoDAF) development may be necessary * Familiar with C2S, S-C2S, AWS, Azure\\MSO365 and Marine Corps Private Cloud technologies and dependencies * Familiarity with the MCISRE and Blueprint services * Familiarity with FEDRAMP, NIST, RMF, and other process standards * Experience with manual and automated security administration, remediation, and hardening (STIG reviews, group policy and permissions, system modifications based on vulnerability scans, audits, and mitigations; patch management) * Experience implementing and supporting enterprise system monitoring tools * Prior project management, team lead, and scrum master experience Travel 0 - 10% Location Quantico, VA - Quantico, VA US (Primary) Education High School Job Type Full-time Career Level Experienced (Non-Manager) Category Information Technology Exemption Type Exempt Date Needed By 7/27/2023 Security Clearance Top Secret/SCI 0.00 1007957 On-site\n\nJob Description Founded in 2007, Markon Solutions is a nationally recognized consulting firm headquartered in Falls Church, Virginia. With employees in seven states and overseas, we support the intelligence community, defense and civilian sectors, as well as commercial clients. Markon is committed to advancing clients through project management and business improvement services offered in a premiere, highly personal environment. Join our winning team! We empower our employees to make decisions and encourage them to take on leadership roles. New ideas from employees are also encouraged. Markon is a place where you can truly demonstrate your capabilities to your full potential. We are consistently named a Washington Post Top Workplace and Best Place to Work in Virginia because we put our employees first. Markon offers an exceptional benefits package that ranks in the top tier of our industry. Markon Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. Description: Markon is seeking qualified candidates to support one of our Department of Defense clients in the Quantico, VA area for a critical IT role. requirements * ServiceNow portal implementation and customization; develops widgets and capabilities, leverages user interface policies, actions, and scripts * Implements ServiceNow users, groups, roles, ACL management * Develops applications and modules using APIs and customized notifications, approvals, and inbound email actions * Engineer ServiceNow implementation to include workflows, business rule customization and scripting * Perform analysis of current infrastructure and business processes * Work with current and identify future customers to ensure their mission needs will be met with minimal impact during transition * Focus on integrating services defined in the MCIEE Blueprint and DCI road map * Ensure requirements are captured and work with teammates to recommend best practices for a smooth cloud transition while maintaining high availability * Create and operate multiple cloud infrastructure environments based on best practices and maximizing vendor strengths * Participate in software development initiatives to support service management enhancements using ServiceNow workflows. * Promotes best practice implementations of a service management system in large-scale environments * Other duties as required Job Requirements Required: * This position requires an active DoD TS//SCI Clearance * Current DoD 8570 IAT Level II Certification (Security+ CE or equivalent) * Must be capable of performing analysis of current business processes and services provided * Knowledge of best practice approach of transition to cloud technologies or hybrid approach * Develop transition plan that keeps current stakeholders informed during the process * Design, Implement, and Automate environment creation * Document procedures, plans, testing, diagrams, maintenance, training, SLAs/OLAs * Must be capable of training peers with administration/migration processes * Must be capable of training customers of new processes/procedures resulting from ServiceNow transition * Excellent communication skills and interpersonal skills * Ability to work well independently and in a collaborative team environment Desired: * Familiar with Marine Corps processes, procedures, and interoperability requirements * Familiar with similar transitions and can inform lessons learned as needed * Department of Defense Architecture Framework (DoDAF) development may be necessary * Familiar with C2S, S-C2S, AWS, Azure\\MSO365 and Marine Corps Private Cloud technologies and dependencies * Familiarity with the MCISRE and Blueprint services * Familiarity with FEDRAMP, NIST, RMF, and other process standards * Experience with manual and automated security administration, remediation, and hardening (STIG reviews, group policy and permissions, system modifications based on vulnerability scans, audits, and mitigations; patch management) * Experience implementing and supporting enterprise system monitoring tools * Prior project management, team lead, and scrum master experience Travel 0 - 10% Location Quantico, VA - Quantico, VA US (Primary) Education High School Job Type Full-time Career Level Experienced (Non-Manager) Category Information Technology Exemption Type Exempt Date Needed By 7/27/2023 Security Clearance Top Secret/SCI"}
{"text": "experience to solve some of the most challenging intelligence issues around data.\n\nJob Responsibilities & Duties\n\nDevise strategies for extracting meaning and value from large datasets. Make and communicate principled conclusions from data using elements of mathematics, statistics, computer science, and application specific knowledge. Through analytic modeling, statistical analysis, programming, and/or another appropriate scientific method, develop and implement qualitative and quantitative methods for characterizing, exploring, and assessing large datasets in various states of organization, cleanliness, and structure that account for the unique features and limitations inherent in data holdings. Translate practical needs and analytic questions related to large datasets into technical requirements and, conversely, assist others with drawing appropriate conclusions from the analysis of such data. Effectively communicate complex technical information to non-technical audiences.\n\nMinimum Qualifications\n\n10 years relevant experience with Bachelors in related field; or 8 years experience with Masters in related field; or 6 years experience with a Doctoral degree in a related field; or 12 years of relevant experience and an Associates may be considered for individuals with in-depth experienceDegree in an Mathematics, Applied Mathematics, Statistics, Applied Statistics, Machine Learning, Data Science, Operations Research, or Computer Science, or related field of technical rigorAbility/willingness to work full-time onsite in secure government workspacesNote: A broader range of degrees will be considered if accompanied by a Certificate in Data Science from an accredited college/university.\n\nClearance Requirements\n\nThis position requires a TS/SCI with Poly\n\nLooking for other great opportunities? Check out Two Six Technologies Opportunities for all our Company’s current openings!\n\nReady to make the first move towards growing your career? If so, check out the Two Six Technologies Candidate Journey! This will give you step-by-step directions on applying, what to expect during the application process, information about our rich benefits and perks along with our most frequently asked questions. If you are undecided and would like to learn more about us and how we are contributing to essential missions, check out our Two Six Technologies News page! We share information about the tech world around us and how we are making an impact! Still have questions, no worries! You can reach us at Contact Two Six Technologies. We are happy to connect and cover the information needed to assist you in reaching your next career milestone.\n\nTwo Six Technologies is \n\nIf you are an individual with a disability and would like to request reasonable workplace accommodation for any part of our employment process, please send an email to accomodations@twosixtech.com. Information provided will be kept confidential and used only to the extent required to provide needed reasonable accommodations.\n\nAdditionally, please be advised that this business uses E-Verify in its hiring practices.\n\n\n\nBy submitting the following application, I hereby certify that to the best of my knowledge, the information provided is true and accurate."}
{"text": "experience in forecasting, particularly in demand or sales forecasting. The ideal candidate will have a strong background in time series analysis, advanced machine learning models, and deep learning techniques. In addition, the candidate should have hands on experience of building ML solutions on AWS.\n**Key requirements, analyze data, and deliver actionable insights.- Collaborate with data engineers to deploy and integrate forecasting solutions into production systems.- Conduct thorough evaluation and validation of forecasting models to ensure accuracy and reliability.- Stay updated on the latest advancements in forecasting techniques, machine learning algorithms, and cloud technologies.\n**Qualifications:**\n- Master's degree in Statistics, Data Science, Computer Science, or related field.- Minimum of 3 years of experience in data science, with a focus on forecasting.- Strong proficiency in time series analysis, statistical modeling, and machine learning algorithms.- Advanced experience with AWS services such as SageMaker, S3, EC2, Lambda, etc.- Demonstrated expertise in building and deploying ML solutions at scale, preferably in a cloud environment.- Excellent problem-solving skills and ability to thrive in a fast-paced, collaborative environment.- Strong communication and presentation skills, with the ability to effectively communicate complex technical concepts to non-technical stakeholders.\n**Great to have:**\n- Familiarity with deep learning techniques for time series forecasting (e.g., LSTM, GRU).- Experience with big data technologies such as Spark."}
{"text": "requirements, and any other documentation that may be needed in support of RBAC.Support coordination and meetings regarding RBAC process, documentation, automated onboarding tool development, and task updates or changes. Meetings will also include appropriate scrums and customer coordination meetings.Assist/support GM CSM with GitLab data entry and updates for the Federated IT team.Position is 6 months but could go up to one year\nNMR Consulting is"}
{"text": "experience, education, geographic location, and other factors\nPosition Summary:Analyze data to produce meaningful and actionable insights for core stakeholders of the business. Build Reports and Dashboards to serve ongoing data needs.\nPrimary Role:Support Digital Domains (Product Teams sitting in Digital) with Reporting requests and DashboardsSupport and work closely with CJA analyst and CX platform Manager\nNecessary SkillsProactive investigation of anomalies/irregularities.Takes initiative to produce results independently; speaks up when challenges arise; communicates proper support neededcritical thinking skills to get to meaningful outcomes;able to incorporate additions and extensions to dashboards on the fly;understanding data processing rules, and fundamentals of the data environmentcircle back to business plan--make results make sense\nTechnical Acumen:\nSQL, Adobe Analytics, Tableau Dashboard Visualization and Tableau Prep, Salesforce /CRMA preferred, Journey Analytics background a plus.\nIf you are interested in this role, then please click APPLY NOW. For other opportunities available at Akkodis, or any questions, please contact Pratiksha Pandey at 610-979-9170 or pratiksha.pandey@akkodisgroup.com.\n\nEqual Opportunity Employer/Veterans/Disabled\nBenefit offerings include medical, dental, vision, term life insurance, short-term disability insurance, additional voluntary benefits, commuter benefits, and a 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria.\nDisclaimer: These benefit offerings do not apply to client-recruited jobs and jobs that are direct hires to a client.\nTo read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy."}
{"text": "experience. Experience with performing security requirements analyses to secure the deployment of large globally distributed cloud-based and/or mobile-embedded platforms. Experience with OWASP Top 10 vulnerabilities and Cryptographic Algorithms: (PKI), X.509 Public Key Certificates, authentication protocols, and transport layer security, OID, OAuth, SAML. Development experience in C++/C, Swift, Java, Scala, Python or other languages and the ability to solve complex operational issues. Experience with IT Security Frameworks such as NIST, ISO27001, PCI, DSS, FedRAMP One or more of the following certifications: AWS Certified Solutions Architect (professional), AWS Certified Security (Specialty), CSA Certificate of Cloud Security Knowledge (CCSK), ISC2 Certified Cloud Security Professional (CCSP), CISSP.\n\n\nIf this is a role that interests you and you’d like to learn more, click apply now and a recruiter will be in touch with you to discuss this great opportunity. We look forward to speaking with you!\n\nAbout ManpowerGroup, Parent Company of:Manpower, Experis, Talent Solutions, and Jefferson Wells\n\nManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands –  Manpower, Experis, Talent Solutions, and Jefferson Wells  – creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2022 ManpowerGroup was named one of the World's Most Ethical Companies for the 13th year - all confirming our position as the brand of choice for in-demand talent."}
{"text": "skills to collect, analyze and interpret large datasets to help develop data and value-driven solutions to solve challenges for our Supply Chain end to end. You will join a newly formed team transforming our analytical and digital culture. Daily responsibilities include partnering with cross-functional teams across Conagra to hypothesize, formulate, develop, deliver and improve data science products to help improve and advance data-driven insights, decisions, simulation, actions and automation\n\nWhat You’ll Do\n\nDevelop and deploy data mining and advanced analytics to monitor, benchmark, and optimize business needs, identifying areas for improvement and deeper, root-cause analysisDevelop and deploy models, simulation models, and other advanced analytics solutions to enable data-driven decision-making to meet Supply Chain objectivesApply business acumen to continuously develop new features to improve analytical modelsPartner with cross-functional business stakeholders on assumptions, opportunities, and solutionsCollaborate to guide standards, best practices, solution innovation, future solution needs and keep current with industry trendsIdentify relationships and trends, perform statistical analysis and implement machine learning algorithms for prediction, forecasting and classificationAdvance our analytics maturity and data-driven culture\n\n\nYou’ll Have\n\nBachelor's Degree3+ years of experience developing and applying operational research models, data mining applications, and advanced analyticsStrong problem solving skills with an emphasis on product developmentExperience using statistical computer languages (R, Python, SQL) to manipulate data and draw insights from large datasetsKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks) and their real-world advantages/drawbacksKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage) and experience with applicationsHands-on experience with Databricks, Snowflake and other Cloud platformsA drive to learn and master new technologies and techniquesExcellent written and verbal communication skills for coordinating across teamsWe ask that you travel up to 20-30% to ChicagoNumber of days in the office: 3\n\n\nRelocation assistance is available for this position. Preference will be given to local candidates\n\nAt this time, we require applicants for this role to be legally authorized to work in the United States without requiring employer sponsorship either now or in the future.\n\nOur Benefits\n\nWe care about your total well-being and will support you with the following, subject to your location and role:\n\nHealth: Medical, dental and vision insurance, company-paid life, accident and disability insuranceWealth: great pay, incentive opportunity, matching 401(k) and stock purchase planGrowth: online courses, virtual and classroom development experiencesBalance: paid-time off, parental leave, flexible work-schedules (subject to your location and role)\n\n\nOur Company\n\nConagra Brands is one of North America's leading branded food companies. We have a rich heritage of making great food, and a team that’s passionate about innovation and growth. Conagra offers choices for every occasion through iconic brands, such as Birds Eye®, Marie Callender's®, Banquet®, Healthy Choice®, Slim Jim®, Reddi-wip®, and Vlasic®, and emerging brands, including Angie's® BOOMCHICKAPOP®, Duke's®, Earth Balance®, Gardein®, and Frontera®.\n\nWe pride ourselves on having the most impactful, energized and inclusive culture in the food industry. For more information, visit www.conagrabrands.com.\n\nConagra Brands is"}
{"text": "skills, including prioritizing, problem-solving, and interpersonal relationship building.Strong experience in SDLC delivery, including waterfall, hybrid, and Agile methodologies.Experience delivering in an agile environment.Skills:Proficient in SQLTableau"}
{"text": "experience with Python who would be able to support algorithm and data model development utilizing traditional and deep learning ML approaches. In this role, you will join an existing team developing a predictive analytics platform which detects infections in tissue samples related to bacterial infection. Demonstrated experience training and optimizing Machine Learning models for biological sample analysis is required. Prior experience building and training Deep Learning models within CNN architecture is strongly preferred. This is a great opportunity to participate in a large, highly visible product development initiative. We are interviewing qualified candidates immediately and will move into the offer stage quickly. If you are interested, please apply with an updated resume. QUALIFICATIONS Demonstrated experience training and optimizing ML models for biological sample analysis Experience building and training DL models in CNN, GAN or RNN architectures, CNN preferred Hands-on expertise with data extraction, alignment, cleansing and storage in Python Prior experience with Random Forests or XGBoost \nEffective written and verbal communication skills are absolutely required for this role. You must be able to work LEGALLY in the United States as NO SPONSORSHIP will be provided. NO 3rd PARTIES."}
{"text": "QualificationsAdvanced degree (Ph.D. preferred) in Computer Science, Electrical Engineering, or a related field.10+ years of experience in engineering leadership, with a proven track record of success in leading and delivering impactful AI projects.Deep understanding of machine learning, deep learning, compilers, and other relevant AI algorithms.Passion for innovation and a commitment to develop AI ethically and responsibly.Proven ability to build and manage high-performing technical teams."}
{"text": "experience in formatting data from Excel spreadsheets and workbooks to be reformatted for new applications. The ideal candidate will have a deep understanding of data manipulation and transformation processes, specifically within the context of Excel data formatting. The primary responsibilities of this role include:\n\n Extracting data from Excel spreadsheets and workbooks. Formatting and restructuring data to meet the requirements of new applications. Developing automated solutions for data formatting tasks. Collaborating with other team members to ensure data accuracy and consistency. Analyzing data quality and identifying improvement opportunities.\n\nQualifications:\n\n Bachelors degree in a relevant field (e.g., Data Science, Statistics, Computer Science). Proven experience in data manipulation and transformation using Excel. Strong proficiency in Excel functions and formulas. Familiarity with data visualization tools is a plus. Excellent problem-solving and analytical skills. Strong attention to detail and accuracy.\n\nIf you are a detail-oriented individual with a passion for data formatting and transformation, we encourage you to apply for this position.\n\n\n\nApex Systems is \n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\n4400 Cox Road\n\nSuite 200\n\nGlen Allen, Virginia 23060\n\nApex Systems is"}
{"text": "experienced data engineer like you to help our clients find answers in their big data to impact important missions from - intelligence to national security. As a big data engineer at Booz Allen, you'll implement data engineering activities on some of the most mission-driven projects in the industry. You'll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you'll work with and guide a multi-disciplinary team of analysts, data scientists, developers, and data consumers in a fast-paced, agile environment. You'll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. Join us. The world can't wait. You Have: * 2+ years of experience in data engineering, software development, machine learning or data science * Experience with application development, including building web APIs * Experience designing, developing, operationalizing, and maintaining complex data applications at scale * Experience building scalable ETL and ELT workflows * Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms * Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud * Experience with distributed data and computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka * TS/SCI clearance * Bachelor's degree Nice If You Have: * Experience with Python, SQL, Scala, or Java * Experience working on real-time data and streaming applications * Experience with NoSQL implementation, including MongoDB or Cassandra * Experience with data warehousing using\n\nStafford, VA, United States Electromagnetic Spectrum Operations Analyst with Security Clearance Stafford VA 22554 United States Employee 2024-04-17 8340_7541082 ClearanceJobs No Yes https://click.appcast.io/track/iid9l5o?cs=i8d&jg=20ym&bid=q948nGl57dNhZO5Va2AQmA==Job Number: R0189950 Electromagnetic Spectrum Operations Analyst\n\nThe Opportunity: As a defense mission professional, you ask questions others don't. You understand the nuances of complex situations. You use your skills to thi nk bigger and push further, solving complex problems. We're looking for an expert like you to create solutions for missions that keep our nation safe. Serve as an Electromagnetic Spectrum Operations ( EMSO ) Analyst to a science and te chn ology client in the information environment. As an EMSO Analyst, you will provide subject matter expertise of electromagnetic spectrum ( EMS ) operations and integration of electronic warfare ( EW ) , space, cyber, joint, and fires and effects processes at the operational, strategic, or national level. Join us. The world can't wait. You Have: * Experience presenting and synthesizing te chn ical information to present to non-te chn ical audiences * Experience analyzing operational data * Knowledge of command and control or spectrum management systems and capabilities * Knowledge of RF Theory and waveforms, Joint Electromagnetic Spectrum Operations ( JEMSO ) environments, and planning, execution, and assessment of EMSO * Ability to analyze foreign capabilities to detect, disrupt, and deny USMC emissions and signals throughout the EMS * Ability to write reports and information papers, prepare graphics, and analyze operational data * Ability to coordinate with universities and other government or non-government agencies for spectrum sharing te chn ologies, evaluate spectrum tools, and support Product Offices * Ability to travel OCONUS up to 20% of the time * TS/SCI clearance * Bachelor's degree and 8+ years of experience in EW, communications, or spectrum operations, capability development, acquisition, or policy , or 15+ years of experience in EW, communications, or spectrum operations, capability development, acquisition, or policy in lieu of a degree Nice If You Have: * Experience with Test and Evaluation ( T & E ) plans and field user evaluations * Experience at Headquarters Marine Corps or on senior military staff * Experience with Joint doctrine, Major Command, or a Combatant Command * Experience in machine learning and predictive analysis * Master's degree Clearance: Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information ; TS/SCI clearance is required. Create Your Career: Grow With Us\n\nYour growth matters to us-that's why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs , tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. A Place Where You Belong\n\nDiverse perspectives cultivate collective ingenuity. Booz Allen's culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you'll build your community in no time. Support Your Well-Being\n\nOur comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401 ( k ) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we'll support you as you pursue a balanced, fulfilling life-at work and at home. Your Candidate Journey At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we've compiled a list of resources so you'll know what to expect as we forge a connection with you during your journey as a candidate with us. Compensation At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen's benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. Salary at Booz Allen is determined by various factors, including but not limited to location, the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $84,600.00 to $193,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen's total compensation package for employees. This posting will close within 90 days from the Posting Date. Work Model\n\nOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. * If this position is listed as remote or hybrid, you'll periodically work from a Booz Allen or client site facility.\n\n If this position is listed as onsite, you'll work with colleagues and clients in person, as needed for the specific role. \n\nJob Number: R0189950 Electromagnetic Spectrum Operations Analyst\n\nThe Opportunity: As a defense mission professional, you ask questions others don't. You understand the nuances of complex situations. You use your skills to thi nk bigger and push further, solving complex problems. We're looking for an expert like you to create solutions for missions that keep our nation safe. Serve as an Electromagnetic Spectrum Operations ( EMSO ) Analyst to a science and te chn ology client in the information environment. As an EMSO Analyst, you will provide subject matter expertise of electromagnetic spectrum ( EMS ) operations and integration of electronic warfare ( EW ) , space, cyber, joint, and fires and effects processes at the operational, strategic, or national level. Join us. The world can't wait. You Have: * Experience presenting and synthesizing te chn ical information to present to non-te chn ical audiences * Experience analyzing operational data * Knowledge of command and control or spectrum management systems and capabilities * Knowledge of RF Theory and waveforms, Joint Electromagnetic Spectrum Operations ( JEMSO ) environments, and planning, execution, and assessment of EMSO * Ability to analyze foreign capabilities to detect, disrupt, and deny USMC emissions and signals throughout the EMS * Ability to write reports and information papers, prepare graphics, and analyze operational data * Ability to coordinate with universities and other government or non-government agencies for spectrum sharing te chn ologies, evaluate spectrum tools, and support Product Offices * Ability to travel OCONUS up to 20% of the time * TS/SCI clearance * Bachelor's degree and 8+ years of experience in EW, communications, or spectrum operations, capability development, acquisition, or policy , or 15+ years of experience in EW, communications, or spectrum operations, capability development, acquisition, or policy in lieu of a degree Nice If You Have: * Experience with Test and Evaluation ( T & E ) plans and field user evaluations * Experience at Headquarters Marine Corps or on senior military staff * Experience with Joint doctrine, Major Command, or a Combatant Command * Experience in machine learning and predictive analysis * Master's degree Clearance: Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information ; TS/SCI clearance is required. Create Your Career: Grow With Us\n\nYour growth matters to us-that's why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs , tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. A Place Where You Belong\n\nDiverse perspectives cultivate collective ingenuity. Booz Allen's culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you'll build your community in no time. Support Your Well-Being\n\nOur comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401 ( k ) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we'll support you as you pursue a balanced, fulfilling life-at work and at home. Your Candidate Journey At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we've compiled a list of resources so you'll know what to expect as we forge a connection with you during your journey as a candidate with us. Compensation At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen's benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. Salary at Booz Allen is determined by various factors, including but not limited to location, the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $84,600.00 to $193,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen's total compensation package for employees. This posting will close within 90 days from the Posting Date. Work Model\n\nOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. * If this position is listed as remote or hybrid, you'll periodically work from a Booz Allen or client site facility.\n\n If this position is listed as onsite, you'll work with colleagues and clients in person, as needed for the specific role."}
{"text": "experience in the U.S. Financial Industry as a business or data analystStrong knowledge of Banking and Investment productsStrong communication skills: Both written and oral with technical and non-technical staffFamiliarity with issues workflow management tools such as JIRAAt least 3+ Years working with Databases such as SQL Server & OracleExperience working with a Data management team and monitoring data quality and/or performing data quality issue remediation activities, inclusive of conducting root cause analysisAdvanced Excel skillsDetail oriented, organized, and thoroughAbility to thrive in a team-based environment"}
{"text": "experience building GenAI at scale (NLP, Cloud, Infra, APIs, etc.) Learn agile working and product development while build GenAI productsWork in a cross functional team with a large number data scientists, engineers, designers, product managers to build GenAI features Enhance existing functionalities by implementing RAG systems\nWhat Gets You The Job:Understanding of NLP or general AI concepts Experiment driven - Design and conduct experiments to test hypotheses for product enhancements Collaborate with team members across different work streams to gain a comprehensive understanding of core product requirements and enhancements Self standing: Ability to structure and drive your own workstream, drive it forward Very strong product mindset: Ability to make trade-offs on DS side for our users and strong critical thinking required to ensure we're doing the right experiments and we’re making the right assumptions Very driven, super strong on execution and output orientation, likes to get stuff done attitude. Able to work in complex and very fast paced environment. Willing to be flexible in hours: Half of team is based in US East Coast, the other half is located in Europe Strong communication skills, holds their ground, opinionated, not afraid to speak up at any level Passionate about building GenAI products Have product development experience, experience in working in software engineering type of set-up (beyond Jupyter Notebooks)Bachelor's degree in quantitative field like Computer Science, Engineering, Statistics, Mathematics or related field required. Advanced degree is a strong plus.\nPlease send your resume to Dave Lim, Senior Technical Recruiter for immediate consideration.\nIrvine Technology Corporation (ITC) is a leading provider of technology and staffing solutions for IT, Security, Engineering, and Interactive Design disciplines servicing startups to enterprise clients, nationally. We pride ourselves in the ability to introduce you to our intimate network of business and technology leaders – bringing you opportunity coupled with personal growth, and professional development! Join us. Let us catapult your career!\nIrvine Technology Corporation provides equal employment opportunities ("}
{"text": "requirements. You will work closely with cross-functional teams to develop and implement data processing solutions that align with business needs. Additionally, you will be responsible for ensuring the quality and integrity of data while optimizing performance and ensuring data security. The successful candidate must have at least 5 years of experience in data engineering, with a strong focus on Azure Databricks and Azure Data Factory. You should be able to design and develop efficient data processing pipelines and should be proficient in SQL queries. Experience in JIRA is a must. Must Have below skills:• SQL Quires • SSIS• Data Factory• Databricks• JIRA.\n\nThanks & RegardsJoshuaDelivery Manager"}
{"text": "skills and domain knowledge. This position involves open-source research and analytic skillsets to create actionable insights. Successful candidate(s) will learn new techniques and approaches on-the-job, working in collaboration with other team members.\n\nBasic Qualifications:\n\nTypically requires a Bachelor’s degree and a minimum of 2 years of related experience; or an advanced degree without experience; or equivalent combination of related education and work experience.Must meet eligibility requirements for TS/SCI (Applicants selected for this position will be subject to a government security investigation). Demonstrated problem-solving skills and capable of working in a cross-functional environment that includes scientists, data analysts, technical software developers, and intelligence analysts.Experience with open-source research.Data analysis around business intelligence, supply chain, economics, cyber, and/or critical infrastructure topics.Familiarity with a modern programming language such as C or C++, Python, or MATLABThis position requires a minimum of 4 days a week on-site\n\nPreferred Qualifications:\n\nExperience cleaning and wrangling real-world messy data.Domain knowledge of machine learning and computer vision techniques for classification, detection, key attribute extraction, segmentation, or activity detection.Experience with modern source control software and methodology, including version control with Git and code reviews in a GitHub Flow-like development workflow.Proficiency with Python, especially data science stack(s) such as Pandas, NumPy, Scikit-Learn, etcCentral Eurasia or East Asia regional studies including history, economics, internal politics, foreign relations, and/or infrastructure.Russian, Chinese Cantonese, or Chinese Mandarin language skills.Interest in national security strategies related to near-peer competition with Russia and China.\n\nThis requisition requires the candidate to have a minimum of the following clearance(s):\n\nNone\n\nThis requisition requires the hired candidate to have or obtain, within one year from the date of hire, the following clearance(s):\n\nTop Secret\n\nWork Location Type:\n\nHybrid\n\nMITRE is proud to be \n\nMITRE intends to maintain a website that is fully accessible to all individuals. If you are unable to search or apply for jobs and would like to request a reasonable accommodation for any part of MITRE’s employment process, please email recruitinghelp@mitre.org.\n\nCopyright © 2024, The MITRE Corporation. All rights reserved. MITRE is a registered trademark of The MITRE Corporation. Material on this site may be copied and distributed with permission only.\n\nBenefits information may be found here"}
{"text": "requirements as well as meeting architectural specifications:\nTransform raw data into merging it with our global provider view enabling health care professionals to make informed business decisions.Review data requirements / data stories and corresponding data sources and data architectureDesign and build a robust data model architecture to support optimal data processing and standardized metric definitionsMaintain high levels of code test coverage while delivering clean concise and understandable codeBuilds and works with distributed computing systems for processing large data sets.Document data requirements / data stories and maintain data models to ensure seamless integration into existing data architecturesDesign, build, and maintain robust and efficient data pipelines that collect, process, and store data from various sources, including NPPES and state license data.\nCollaborate with cross-functional teams, including Data Analysts, Product Managers, and Software Engineers, to define data requirements, and deliver data solutions that drive internal alignment and process improvements:\nProvide constructive feedback on architectural designs and peer code reviews\nContribute to building a best in class Provider Data Management system to empower users to collect, analyze and react to provider data in new ways.\nBehavioral Competencies\nIndependent worker: Need to be able to communicate but also work independently Cross-team collaboration: Collaborate across teams including but not limited to Engineering, Operations, and Client SuccessCuriosity and drive: Demonstrate curiosity and a well-developed drive to find answers to questions that are currently being asked or haven’t yet been askedExcellent communicator: comfort explaining technical problems in person and in writingSelf-directed: Seeks responsibility, and strives for excellence. Proactively identifies problems and presents solutions.\nDesired Attributes / Qualifications\n5+ years professional full time software development experienceAdvanced knowledge and experience with Python. Experience with RubyExperience with Spark or PySpark or Map ReduceExperience with AWS or other cloud services4+ years experience with PostgreSQL or other RDBMS2+ years experience with ElasticSearchProficient in operating system concepts, specifically LinuxOutstanding coding skills, knowledge of patterns and best practices in a object oriented style"}
{"text": "skills and be able to understand business needs.\n\nrequirements with supporting teams through to executionAssist in forecast model management to effectively track performance against targets. Continuously ensure data integrity, verifying info through frequent data validation of informationSupport data needs for marketing campaign initiatives. Support key business teams through data support as needed. Support Salesforce data pulls as needed. Ensure alignment on data with identified cross-functional teams. Own requirement to deliver on specified report needs on weekly basis and other cadences as defined. Drive efficiency in data and reporting management. Cultivate relationships with cross-functional teams. Some travel may be required. Other duties as assigned. \n\nMINIMUM QUALIFICATIONS:\n\nTo perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required.\n\nProven experience as Data Analyst, Support Coordinator, IT Related Analysts/ Consultant, or similar role. Moderate understanding and practical experience with Microsoft, Salesforce, and SQLProficient in Microsoft Outlook, Power BI, Excel, Word, and PowerPointKnowledge of cable and telecommunications products and services is a plus. Strong interpersonal skillsSkilled at working effectively with cross-functional teams. Must embrace and display company principles and demonstrate an understanding of Hotwire culture. Ability to interact and support executive level leadership. Associate degree in general business and/or a minimum of 1 year of operational experienceGrammatically correct speech (in English)Must be able to travel on occasion. \n\nBENEFITS:\n\nHotwire Communications has paved the way in fiber optic telecommunications for over two decades, offering our partners high-speed internet, cable, phone, and security service. As we expand nationwide, we are looking for innovators who are passionate about technology and serving their community. And since our employees care about their work, we make sure we take care of them with:\n\nComprehensive Healthcare/Dental/Vision Plans401K Retirement Plan with Company MatchPaid Vacation, Sick Time, and Additional HolidaysPaid Volunteer TimePaid Parental LeaveHotwire Service DiscountsEmployee Referral BonusesExclusive Entertainment Discounts/Perks"}
{"text": "experience and development skills in the Judicial Sector as an AWS Data Engineer? CGI is seeking an AWS Data Engineer who can bring in expertise and industry best practices define better development and Engineering approaches. This is an exciting opportunity to augment your current skills, as well as learn new technologies.\n\nIf you are looking for a new challenge and want to make a difference in the Judicial Sector, this role is for you.\n\nYour future duties and responsibilities\n\nOur AWS Data Engineer will be a key contributor with the below Qualifications To Be Successful In This Role\n\n Any Applicants should have hands on experience with AWS services such as Glue, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM. Proficient in Python, including data wrangling. Experience with Data application development and version control systems such as Git. Experience in implementing data ingestion processes incorporating ETL processes. Experience in data modeling and relational database design Knowledge of application development lifecycles, & continuous integration/deployment practices. 3-5 years' experience delivering and operating large scale, highly visible distributed systems. Knowledge of IAC using terraform is preferred.\n\nDesired qualifications and skills of our AWS Data Engineer include:\n\n Agile development experience Knowledge of DevOps practices Experience working with the Atlassian toolset Experience with DynamoDB or other NoSQL databases; Redshift API design; API Gateway Experience ElasticSearch/OpenSearch Experience\n\n#CGIFederalJob\n\nCGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to: skill set level; experience and training; and licensure and certifications. CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $84,000 - $161,600.\n\nInsights you can act on\n\nWhile technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.\n\nWhen you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees “members” because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.\n\nAt CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.\n\nReady to become part of our success story? Join CGI — where your ideas and actions make a difference.\n\nQualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics.\n\nCGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned.\n\nWe make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.\n\nAll CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.\n\nCGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information."}
{"text": "experience with the Refactor the Macro code from local Python/R implementation to Databricks (Python/Pyspark) Analytical expert who utilize his/her skills in both technology and social science to find trends and manage data.They use industry knowledge, contextual understanding, skepticism of existing assumptions – to uncover solutions to business challengesCollecting, analysis and clean up dataCreating algorithms for processing catalog products using different data sourcesExperimenting with different models and neural networks, creating model ensemblesCreating a workflow for publishing algorithms to productionStrong skills in a machine and/or deep learning algorithms, data cleaning, feature extraction, and generationDemonstrated computational skills and experience with PythonExperience executing and presenting independent analysis Must have skills:Python(Programming Language)R (Programming Language)PySparkDatabricks"}
{"text": "skills to produce analyses in various topics including, but not limited to, customer segmentation, campaign performance, marketing attribution, return on investment/ad spend, and customer lifecycle.Interact with multiple functional teams across the organization to identify potential issues, implement solutions, improve throughput, and automate recurring processes.Write queries and build analyses on an ad hoc basis to answer strategic stakeholder questions, balancing short term solutions with long term operations.Build processes and dashboards to address ongoing and repetitive business needs, working with data engineering, data product managers, and other teammates as needed.\n\n\nQualifications\n\nWhat you’ll bring: \n\n8+ years of hands-on analytic experience in the financial services industry, specifically in consumer lending, with fintech preferred.Strong experience working with both internally owned digital acquisition channels (paid social, paid search, display, etc.) and external lead generation channels (partners, affiliates, etc.), and understanding measurement of each channel’s relative and absolute efficiency and effectiveness.Advanced SQL skills and experience working with reporting tools such as Tableau, Looker or othersExperience in Python or R is preferred but not requiredStrong understanding on how the data should be captured, stored and structured in data warehousing environmentAn effective cross functional communicator in both written (PowerPoint or Google Slides) and verbal skills with experience working with executive-level stakeholdersAbility and willingness to learn in a fast-paced environment and adapt to various situations and changing prioritiesCollaborate with Data Science and Data Engineering teams to implement predictive models into various stages of marketing funnels,Experience with data analytics and statistical modeling concepts such as multivariate regression, segmentation modeling, optimization, and forecasting is a plusBachelor’s degree or better in a quantitative field such as Mathematics, Statistics, Computer Science, Economics or equivalent\n\n\nAdditional Information\n\nAchieve well-being with:\n\nHybrid and remote work opportunities401 (k) with employer matchMedical, dental, and vision with HSA and FSA optionsCompetitive vacation and sick time off, as well as dedicated volunteer daysAccess to wellness support through Employee Assistance Program, Talkspace, and fitness discountsUp to $5,250 paid back to you on eligible education expensesPet care discounts for your furry friendsFinancial support in times of hardship with our Achieve Care FundA safe place to connect and a commitment to diversity and inclusion through our six employee resource groups\n\n\nWork from home/hybrid:\n\nWe are proudly offering hybrid options in the Phoenix, AZ or San Mateo, CA metro markets. In other locations throughout the country, we offer work from home in the following states: WA, OR, NV, UT, TX\n\nSalary Range: $160,000 to $175,000 annually + bonus + benefits. This information represents the expected salary range for this role. Should we decide to make an offer for employment, we'll consider your location, experience, and other job-related factors.\n\nJoin Achieve, change the future.\n\nAt Achieve, we’re changing millions of lives.\n\nFrom the single parent trying to catch up on bills to the entrepreneur needing a loan for the next phase of growth, you’ll get to be a part of their journey to a better financial future. We’re proud to have over 3,000 employees in mostly hybrid and 100% remote roles across the United States with hubs in Arizona, California, and Texas. We are strategically growing our teams with more remote, work-from-home opportunities every day to better serve our members. A career at Achieve is more than a job—it’s a place where you can make a true impact, have a sense of belonging, establish a fulfilling career, and put your well-being first.\n\nAttention Agencies & Search Firms: We do not accept unsolicited candidate resumes or profiles. Please do not reach out to anyone within Achieve to market your services or candidates. All inquiries should be directed to Talent Acquisition only. We reserve the right to hire any candidates sent unsolicited and will not pay any fees without a contract signed by Achieve’s Talent Acquisition leader.\n\nCompany Description\n\nMeet Achieve\n\nAchieve is a leading digital personal finance company. We help everyday people move from struggling to thriving by providing innovative, personalized financial solutions. By leveraging proprietary data and analytics, our solutions are tailored for each step of our member's financial journey to include personal loans, home equity loans, debt consolidation, financial tools and education. Every day, we get to help our members move their finances forward with care, compassion, and an empathetic touch. We put people first and treat them like humans, not account numbers.\n\nAttention Agencies & Search Firms: We do not accept unsolicited candidate resumes or profiles. Please do not reach out to anyone within Achieve to market your services or candidates. All inquiries should be directed to Talent Acquisition only. We reserve the right to hire any candidates sent unsolicited and will not pay any fees without a contract signed by Achieve’s Talent Acquisition leader."}
{"text": "requirements to concrete solutions for exploring data, designing and/or applying appropriate algorithms, documenting the findings, and incorporating the analysis into end-to-end solutions, systems, and platforms. Effective communication with other job disciplines is required. Contributions are expected at a level of results above and beyond entry-level and mid-level Data Scientists.\n\nKey Duties & Responsibilities\n\nHave a wider impact by providing insights and effective leadership into data science, digital media, and data engineering. This individual will have the hands-on skills to be an individual contributor and the experience for mentoring and leading other data scientists (25%)Act often as a technical lead, determining approach, objectives, requirements, features, milestones, implementation tasks, and tradeoffs of end-to-end large scale data science projects, platforms, and systems (25%)Act as a subject matter expert in data science (ML/AI) algorithms and underlying technologies (programming languages and systems) (15%)Design, conduct, and incorporate analyses of large-scale data from a wide variety of sources (15%)Work within the scrum practices in team projects (10%)Contribute to hiring process by screening higher level candidates, team interviews, manager candidates, i.e., act as a \"Bar Raiser\" (10%)\n\nQualifications\n\nEducation\n\nBachelor's Degree in a quantitative discipline (Computer Science, Mathematics, Engineering, Statistics) (Required)Master's Degree in a quantitative discipline (Computer Science, Mathematics, Engineering, Statistics) (Desired)Doctorate Degree (Preferred)In lieu of the above education requirements, a combination of experience and education will be considered.\n\nExperience\n\n8 - 10 years Relevant Experience (Required)\n\nKnowledge/Skills/Abilities\n\nStrong analytical skills, with expertise and solid understanding of multiple statistical/analytical machine learning techniques applied at large scale.Technical proficiency in ML algorithms, scalable ML platforms, languages, and tools (Python, Spark, ML/Ops) in a corporate setting is highly desirable.Ability to communicate effectively across multi-disciplinary teams (e.g., data science, engineering and product management, org leadership).Prior experience in applying Data Science in Digital Marketing Technology, Graph Theory, Privacy and Geolocation Data is a plus.\n\nAdditional Information\n\nSalary:$160,000-175,000\n\nThe ultimate compensation offered for the position will depend upon several factors such as skill level, cost of living, experience, and responsibilities.\n\nVericast offers a generous total rewards benefits package that includes medical, dental and vision coverage, 401K and flexible PTO. A wide variety of additional benefits like life insurance, employee assistance and pet insurance are also available, not to mention smart and friendly coworkers!\n\nAt Vericast, we don’t just accept differences - we celebrate them, we support them, and we thrive on them for the benefit of our employees, our clients, and our community. As"}
{"text": "Requirements\n\n0-10% overnight travel required.\n\nQualifications\n\n Bachelor’s Degree from an Accredited University and 6 years of experience from relevant industry (life sciences or technology research) is required.  Advanced degree in Data Science or related field strongly preferred.  Excellent organizational, communication, and presentation skills.  Advanced skills in text mining, data mining, information visualization concepts and tools is required.  Experience with cloud analytics platforms & tools (AWS platform & services, R, Python, SQL) is preferred.  Experience researching and implementing Machine Learning & Deep Learning models is required.  Deep expertise in interpreting and explaining data trends is required.  Subject matter familiarity in one or more disease areas of interest to Novo Nordisk is preferred. \n\nWe commit to an inclusive recruitment process and equality of opportunity for all our job applicants.\n\nAt Novo Nordisk we recognize that it is no longer good enough to aspire to be the best company in the world. We need to aspire to be the best company for the world and we know that this is only possible with talented employees with diverse perspectives, backgrounds and cultures. We are therefore committed to creating an inclusive culture that celebrates the diversity of our employees, the patients we serve and communities we operate in. Together, we’re life changing.\n\nNovo Nordisk is \n\nIf you are interested in applying to Novo Nordisk and need special assistance or an accommodation to apply, please call us at 1-855-411-5290. This contact is for accommodation requests only and cannot be used to inquire about the status of applications."}
{"text": "Data AnalystDakota Dunes, SD\nEntry Level SQL, Run SQL The queries. Client is using ThoughtspotUnderstanding of Dashbord and Proficient in Microsoft Office and excel \nPlease share your profile to Lokesh.Kumar@infovision.com or reach me on 619 771 1188."}
{"text": "Requirements\n\nAssociate’s degree and three or more years of data analytics, or related; or equivalent combination of education and experience.\n\n\nCompetencies\n\nStrong technical skills, including but not limited to: SQL, Microsoft Excel & Access, General Database Administration, General Programing knowledge.Database management.Proficiency with both technical and non-technical communication.Excellent organizational skills, with a focus on accuracy and comprehension.\n\n\nPreferred, But Not Required\n\nUndergraduate degree in related area of study.\n\n\nThanks for considering Old Second!"}
{"text": "Qualifications\n\nEither a PhD in a quantitative subject area (computer science, mathematics, statistics, economics, physics, engineering, or related field), orAn MS degree in a quantitative field plus 3+ years of professional experience in optimization, machine learning, statistics, exploratory data analysis, and other aspects of the data science processStrong theoretical background in and practical experience using optimization, statistical techniques, and machine learning, preferably shown through academic publications or open-source codebases.Strong familiarity with programming languages such as Python, Julia, or R.Experience with the analysis or application of data in finance, economics, sociology, or related fields is a plus.\n\nFor California and New York City only the salary range for this position is $157,500 - $214,500. Additionally, employees are eligible for an annual discretionary bonus, and benefits including heath care, leave benefits, and retirement benefits. BlackRock operates a pay-for-performance compensation philosophy and your total compensation may vary based on role, location, and firm, department and individual performance.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock’s hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person – aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children’s educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment – the one we make in our employees. It’s why we’re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity and Affirmative Action Employer. We evaluate qualified applicants without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, disability, protected veteran status, and other statuses protected by law.\n\nWe recruit, hire, train, promote, pay, and administer all personnel actions without regard to race, color, religion, sex (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), sex stereotyping (including assumptions about a person’s appearance or behavior, gender roles, gender expression, or gender identity), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. We interpret these protected statuses broadly to include both the actual status and also any perceptions and assumptions made regarding these statuses.BlackRock will consider for employment qualified applicants with arrest or conviction records in a manner consistent with the requirements of the law, including any applicable fair chance law."}
{"text": "experience.Required Skills: ADF pipelines, SQL, Kusto, Power BI, Cosmos (Scope Scripts). Power Bi, ADX (Kusto), ADF, ADO, Python/C#.Good to have – Azure anomaly Alerting, App Insights, Azure Functions, Azure FabricQualifications for the role 5+ years experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Specific experience working with COSMOS and Scope is required for this role. Experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases is a plus. Experience with investigating and on-boarding new data sources in a big-data environment, including forming relationships with data engineers cross-functionally to permission, mine and reformat new data sets. Strong analytic skills related to working with unstructured data sets. A successful history of manipulating, processing and extracting value from large disconnected datasets."}
{"text": "experience:\n\n Software Development for Infrastructure as Code (IaC) with Python or TypeScript: Engineer will need to create and manage infrastructure components using code. Terraform and CloudFormation (CDKTF): Engineer will orchestrate infrastructure provisioning and management. Snowflake Object Creation: Engineer will write code to create and manage Snowflake objects. Public Cloud Integration: must be able to Deploy Snowflake on a public cloud provider (e.g., AWS, Azure, Google Cloud).\n\n\n Desired (yet Not Required) Qualifications \n\nAWS: Candidates must have AWS engineering experience to build environments and deploy applications in AWS Services such as: Lambdas, API Gateway, AMIs, EC2, S3, Kinesis, Event Bridge, Cloudformation, Lambdas SNS.\n\nData: Experience with algorithms and data structures. Has experience resolving issues with scalability, low latency or receiving real time data retrieval from multiple sources at once.\n\nKnowing the trade-offs between different data storage systems and architectures (data warehouses, SQL vs NoSQL, partitioning, etc.)\n\n Responsibilities \n\nDesign and implement reliable, high-throughput, low latency, scalable and well-documented code to tackle sophisticated algorithms and build cloud infrastructures components.\n\nCollaborate with engineers, data scientists, and other partners to make proposals across teams on their engineering work and practices.\n\nDesign low-level implementation details of software, which data structures, class/function/code organization, etc.\n\nPropose technical implementation approaches which support architectural changes that address scaling and performance problems.\n\nProactively seek opportunities to reuse or generalize existing services and implementations.\n\nKnow the latest technologies and best approaches in data management and software engineering.\n\nProvide technical mentorship and guidance to other members of the team.\n\nConduct code review pull-requests with consistent scrutiny.\n\nThis role is a hybrid schedule in either: Seattle, Anaheim, or Orlando, FL\n\n About TEKsystems \n\nWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.\n\nThe company is"}
{"text": "experiences. We own and operate leading entertainment and news brands, including NBC, NBC News, MSNBC, CNBC, NBC Sports, Telemundo, NBC Local Stations, Bravo, USA Network, and Peacock, our premium ad-supported streaming service. We produce and distribute premier filmed entertainment and programming through Universal Filmed Entertainment Group and Universal Studio Group, and have world-renowned theme parks and attractions through Universal Destinations & Experiences. NBCUniversal is a subsidiary of Comcast Corporation.\n\nHere you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. Our Diversity, Equity and Inclusion initiatives, coupled with our Corporate Social Responsibility work, is informed by our employees, audiences, park guests and the communities in which we live. We strive to foster a diverse, equitable and inclusive culture where our employees feel supported, embraced and heard. Together, we’ll continue to create and deliver content that reflects the current and ever-changing face of the world.\n\nJob Description\n\nOur Direct-to-Consumer (DTC) portfolio is a powerhouse collection of consumer-first brands, supported by media industry leaders, Comcast, NBCUniversal, and Sky. When you join our team, you’ll work across our dynamic portfolio including Peacock, NOW, Fandango, SkyShowtime, Showmax, and TV Everywhere, powering streaming across more than 70 countries globally. And the evolution doesn’t stop there. With unequaled scale, our teams make the most out of every opportunity to collaborate and learn from one another. We’re always looking for ways to innovate faster, accelerate our growth, and consistently offer the very best in consumer experience. But most of all, we’re backed by a culture of respect. We embrace authenticity and inspire people to thrive.\n\nNBCU Entertainment is looking for a motivated Lead, Data Engineering that can deliver results in a fast-paced environment. In this position, you will Analyze various data sources and pipelines, Design and build Data Engineering pipelines for NBCU Entertainment networks including NBC Entertainment, NBC Sports, Bravo, E!, Oxygen, Syfy, USA, Universo and Telemundo.\n\nThe ideal candidate is an experienced data engineer who has previous success designing, building & modernizing data transformation at a large direct-to-consumer organization. This person must have had success in building and maintaining Data Engineering pipelines to solve complex problems across areas such as advanced analytics, marketing, product, monetization & forecasting. This candidate will also be expected to deploy machine learning models at scale for consumer-facing products with millions of users in conjunction with App Support Teams.\n\nEssential Responsibilities\n\nHelp coordinate with Advanced Analytics team, work with other onsite/offshore Data Engineers and own deliverables.Build out data engineering pipelines and optimizations of a cloud-based Data Lake, Data Warehouse, and File System.Identify pipeline bottlenecks as well as opportunities for optimizations and create implementation plans.Work in an onsite and offshore team model to build out ETLs to support Business Intelligence initiatives, including real-time processing, distributed computing, & containerized solutions.Help in Data QA and ensure required auditing and alerts are implemented for every data engineering pipeline.Create, define, and document processes & help in data governance and estimation initiatives within the organization.\n\nQualifications\n\nBachelors or Masters degree required from a quantitative field from such as Data Science, Engineering, Mathematics or Statistics10+ years of applied experience in Data Engineering, including but not limited to building Data Pipelines, Orchestration, Data Modeling & Lakehouse.Experience with processing large datasets (100s of TBs) and building code using SQL, Python, pySpark & Airflow.Strong working experience with variety of data sources such as APIs, real-time feeds, structured and semi structured file formats.Strong Experience working on AWS cloud services like S3, EC2, EMR & Secrets and cloud platforms like Snowflake Datawarehouse & DatabricksExperience implementing CI/CD pipelines for Data Engineering with GIT and/or similar repo tools.Expert level experience in distributed processing frameworks such as Spark and/or Hadoop with skills for building and optimizing pipelines on Databricks or similar platforms.Knowledge and/or experience integrating 1st, 2nd and 3rd party data and using data clean rooms technology & policies.Strong interpersonal skills and ability to partner and earn the trust of stakeholders.Experience in large media, technology, or other direct to consumer business driven companies.\n\nHybrid: This position has been designated as hybrid, generally contributing from the office a minimum of three days per week.\n\nThis position is eligible for company sponsored benefits, including medical, dental and vision insurance, 401(k), paid leave, tuition reimbursement, and a variety of other discounts and perks. Learn more about the benefits offered by NBCUniversal by visiting the Benefits page of the Careers website. Salary range: $130,000 - $170,000, Bonus eligible\n\nAdditional Information\n\nIf you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations by emailing AccessibilitySupport@nbcuni.com."}
{"text": "skills, data manipulation capabilities and business insight. Define, instrument and maintain metrics and build dashboards. Proactively identify impactful opportunities and autonomously implement data analysis. Be a bridge between business and digital organizations, combining in-depth understanding of both areas. If needed, build and maintain data pipelines to unblock your analysis. Adhere to and advocate for data analytics standard processes Present results to peers and senior management, influencing decision making Mentor others. \n\nQualifications\nEssential\n\nMSc in a quantitative field, preferably statistics. Hands-on experience (typically 5+ years) carrying out data analytics, data mining and product analytics in complex, fast-paced environments. Applied knowledge of data analytics and data pipelining tools and approaches across all data lifecycle stages. Thorough understanding of underlying mathematical foundations of statistics, including knowledge of experimental design and analysis and causal inference from observational data. Expert SQL knowledge Advanced scripting experience in R or python. Ability to write and maintain moderately complex data pipelines Customer-centric and pragmatic mentality. Focus on value delivery and swift execution, while maintaining attention to detail. Good communication and collaborator management skills. Ability to lead large organizations through influence. Continuous learning and improvement attitude. \n\nDesired\n\nAdvanced analytics degree Experience with big data technologies (e.g. Hadoop, Hive, and Spark) is a plus. No prior experience in the energy industry required. \n\nWhy join us\n\nAt bp, we support our people to learn and grow in a diverse and exciting environment. We believe that our team is strengthened by diversity. We are committed to fostering an inclusive environment in which everyone is respected and treated fairly.\n\nThere are many aspects of our employees’ lives that are important, so we offer benefits to enable your work to fit with your life. These benefits can include flexible working options, a generous paid parental leave policy, and excellent retirement benefits, among others!\n\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n\nTravel Requirement\n\nNegligible travel should be expected with this role\n\nRelocation Assistance:\n\nThis role is not eligible for relocation\n\nRemote Type:\n\nThis position is a hybrid of office/remote working\n\nSkills:\n\nCommercial Acumen, Commercial acumen (Inactive), Communication, Data Analysis, Data cleansing and transformation, Data domain knowledge, Data Integration, Data Management, Data Manipulation, Data Sourcing, Data strategy and governance, Data Structures and Algorithms, Data visualization and interpretation, Digital Security, Extract, transform and load, Group Problem Solving\n\nLegal Disclaimer:\n\nWe are \n\nIf you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks."}
{"text": "Skills/Domain: Microsoft Azure, Synapse, Spark, Python, Angular, C#, .NET, DevOps, Azure Function,Microservice/API Development, Power BIRoles and"}
{"text": "Qualifications: • Bachelor's degree in Computer Science, Information Technology, or a related field.• 10+ years of experience in data warehouse architecture and development.• Proven expertise in Microsoft Azure Data Services (ADLS, Synapse Analytics, Data Factory).• Strong understanding of data warehousing concepts, data modeling, ETL/ELT processes, and big data platforms.• Experience with data integration techniques, self-service data preparation, and DevOps tools (Azure DevOps, Jenkins, etc.).• Excellent communication and presentation skills to collaborate effectively with technical and non-technical stakeholders.• Strong analytical skills and a passion for learning new technologies.• Ability to work independently and as part of a team, prioritizing workload effectively."}
{"text": "experience for both buyers and sellers. In this role, you would partner closely with the Etsy Ads group, whose mission is to empower Etsy sellers to accelerate and sustain their growth through advertising.\n\nThis is a full-time position reporting to the Senior Manager, Product Analytics. In addition to salary, you will also be eligible for an equity package, an annual performance bonus, and our competitive benefits that support you and your family as part of your total rewards package at Etsy.\n\nThis role requires your presence in Etsy’s Brooklyn Office once or twice per week depending on your proximity to the office. Candidates living within commutable distance of Etsy’s Brooklyn Office Hub may be the first to be considered. Learn more details about our work modes and workplace safety policies here.\n\nWhat’s this team like at Etsy?\n\nData scientists at Etsy use rigorous methods to generate insights that inform product, engineering, and business decisions across the company. We collaborate with partner teams through all stages of development: actively uncovering opportunity areas, crafting experiments to test hypotheses, analyzing the impact of our efforts, and highlighting takeawaysLearning new skills and techniques is not only a requirement but a perk of the job! We are always looking for opportunities to grow. Our mission is to guide our partner teams with data and insights and tell the story of how we attract and retain our users – to teams, to senior management, and to the community\n\nWhat does the day-to-day look like?\n\nWork closely and collaboratively with management within the Product org to help shape Etsy’s strategy and visionConduct analysis on buyers’ and sellers’ behavior, helping us better optimize the features that are most important to our membersDesign and analyze rigorous experiments, help teams set great hypotheses, and deliver robust analysis of experiment resultsTransform raw data into important and impactful analysis characterized by strong data governance, technique clarity, and clear documentationImprove or automate internal analytics processes to drive efficiency Of course, this is just a sample of the kinds of work this role will require! You should assume that your role will encompass other tasks, too, and that your job duties and responsibilities may change from time to time at Etsy's discretion, or otherwise applicable with local law.\n\nQualities that will help you thrive in this role are:\n\n2+ years experience as a data scientist or data analyst during which you extracted insights from large datasetsExperience in A/B experimentation and statistical analysis of experimental dataMastery of SQL, and experience with R/Python and other scripting/automation techniques. Bonus points for experience with Looker, Tableau, or other data visualization softwareCurious mindset to drive creative problem-solving and business impact Proficiency in causal inference analysis is strongly preferredExperience in an e-Commerce setting is a plus\n\nAdditional Information\n\nWhat's Next\n\nIf you're interested in joining the team at Etsy, please share your resume with us and feel free to include a cover letter if you'd like. As we hope you've seen already, Etsy is a place that values individuality and variety. We don't want you to be like everyone else -- we want you to be like you! So tell us what you're all about.\n\nOur Promise\n\nAt Etsy, we believe that a diverse, equitable and inclusive workplace furthers relevance, resilience, and longevity. We encourage people from all backgrounds, ages, abilities, and experiences to apply. Etsy is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to"}
{"text": "Requirements:Solid technical expertise, especially in data processing and exploration, with a keen interest in staying abreast of emerging technologies.A fervent commitment to automation and continuous improvement, demonstrated by a history of identifying valuable automation opportunities.Proficiency in recognizing patterns and establishing standards to streamline development processes and enhance reliability.Strong interpersonal skills, fostering positive and collaborative relationships across teams and locations.Methodical and systematic problem-solving approach.\nPreferred Technologies:Proficiency in one or more programming languages such as Python, C#, Scala, Java, or Go.Experience with various data storage and manipulation tools including SQL, Pandas, Elasticsearch & Kibana, and Snowflake.Familiarity with containerization and orchestration technologies like Docker, Kubernetes, Helm, and Flux.Exposure to ETL/ELT technologies such as Airflow, Argo, Dagster, Spark, and Hive.\nAdditional Beneficial Skills:Familiarity with data visualization tools.Experience with stream processing platforms like Apache Kafka.Knowledge of cross-asset financial markets, including Equities, FX, Options, Futures, and Fixed Income."}
{"text": "experience.Collaborate with other solution and functional teams (e.g., commercial operations, professional services, clinical education, financial administration) to find practical and ambitious solutions to these gaps and aspirations.Identify critical success metrics with which to gauge the relative performance and progress of our managed service customers over time.\n\nYou're the right fit if: \nYou’ve acquired 7+ years of experience in programming, data visualization, and healthcare informatics experience as well as knowledge of physiologic monitoring systems.Your skills include proficiency with R and/or Python libraries commonly used in data science, Python programming experience, knowledge and understanding of hospital data flows such as CPOE, EMR, RIS, LIS and PACS and related data format standards (e.g., HL7, DICOM, FHIR and IHE), healthcare terms and classifications (SNOMED CT, ICD10), Azure or AWS DevOps, GIT, PowerBI, Power Query M, Visual Basic for Applications, and a high affinity with applying new IT platforms/dash boarding software tools for reporting and experience. You have a Master’s in Computer Sciences, Biomedical Engineering, Bioinformatics, or a related field preferred OR 10 years of related work experience.You must be able to successfully perform the following minimum Physical, Cognitive and Environmental job requirements with or without accommodation for this position. You also have the ability to work with cross-functional teams, are self-motivated, committing to results, are flexible and a quick-learner. You must have excellent verbal and written communication, the ability to manage complex projects and demonstrated operational analytics/financial analysis capabilities.\n\nAbout Philips\n\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help improve the lives of others.\n\nLearn more about our business.Discover our rich and exciting history.Learn more about our purpose.Read more about our employee benefits.\n\nIf you’re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion here.\n\nAdditional Information\n\nUS work authorization is a precondition of employment. The company will not consider candidates who require sponsorship for a work-authorized visa, now or in the future.\n\nCompany relocation benefits will not be provided for this position. For this position, you must reside in or within commuting distance to locations listed.\n\nThis requisition is expected to stay active for 45 days but may close earlier if a successful candidate is selected or business necessity dictates. Interested candidates are encouraged to apply as soon as possible to ensure consideration.\n\nPhilips is an Equal Employment and Opportunity Employer/Disabled/Veteran and maintains a drug-free workplace."}
{"text": "requirements and industry practices.Build high-performance algorithms, prototypes, predictive models, and proof of concepts.Research opportunities for data acquisition and new uses for existing data.Lead and develop data set processes for data modeling, mining, and production.Direct and integrate new data management technologies and software engineering tools into existing structures.Employ a variety of techniques and tools to merge multiple data sets in a centralized data repository.Recommend and execute ways to improve data reliability, efficiency, and quality.Manage projects, resources, internal customer expectations, and business priorities to achieve customer satisfaction.Collaborate with data architects, modelers, and IT team members on project goals.Adhere to all company policies and procedures, including Information Security Policies and ensure that AMSURG remains as secure as possible.Regular and reliable attendance is required.\n\nKnowledge And Skills\n\nTo perform this job successfully, an individual must be able to perform each essential responsibility satisfactorily. The requirements listed below are representative of the knowledge, skills and/or abilities required:\n\nExcellent quantitative and analytical skills as well as the ability to translate findings into meaningful information appropriate to the audience/stakeholder.High level of comfort with many types of data including financial, quality, clinic, and security.Relational database training and data modeling skills. Must demonstrate a history of project management, technology investigation, technology implementation, and technology oversight in various capacities.Ability to be a self-starter that can provide leadership, managing and mentoring team members.Strong ability to understand and analyze user requirements as they relate to organizational goals and objectives.Strong attention to detail with the ability to work under deadlines and switch quickly and comfortably between projects, as business needs dictate.Superior written and oral communication skills.Strong interpersonal skills with the ability to effectively collaborate across teams.Strong work ethic and ability to work autonomously in a high production environment.Ability to work independently and prioritize work appropriately.Strong communication skills, with experience presenting to executive and senior leadership teams.\n\nEducation/Experience\n\nBachelor's Degree from a four-year College or University, or equivalent combination of education and software development experience.Experience in Azure Data Factory and SSIS.Extensive experience with Microsoft SQL Server.Advanced knowledge of relational database principles including SQL and MS-Office products.Advanced / Power user of Excel.Demonstrated presentation skills working with PowerPoint, with ability to tell a data story to executive leadership. Comprehensive understanding of the Agile Development process.\n\nWe are \n\nMust pass a background check and drug screen.\n\nWe do not discriminate in practices or employment opportunities on the basis of an individual's race, color, national or ethnic origin, religion, age, sex, gender, sexual orientation, marital status, veteran status, disability, or any other prohibited category set forth in federal or state regulations."}
{"text": "requirements and design solutions that include go-to-market metrics tracking, analyzing telemetry data, and building models for BI Reporting dashboards.Regularly use SDF CLI and Console for day-to-day tasks and play a pivotal role in testing new features. Engage with our engineering and product management teams in the conceptualization, rapid prototyping, and launch of innovative features and functionalities for SDF.\nWe’d love to hear from you if you have:\nBachelors or Masters Degree in Computer Science or associated area of studiesAt least 2 years of experience of Data Engineering (strong fundamentals with ELT pipelines, workflow automation, and data quality/governance)1+ years of experience with AWS RedShift, Snowflake, or GCP BigQueryExpertise in applying Python and SQL to execute complex data operations, customize ETL/ELT processes, and perform advanced data transformations across the platform.Expertise in metric definitions, and unificationInterest in the data domain, especially knowledge of the general data landscape and other tooling/providersExcellent written and verbal communication skills: Ability to effectively communicate technical concepts to both technical and non-technical team membersHighly self-motivated and enjoys self-directed learning.\nGreat team, great benefits:\nHealth Insurance - We provide medical, dental and vision benefits to employees and their dependents. Health benefits are paid in part by the company.Retirement Plan - Every employee has the option to contribute to a 401k plan because we care about the future of our employees.Hybrid Work Environment - Our hybrid work environment blends office collaboration with the comfort of home, offering flexibility and fun in your work life!Mandatory Vacation - Everyone at a startup works hard. We expect all employees to take 2 solid weeks of paid time off each year.Trust & Flexibility - Trust is key for us. Need a break to grab coffee? Go for it. Need to leave early to grab the kids from school? No problem. Talk to us, get your stuff done, and don't forget to live your life.Learning and Development - We believe in investing in the development of our team. We provide coaching and mentorship opportunities to every employee.Skill Acquisition - On our small and collaborative team, there is opportunity to learn all aspects of development, including specialized languages/softwares such as Rust and Kubernetes.\nMore About Us:You can read more about us at https://sdf.com. You can also find us on Linkedin, or Twitter, or our Blog.\nNo agencies please. SDF Labs is"}
{"text": "Qualifications\nDegree 1-3 Years of Experience (industry experience required for years) or Ph.D. Degree 0-2 Years of Experience (in school experience will be considered)with scientists to define/understand work and data pipelines in-labBenchling protocols and templates to capture necessary data and align across teams.Have coding experience SQL, Python, and LIMS Lab Information Systemexperience, industry setting (biotech)Experience (or Gene Data or comparable), Bench Experience in Molecular Biology"}
{"text": "requirements and options available within customer source systems to meet the data and business requirements. Become a Subject Matter Expert in existing solutions.Analyze business needs and align data integration solutions to support i2i Population Health data architecture and strategy. Knowledge and Skills Proficiency with SSMSKnowledge of healthcare data terminology (LOINC, CPT, ICD-10, etc.) preferred.A working knowledge of Electronic Health Record software solutions, e.g., NextGen, Allscripts, Epic, Cerner, etc., is preferred.Strong Internet and computer literacy skills in Microsoft Office (Word, PowerPoint, and Excel)Proficiency in communicating with a variety of stakeholders and customers. Experience:2 -4 years’ experience with relational databases (MS SQL, Oracle, Postgres, MySQL).Experience with Git and Salesforce preferred.Healthcare and/or software product company software experience is preferred. Education:Bachelor’s Degree preferred, or deep and wide industry experience in lieu of a degree. i2i Population Health Offers:Great coworkers who describe our award-winning culture as collaborative, friendly, fun, and supportive! Remote/Hybrid work environmentA relaxed work environment, with flexibility and a work-life balance focus.Competitive Compensation and Benefits PackageUnlimited PTO"}
{"text": "requirements, ultimately driving significant value and fostering data-informed decision-making across the enterprise.\n\nAdditional Information\n\nJob Site: Atlanta, GA40 hours/weekEligible for Employee Referral Program: $1500If offered employment must have legal right to work in U.S. \n\nYou Must Have\n\nQualified applicants must have a Master’s degree or foreign equivalent in Business Analytics, Data Science, Statistics, Applied Mathematics, or related field and five (5) years of IT experience. Full term of experience must include: data science, machine learning; commercial analytics; and implementing advanced analytical solutions in a business context. Must possess (2) two years of experience in the following: managing analytics projects and interfacing with internal / external project stakeholders; advanced programming skills in Python and SQL; big data technologies, including Hadoop and Spark; on-the-job experience developing, validating, and deploying a wide variety of machine learning algorithms; applying advanced statistical methods (Bayesian inference), multivariate regression, time-series analysis, and deep learning; Advanced skills in Tableau and Power BI to create data visualizations; effectively communicating complex analytical findings to both technical and non-technical stakeholders; basic knowledge of Cloud Computing platforms, including AWS, Azure, or Google Cloud, and their respective data storage, processing, and machine learning services. Telecommuting permitted up to two (2) times per week.\n\nAdditional Information\n\nJOB ID: req443422Category: EngineeringLocation: 715 Peachtree Street, N.E.,Atlanta,Georgia,30308,United StatesExempt\n\nHoneywell is"}
{"text": "experience in AI applications for the Hydrocarbon Processing & Control Industry, specifically, in the Gas Processing and Liquefaction business. Key ResponsibilitiesYou will be required to perform the following:- Lead the development and implementation of AI strategies & roadmaps for optimizing gas operations and business functions- Collaborate with cross-functional teams to identify AI use cases to transform gas operations and business functions (AI Mapping)- Design, develop, and implement AI models and algorithms that solve complex problems- Implement Gen AI use cases to enhance natural gas operations and optimize the Gas business functions- Design and implement AI-enabled plant optimizers for efficiency and reliability- Integrate AI models into existing systems and applications- Troubleshoot and resolve technical issues related to AI models and deployments- Ensure compliance with data privacy and security regulations- Stay up-to-date with the latest advancements in AI and machine learning As a Gas Processing AI Engineer, you will play a crucial role in developing, implementing, and maintaining artificial intelligence solutions that drive business growth and optimized operations. You will collaborate with cross-functional teams to understand business requirements, map new AI trends to address business challenges / opportunities, design AI models, and deploy such models in the gas plants. The ideal candidate should have a strong background in AI and machine learning with hands-on programming and problem-solving skills.  Minimum Requirements\nAs a successful candidate, you must have a Bachelor's or Master's degree in Chemical Engineering with (10) years of experience in the Oil/Gas industry and significant hands-on experience of AI applications in the Gas Industry.Preferred Qualifications:- PhD or Master's degree in Chemical Engineering- Minimum 10 years of experience in Oil & Gas Industry- Minimum 5 years of Hands-on experience in implementing successful AI projects in the Gas Processing sector- Strong programming skills in Python, TensorFlow, and PyTorch- Experience with reinforcement learning and generative AI (LLM) models- Experience with natural language processing (NLP) and AI Computer Vision- Excellent communication and leadership abilitiesRequirements:- Bachelor's or Master's degree in Chemical Engineering with demonstrated hand-on experience in AI applications and projects- Proven work experience as a Gas Processing AI Engineer or in a similar role- Strong knowledge of machine learning algorithms, neural networks, and deep learning frameworks (e.g., TensorFlow, PyTorch)- Strong knowledge of plant networks and infrastructure requirements to deploy and scale AI in gas plants- Proficiency in programming languages such as Python, Java, or C++- Excellent problem-solving and analytical skills- Strong communication and teamwork abilities- Ability to work on multiple projects and prioritize tasks effectivelyMinimum Years of Experience :09"}
{"text": "experienced Data Engineer to join our world leading footwear client. The ideal candidate will have 6-7 years of relevant experience, with a focus on practical application in AWS tech stack. Experience with Databricks, Spark, and Python for coding is essential.\n\nW2 ONLY, NO C2C*\n\n\nKey Qualifications:\n\nBachelor’s degree in Computer Science or related field.6-7 years of data engineering experience.Proficiency in AWS, Databricks, Spark, and Python.Ability to work in complex environments with diverse projects.Strong communication and collaboration skills.\n\n\nMainz Brady Group is a technology staffing firm with offices in California, Oregon and Washington. We specialize in Information Technology and Engineering placements on a Contract, Contract-to-hire and Direct Hire basis. Mainz Brady Group is the recipient of multiple annual Excellence Awards from the Techserve Alliance, the leading association for IT and engineering staffing firms in the U.S.\n\nMainz Brady Group is"}
{"text": "Experience: Minimum 2-3 years of relevant experienceEmployment Type: W2/1099 position with visa sponsorship provided for successful candidates\nrequirements and objectivesDevelop and maintain data models, dashboards, and reports to support business decision-makingIdentify trends, patterns, and anomalies in data to inform strategic initiativesUtilize statistical techniques and predictive modeling to drive data-driven solutionsCommunicate findings and recommendations to stakeholders through visualizations and presentationsAssist in the design and implementation of data collection processes and toolsConduct quality assurance checks to ensure data accuracy and integrityStay up-to-date with industry trends and best practices in data analysis and visualization\nQualifications:Bachelor's or Master's degree in Data Science, Statistics, Mathematics, Computer Science, or related fieldProficiency in SQL, Python, R, or other programming languages used for data analysisExperience with data visualization tools such as Tableau, Power BI, or matplotlibStrong analytical and problem-solving skills with a keen attention to detailExcellent communication and collaboration abilities to work effectively with cross-functional teamsFamiliarity with machine learning algorithms and techniques is a plusAbility to work independently and manage multiple priorities in a fast-paced environmentUnderstanding of data governance and privacy regulations"}
{"text": "Qualifications and Skills Education: Bachelor's degree in Computer Science or a related field. Experience: 5+ years in Software Engineering with a focus on Data Engineering. Technical Proficiency: Expertise in Python; familiarity with JavaScript and Java is beneficial. Proficient in SQL (Postgres, Presto/Trino dialects), ETL workflows, and workflow orchestration systems (e.g. Airflow, Prefect). Knowledge of modern data file formats (e.g. Parquet, Avro, ORC) and Python data tools (e.g. pandas, Dask, Ray). Cloud and Data Solutions: Experience in building cloud-based Data Warehouse/Data Lake solutions (AWS Athena, Redshift, Snowflake) and familiarity with AWS cloud services and infrastructure-as-code tools (CDK, Terraform). Communication Skills: Excellent communication and presentation skills, fluent in English. Work Authorization: Must be authorized to work in the US. \nWork Schedule Hybrid work schedule: Minimum 3 days per week in the San Francisco office (M/W/Th), with the option to work remotely 2 days per week. \nSalary Range: $165,000-$206,000 base depending on experience \nBonus: Up to 20% annual performance bonus \nGenerous benefits package: Fully paid healthcare, monthly reimbursements for gym, commuting, cell phone & home wifi."}
{"text": "requirements this role is only open to USC or GC candidates***\n \n Job Summary:\n Project Details:\nCorporate treasury data horizontal team is migrating off of Oracle RDBMS to a Hadoop Infrastructure. Processing 4 billion records of treasury data per day. POCs are complete, they are now processing data in about 1 hour. \nThey need to hire this \"techno functional analyst\" not to face off with business, but more so to act like a psuedo data engineer. They will go in look and and understand data lineage, look at feed files trace it thru schemas, understand data movements, help developers do a huge migration, see where breaks are coming from as they happen, help developers understand current code and changes needed, look at legacy feeds and work them thru the new platform. dealt with large data sets in the past.\n Must Haves\nmid senior exp minimum 5-7 yrs \n data analysis, lineage not as important- wont do that for 8+ months,\n SQL at least a 4,\n rdbms,\n experience working with large data sets\n \n Desired Skills\nHadoop \nFinancial industry experience\n About Matlen Silver\n Experience Matters. Let your experience be driven by our experience. For more than 40 years, Matlen Silver has delivered solutions for complex talent and technology needs to Fortune 500 companies and industry leaders. Led by hard work, honesty, and a trusted team of experts, we can say that Matlen Silver technology has created a solutions experience and legacy of success that is the difference in the way the world works.\n \n Matlen Silver is \n If you are a person with a disability needing assistance with the application or at any point in the hiring process, please contact us at email and/or phone at: info@matlensilver.com // 908-393-8600"}
{"text": "requirements into data models supporting long-term solutions.Data Governance and Quality:Leading the initiative in establishing a data governance strategy.Implementing frameworks to ensure data quality and consistency across the data pipeline.Project Management and Scoping:Scoping new projects, setting up priorities and dependencies in collaboration with the Data Engineering Manager.Adhering to a loose version of agile project management with bi-weekly sprints.API and Web Application Development:Maintaining and optimizing existing APIs exposing data warehouse tables.Collaborating with full-stack engineers on internal web applications allowing business users to interact with the database and S3.\n\nWhat you have:\n\nBachelor's degree in Computer Science, Engineering, or a related field; or equivalent work experience.7+ years of experience in data management.Advanced knowledge of database and data warehouse systems set up and management.Advanced proficiency in SQL and Python.Experience with AWS services including RDS, S3, Lambda, and API Gateway.Experience with serverless architectures for data pipelines.Proficiency in containerization and orchestration technologies such as Docker and Kubernetes, with a solid understanding of container architecture and its role in developing scalable, efficient, and portable data pipelines and applications.Experience implementing and managing robust logging systems to monitor, troubleshoot, and optimize data operations and infrastructure.Experience with “infrastructure as code” using tools like AWS Cloud Formation or Terraform.Excellent communication and project management skills.\n\nNice to haves:\n\nHands on experience with Snowflake and dbt.Experience with on-premises to cloud migrations.Understanding of hardware development/manufacturing business logic.Keywords: AWS, dbt, SQL, Snowflake, data modeling, data warehouse, Tableau, Python\n\nWhat's in it for you: \n\nBe part of a fast paced and dynamic teamVery competitive compensation and meaningful equity!Exceptional benefits: Medical, Dental, Vision, and more!Unlimited PTO: Take all the time you need.Paid lunches, ping pong tournaments, and fun team off-sites!\n\n$208,000 - $282,000 a year\n\nSalary pay ranges are determined by role, level, and location. Within the range, the successful candidate’s starting base pay will be determined based on factors including job-related skills, experience, certifications, qualifications, relevant education or training, and market conditions. These ranges are subject to change in the future.\n\nDepending on the position offered, equity, bonus, and other forms of compensation may be provided as part of a total compensation package, in addition to comprehensive medical, dental, and vision coverage, pre-tax commuter and health care/dependent care accounts, 401k plan, life and disability benefits, flexible time off, paid parental leave, and 11 paid holidays annually."}
{"text": "Skills/Tech: 5-7 years experience\n\n Hands-on experience with Databricks and Azure , including implementation using Unity Catalog.  Experience in platform modernization projects and implementation.  Deep understanding of components of data and analytics (building data platforms, data platform management, data extraction, ETL/ELT, data security, and data/insight reporting) \n\n\nJob requirements are met. \n\n\nRequired Skills and Qualifications:\n\n 5-7 years of proven experience in Data Engineering roles, with a strong understanding of Data and Analytics components.  Expertise in cloud platforms, particularly Databricks and Azure, with hands-on experience in architecting and implementing data solutions.  Proficiency in data platform management, data extraction, UI/UX presentation, and generative AI, with a strong technical background in data ops and data governance.  Ability to lead technical discussions, document decisions, and communicate effectively with both technical and non-technical stakeholders.  Proactive, independent, and self-motivated with a natural curiosity and desire to explore emerging technologies and industry trends.  Bachelor’s degree in Computer Science, Engineering, or related field (or equivalent work experience)"}
{"text": "skills, Excellent planning and organization skills, Ability to deal with ambiguity and a fast-paced business culture.\n\nJob Title: Data Analyst for Talent Management \n\nLocation: Princeton, NJ \n\nWhat’s the Job?\n\nThe Talent Management Consultant will work with other members of the Talent team and is responsible for participating in the implementation of Talent Management initiatives launched throughout the organization. Specializing in talent data science, analysis, and insights.\n\nRole Duties Include But Are Not Limited To\n\n Talent management data analysis, reporting and insights across talent practices. Assist with validation projects, content and criterion, in whatever capacity needed. Assist with Talent data auditing, coding for NLP/machine learning initiatives and measures. Specifically, but not limited to, manager performance coaching documentation quality. Assist with the coordination of talent practice process documentation and control points. Assist with the implementation of mentoring practices and coordination of professional development data. Assist the Talent Management Team as needed with consultation and collaboration with internal stakeholders to ensure that Talent solutions align with organizational priorities and needs. Assist with talent process documentation, job aids, training materials, and other change management tools and resources. Assist with project specific communications and broader Talent Management messaging to ensure that the organization is aware of Talent Management practices and initiatives. Assist with the coordination of executive assessments as needed. Assist with orienting new 360 participants, launching cohorts, monitoring progress, partnering with participants, and making decisions related to timing. Partner with subject matter experts throughout the business to design and finalize skills associated with varied jobs throughout Otsuka. Assist with Veeva submissions for all applicable job aids and training materials. Assist with post-training survey design and analyses, and results report outs. Potentially assist with survey content proposals and Workday/Qualtrics integration. Provide support for additional, varied Talent Management project implementations and other duties as assigned.\n\n\nRequired Experience And Skills\n\n Four-year Degree in Social Science, preferred Sociology, Psychology, Anthropology Minimum of 3 years’ related work experience Strong data programming, analytic, and reporting skills. Experience working in Excel and PowerPoint, intermediate level. Ability to do basic graphic design in Word/PowerPoint Professional level communication skills, written and verbal. Handle confidential information with professionalism and integrity. Ability to interact independently with internal and external stakeholders. Demonstrated ability to work effectively both independently and collaboratively as part of a team. Proficient project management skills Excellent planning and organization skills Ability to deal with ambiguity and a fast-paced business culture.\n\n\nIf this is a role that interests you and you’d like to learn more, click apply now and a recruiter will be in touch with you to discuss this great opportunity. We look forward to speaking with you!\n\nAbout ManpowerGroup, Parent Company of:Manpower, Experis, Talent Solutions, and Jefferson Wells\n\nManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands –  Manpower, Experis, Talent Solutions, and Jefferson Wells  – creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2022 ManpowerGroup was named one of the World's Most Ethical Companies for the 13th year - all confirming our position as the brand of choice for in-demand talent."}
{"text": "Qualifications\n\n3 to 5 years of experience in exploratory data analysisStatistics Programming, data modeling, simulation, and mathematics Hands on working experience with Python, SQL, R, Hadoop, SAS, SPSS, Scala, AWSModel lifecycle executionTechnical writingData storytelling and technical presentation skillsResearch SkillsInterpersonal SkillsModel DevelopmentCommunicationCritical ThinkingCollaborate and Build RelationshipsInitiative with sound judgementTechnical (Big Data Analysis, Coding, Project Management, Technical Writing, etc.)Problem Solving (Responds as problems and issues are identified)Bachelor's Degree in Data Science, Statistics, Mathematics, Computers Science, Engineering, or degrees in similar quantitative fields\n\n\nDesired Qualification(s)\n\nMaster's Degree in Data Science, Statistics, Mathematics, Computer Science, or Engineering\n\n\nHours: Monday - Friday, 8:00AM - 4:30PM\n\nLocations: 820 Follin Lane, Vienna, VA 22180 | 5510 Heritage Oaks Drive, Pensacola, FL 32526\n\nAbout Us\n\nYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:\n\n Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services\n\n\n\n\nDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market position\n\nBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."}
{"text": "requirements of health plan as it pertains to contracting, benefits, prior authorizations, fee schedules, and other business requirements.\n•Analyze and interpret data to determine appropriate configuration changes.• Accurately interprets specific state and/or federal benefits, contracts as well as additional business requirements and converting these terms to configuration parameters.• Oversees coding, updating, and maintaining benefit plans, provider contracts, fee schedules and various system tables through the user interface.• Applies previous experience and knowledge to research and resolve claim/encounter issues, pended claims and update system(s) as necessary.• Works with fluctuating volumes of work and can prioritize work to meet deadlines and needs of user community.• Provides analytical, problem-solving foundation including definition and documentation, specifications.• Recognizes, identifies and documents changes to existing business processes and identifies new opportunities for process developments and improvements.• Reviews, researches, analyzes, and evaluates all data relating to specific area of expertise. Begins process of becoming subject matter expert.• Conducts analysis and uses analytical skills to identify root cause and assist with problem management as it relates to state requirements.• Analyzes business workflow and system needs for conversions and migrations to ensure that encounter, recovery and cost savings regulations are met.• Prepares high level user documentation and training materials as needed.\n• Works to identify opportunities for continuous improvement, standardization, and reduction of rework across health plan and shared services• Monitors, coordinates, and communicates the strategic objectives of health plan across shared services to optimize performance/results.• Aggregates and assists with the analysis of health plan and shared service data.\n2-5 years• Bachelor’s Degree or equivalent combination of education and experience• 1-3 years formal training in Business Analysis and/or Systems Analysis"}
{"text": "requirements and forward your/Consultant resume, and contact details if you are interested and comfortable with the below job description feel free to call/mail me at goutham@cliecon.com or O: 732-626-9717 Ext 107\nRole: Senior Data engineer with Python, Spark, AWSLocation: Jersey City NJ only (Must report onsite day 1, Mon, Tues, and Thursday in office)Duration: 12 Months\nJob Description: Need strong Python (backend development), Spark (performance tuning, expertise), and AWS. \nSummary:• Executes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems• Develops secure and high-quality production code, and reviews and debugs code written by others• Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems\nRequired Skills:• Min 8 years of hands-on experience• AWS, Python, Spark• Data/Backend Software Developer\nNice to have:• Certifications - AWS Solutions architect• Financial Domain\n\n\nAbout us: Cliecon Solutions Inc.,( headquartered in central NJ ) is one of the fastest-growing and leading consulting and management firms with 14 years of experience in Staff Augmentation. We handle a complete recruiting cycle for fortune 500 clients, major implementing partners, and tier -1 vendors. We specialized in recruiting for Application development, Bigdata, Databases, Infrastructure, Cloud, Mobile, and ERP-based solutions projects\n\n\nThanks & Regards,\nGoutham Musham,Technical Lead.Cliecon Solutions Inc.,(Client + Consultants)O: 732-626-9717 Ext 107Direct: 609-901-9002E: goutham@cliecon.com || http://www.cliecon.comContact me on LinkedIn: linkedin.com/in/goutham-m-640035a2"}
{"text": "Skills & Experience\n\n4+ years of experience as a Data Engineer. Experience in automation, data management, data quality, financial or regulatory reporting. Strong experience with relational and non-relational data stores. Experience using ETL/ELT tools like Ab Initio, Informatica, and DataStage. Experience writing Python. Understand database performance concepts like indices, segmentation, projections, and partitions. Shell scripting in Unix environment. \n\nWhat You Will Be Doing\n\nSupport data warehouse batch and drive continuous optimization and improvement. Identify and implement process improvements: infrastructure that scales, automating manual processes, etc. Take ownership of the various tasks that will allow to maintain high-quality data; ingestion, validation, transformation, enrichment, mapping, storage, etc. Improve observability across the data infrastructure to ensure data quality from raw sources to downstream systems. Collaborate with the teams to deploy and support reliable, scalable tooling for analysis and experimentation. Collaborate with the dev teams to anticipate and support changes to the data. \n\nPosted By: Melissa Klein"}
{"text": "experience, including Machine Learning using SQL and Python (including pandas). (PR12690A)"}
{"text": "requirements\n\nSource to target mapping\n\nSQL Skills- running queries\n\nAWS and Databricks environment is preferred\n\nSkills needed: Business data Analyst, preferably understanding of data products. Being able to independently work on source to target mappings, translate business requirements around location data (building, occupancy, traffic, persons, etc) into a structured mapping to further build out the data product.\n\nAny specific Tools/ technologies; Understanding of SQL, database structures. Nice to have; worked with Databricks."}
{"text": "Qualifications:SKILLS NEEEDED: Teradata, GCP BigQuery, Python Data Processing- Scripting, Kafka, SQLExpertise with the Technology stack available in the industry for data management, data ingestion, capture, processing and curationETL development experience with strong SQL backgroundExperience in building high-performing data processing frameworks leveraging Google Cloud PlatformExperience in building data pipelines supporting both batch and real-time streams to enable data collection, storage, processing, transformation and aggregation.Experience in utilizing GCP Services like Big Query, Composer, Dataflow, Pub-Sub, Cloud MonitoringExperience in performing ETL and data engineering work by leveraging multiple google cloud components using Dataflow, Data Proc, BigQueryExperience in scheduling like Airflow, Cloud Composer etc.Experience in JIRA or any other Project Management ToolsExperience in CI/CD automation pipeline facilitating automated deployment and testingExperience in bash shell scripts, UNIX utilities & UNIX Commands Nice to have Qualifications:Strong understanding towards Kubernetes, Docker containers and to deploy GCP services is a plusKnowledge of Scrum/Agile development methodologies is a plusAny experience with Spark, PySpark, or Kafka is a plusData analysis / Data mapping skills is a plusKnowledge in data manipulation JSON and XML Technical Skills:GCP Services: DataFlow, BigQuery, Cloud Storage, DataProc, Airflow, Composer, Pub/Sub and Memorystore/RedisProgramming languages: Java, PythonStreaming ETL: Apache Beam, KafkaDatabase: Teradata, BigQuery / BigTable"}
{"text": "skills and discover what you excel at—all from Day One.\n\nJob Description\n\nBe a part of transformational change where integrity matters, success inspires, and great teams collaborate and innovate. As the fifth-largest bank in the United States, we’re one of the country's most respected, innovative, ethical, and successful financial institutions. We’re looking for people who want more than just a job – they want to make a difference! U.S. Bank is seeking a Lead Software Engineer who will contribute toward the success of our technology initiatives in our digital transformation journey.\n\nThis position will be responsible for the analysis, design, testing, development, and maintenance of best-in-class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.\n\nKey Responsibilities\n\nUnderstand the data needs of business teams and suggest appropriate solutions.Act as liaison between Data Architecture Team and the customers to bring efficiency. Ensure best practices in data management are being followed.Help identify data governance, data quality and protection issues.Work closely with cross functional teams and leadership to improve the quality and value of core data assets.Evaluate implemented data systems for variances, discrepancies, and efficiency, and identify areas of improvement in current systems.Work with application teams to find ways of optimizing data access and throughput.Play a key role in the process of data transformation required for effective reporting, analytics.Determine the requirements for new database architecture.Identify solutions for new databases and new data architecture.Determine the data storage needs, create, and maintain data infrastructure for the company.Coordinate with other team members to reach project milestones.Act as a mentor to members of the team.\n\n\nBasic Qualifications\n\nBachelor’s degree, or equivalent work experienceSix to eight years of relevant experience\n\n\nPreferred Skills/Experience\n\nExperience with developing SQL queries and stored procedures.3+ years of experience with non-SQL databases preferably Cassandra.3+ years working with managed/self-managed data stores on public cloud.Designing and building complex data solutions using SQL and non- SQL databases.Experience in designing data solutions to support analytic needs of the company.Strong understanding of and experience with data management concepts, data governance, and data security.Strong experience with cloud technologies (Google Cloud, Azure, AWS, Azure is preferred) and cloud data engineering tools and services.Good understanding of streaming technologies like Kafka.Basic understanding of applications running Kubernetes.Excellent verbal and written communication skills.\n\n\nThe role offers a hybrid/flexible schedule, which means there's an in-office expectation of 3 or more days per week and the flexibility to work outside the office location for the other days. \n\nIf there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.\n\nBenefits:\n\nOur approach to benefits and total rewards considers our team members’ whole selves and what may be needed to thrive in and outside work. That's why our benefits are designed to help you and your family boost your health, protect your financial security and give you peace of mind. Our benefits include the following (some may vary based on role, location or hours):\n\nHealthcare (medical, dental, vision)Basic term and optional term life insuranceShort-term and long-term disabilityPregnancy disability and parental leave401(k) and employer-funded retirement planPaid vacation (from two to five weeks depending on salary grade and tenure)Up to 11 paid holiday opportunitiesAdoption assistanceSick and Safe Leave accruals of one hour for every 30 worked, up to 80 hours per calendar year unless otherwise provided by law\n\n\n\n\nU.S. Bank is \n\nE-Verify\n\nU.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.\n\nThe salary range reflects figures based on the primary location, which is listed first. The actual range for the role may differ based on the location of the role. In addition to salary, U.S. Bank offers a comprehensive benefits package, including incentive and recognition programs, equity stock purchase 401(k) contribution and pension (all benefits are subject to eligibility requirements). Pay Range: $129,455.00 - $152,300.00 - $167,530.00\n\nU.S. Bank will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance.\n\nJob postings typically remain open for approximately 20 days of the posting date listed above, however the job posting may be closed earlier should it be determined the position is no longer required due to business need. Job postings in areas with a high volume of applicants, such as customer service, contact center, and Financial Crimes investigations, remain open for approximately 5 days of the posting listed date."}
{"text": "Experience with AI Technologies\n\nWhat the Client Needs you to Do:\n\nIn this role you will leverage your expertise in Artificial Intelligence to drive data-driven solutions. Responsibilities include optimizing data pipelines, implementing AI algorithms, and leveraging cloud technologies for scalable data solutions. Bring your proficiency in SQL, Python, and cloud platforms to architect, develop, and maintain data infrastructure. Collaborate with cross-functional teams to deliver impactful insights. Must have a strong background in AWS, SQL, Python, and experience with Snowflake, Redshift, Databricks, and AI technologies.\n\nEducational Requirements:\n\nBS in Math, Stats or a related Computer ScienceMS is preferred\n\nAdditional Information:\n\nHybrid work environment, candidates should be within commuting distance to the Greater Boston areaCandidates must be eligible to work in the United States\n\nSenior Data Engineer"}
{"text": "Experience in crunching data? Love working with data and providing business insights? Power BI Rockstar? We'd love to talk to you! \n\nResponsibilities\n\n Work closely with Business stakeholders to access reporting requirements and confirm existing reporting capabilities  Develop reporting and analytics to identify opportunities for process improvement; provide expert-level advice on the implementation of operational process; continual refinement of analytics to drive operational excellence  Develop quality assurance process relating to business intelligence reporting; conduct reviews of output; consult with end users, implement resolution to any deficiencies  Develop and implement reporting audits to ensure accuracy and compliance  Collaborate with cross-functional teams and senior stakeholders to identify and understand key business challenges, translating them into data-driven insights and actionable recommendations  Create compelling visualizations and interactive dashboards to effectively communicate analytical findings to non-technical stakeholders, present insights, and recommendations to senior leadership in a clear and concise manner  Develop data models and frameworks to organize and structure data effectively and create visually appealing and informative reports, dashboards, and presentations  Present complex data in a simplified and understandable format for non-technical stakeholders and utilize data visualization tools like Power BI to enhance data storytelling  Manage data Gathering, Analyzing, Cleaning, transforming, and manipulating various sorts of data using SQL, Microsoft Excel (Pivot tables, VLOOK UP, etc.) and Power BI to ensure data accuracy and consistency  Provide prompt, effective day-to-day support for stakeholders on data, dashboarding, tooling, and reporting  Accountable for efficient transition and delivery of scheduled and support ad-hoc reports and analysis requests \n\nQualifications\n\n Must possess one or more of the following:  Associate degree in Business Administration, Management, or related field with a minimum of three (3) years of management experience in the financial and team leadership aspects of a large semi-independent business enterprise  High school diploma or equivalent with a minimum of five (5) years of management experience in the financial and team leadership aspects of a large semi-independent business enterprise  Experience with data visualization tools such as Power BI  Exceptional oral, written, and presentation skills  Ability to work effectively both independently and as part of a team  Knowledge of file management and other administrative procedures  Ability to work on tight deadlines  Must possess strong oral, written, and analytical skills to effectively convey complex concepts and findings to both technical and non-technical stakeholders  Effective oral and written communication  Planning and organizing  Proficiency with Microsoft Office Applications  Problem solving  Analyzing, predicting  Active listening  Write informatively, clearly, and accurately  Identify critical issues quickly and accurately  Teamwork  Attention to detail \nPreferred Qualifications\n\n Working knowledge of Finance-related processes in ERP environment, PeopleSoft, WinTeam  Intermediate skill level in Microsoft Office; Excel in particular  Experience working with internal and external clients \n\nBenefits\n\n Medical, dental, vision, basic life, AD&D, and disability insurance  Enrollment in our company’s 401(k)plan, subject to eligibility requirements  Eight paid holidays annually, five sick days, and four personal days  Vacation time offered at an accrual rate of 3.08 hours biweekly. Unused vacation is only paid out where required by law. \n\n Closing \n\nAllied Universal® is \n\nIf you have any questions regarding \n\n Requisition ID \n\n2024-1200911"}
{"text": "Qualifications & Desired Skillsin a CS-related fieldyears of relevant experience (Healthcare solutions focused experience is a plus)understanding of product/application designs and software developmentdevelopment experienceon Data and Analytics technologies and trends - current on new ideas and tools/ Scripting (Python, Scala, Bash, Korn Shell)Data (Hadoop, Spark, Kafka)Platforms (AWS, Azure, GCP)Concepts deep knowledge (near-/real-time streaming, data ingestion, data transformations, data structures, metadata, master data, data flow management)(SQL and NoSQL data bases)Languages (JSON, XML)Management Tools (Git/GitHub)Containerization, Workflow experience is a plus: (Terraform, Docker, Kubernetes, Airflow)management and agile tools: (Jira)\nGray Matter Analytics is"}
{"text": "skills towards solving the climate crisis.\n\nMain Responsibilities\n\nYou will help further develop the Business Intelligence (BI) / data analytics system within EnergyHub’s platform, including designing and building dashboards and reports and developing and maintaining data models and data quality procedures to ensure accuracy and timeliness of data.You will collaborate with engineering teams, data scientists, product managers and client success managers to help develop business logic and detailed system requirements and to help identify and answer research questions of interest to the company and our clients.You will become an expert in smart device data and the aggregation of such into VPPs; including that of electric vehicles, smart thermostats, EV chargers, batteries, solar inverters, etc.Other duties as assigned\n\n\nKey Skills And Experience\n\nYou are passionate about finding valuable insights in large, complex datasets3+ years of experience as a data analyst and have previously worked in a multi-person business intelligence teamWhen you can’t find the data that you need, you are creative enough to infer and/or generate the data needed from other information that is availableYou are skilled at developing analytics dashboards and data visualization that are useful and visually compelling.You have an understanding of and can develop basic statistical models and perform complex data analysis to surface insights or inform decisions.You have experience developing automated reporting systems with business intelligence toolsYou enjoy communicating with others and and developing real relationships with colleagues, clients and external partnersYou are excited about the opportunities of working with very large datasets from disparate sourcesYou have the ability to describe your ideal analytics schema to data engineering team members and work with them to improve a data warehouseYou are share our vision of a carbon-free distributed energy futureYou are passionate about empowering users through timely, accurate and actionable dataYou are very skilled at using one or more BI tools (e.g. Tableau, Sigma, Looker), cloud data warehouses (e.g. Snowflake, Redshift, Big Query), data transformation frameworks (e.g. dbt), and data science tools (e.g. Python/Jupyter, Julia, R)You have a deep understanding of data warehouse architecture \n\n\nPreferred Skills And Experience\n\nExtensive experience with dbt / Snowflake / SQLA strong background in mathematics and statistics in order to provide support to clients who need to understand complex statistics and to collaborate with data scientists who will use the analytics platform for statistical analysis\n\n\nThe salary range for this position is $120,000 - $150,000. Base pay offered may vary depending on location, job-related knowledge, skills and experience.\n\nWhy work for EnergyHub?\n\nCollaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other. Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team!Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business.Work with the latest technologies: You’ll gain exposure to a broad spectrum of IoT, SaaS and machine learning obstacles, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs. Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate.Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what’s also important is how our employees feel every single day. \n\n\nCompany Information\n\nEnergyHub is a growing enterprise software company that works with the most forward-thinking companies in smart energy. Our platform lets consumers turn their smart thermostats, electric cars, water heaters, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power. We work on technology that already provides energy and cost savings to millions of people through partnerships with the most innovative companies in the Internet of Things.\n\nCompany Benefits\n\nEnergyHub offers a generous benefits package including 100% paid medical for employees and a 401(k) with employer match. We offer a casual environment, the flexibility to set your own schedule, a fully stocked fridge and pantry, free Citi Bike membership, secure bike rack, gym subsidy, paid parental leave, and an education assistance program.\n\nEnergyHub is \n\nIn connection with your application, we collect information that identifies, reasonably relates to or describes you (“Personal Information”). The categories of Personal Information that we may collect include your name, government-issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future positions, recordkeeping in relation to recruiting and hiring, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.\n\nNotice To Third Party Agencies:\n\nEnergyHub understands the value of professional recruiting services. However, we are not accepting resumes from recruiters or employment agencies for this position. In the event we receive a resume or candidate referral for this position from a third-party recruiter or agency without a previously signed agreement, we reserve the right to pursue and hire those candidate(s) without any financial obligation to you."}
{"text": "experience, and job responsibilities, and does not encompass additional non-standard compensation (e.g., benefits, paid time off, per diem, etc.). Job Description:Work with Material Master product team to gather requirements, collect data, lead cleansing efforts and load/support data loads into SAP.Will need to bridge the gap between business and IT teams to document and set expectations of work/deliverables.Create and maintain trackers that show progress and hurdles to PM’s and stakeholders.Assist in go live of site including, collecting, cleansing and loading data into SAP system.Middleman between IT and business stakeholderAble to communicate data models.Knowledge in SAP and MDG is preferred.Years of experience: 2+ in data analytics spaceStrong communication skills are a must.Will be working on multiple high priority, high paced projects where attention to detail and organization is required.Intermediate to Senior position – great opportunity to learn an in-demand area of SAP MDG.Strong willingness to learn – no ceiling on learning and growth potential and plenty of work to go around. About BCforward:Founded in 1998 on the idea that industry leaders needed a professional service, and workforce management expert, to fuel the development and execution of core business and technology strategies, BCforward is a Black-owned firm providing unique solutions supporting value capture and digital product delivery needs for organizations around the world. Headquartered in Indianapolis, IN with an Offshore Development Center in Hyderabad, India, BCforward’s 6,000 consultants support more than 225 clients globally.BCforward champions the power of human potential to help companies transform, accelerate, and scale. Guided by our core values of People-Centric, Optimism, Excellence, Diversity, and Accountability, our professionals have helped our clients achieve their strategic goals for more than 25 years. Our strong culture and clear values have enabled BCforward to become a market leader and best in class place to work.BCforward is"}
{"text": "Requirements\n\nExperience: At least 6 years of hands-on experience in deploying production-quality code, with a strong preference for experience in Python, Java, or Scala for data processing (Python preferred).Technical Proficiency: Advanced knowledge of data-related Python packages and a profound understanding of SQL and Databricks.Graph Database Expertise: Solid grasp of Cypher and experience with graph databases like Neo4j.ETL/ELT Knowledge: Proven track record in implementing ETL (or ELT) best practices at scale and familiarity with data pipeline tools.\n\nPreferred Qualifications\n\nProfessional experience using Python, Java, or Scala for data processing (Python preferred)\n\nWorking Conditions And Physical Requirements\n\nAbility to work for long periods at a computer/deskStandard office environment\n\nAbout The Organization\n\nFullsight is an integrated brand of our three primary affiliate companies – SAE Industry Technologies Consortia, SAE International and Performance Review Institute – and their subsidiaries. As a collective, Fullsight enables a robust resource of innovative programs, products and services for industries, their engineers and technical experts to work together on traditional and emergent complex issues that drive their future progress.\n\nSAE Industry Technologies Consortia® (SAE ITC) enables organizations to define and pilot best practices. SAE ITC industry stakeholders are able to work together to effectively solve common problems, achieve mutual benefit for industry, and create business value.\n\nThe Performance Review Institute® (PRI) is the world leader in facilitating collaborative supply chain oversight programs, quality management systems approvals, and professional development in industries where safety and quality are shared values.\n\nSAE International® (SAEI) is a global organization serving the mobility sector, predominantly in the aerospace, automotive and commercial-vehicle industries, fostering innovation, and enabling engineering professionals. Since 1905, SAE has harnessed the collective wisdom of engineers around the world to create industry-enabling standards. Likewise, SAE members have advanced their knowledge and understanding of mobility engineering through our information resources, professional development, and networking."}
{"text": "experience and financial performance of hospitals, health systems and medical groups. We are the one company that combines the deep expertise of a global workforce of revenue cycle professionals with the industry’s most advanced technology platform, encompassing sophisticated analytics, AI, intelligent automation, and workflow orchestration.\n\nAs our Power BI Semantic Layer Engineer, you will be responsible for creating and optimizing data models, DAX measures, and data security roles that enable self-service reporting and analysis for our business users. Each day, you will design, develop, and maintain Power BI semantic models for our data analytics platform. To thrive in this role, you must have experience with DAX, SQL, and Power BI as well as knowledge and experience in data modeling concepts, data warehouse design, and dimensional modeling.\n\nHere’s what you will experience working as a Power BI Semantic Layer Engineer:\n\nBuild and maintain a semantic data layer on top of data marts that will be utilized by BI tools like Power BI to serve enterprise-level reporting needs like paginated reports, dashboards, and self-serve capabilities. Create and support Power BI datasets and dataflows that connect to various data sources such as Snowflake, SQL Server, Azure Data Lake, Snowflake, and Azure Synapse Analytics. Develop and test DAX measures, calculations, and dynamic filters that meet business requirements and adhere to data quality standards. Implement data security roles and row-level security to ensure data access is controlled and compliant with data governance policies. Document and support metadata, data lineage, and data dictionary for Power BI semantic models. Lead the design and ensure the quality, reliability, and scalability of the semantic layer architecture. Implement best practices for testing, monitoring, and troubleshooting to support optimal system performance. \n\nQualifications\n\nBachelor’s degree in a related field. At least 3 years of experience in developing Power BI semantic models or similar tools. Strong knowledge of data modeling concepts, data warehouse design, and dimensional modeling. Proficient in DAX, SQL and working with relational and non-relational data sources. Experience in using Power Query and M language to transform and cleanse data. Experience in using Power BI service and Power BI desktop features such as report design, data visualization, and data refresh. Experience in implementing data security and data governance best practices for Power BI semantic models. Excellent communication, analytical, and critical thinking skills. Ability to work independently and as part of a team. \n\nFor this US-based position, the base pay range is $53,812.50 - $93,375.00 per year . Individual pay is determined by role, level, location, job-related skills, experience, and relevant education or training.\n\nThe healthcare system is always evolving — and it’s up to us to use our shared expertise to find new solutions that can keep up. On our growing team you’ll find the opportunity to constantly learn, collaborate across groups and explore new paths for your career.\n\nOur associates are given the chance to contribute, think boldly and create meaningful work that makes a difference in the communities we serve around the world. We go beyond expectations in everything we do. Not only does that drive customer success and improve patient care, but that same enthusiasm is applied to giving back to the community and taking care of our team — including offering a competitive benefits package.\n\nR1 RCM Inc. (“the Company”) is dedicated to the fundamentals of \n\nIf you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.\n\nCA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent\n\nTo learn more, visit: R1RCM.com\n\nVisit us on Facebook\n\n#powerbi #dax #semanticmodels #remotework #dataengineer"}
{"text": "skills for this role:\n\nSnowflakeDataStageAWSS3 LambdaHadoop (little experience is enough), python experience, SQL.\n\nCompensation:\n\nThe pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on a number of factors, including but not limited to, a candidate’s qualifications, skills, competencies, experience, location and end client requirements).\n\nBenefits and Ancillaries:\n\nMedical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee."}
{"text": "skills to join a high-profile, high-visibility organization that powers Verizon's Network capital analytics. You will be part of a team that builds strategy, analysis, and insights that inform, quantify, and enable business decisions and investments for Verizon Networks. As a member of NCM, you will shape multibillion-dollar investments for the nation's largest network.\n\nFor this role, you will be a member of the Waypoint Project Team. Waypoint creates and ingests model data to produce comprehensive optimized multi-year capital plans across a series of data-driven dashboards with dependencies between major network programs allowing for a strong focus on capital-efficient performance outcomes that are feasible based on various build pipelines while emphasizing trade-offs between plans, collaboration, and feedback.\n\nIn order to be successful, decisions need to be made based on high-quality data insights that are aligned with NCM's partners & stakeholders.\n\nThe responsibilities associated with this role are:\nPartner with the existing team to identify critical focus areas for upcoming work\nDevelop new features and maintain the existing Waypoint data platform \nWork on critical focus areas by meeting with stakeholders to identify the business process, and relevant data to create meaningful high-value models \nMeasure model outcomes as input actuals shift to understand the impacts on future plans\nPerform quality assurance testing and maintenance on existing and new dashboards to maintain a high user experience\nLead efforts to boost engagement via demonstrations and gathering stakeholder feedback for future enhancements that boost value/use \nAid in transitioning the tool's architecture from external Verizon sources to NCM sources. \n\nWhat we're looking for...\n\nYou'll need to have:\nMust be a current participant of the Department of Defense Skillbridge Program or of the Hiring our Heroes Corporate Fellowship Program Cohort 2-2024.\nBachelor's degree or higher in data science, engineering, or computer science\nSix or more years of Experience with logical data modeling development tools & languages, visualization tools (Tableau), database management (SQL), & API integration\nStrong understanding of SQL, Python (or other relevant programming languages), problem-solving using logical representations of real-world systems\nAnalytical capability, including the ability to analyze a large amount of data between and amongst various systems\nProven ability to prioritize, manage, and deliver multiple complex projects against key deliverables and timelines\nStrong written and verbal communication skills, including the ability to distill complex issues quickly, effectively communicate strategies, and summarize key points\nVisualization/storyboarding experience to present insights and recommendations in concise, intuitive, narratives\nDemonstrated effective communication skills with the ability to influence and set expectations across various functional teams; and across all levels within the organization \n\nEven better if you have one or more of the following:\nMaster's degree in data science, engineering, or computer science \nHigh level of curiosity and investigative mindset, with strong attention to detail\nExperience with Alteryx, Tableau, and other data processing programs\nExperience with Agile methodologies\nStrong project and process management skills and ability to manage multiple competing projects/priorities simultaneously\nAble to translate data into Executive Presentations in an accurate, clear, and concise manner, including graphical displays\nStrong Google Suite & Microsoft Office skills\n\nIf Verizon and this role sound like a fit for you, we encourage you to apply even if you don't meet every \"even better\" qualification listed above.\n\nWhere you'll be working\n\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\n\n\nWe're proud to be"}
{"text": "Qualifications\n Analytical Skills, Data Analytics, and StatisticsStrong communication skills to effectively convey complex data insightsData Modeling skills to organize and structure data for analysisExperience in working with large datasets and applying statistical techniquesProficiency in programming languages such as Python, R, or SQLExperience with data visualization tools and techniquesAttention to detail and ability to work independently and remotelyBachelor's degree in a quantitative field such as Mathematics, Statistics, or Computer Science"}
{"text": "skills and the ability to connect and communicate across multiple departments.Adept at report writing and presenting findings.Ability to work under pressure and meet tight deadlines.Be able to read and update project and program-level resource forecasts.Identify recurring process issues and work with the manager to find solutions and initiate improvements to mitigate future recurrence.\nSkills and Qualifications:Minimum of 5 years of experience with Clarity PPM and 5-8 years in an analyst capacity.Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL, etc), and programming (XML, Javascript, etc).Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SAS, etc).You have a high understanding of PPM disciplines, have worked in a team, and covered strategic projects. Experience with Dashboard customization, configuration, user interface personalization, and infrastructure management will be helpful.Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail, accuracy, and actionable insights.Excellent communicator, adjusting communication styles based on your audience.Quick learner, adaptable, and able to thrive in new environments.Proactive, confident, and engaging; especially when it comes to large stakeholder groups.Capable of critically evaluating data to derive meaningful, actionable insights.Demonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background. Required Education:BA or equiv.\nBenefits:401(k).Dental Insurance.Health insurance.Vision insurance.We are \nAdditional"}
{"text": "RequirementsExperience in at least one of these relevant programming languages: C#, Python, Java, etc.Experience with Elasticsearch, MongoDB, or other NoSQL experienceExperience with containerization platforms (Docker, Kubernetes, etc)Experience with schema design and writing queries for SQL Server, Postgres or similarAzure experienceKanban/Agile experienceFamiliarity with machine learning and NLP is nice to have but not requiredAt least 2 years. This is not a “junior” position.\n\nChmura is not able to provide sponsorship for this role. We back our colleagues with the following benefits/programs: \nCompetitive base salaries Comprehensive medical, dental, and vision benefitsLife Insurance and Disability Insurance benefits, 100% of premium paid by ChmuraParking and Transit Program Up to a 4% Company Match on retirement savings planPaid parental leave for expecting parents, regardless of gender, offered for pregnancy, adoption or surrogacy Free and confidential support for counseling, personal and work-related issues through our employer-sponsored service with Cigna (Employee Assistance Program)Employee Development ProgramTuition Reimbursement Program\nChmura is \n#LI-Hybrid #LI-CHMURA"}
{"text": "requirements for our direct client, please go through the below Job Description. If you are interested please send me your updated word format resume to sudi.reddy@exlservice.com and reach me @ 520-231-4672.\n Title: GCP Data EngineerLocation: Hartford, CTDuration: Full Time\n6-8 Years of experience in data extraction and creating data pipeline workflows on Bigdata (Hive, HQL/PySpark) with knowledge of Data Engineering concepts.Experience in analyzing large data sets from multiple data sources, perform validation of data.Knowledge of Hadoop eco-system components like HDFS, Spark, Hive, Sqoop.Experience writing codes in Python.Knowledge of SQL/HQL to write optimized queries.Hands on with GCP Cloud Services such as Big Query, Airflow DAG, Dataflow, Beam etc."}
{"text": "experience. While operating within the Banks risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.\n\nESSENTIAL DUTIES AND SKILLS, AND ABILITIES REQUIRED:\n\n Bachelors degree in Computer Science/Information Systems or equivalent combination of education and experience. Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group. Fundamental understanding of distributed computing principles Knowledge of application and data security concepts, best practices, and common vulnerabilities. Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development.\n\n\n\nApex Systems is \n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\nApex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.\n\n4400 Cox Road\n\nSuite 200\n\nGlen Allen, Virginia 23060\n\nApex Systems is"}
{"text": "Experience: 10+ yrsLocation: RemoteAzure Lead Data Engineers 10 years of experience with Databricks and ADF Payer Domain with Medicare and MedicaidMust have skills : Azure Datalake Store, Azure Data FactoryOther skill: Databricks workspace admin, Spark, Scala, Databricks CLI, Databricks SQLNice to have skills include Python Azure Event Hub and Azure DevOpsAWS Redshift, Netezza and Data Modelling is a MUSTMust have led a team for minimum 5 peopleMust have good collaboration and communication skillsPerformance optimization skill and code review skill is a mustVery hands on heavy on quickly learning Databricks and DBT.Data Migration experience if possible\nThanks & Regards Shreyas LExecutive RecruiterNam Info IncPhone- 732-851-0065 (126)Email – shreyas@nam-it.com"}
{"text": "Qualifications\n\n 1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Bachelor's or Master's degree in Engineering with 0-2 years of experience.\n\nPreferred Qualifications\n\n Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.\n\nAmazon is committed to a diverse and inclusive workplace. Amazon is \n\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $81,000/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.\n\n\n\nCompany - Amazon.com Services LLC\n\nJob ID: A2605789"}
{"text": "experiences using just their creativity and imagination.\n\nOur founders bring a wealth of experience at scale and a deep understanding of cutting-edge AI technologies from their combined 15 years at Amazon, spanning both Amazon Web Services (AWS) and Alexa. The founding duo also boasts significant game industry experience at companies such as Zynga, building games that have been played by millions.\n\nWhat you will do in this role\n\nAs the first ML Engineer at RenderWolf, you will work with our Science and Product teams to implement cutting-edge generative AI models to power creative products for game studios. You will\n\nResponsibilities\n\nImplement cutting edge AI models and techniques to build product features that solve the needs of art teams at game studiosOptimize our AI pipelines and techniques to maximize feature performance and operational efficiencyDevelop internal software tooling to automate tasks, facilitate rapid experimentation and prototyping of new models and techniquesKeep up to date with the state of the art in the field and quickly adopt breakthrough techniques\n\nWhat you need to excel in this role: We are looking for candidates with a strong background building impactful and novel machine learning projects, strong software engineering skills and a desire to convert research into products that people love.\n\nMust have BS or advanced degree in Computer Science; Computer Vision and/or AI research experience Experience developing and executing major AI/ML project(s) at a company or as part of an academic research teamFluency with Python, C++, CUDA, and deep learning frameworks such as TensorFlow and PyTorchAble to build simple software tools to improve research productivity through automation, experimentation, prototyping and evaluation. \n\nWhat We Offer\n\nThe chance to work at the forefront of AI and gaming technology.A collaborative and inclusive work environment that values diverse perspectives.Compensation: Salary, early stage stock optionsRemote workCompetitive benefitsCompetitive PTO"}
{"text": "requirements, provide data solutions, and deliver reports and dashboards using Power BI and Athena.\n\nKey requirements, provide data solutions, and support data-driven decision making.Research and evaluate new data technologies and best practices to improve data performance and quality.Provide technical guidance and mentorship to peers.\n\nSkill Requirements: \n\nAt least 5 years of experience in data analysis, data engineering, or business intelligence.Strong knowledge and experience with AWS, Data Lake, ETL transformations, Athena, and Redshift.Proficient in SQL and Python for data manipulation and analysis.Experience with Power BI and other data visualization tools.Experience with medical payer and patient data, such as claims, eligibility, enrollment, billing, and quality measures.Excellent communication, presentation, and problem-solving skills.Nice to have: Experience with Azure and Fabric.\n\nEducation/Certification Requirements:\n\nBachelor's degree in Computer Science, Statistics, Mathematics, or related field.\n\nAdditional Requirements:\n\n Experience and ability to work in a constantly changing, fast paced, deadline driven environment. All prospective employees must pass a background check & drug test (Federal contractor requirement)\n\nCOMMITMENT TO DIVERSITY & INCLUSION\n\nWe are an Equal Employment/Affirmative Action employer. We do not discriminate in hiring on the basis of sex, gender identity, sexual orientation, race, color, religion, creed, national origin, physical or mental disability, protected Veteran status, or any other characteristic protected by federal, state, or local law.\n\nIf you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us by email at careers@mhk.com . Please note that only inquiries concerning a request for reasonable accommodation will be responded to from this email address and the email address cannot be used to inquire about the status of applications.\n\nFor more information, please visit  Know Your Rights ,  Pay Transparency , and  MHK \n\nBenefits Snapshot:\n\nMedical, vision, and dental plans for full time employees401(k) offered with a generous matchBenefits begin on first day of the month following employment Exercise/Health Club reimbursement opportunity Monthly dependent care reimbursement opportunity Short Term and Long Term disability Basic Term Life and AD&D Insurance\n\nPaid Time Off\n\n15 days Paid Time Off 13 Company Paid Holidays 3 Personal Days 2 Community Service days"}
{"text": "experience. We're a passionate team dedicated to building intelligent systems that blend expert analysis, cutting-edge machine learning, and real-time odds to give sports fans an unparalleled predictive edge and create a thrilling new polling experience.The OpportunityWe're seeking a talented and enthusiastic intern to join our dynamic team. You'll dive into the world of sports analytics, machine learning model development, and user-facing applications. Help us create an industry-leading prediction platform and reimagine how fans interact with sports!Key ResponsibilitiesData Collection and Preprocessing: Explore and implement techniques to acquire relevant sports data from sources like social media, sports statistics websites, and odds services (OddsJam integration a plus). Clean and shape this data for machine learning input.Machine Learning Model Development: Experiment with various regression and classification algorithms to predict fantasy points, game outcomes, and other sports metrics. Fine-tune models for accuracy and performance.Prediction Polling System: Design and implement a user-friendly polling interface where users can interact with the AI, express opinions and predictions on upcoming games and potentially earn rewards based on their accuracy.Evaluation and Improvement: Develop strategies to measure the performance of our predictive models. Continuously iterate to enhance accuracy and user experience.  QualificationsIn Pursuit of: Undergraduate or graduate degree in Computer Science, Data Science, Statistics, or a related field.Strong Programming Skills: Proficiency in Python and familiarity with data analysis and machine learning libraries (pandas, NumPy, scikit-learn, TensorFlow, PyTorch).Passion for Sports: Knowledge of popular sports (e.g., NFL, NBA, MLB) and an understanding of sports statistics.Creative Problem-Solver: Exceptional analytical thinking and an eagerness to tackle complex challenges."}
