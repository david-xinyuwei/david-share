# 深入理解 SGLang 与 vLLM：LLM 推理引擎技术对比解析

随着大语言模型（Large Language Models，简称 LLM）持续快速发展，推理引擎逐渐成为模型生产部署的关键基石。在诸多开源推理引擎中，SGLang 和 vLLM 无疑是近期社区瞩目的焦点。本文旨在从底层架构与核心技术角度，对二者进行全面、准确的对比和解析。

## 一、核心架构差异解析

大语言模型规模庞大（如 DeepSeek-V3、GPT 系列模型），通常拥有数百亿至千亿参数。模型在真实生产环境中运行时需要高效管理 GPU 显存与计算资源，并通过优化程序（Kernel）与调度策略实现低延迟、高吞吐。这便是推理引擎的作用——高效调度用户请求并优化模型计算，支撑 LLM 应用高效运行。

当前较流行的开源引擎——**SGLang** 与 **vLLM**，在实现技术上各具特色，各有优势。



几个重要技术点展开剖析它们之间的差异及设计取舍：

### （1）KV-Cache 存储方式差异：Block-KV VS. PagedAttention

在LLM生成文本时，为避免反复计算之前的 token，需要缓存历史计算的 Key-Value 值，这称为 KV-Cache。KV-Cache的存储方式会直接影响显存利用率与性能表现：

- **SGLang：Block-KV（连续块）结构**
  将每个请求所需的 KV-Cache 存储于一片连续的显存区域，结构简单易于管理。但当多个请求长度不同时易产生显存碎片，导致资源利用率降低。
- **vLLM：PagedAttention（分页）结构**
  通过固定大小的「页」存储 KV-Cache，并以索引表维护每个请求的数据位置。结构复杂度稍高，但灵活调配页的分配，有效降低显存碎片率，综合利用效率更高。

| 维度         | vLLM                     | SGLang                          |
| ------------ | ------------------------ | ------------------------------- |
| 物理存储布局 | PagedAttention（页）     | Block-KV（连续块）              |
| 前缀复用策略 | Block Hash（固定块匹配） | Radix Tree（可变长 token 匹配） |

| 场景                      | vLLM Block-Hash                                              | SGLang Radix-Tree                                       |
| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------- |
| A. 单轮长上下文           | 基本命中不了（每题都不同）→ 性能差异≈0                       | 同左，维护树也用不上                                    |
| B. 多轮递进式对话         | 每一轮大概率前 N 个 16/32 token 块不变 → 命中率高，查表非常快 | 也能命中，但树遍历和节点维护稍重；收益≈vLLM，但成本更高 |
| C. Agent/工具链树状上下文 | 很多分支只改了末尾几 token，块哈希很容易失配 → 复用率低      | 可复用任意长度公共前缀，非常适合此类“分叉”上下文        |

接下来简单对比一下Block-KV和PagedAttention的差异：

**把 LLM 当成“算一次很贵的函数”**

假设我们让模型一步步推理：

① 系统提示：You are ChatGPT
② 用户提问：给我证明费马小定理
③ 模型回答 …
④ 用户追问：请把证明写成 Python

步骤 ① ② ③ 的中间计算结果（KV-cache）已经花费显卡算过一遍。
当进行第 ④ 步时，如果能直接拿到①②③算好的结果，就能省下一半以上的时间——这就叫“前缀复用”。



两种做法，对同一个序列怎么记

1. vLLM：把 token 按 **固定 16 个** 切块，然后给每块拍一张“指纹照片”（hash）。

   ```
   块1 = token[0‒15] → 指纹A
   块2 = token[16‒31]→ 指纹B
   块3 = token[32‒47]→ 指纹C
   ```

   

   以后只要又遇到 **完全相同的整块**（指纹匹配），直接拿来用，省时。

2. SGLang：把整段 token 接成一棵 **前缀树**：

   ```
   根 ─► "You" ─► "are" ─► "ChatGPT"
                       └──► "你好"
   ```

   

   任何新请求，只要开头对得上，都能沿着树走到“最长公共前缀”位置，然后继续长出新枝。

------

哪个场景各自更好？

• 一次性大 prompt、不再改动（比如“给我整篇论文摘要”）
‑ 根本没有第 ④ 步，前缀复用用不上 → 两家都差不多。

• 线性多轮对话（一步接一步提问，没有回溯）
‑ 第 ④ 步的前 16 的倍数 token 多半不变，vLLM 一次命中几个整块就够了 → 查表速度最快。
‑ SGLang 也能命中，但维护树略微多一些操作 → 没优势。

• 分叉式推理 / Agent（同一个对话里“试 3 种思路”，或 RAG 插入检索段落）
‑ 老内容前缀很长，但每条思路末尾稍有不同。
‑ vLLM：末尾那几个 token 改动→ 对齐块后指纹全变 → 复用失败，需要重算。
‑ SGLang：沿树共用到分叉点，后面再长新枝 → 只算差异部分，省得更多。

**一句话记忆**

固定 16 token 一块、不许改 → vLLM（快）
前缀长度随便改、允许分叉 → SGLang（灵活）

------

### （2）计算优化方式差异：Torch Compile 与 CUDA Graph

在大模型推理中，GPU kernel 启动开销与计算效率至关重要。两种引擎分别用不同方案实现优化：

- **CUDA Graph（SGLang 主推的默认方案）**：
  通过第一次计算时捕获完整执行序列，后续推理再执行采集到的「计算图回放」过程。省去了多次执行重复的 CPU-GPU 间命令传输开销，显著提高相同任务的执行速度，特别适合批次大小固定的推理作业。
- **Torch Compile（vLLM默认开启）**：
  由 PyTorch 官方提供的编译优化工具链，通过动态追踪（Dynamo）模型执行路径，并进一步由 Inductor 转译为高效的 GPU kernel。这种方式在首次编译耗费数分钟，但后续同质任务执行显著加速，是一种「首次启动慢，但长期运行效率更高」的路线。

| 维度                       | CUDA Graph（录“顺序”）                                       | Kernel Fusion / Inductor（改“代码”）                         |
| -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 核心思路                   | 把 **原来就存在的 N 个 kernel** 的“调用顺序”一次性录下来，后续直接 **批量回放** | 把 **N 个小 kernel 重新生成一段新代码**，合成 **1 个大 kernel** |
| GPU 层面留下的 kernel 数量 | 仍然是 N 个；只是不再逐个 launch                             | 变成 1 个（或少数几个）                                      |
| 是否改写算子内部逻辑       | 否，完全复用原始 kernel                                      | 是，重新生成、更大、更复杂的 kernel                          |
| 首次成本                   | 捕获时真正跑一遍 → 毫秒～秒级                                | Trace + 编译（NVCC/Triton）→ 分钟级                          |
| 运行期要求                 | 批次形状必须一致，否则需要重新 capture                       | 对形状更宽容（Inductor 支持 shape-polymorphic）              |
| 中间张量是否仍写回显存     | 会——因为 kernel 之间边界还在                                 | 很多临时张量可留在寄存器/共享内存，不写显存                  |
| 典型命中场景               | 固定批、快速冷启                                             | 长期 7×24 服务、追求极限吞吐                                 |
| 在引擎里的默认策略         | SGLang 默认开、无编译                                        | vLLM 默认开（Graph + Fusion）                                |

一句话区分
• **CUDA Graph**：不碰 kernel 本体，只把 *N 次启动* 合并成 *一次启动同样的 N 个 kernel*。
• **Kernel Fusion / Inductor**：直接把 *N 个 kernel* 「炼」成 *1 个 kernel*，连中间临时结果都在 GPU 内部就地传递。

所以两者面向的“省”不一样：

- CUDA Graph 省 **启动指令**；
- Kernel Fusion 既省 **启动** 又省 **内存往返**，代价是编译时间和更高复杂度。

### （3）**调度策略差异：Async-Python 与双层调度架构（Python+ExecutionEngine）**

针对大量推理请求时，如何高效地进行任务调度成为关键：

- **SGLang 采用异步 Async-Python 指令调度**： 基于 Python 异步编程（asyncio）完成简单直观的任务切换与排队机制。适合快速开发迭代和轻量需求，但扩展性与低延迟调度控制相对简单。
- **vLLM 采两级调度架构**：
  设计一个Python实现的高层次调度器（用户请求优先级排序）与底层的C++实现的高效执行引擎（ExecutionEngine）分离配合，复杂性略高但可实现精细调度控制，支持更大规模、更严苛并发场景的调度要求。

**对比总览**

| 对比维度     | SGLang（异步 Async-Python）                     | vLLM（双层调度：Python Scheduler + C++ Execution Engine）  |
| ------------ | ----------------------------------------------- | ---------------------------------------------------------- |
| **代码组成** | 纯 Python，单层 asyncio 异步协程                | 上层 Python（排队、策略） + 底层 C++（推理计算、显存管理） |
| **整体思路** | 像一个全能服务员，单线程管理所有服务请求        | 像高效餐厅：前厅经理（管理）+ 专业厨师团队（实际执行）     |
| **执行粒度** | Python 协程事件循环，每轮合并请求执行           | 上层批量分发任务，底层拆解为小批次微任务(micro-batch)执行  |
| **优点**     | 简单、易于理解、快速迭代                        | 功能丰富、吞吐量高、支持复杂调度和多模型并发               |
| **缺点**     | 幸运情况吞吐足够；大量并发会遇到瓶颈；CPU压力大 | 代码复杂，开发、调试成本相对更高                           |
| **适合场景** | 简单服务、轻量需求、测试demo、批量脚本          | 大规模生产服务、高并发、严格的延迟和QoS要求                |

------

**详细分析：SGLang 异步调度模式（Async-Python）**

#### 工作流程图示

```
用户请求 → Python asyncio事件循环 → 凑成批次(batch) → 调用GPU kernel执行
```



#### 流程解析：

- **事件循环**：通过 Python 标准库 asyncio，自带事件循环结构，优雅管理多个请求；
- **批处理方式**：每收到请求先暂存，条件满足（如达到最大批次大小或时间），即拼成 GPU 可执行批次；
- **执行方式**：直接在Python代码中调用 GPU kernel 执行；
- **适用场景**：少量并发请求、简单推理任务、需要快速开发的场景。

#### 优势：

- 编程简单直观、代码量少、易于维护子迭代；
- 最适合快速原型开发、低吞吐量推理任务。

#### 不足之处：

- 单线程执行，CPU繁忙时吞吐受限制；
- 缺乏QoS、任务抢占、多优先级调度等高级能力。



**详细分析：vLLM 双层调度模式（Python Scheduler + C++ Execution Engine）**

#### 工作流程图示

```
[上层Python Scheduler(请求优先级排序、任务划分)]
              ↓
[下层C++ Execution Engine(高性能拼batch、显存管理、调用GPU kernel)]
```

#### 流程解析：

- **上层Scheduler（Python）**：决定任务优先级，负责请求调度策略，例如先后顺序、批次组合方式等；
- **下层Execution Engine（C++）**：实际执行计算，负责准确分配显存和 GPU 执行微批次任务，作精细化性能优化；
- **适用场景**：大规模服务、较高并发、复杂 QoS、任务抢占、多模型混合模型管理。

#### 优势：

- 上层业务逻辑（Python）直观，下层GPU执行（C++）高效；
- 支持复杂调度策略、多模型、高QPS低延迟等生产级特性；
- CPU调度与GPU执行解耦，性能潜力更高、更稳定。

#### 不足之处：

- 架构复杂，开发成本与调试复杂度较高；
- 冷启动略慢，额外调优参数多，需更多调试工作量。

------

## 二、DeepSeek在DeepSeek方面的优化

DeepSeek 模型在 FP8 精度推理任务中表现尤其出色，其卓越的推理性能主要得益于多项底层优化技术的成功应用。这些优化技术由 DeepSeek 模型官方与推理引擎 SGLang 团队联合打造，针对 DeepSeek 的模型特点做了精细和深度的集成。下面我们逐条清晰地解释这些关键技术：

#### （1）**FP8 Blockwise Scaling 量化优化**

- **FP8是什么**？
  FP8 是一种 8 位浮点数精度表示。相较 FP16（16 bit）、FP32（32 bit），FP8 使用的比特数更少、更节省显存，运算速度更快；但同时精度降低也容易损失计算准确性。
- **Blockwise Scaling含义**：
  为避免低精度 FP8 运算中的精度损失，这种技术将大 tensor（比如权重和激活）分成小块(block)。每一小块单独对数值范围(scale)进行精细调整缩放，保证 FP8 精度“够用”但又不浪费精度。
- **技术价值**：
  以极低的精度（FP8）实现高准确性与高执行效率的平衡。

------

#### （2）**基于树状结构的NextN Speculative Decoding优化**

- **什么是Speculative decoding？**
  推测式解码指的是，模型生成token（文字）过程中，让小模型提前并快速推测多个可能的token输出，再由大模型事后统一批量验证这些推测结果。若推测正确则直接使用，大幅提高生成效率。
- **NextN模式与树状结构优化的含义**：
  与逐一线性推测不同，NextN模式一次性预测多个可能轨迹，组织成类似“树”般结构；之后大模型一次性全局检查并确认树上的节点路径。这种树状结构能够更高效地批量探索多个候选生成路径。
- **技术价值**：
  明显提高推理速度与吞吐量，极大降低端到端延迟。

------

#### （3）**Mixture-of-Experts（MoE）算子融合与调优优化**

- **MoE是啥？**
  MoE把模型分为多个专家（子模型），每次推理只激活少数专家进行计算，而不是调用整个巨大模型，从而提升模型推理效率。
- **融合与调优的含义**：
  将“专家选择→专家计算→结果汇总”等多个步骤算子融合在一起，以减少中间步骤的算力开销；并针对硬件资源精细调优专家数量与运行方式，进一步提升推理效率。
- **技术价值**：
  模型大却运行成本低、精细化资源控制与优化，获得超高性价比推理。

------

#### （4）**Multi-Head Latent Attention（MLA）并行优化**

- **什么是MLA？**
  传统多头自注意力（Attention）机制需要每头注意力都关注所有历史token，计算复杂；MLA则关注更高效、更具信息浓度的潜在(latent)表示，显著降低了注意力矩阵计算量。
- **并行优化含义**：
  在GPU上高效并行执行多个注意力头计算，避免计算和内存之间的等待瓶颈，实现真正并行。
- **技术价值**：
  减少Attention计算量，提升并行计算效率，降低推理延迟。

------

#### （5）**CUTLASS CUDA 模板库的FP8 GEMM深度优化**

- **什么是CUTLASS？**
  NVIDIA开发的开源GPU矩阵计算优化库，高度定制化优化GEMM（General Matrix Multiplication，矩阵相乘）运算，通过CUDA内核模板实现极致性能。
- **FP8 GEMM优化是啥？**
  FP8 GEMM就是使用8位浮点（FP8）精度执行矩阵乘法，CUTLASS通过精细kernel优化与硬件特性充分结合，进一步发挥 GPU 性能极限。
- **技术价值**：
  在超低精度 FP8 状态下，帮助 DeepSeek 模型获得极致高效的矩阵计算效率。



GLang 与 DeepSeek 模型团队高度协作，针对上述技术做了深入定制化与集成。相比之下，**vLLM** 等通用推理框架并非无法实现，但其开发资源的投入优先级问题，使得当前尚未完成同等级别的专用优化与深度适配，表现暂时明显落后。
**但长期而言**，社区力量与生态建设不断推进，vLLM 可能逐步赶上并缩小差距。

## **三、推理引擎选型指南：根据应用场景做出合适选择**

SGLang 在并行维度的支持现状：

| 并行类别            | SGLang 支持度                         | 典型用法 / 官方示例                                          | 适用场景                                                   | 备注                                                         |
| ------------------- | ------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ |
| **张量并行 (TP)**   | **✅ 完整支持**<br>单节点 & 多节点     | `--tp 8`（单机 8×GPU）<br>`--tp 16`（两机各 8×GPU）          | DeepSeek V3/R1 默认并行方式，用来把超大权重切成 N 份       | “Multi-Node Tensor Parallelism” 章节给出 2×8 H200 / 4×8 A100 等示例 |
| **数据并行 (DP)**   | **✅ 支持**（仅限 MLA Attention 路径） | `--enable-dp-attention --tp 8 --dp 8`（8×H200 纯 DP）<br>`--enable-dp-attention --tp 16 --dp 2`（跨 2 节点） | 高批量解码、KV-cache 容量瓶颈场景，可提升解码吞吐 1.5–1.9× | 只对 DeepSeek 系列 MLA 有效；小批低延迟场景不建议开启        |
| **流水线并行 (PP)** | **❌ 暂无正式实现**                    | —                                                            | 需要按层拆分模型的超大规模部署                             | 若必须 PP，请使用 vLLM 等其它框架                            |

根据以上技术差异，我们给出了以下清晰推荐：

| 使用场景或需求                                               | 推荐推理引擎 | 推荐原因与技术优势                                           |
| ------------------------------------------------------------ | :----------: | ------------------------------------------------------------ |
| 专注 DeepSeek或特定巨型模型推理，以 FP8 精度运行<br>(对 TP 要求较高，无需频繁DP伸缩） |  **SGLang**  | - 特定模型 kernel 深度优化 (FP8 GEMM、NextN Speculative 等)<br>- 和 DeepSeek 官方协作紧密<br>- TP 支持已较好 |
| 通用大型模型快速切换与广泛兼容性<br>（TP、PP、DP 全面支持）  |   **vLLM**   | - torch.compile 覆盖广泛模型，生态建设更完善<br>- TP/PP/DP 并行方案成熟好用<br>- 文档和社区生态活跃，各种新模型快速适配 |
| 大企业级、高QPS服务、大规模集群 GPU 并行管理<br>（DP、PP、TP 并行需求全面） |   **vLLM**   | - C++ Execution Engine 更成熟稳定<br>- 支持 Kubernetes、Ray 等生产级调度与运维工具<br>- PP、DP 组合极佳，更便于企业生产场景 |