## **Gemma 3 270M 工程化微调最佳实践：从零样本到可商用翻译模型**

### 结论

1. **模型选型上**：对于纯任务型（翻译、抽取等），270M Base > 270M Instruct，因为 Instruct 会保留安全规避和对话习惯，不利于目标任务收敛。
2. **训练轮次**：验证集 BLEU 和 Loss 在第 3 轮左右是最佳点 → 应用早停策略，避免第 4 轮开始的性能回落（过拟合）。
3. **任务方向性**：如果数据方向匹配度差（法→英），即使训练 Loss 下降，BLEU 也不会提升，提示需要换更匹配的数据集。
4. **训练动态**：Base 模型在训练集 Loss 和验证集 Loss 上同时优于 Instruct，说明它不仅记得住，还能更好泛化。
5. **工程建议**：
   - 低资源场景优先 Base 模型全量微调
   - 在监控 BLEU 变化的同时用验证集 Loss 作为早停指标
   - 数据方向和领域匹配比纯 epochs 增加更重要