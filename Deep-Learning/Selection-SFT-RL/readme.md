# 强化学习（RL）与监督微调（SFT）的选择与对比

### 从例子入手理解SFT和RL

##### **监督微调（SFT）- 像老师教学生**

监督微调（Supervised Fine-Tuning，简称SFT）相当于作为老师，自己先列出很多问题，再告诉模型标准的回答：

比如用数据（训练集）教它：

| 问题             | 标准答案 |
| ---------------- | -------- |
| 1加1多少？       | 2        |
| 苹果什么颜色？   | 红色     |
| 太阳从哪边升起？ | 东方     |

你让模型一遍又一遍模仿你的标准答案，直到你符合要求。

**SFT具体步骤（算法的介绍）：**

1. 你拿出一个问题：`苹果什么颜色？`
2. 鹦鹉自己尝试回答：比如它乱回答成 `蓝色`。
3. 你就立马纠正，告诉它正确的答案应该是`红色`，给它一个明确的误差信号： [ 误差 = - log P("红色") ]
4. 然后用这个误差信号帮助它更新自己说法，让下次“红色”概率增加。

所以，监督学习过程如下：

```
# 简单说SFT过程:
for 问题, 标准答案 in 数据集:
    模型答案 = 模型生成(问题)
    误差 = 计算交叉熵Loss(模型答案, 标准答案)
    模型更新(误差)
```

优点：安全、稳定、几乎不会出大问题
缺点：模型永远只能模仿，不太能创造性地发现新答案。

##### 强化学习（RL）– 让模型自己摸索

强化学习就不一样了，你不直接教标准答案，而是用“鼓励”和“惩罚”引导模型。

你问它：“1加1等于？”

- 它如果乱说了：“香蕉！”，你立刻给个负面奖励(-1);
- 如果它说对了：“2”，你给它正面奖励(+2)。

模型得到这些奖励和惩罚之后，会慢慢去摸索和记忆，知道怎么才能得到更多奖励（而不是直接告诉它标准答案）。

强化学习大致算法：

```
# RL过程:
for 问题 in 数据集:
    # 让鹦鹉自由生成多个答案（探索）
    多个答案 = 模型生成多个可行答案(问题) 

    # 每个答案给奖励
    for 每个答案 in 多个答案:
        奖励 = 奖励函数(每个答案)
        更新策略(奖励 * log(生成该答案概率))
```

优势：模型能够自己发现最优策略，能主动“探索”，学得更主动；
危险：但探索过猛容易出岔子（比如变得“过激”，只重复高奖励的句子），产生 KL 爆冲、梯度爆炸、最终模型崩盘。

| 方面     | 监督微调（SFT）        | 强化学习（RL）               |
| -------- | ---------------------- | ---------------------------- |
| 本质     | 模仿老师标准答案       | 只靠鼓励&惩罚自己摸索        |
| 学习速度 | 快，稳定               | 慢，可能反复波动             |
| 创造性   | 低，比较死板           | 高，能探索创造               |
| 数据需求 | 标准答案必须充足       | 只需奖励的反馈信号           |
| 常见问题 | 很少出现大的稳定性问题 | 易出现KL爆冲→ 梯度爆炸→ 崩盘 |

大多数情况下训练模型“先 SFT 再 RL”更安全、更高效，尤其是对能力尚弱的小模型或需要严格格式输出的任务。不过这并不是绝对法则，下面补充几点可作为快速校验的要点。

**一、为什么“先 SFT 后 RL”通常更好**

1. 训练稳定性
   • 直接 RL（尤其是小模型）容易出现 KL 爆冲、梯度爆炸，模型甚至崩盘。
   • SFT 先把策略锚定在“基本正确、格式合规”的空间，再让 RL 微调，KL 跳变小很多，收敛更稳。
2. 数据利用效率
   • SFT 等价于“先喂答案教基础功”；RL 更像“在掌握基础后练举一反三”。
   • 如果一开始就 RL，模型会在大量无意义探索上浪费步数。
3. 人工标注成本
   • SFT 阶段可用少量高质量标注（或合成高质量标注）直接模仿；RL 阶段只用奖励信号即可继续放大效果，二者配合能节省标注量。

**二、直接 RL 的合理场景**

1. 几乎没有标注数据、但可以自动计算奖励
   例如：解数独、玩 Atari 游戏，环境本身给出分数。
2. 大模型已具备强基础能力
   GPT-4、Claude 3-Sonnet 这一级别，格式和基本推理已比较稳，直接 RL（或 RLAIF）效果也可接受。
3. 任务鼓励高多样性、无法提供单一“标准答案”
   如创意写作、对话风格优化，仅用偏好打分即可训练。

**三、实践经验速查表**

| 情况               | 建议策略                            | 备注                    |
| ------------------ | ----------------------------------- | ----------------------- |
| 有一批高质量标注   | 先 SFT，后 RL                       | 主流 RLHF/GRPO Pipeline |
| 只有合成弱标注     | 可尝试短 SFT + RL                   | 先对齐格式再放大能力    |
| 纯交互式/环境奖励  | 直接 RL/在线 RL                     | 如游戏、机器人控制      |
| 预算极低、模型极小 | 先小规模 SFT，再视情况决定是否加 RL | RL 计算开销更大         |

**四、快速自测**

1. 你的奖励函数是不是完全依赖“答案==标准答案”？
   → 如果是，说明你已经有明确标注；SFT 通常先做更划算。
2. 你有多大 GPU/TPU 预算？
   → RL（尤其 GRPO/PPO）往往需要比 SFT 高 2-4 倍的算力。
3. 任务对“推理链”可解释性要求高吗？
   → 先 SFT（教会标签格式）再 RL（提升正确率）更容易满足可解释输出。

**结论 ：**

“先 SFT 再 RL”并非硬性规定，但在绝大多数需要结构化输出、且有可用标注的场景下是最省心、最稳妥的路径。只有当标注极少或任务天然提供可计算奖励时，才会优先考虑“直接 RL”。

### RL常见问题

| 术语     | 问题本质（到底是哪里出了问题）           | 属于哪种概念                                          | 错误的具体表现（学术描述）                                   |
| -------- | ---------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| KL爆冲   | 模型输出分布变化太剧烈、速度太快         | 输出分布问题 （Distribution-level issue）             | KL散度指标短期内急剧上升（如超过10或更高）；<br>策略模型与参考模型的概率分布差异迅速扩大；<br>导致模型输出质量急剧下降，如明显的文本内容混乱、重复或断句异常； |
| 梯度爆炸 | 训练过程参数更新数值过大、模型变得不稳定 | 训练过程中参数更新的问题 （Training stability issue） | 反向传播过程中梯度范数（Gradient norm）异常剧增（数值巨大甚至趋于无穷或NaN）；<br>训练损失(loss)数值异常增大甚至跳跃至无穷大或NaN；<br>模型权重参数技术层面上更新幅度异常大，导致网络计算存溢出或数值退化； |
| 模型崩盘 | 模型生成的内容变得单一、呆板、无法泛化   | 模型最终表现问题 （Final-generation quality issue）   | 模型生成内容多样性急速降低，信息熵（Entropy）显著减小；<br>输出分布退化到极少的模式（如mode collapse），文本表现为反复生成单一或少数固定答案；<br>在训练集之外的数据上表现能力急剧下降，泛化能力严重受损。 |

一般情况下，这三个问题会组成一条「连锁反应」：

```
奖励函数设计不佳或超参错误
      ↓↓导致↓↓
   KL爆冲 --> 梯度爆炸 --> 模型参数剧烈变化或NaN
      ↓↓进一步导致↓↓
   模型崩盘 (输出单一、低质)
```

**① KL爆冲**

**KL 散度 (Kullback–Leibler Divergence)** 本质上衡量的确实是 **两个概率分布之间的差距**。在 **DPO (Direct Preference Optimization)** 方法中，**参考模型(reference model)** 和 **训练中模型(policy model)** 之间计算的就是 **KL散度**。

**用简单例子解释一下：**

还是回到刚才鹦鹉的例子：

假设你的鹦鹉只会讲三种话：“你好”、“谢谢”、“再见”。

它现在的“说话概率”（也可以叫“原始概率分布”）是：

| 鹦鹉自己的当前概率分布（P分布） | 概率 |
| ------------------------------- | ---- |
| 你好                            | 0.6  |
| 谢谢                            | 0.3  |
| 再见                            | 0.1  |

你心目中理想的“鹦鹉应该说话的概率分布”（目标概率分布）是：

| 你想要的目标概率分布（Q分布） | 概率 |
| ----------------------------- | ---- |
| 你好                          | 0.2  |
| 谢谢                          | 0.7  |
| 再见                          | 0.1  |

你希望模型朝着**目标概率(Q分布)\**学习，但它原本的习惯是\**当前概率(P分布)**。

这时候，为了知道你的鹦鹉**目前的概率分布 P** 与 **目标概率分布 Q** 差距有多远。

- KL散度 **越小** = 两个概率 **越接近**。
- KL散度 **越大** = 两个概率分布的差距 **越明显**。
- 

鹦鹉例子中，如果原来鹦鹉会说:“你好(Hello)”，但你想教它说:"谢谢(Thank you)"，那么就有了：

- 一个**原始鹦鹉**的分布（Original distribution）：擅长说“你好”；
- 一个**目标鹦鹉**的分布（Target distribution）：你希望它能学会说“谢谢”。

假设你给了鹦鹉过分高的奖励，比如只要提到“谢谢”，你就给它100块巧克力。

鹦鹉在几步内学得太猛，突然所有问题只会无脑喊：“谢谢谢谢！”（变化太激烈，这就叫KL距离瞬间爆发）



KL爆冲 → 用算法调整KL惩罚系数 (β)

```
Loss总 = 奖励损失 + β × KL散度
```

提高β，比如0.01 → 0.1，约束鹦鹉变化的幅度。

**② 梯度爆炸**

深度学习中很常见的梯度爆炸问题主要是指：

- 网络在**训练过程中**因为某次更新的**梯度过大**，导致**模型参数突然变化过大**，从而网络可能变得不稳定甚至崩溃。

最常见导致梯度爆炸的情况，很少是简单的代码Bug；事实上更多是**算法超参设置不当或数值计算不稳定**导致的：

- **学习率（LR）过大**：
  如原本建议的学习率是`1e-5`，但使用了过高学习率（如`1e-2`或者更高），一次参数更新迈步过大，造成梯度过大。
- **奖励信号设计不合理（尺度过大）**：
  有时设计奖励信号时，没有进行归一化处理，例如你奖励的值过大（比如正常奖励是±1，却给了数百甚至上万），导致更新步幅过猛，产生极大的梯度数值。
- **网络结构本身设计或优化器配置不好**：
  比如神经网络某些层的初始化不合理，或梯度累计出现了数值问题，使得运动过程中梯度持续放大。
- **未使用梯度裁剪或裁剪设置值过大**：
  如果训练过程中未用梯度裁剪方法，或梯度裁剪的上限值设置过大（如10以上），一旦梯度猛增就不能约束，即可引发梯度爆炸。

算法表现为梯度值剧烈变大甚至NaN。

**③ 模型崩盘**

模型崩盘的本质含义是：

- 模型的参数被 “过度优化” 到单一或极少数的策略上（也称为Mode Collapse）；
- 策略分布发生严重的退化，模型无法再生成丰富、多样化的内容。

模型崩盘有典型的指标，例如：

- 输出的**熵大幅降低**（Entropy↓），表示语言多样性消失；
- **生成内容变得单一固定**，重复度极高；
- 在训练数据以外的泛化能力和稳健性大幅下降。

算法上，熵的定义是：

```
熵值 = -sum( p(X_i)*log(p(X_i)) )
# 熵越低，表示模型生成的语言越单调单一，越接近崩盘
```



一种典型的模型崩盘的表现是：

- 训练前语言多样性熵值 ≈ 8 到 10；

- 训练后模型崩盘，语言熵值 下降至 1～2 左右。

  

模型崩盘最常见的直接原因是源于强化学习训练过程本身的一系列内在问题（尤其是强化学习），例如：

- **奖励函数过于单一和简单**：导致模型倾向走极端，重复一种行为；
- **长时间训练、KL问题持续未解决**：模型能力持续退化，最终彻底丢失多样性；
- **连续出现梯度爆炸但未干预**：参数持续异常更新，模型能力根本不能正常保留；
- **数据质量较低或过拟合于一种模式**：模型长时间反复学习有限模式，无法泛化。

如果出现上述问题你还继续训练，鹦鹉最后脑袋就真的弄坏了。比如它彻底只会一招，一问就吐出“苹果苹果”或彻底傻掉不回话，再训练也没用（模型崩溃）。

## SFT与GRPO的两阶段训练

| 阶段 | 代码调用                                                     | HF 数据集仓库名 | 配置（子集）     | 网址示例                                     |
| ---- | ------------------------------------------------------------ | --------------- | ---------------- | -------------------------------------------- |
| SFT  | `get_limo()`<br>`load_dataset("GAIR/LIMO")`                  | `GAIR/LIMO`     | 无子配置（默认） | https://huggingface.co/datasets/GAIR/LIMO    |
| GRPO | `get_gsm8k_questions()`<br>`load_dataset("openai/gsm8k", "main")` | `openai/gsm8k`  | `"main"`         | https://huggingface.co/datasets/openai/gsm8k |

说明

1. SFT 阶段脚本里会对 `GAIR/LIMO` 做 `.select(range(1600))` 之类抽样；原始仓库约 817 条（train），938 条（dev+test）。
2. GRPO 阶段在 `openai/gsm8k` 的 `"main"` 配置上取 `train` split，再 `select(range(3500))` 抽子集做 RL；`test` split 用于离线评测。

只要在 Hugging Face Hub 搜索 “GAIR/LIMO” 和 “openai/gsm8k” 即可查看与下载完整数据。

| 阶段 | 数据集 (行数)         | 内容示例                                                    | 目的                                |
| ---- | --------------------- | ----------------------------------------------------------- | ----------------------------------- |
| SFT  | GAIR/LIMO 约 1 600 条 | K-12 数学题 + 官方解答<br>已包好 `<reasoning>` & `<answer>` | 教模型先学「写作模板 + 推理语气」   |
| GRPO | GSM8K-train 3 500 条  | 小学应用题，只有真值数字                                    | 用奖励（格式+数值）做 RL 提升正确率 |

因此 SFT 并不会看到后续 GRPO 使用的真值答案行；GRPO 也不会再用 LIMO 标注的解答文本。

**两阶段各自“训练了什么”？**

阶段① SFT (Supervised Fine-Tuning)
• 训练信号：交叉熵 (Cross-Entropy)，对教师答案逐 token 强制对齐。
• 学到内容

1. XML 模板必须完整闭合。
2. `<reasoning>` 里如何写链式思考（First … Therefore …）。
3. 在 `<answer>` 标签里只出现一个纯数字。
4. LoRA 参数被拉近“正确格式 + 基本推理”的低损失区。
   • 不学/很少学到
   – GSM8K 真值数字（因为数据集不同）。
   – 高阶数学技巧（量太少、只有 1 epoch）。

阶段② GRPO (Reinforcement Learning, KL-regularized)
• 训练信号：
– 数值奖励 cor_reward：完全命中 +2；其余 0。
– 格式奖励 fmt_reward：模板满足 +1；否则 0。
– 惩罚项 KL：防止行为过度偏离基座。
• 学到内容

1. 如何把 `<answer>` 数字精确等于真值（Exact-Match）。
2. 在保持模板的同时优化上一步数字。
3. 探索 ‑> 投票 ‑> 精修的策略（num_generations=8 + 众数投票）。
   • 不再关注
   – 语言流畅度/用词：奖励里没有相应项。
   – 训练集 LIMO 里的叙述风格（如果在奖励里没加 BLEU/Rouge）。

──────────────────────

3. 可以简化为「SFT 管格式，GRPO 管数学」吗？

──────────────────────
• 概念上接近，但要加几个限定：

1. SFT **确实主要** 把模型往“格式正确 + 推理语气自然”方向拉；
   在真值层面，由于 LIMO 的答案和 GSM8K 不重叠，加的数值知识有限。
2. GRPO 不仅训练数学，还继续用 `fmt_reward` 维持格式；
   如果把格式奖励权重调成 0，格式率会显著下降。
3. SFT 阶段也会略提升数学（因为 LIMO 题目是算数题），只是提升幅度小；
   GRPO 阶段才用 3 500 条 GSM8K + 180 步强化专门优化数字。
4. 最终格式 90 %+ 依然是两阶段共同作用的结果——SFT 给起点，GRPO 用奖励守住。

一句话总结
• 数据：SFT ≠ GRPO，用不同语料。
• 学习内容：SFT ＝「写得像样」；GRPO ＝「算得准确」。
• 格式主要靠 SFT 打基础、GRPO 维持；Exact-Match 主要靠 GRPO。





## 设计欠佳奖励函数

在强化学习训练中，**答案正确性的判断**通常通过**自动化脚本**实现，而非依赖人工标注的表格。以下是具体实现逻辑。

##### **1. 格式奖励函数（`format_reward_func`）**

**目标**

确保模型输出符合预设的XML标签结构 `<reasoning>...</reasoning><answer>...</answer>`。

**代码实现**

```
import re

def format_reward_func(completions, **kwargs):
    """检查输出是否符合XML标签格式"""
    pattern = r"^<reasoning>[\s\S]*?<\/reasoning>\s*<answer>[\s\S]*?<\/answer>$"
    responses = [completion[0]["content"] for completion in completions]
    rewards = [1.0 if re.match(pattern, response) else 0.0 for response in responses]
    return rewards
```



**逻辑解析**

- **正则表达式匹配**：
  使用正则表达式 `r"^<reasoning>[\s\S]*?<\/reasoning>\s*<answer>[\s\S]*?<\/answer>$"` 严格检查输出是否包含完整的 `<reasoning>` 和 `<answer>` 标签，且顺序正确。
- **奖励分配**：
  符合格式则奖励 **1.0 分**，否则 **0.0 分**。

**2. 正确性奖励函数（`correctness_reward_func`）**

**目标**

验证模型输出的数值答案是否与标准答案一致。

**代码实现**

```
def correctness_reward_func(completions, answer, **kwargs):
    """检查答案是否正确"""
    responses = [completion[0]["content"] for completion in completions]
    extracted_responses = [extract_last_xml_answer(response) for response in responses]
    rewards = [
        2.0 if extracted == correct else 0.0
        for extracted, correct in zip(extracted_responses, answer)
    ]
    return rewards
```

**依赖函数 `extract_last_xml_answer`**

```
def extract_last_xml_answer(response):
    """从XML标签中提取答案（若格式错误，则取最后一个数字）"""
    try:
        # 尝试解析XML标签
        answer = re.search(r"<answer>(.*?)</answer>", response).group(1).strip()
        return answer
    except:
        # 格式错误时，提取最后一个数字
        numbers = re.findall(r"\d+\.?\d*", response)
        return numbers[-1] if numbers else ""
```

#### **逻辑解析**

- **答案提取**：
  优先从 `<answer>` 标签中提取答案；若标签缺失或格式错误，则提取输出中的最后一个数字。
- **奖励分配**：
  答案与标准答案一致则奖励 **2.0 分**，否则 **0.0 分**。

**3. 总奖励计算**

- **总分范围**：`0.0 ~ 3.0`
  总奖励（`1.0`） + 正确性奖励（`2.0`）。
- **归一化处理**：
  GRPO算法会对组内奖励进行相对归一化（组内个体奖励减去组平均奖励），以平衡探索与利用。

**关键设计考量**

1. **格式与正确性的权重**
   正确性奖励（2.0）权重高于格式奖励（1.0），体现“答案正确性优先于格式”的设计原则。
2. **容错机制**
   即使格式错误，仍尝试提取最后一个数字作为答案，避免因格式问题完全丢弃有效答案。
3. **正则表达式严格性**
   格式检查使用严格匹配（`^...$`），确保标签闭合且无多余内容，强制模型学习结构化输出。

## 奖励函数优化思路

 二、格式奖励 fmt_reward （0 or 1 分） ────────────────────────────────── 源码（gemma-grpo.py）：

```
pat = r"^<reasoning>[\s\S]*?</reasoning>\s*<answer>[\s\S]*?</answer>$"

def fmt_reward(completions, **_):
    return [
        1.0 if re.match(pat, c[0]["content"]) else 0.0
        for c in completions
    ]
```



逐行解释

1. ```
   pat
   ```

   ：一条严格正则，要求：

   ```
   <reasoning> … </reasoning><answer> … </answer>
   ```

   

   前后可有任意空白，除此之外不能有别的 Token。

2. `for c in completions`：遍历 **每个 prompt** 的回答列表 （长度 = num_generations = 8）。
   这里只检查 `c[0]` ——即**第一条回答**，是设计简化；如果想保守 可把所有回答都检查再取 max。

3. `re.match(...)` 成功 → 1.0；否则 0.0。

4. 返回如 `[1.0, 0.0, 1.0, …]`，每个 batch 元素 0/1 分。

设计目的
• 把模型限制在固定 XML 模板，方便后续用正则抽答案，
也利于上线接口直接 parse。
• 前期 reward 非零易得，帮助模型在搜索空间内“活下来”。

────────────────────────────────── 三、数字奖励 cor_reward （0 / 1 / 2 分） ────────────────────────────────── 源码（经修正后的版本）：

```
XML_RE  = re.compile(r"<answer>(.*?)</answer>", re.S)
_num    = lambda x: re.sub(r"[%$,]", "", x).strip()

def _extract_nums(text: str):
    return [_num(m) for m in XML_RE.findall(text)]

def cor_reward(completions, **kw):
    answers = kw.get("answer") or kw.get("answers") or []
    rewards = []

    for cand_list, gt in zip(completions, answers):
        # 1) 收集 8 条回答里的所有 <answer>…</answer> 数字
        nums = [
            n
            for c in cand_list
            for n in _extract_nums(c["content"])
        ]

        # 2) 若一个数字都没抓到 → 直接 0 分
        if not nums:
            rewards.append(0.0)
            continue

        # 3) 群组投票：出现次数最多的数字
        vote = Counter(nums).most_common(1)[0][0]

        # 4) 评分：完全对 +2，差 1 +1，其余 0
        diff = abs(int(vote) - int(gt)) if vote.isdigit() and gt.isdigit() else 999
        if   diff == 0: rewards.append(2.0)
        elif diff == 1: rewards.append(1.0)
        else:           rewards.append(0.0)

    return rewards
```



详细步骤

1. `_extract_nums()`
   • 用正则在单条回答文本里找所有 `<answer>…</answer>`；
   • `_num()` 去掉 `$ % ,` 等符号，得纯数字字符串。

2. 组内投票（majority vote）

   ```
   vote = Counter(nums).most_common(1)[0][0]
   ```

   

   – 把 8 条回答汇总得到的数字列表 `nums` 做统计；
   – 选出现频率最高的那一个（若并列，Counter 取第一出现）。
   投票的好处：
   • 抑制偶然的随机数；
   • 让模型有动力让多个回答趋向一致＝正确数值。

3. 分级奖励
   diff = |vote - ground_truth|
   – diff == 0 → +2 （完全正确）
   – diff == 1 → +1 （只差 1，也给部分梯度）
   – else     → 0 （远离真值）
   这样 early 训练阶段更容易拿到非零 reward，梯度稠密，KL 更平滑。

输出示例

```
batch_size = 8
cor_reward → [2,1,0,2,0,1,0,2]
fmt_reward → [1,1,0,1,1,1,0,1]
total_reward → [3,2,0,3,1,2,0,3]
```



────────────────────────────────── 四、设计动机对照旧版 ──────────────────────────────────

|          | 旧版 cor_reward   | 新版 cor_reward (群组投票+部分分) |
| -------- | ----------------- | --------------------------------- |
| 采样数目 | 只看第 1 条回答   | 利用 8 条回答，众数投票           |
| 评分标准 | 完全对 +2，否则 0 | 完全对 +2；差 1 +1；其余 0        |
| 梯度稀疏 | 早期大量 0 分     | 早期平均 reward 即可达 1.0+       |
| 格式耦合 | 无                | 格式奖励独立；数字奖励看所有回答  |

结果：
• fmt_reward_mean 更快爬到 0.9；
• cor_reward_mean 抬到 ~1.2（≈30% 完全对 + 35% 差 1）；
• KL 控制在 <0.2，训练稳定；
• 总 reward 1.8→2.1 左右，比旧版提升约 10 %。

────────────────────────────────── 五、可调超参/扩展 ──────────────────────────────────

1. 把“差 1”阈值调宽

   ```
   elif diff <= 2: rewards.append(1.0)
   ```

   

2. 要求投票众数出现 ≥4 次才算有效

   ```
   vote, cnt = Counter(nums).most_common(1)[0]
   if cnt < 4: rewards.append(0.0); continue
   ```

   

3. 只对格式合规的回答计入投票

   ```
   for c in cand_list:
       if not re.match(XML_FMT, c["content"]): continue
       nums.extend(_extract_nums(c["content"]))
   ```

   

这样您可以根据收敛速度、准确率、格式稳定性在项目中继续迭代奖励策略。

下面用一个具体示例把“8 条回答 → 汇总数字 → 选众数（vote）”这一步拆成最直观的过程。

假设本 prompt（某道题）生成了 8 条回答，每条都包含 `<answer>` 标签：

```
回答 0：… <answer>42</answer>
回答 1：… <answer>41</answer>
回答 2：… <answer>42</answer>
回答 3：… <answer>43</answer>
回答 4：… <answer>41</answer>
回答 5：… <answer>42</answer>
回答 6：… <answer>40</answer>
回答 7：… <answer>41</answer>
```



（一）抽数字 → 得到 nums 列表

```
nums = ['42', '41', '42', '43', '41', '42', '40', '41']
```



• 对 8 条回答逐条做 `_extract_nums(c["content"])`
• 把结果用双层列表推导展平成一个长列表

（二）统计出现次数

```
from collections import Counter
cnt = Counter(nums)
# cnt == {'42': 3, '41': 3, '43': 1, '40': 1}
```



（三）取出现次数最多的那一个

```
vote = cnt.most_common(1)[0][0]
# cnt.most_common(1) 返回 [('42', 3)]
# 因为 42 和 41 同为 3 次，但 Counter 保留“第一次出现”顺序，
# nums 列表里 '42' 比 '41' 先出现，所以 42 胜出
```



最终 `vote == '42'`

（四）用 vote 对真值打分
假设正确答案 `gt = '42'`

```
diff = abs(int(vote) - int(gt))   # → 0
reward = 2.0                      # 完全命中 +2 分
```



如果真值是 41

```
diff = abs(42 - 41) = 1   # 只差 1
reward = 1.0              # 得部分分
```



如果真值是 39

```
diff = abs(42 - 39) = 3   # 相差>1
reward = 0.0
```



小结

1. 列表 `nums` 聚合了“这一道题所有回答中的数字”。
2. `Counter(nums).most_common(1)[0][0]` 取“出现次数最多且最先出现”的数字，代表「群组投票」结果。
3. 用这个众数 `vote` 与真值 `gt` 比较：
   ‑ 完全一致 → +2
   ‑ 相差 1 → +1
   ‑ 其余 → 0

这样就充分利用了并行生成的 8 条回答，把它们“投票”成一个更稳妥的预测，再给奖励。



## 群组vote的合理性研究

先投票 → 再对真值”合不合理，要看你希望奖励函数起什么作用。
**合理方面**

1. 自洽性（Self-Consistency）的经验规律
   OpenAI、Google 论文都表明：
   “同一 prompt 让模型多生成几条推理，用众数/平均值作为最终答案， 准确率往往高于单条输出。”
   投票奖励把这个经验直接注入 RL：
   ‑ 如果 8 条里 ≥4 条写 42，那 42 很可能就是正确答案；
   ‑ 早期即使 8 条回答各不相同，也能把出现次数最多的那个作为 “模型当前最确信” 的猜测。
2. 梯度密度更高
   ‑ 纯 0/2 模式：完全错 = 0，很容易 reward 全 0；
   ‑ 投票 + 差 1 给 1 分：早期也能拿到非零 reward，梯度方向更连续。
3. 利用并行生成的计算成本
   既然你已经花显存一次性生成了 8 条回答，把它们全都用来评奖要 比只看第一条更物超所值。
4. 格式门控 + 数值投票分离
   先用格式奖励约束输出形状，再用投票奖励评数值；两部分可独立调 权重，互不干扰。

**局限性**

1. “集体跑偏”
   如果模型内部存在系统性错误（8 条都写 41，但真值 42），投票仍会 选错。此时 reward 仍给 0 / 1，梯度作用有限。
2. 并列众数的歧义
   `Counter.most_common(1)` 默认返回先出现的数字；
   若票数打平，选择具有随机性，可能带来噪声。
   → 可以设阈值：只有票数 ≥4 才用众数，否则 reward=0。
3. 差值阈值的 trade-off
   ‑ 差 1 给 1 分能 densify 梯度；
   ‑ 但如果阈值太宽（差 5 也给分）会削弱“完全正确”的驱动力。
4. 生成条数与开销
   num_generations=8 对 A100 2B 模型还算轻；如果用更大的模型或者 更长 completion，生成 8 条会拖慢训练。

**如何让投票更鲁棒** 

| 表格       | 建议                                                         | 作用                        |
| ---------- | ------------------------------------------------------------ | --------------------------- |
| 票数阈值   | `vote, cnt = Counter(nums).most_common(1)[0]; if cnt < 4: reward=0` | 避免 2∶2∶2∶2 平票时的随机性 |
| 格式过滤   | `if not XML_RE.match(c["content"]): continue`                | 不让无效回答影响投票        |
| 置信度加权 | 用 logits 概率给数字加权平均，而非纯计数                     | 兼顾概率信息                |
| 多众数比较 | 如果出现两个众数且都差 0/1，各给 1.5 分                      | 减少随机性噪声              |
| 差值梯度   | `score = max(0, 2 - diff)`（差 2 得 0、差 1 得 1）           | reward 曲线更平滑           |

**结论**

 • 对 **小批量、有限步数的 RL 微调** 而言，
“投票→对真值” 的奖励能显著缓解 0/2 稀疏问题，让 early reward 更快爬升，并在格式合规率上带来明显增益，是一个合理且常用的技巧。

• 如果你更在意“对/错的严格区分”，可以保持 diff==0 才给分；
若更在意收敛速度和平滑梯度，保留差 1 + 部分分会更友好。

因此，**是否保留投票机制**取决于：
– 你能否接受多生成几条回答的时间 / 显存成本；
– 你更关注最终极限正确率（可考虑后期关闭差 1 奖励），
还是关注训练效率和稳定性（保留投票 + 部分分）。

## 训练结果指标解读

日志里看到的主要字段分成 3 组：
A) SFT 专有、B) GRPO 共同、C) GRPO 专有。
每一列解释字段含义、典型取值范围以及它是怎么来的。

──────────────────────────────────
A. 只在 SFTTrainer 日志里出现
──────────────────────────────────

| 字段                     | 含义                                     | 典型范围          | 计算方式                        |
| ------------------------ | ---------------------------------------- | ----------------- | ------------------------------- |
| loss                     | teacher-forcing 交叉熵平均值（越低越好） | 0.7 → 0.3         | `CrossEntropy(outputs, labels)` |
| mean_token_accuracy      | token 级 top-1 准确率                    | 0.65 → 0.80       | `1 - ppl` 近似值                |
| num_tokens               | 当前 step 处理的 token 数                | batch×seq_len     | 统计 tokenizer 输入长度         |
| train_runtime            | 整个 epoch 耗时 (最终行)                 | 280-300 s         | end_time - start_time           |
| train_samples_per_second | 每秒处理样本数                           | ≈(batch/step)/sec | HF Trainer 统计                 |
| train_steps_per_second   | 每秒更新步数                             | ≈1 / step_latency | HF Trainer 统计                 |
| train_loss               | 全 epoch 的 loss 平均值（最终行）        | 0.85              | 所有 step loss 加权平均         |

──────────────────────────────────
B. SFT、GRPO 都有的通用字段
──────────────────────────────────

| 字段          | 含义                           | 备注                           |
| ------------- | ------------------------------ | ------------------------------ |
| epoch         | 当前步对应的 epoch 比例        | 0.08 = 8% 已训练进度           |
| loss          | SFT：交叉熵；GRPO：KL − reward | GRPO 中越“低”未必越好          |
| grad_norm     | 当前梯度 L2 范数，过大可能爆炸 | 通常 0.1-3 之间                |
| learning_rate | 每 step 动态学习率             | 线性/余弦调度                  |
| num_tokens    | step 内处理 token 数           | 生成任务包含 prompt+completion |
| logging_steps | n 步打印一次，决定日志行粒度   | 配置里的 `logging_steps`       |

──────────────────────────────────
C. GRPOTrainer 特有字段
──────────────────────────────────

| 字段名 (日志 key)             | 含义                                           | 判读规则   | 好/坏典型阈值                 |
| ----------------------------- | ---------------------------------------------- | ---------- | ----------------------------- |
| **rewards/cor_reward/mean**   | 数字奖励均值（+2 完全正确；+1 仅差 1；0 其余） | ↑ 越高越好 | ≥1.2 ⇒ ~30 % 完全对           |
| **rewards/fmt_reward/mean**   | XML 格式奖励均值（满足模板得 +1）              | ↑ 越高越好 | ≥0.90 ⇒ 90 % 合规             |
| **reward**                    | cor + fmt 的批均值 ∈ [0, 3]                    | ↑ 越高越好 | ≥2.0 ⇒ 整体表现佳             |
| **reward_std**                | 批内 reward 的标准差                           | 中等即可   | 0.3–0.8 正常；>1 波动大       |
| **frac_reward_zero_std**      | reward＝0 的样本比例                           | ↓ 越低越好 | <0.3 ⇒ 梯度稠密               |
| **kl**                        | 策略与底座模型的 KL 散度                       | 中等最好   | 0.05–0.25 安全；>0.4 可能发散 |
| **loss**                      | β·KL – reward（GRPO 目标）                     | 趋势即可   | 上下波动属正常                |
| **grad_norm**                 | 当前梯度 L2 范数                               | ↓ 避免爆   | ≤1 稳定；>5 需调梯度裁剪      |
| **completions/mean_length**   | 8 条回答平均 token 长度                        | 监控长度   | 80–110 正常；<50 推理不足     |
| **completions/clipped_ratio** | 回答被 `max_completion_length` 截断的比例      | ↓ 越低越好 | <0.2 最佳；>0.4 考虑加长      |
| **epoch**                     | 已训练进度 (0-1 = 0-100 %)                     | —          | 用来对齐时间点                |

**快速读表**

1. `fmt_reward/mean` ≥ 0.9 → 模板输出稳定。
2. `cor_reward/mean` ≥ 1.2 → 30 % 以上完全正确（好）。
3. `kl` < 0.3 → 更新稳定；若突涨，需减小学习率 / β。
4. `frac_reward_zero_std` < 0.3 → 奖励信号足够密集。
5. `completions/clipped_ratio` > 0.4 → 说明 128 token 不够，可调大。



## 训练参考例子

Refer: *https://github.com/xinyuwei-david/Gemma-2-2B-IT-GRPO*

### 奖励函数优化前：

训练：

```
source .venv/bin/activate
root@a100vm:~/Gemma-2-2B-IT-GRPO# pwd
/root/Gemma-2-2B-IT-GRPO
root@a100vm:~/Gemma-2-2B-IT-GRPO# python gemma-grpo2.py 
root@a100vm:~/Gemma-2-2B-IT-GRPO# python  gemma-instruct-grpo2.py
```

训练中的资源利用率：

```
(Gemma-2-2B-IT-GRPO) root@a100vm:~/Gemma-2-2B-IT-GRPO# nvidia-smi
Sun Jun 15 11:44:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000001:00:00.0 Off |                    0 |
| N/A   49C    P0            109W /  300W |   80793MiB /  81920MiB |     48%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    449318      C   python                                      80780MiB |
+-----------------------------------------------------------------------------------------+
```

评估：

```
python3 -m venv ~/eval-env
source ~/eval-env/bin/activate
pip install "torch>=2.1" "transformers>=4.49" datasets tqdm
pip install accelerate
python gsm8k-eval-tf2.py --model_dir gemma-grpo-only
python gsm8k-eval-tf2.py --model_dir gemma-sft-grpo
```

训练输出对比分析：

| 指标                        | 纯 GRPO (epoch≈0.09) | SFT+GRPO (epoch≈0.18) |
| --------------------------- | -------------------- | --------------------- |
| cor_reward_mean（数字正确） | 1.10 (55%)           | 1.09 (54%)            |
| fmt_reward_mean（格式合规） | 0.76 (76%)           | 0.88 (88%)            |
| 总 reward_mean              | 1.86                 | 1.97                  |
| KL                          | 0.19                 | 0.12                  |
| 训练步数（num_tokens≈）     | 357k                 | 707k                  |
| train_loss                  | 0.0077               | 0.0091                |
| 训练时长                    | 22 min               | 29 min                |

**评估脚本执行结果**

纯GRPO

```
----------------------------------------
Input tokens  avg=140.5  max=269
Output tokens avg=90.9  max=257
Correct format     : 1142/1319 (86.6%)
Plausibly correct  : 566/1319 (42.9%)
Exact correct      : 559/1319 (42.4%)
========================================
```

SFT+GRPO

```

----------------------------------------
Input tokens  avg=140.5  max=269
Output tokens avg=74.7  max=257
Correct format     : 1192/1319 (90.4%)
Plausibly correct  : 504/1319 (38.2%)
Exact correct      : 500/1319 (37.9%)
========================================
(eval-env) root@a100vm:~/Gemma-2-2B-IT-GRPO# 
```

### 奖励函数优化后

训练

```
source .venv/bin/activate
root@a100vm:~/Gemma-2-2B-IT-GRPO# pwd
/root/Gemma-2-2B-IT-GRPO
root@a100vm:~/Gemma-2-2B-IT-GRPO# python gemma-grpo3.py 
root@a100vm:~/Gemma-2-2B-IT-GRPO# python  gemma-instruct-grpo3.py
```

评估

```
python3 -m venv ~/eval-env
source ~/eval-env/bin/activate
pip install "torch>=2.1" "transformers>=4.49" datasets tqdm
pip install accelerate
python gsm8k-eval-tf2.py --model_dir gemma-grpo-only
python gsm8k-eval-tf2.py --model_dir gemma-sft-grpo
```

**GRPO only的输出分析：**

1. 新版奖励（投票＋差值≤1 给 1 分）显著提高了 early-reward： 

   ‑ epoch 0.02 时总 reward=1.70（旧版 1.18）
   ‑ 格式合规率直接跃到 86 %（旧版 7 %→60 %）

2. 数字部分不仅看 “完全正确”，还能看到 “差 1” 的部分分。 cor_reward_mean≈1.25 ⇒ 约 30 % 完全命中、35 % 差 1。

3. 训练 180 步后： • 新版总 reward ≈2.05（旧版 1.86）
   • 格式 93 %（旧版 76 %）
   • 完全正确率略升，近+1 个百分点；但“近似正确”样本大幅增加。

4. KL 始终 <0.2，说明在 β=0.005 配置下模型没有过度偏离 初始分布，训练稳定。

指标拆解，下面取日志中的 6 个时间点（步数间隔 ≈20）：

| epoch | cor_mean | fmt_mean | reward | 说明                    |
| ----- | -------- | -------- | ------ | ----------------------- |
| 0.01  | 0.71     | 0.32     | 1.03   | 格式刚开始受奖励约束    |
| 0.02  | 0.85     | 0.86     | 1.70   | 投票 + 格式门控发力     |
| 0.03  | 0.99     | 0.91     | 1.90   | 一半以上数值近真值      |
| 0.05  | 1.25     | 0.95     | 2.20   | 25 % 完全对；40 % 差 1  |
| 0.07  | 1.10     | 0.89     | 1.98   | reward 稍回落——探索多样 |
| 0.09  | 1.22     | 0.90     | 2.12   | 收尾：保持 2+ reward    |

与旧版对比 

| 指标（训练末尾） | 旧 版 (2 k) | 新 版 (3.5 k+投票) |
| ---------------- | ----------- | ------------------ |
| cor_reward_mean  | 1.10        | 1.22 (+0.12)       |
| fmt_reward_mean  | 0.76        | 0.93 (+0.17)       |
| reward_mean      | 1.86        | 2.05 (+0.19)       |
| KL               | 0.19        | 0.15 (↓)           |
| 训练时长         | 22 min      | 25 min (+3 min)    |
| 显存峰值         | ~55 GB      | ~55 GB（同）       |

可见新脚本：

1. 用更大的训练集 + 投票奖励，把总 reward 提高约 10 %。
2. 格式合规显著改善；数字完全正确率略提升但更稳。
3. KL 下降，说明奖励更平滑，梯度噪声更小。



新奖励函数两种训练方法对比

| 指标          | 时段  | 纯 GRPO | SFT+GRPO | 结论                     |
| ------------- | ----- | ------- | -------- | ------------------------ |
| 格式 fmt_mean | Early | 0.32    | 0.76     | SFT 热身大幅提高格式     |
|               | End   | 0.90    | 0.93     | 二者收敛都高，SFT 略高   |
| 数字 cor_mean | Early | 0.71    | 0.72     | 起步相近                 |
|               | Mid   | 0.99    | 0.97     | 持平                     |
|               | End   | 1.22    | 1.03     | 纯 GRPO 稍优             |
| 总 reward     | Best  | 2.20    | 2.07     | 最高 reward 纯 GRPO 略高 |
| KL            | End   | 0.15    | 0.17     | 均处安全区               |

这样就能看到：
• SFT 带来「格式率早期就上线」，但数值 reward 最高值还是纯 GRPO 稍高；
• 如果延长步数，观察 Mid→End 的趋势线谁仍在上升，就能决定要不要再加步数 / 数据。

本质结论依旧：在 3 500 GSM8K / 180 step 的小训练窗口里，SFT 的收益主要体现在格式；想让 SFT 跑赢数值需要更多数据或更长 RL 训练。但用完整时间序列 + 分段统计，而非单条 final log，会让决策更有依据。

**与旧代码对比**

| 版本 & 数据/奖励        | 纯 GRPO<br>旧 (2 k) | SFT→GRPO<br>旧 (2 k+SFT) | 纯 GRPO<br>新 (3.5 k+投票) | SFT→GRPO<br>新 (3.5 k+投票+SFT) |
| ----------------------- | ------------------- | ------------------------ | -------------------------- | ------------------------------- |
| fmt_mean (格式率)       | **0.90**            | **0.93** ↑(+0.03)        | **0.90** (≈)               | **0.93** (≈)                    |
| cor_mean (数值奖励均值) | **1.22**            | 1.10 ↓                   | **1.22** (≈)               | 0.97 ↓（投票后均摊）            |
| 总 reward_mean          | **2.05**            | 1.90 ↓                   | **2.05** (≈)               | 1.90 ↓                          |
| 完全正确估算¹           | ≈30 %               | ≈24 % ↓                  | ≈30 % (≈)                  | ≈24 % ↓                         |
| 近似正确(差1) 估算¹     | ≈0 %                | ≈6 %                     | ≈31 % ↑                    | ≈49 % ↑                         |
| KL 末值                 | 0.15                | 0.17 ↑                   | 0.15 (≈)                   | 0.17 (≈)                        |
| 训练时长 (min)          | 22                  | 29                       | 25                         | 25                              |
| frac_reward_zero        | 0.30                | 0.40 ↑                   | 0.30 (≈)                   | 0.40 (≈)                        |



**评估脚本执行结果：**

仅GRPO：

```
----------------------------------------
Input tokens  avg=140.5  max=269
Output tokens avg=92.2  max=257
Correct format     : 1120/1319 (84.9%)
Plausibly correct  : 665/1319 (50.4%)
Exact correct      : 657/1319 (49.8%)
========================================
(eval-env) root@a100vm:~/Gemma-2-2B-IT-GRPO# 
```

SFT+GRPO

```
----------------------------------------
Input tokens  avg=140.5  max=269
Output tokens avg=75.5  max=257
Correct format     : 1161/1319 (88.0%)
Plausibly correct  : 506/1319 (38.4%)
Exact correct      : 505/1319 (38.3%)
========================================
(eval-env) root@a100vm:~/Gemma-2-2B-IT-GRPO# 
```



与前一版奖励函数进行对比（仅GRPO）：

| 指标                                       | 旧奖励             | 新奖励                | 差值 (B-A) | 谁更优 |
| ------------------------------------------ | ------------------ | --------------------- | ---------- | ------ |
| **Correct format**<br>(格式合规率)         | 86.6 % (1142/1319) | 84.9 % (1120/1319)    | – 1.7 pt   | A      |
| **Exact correct**<br>(完全命中真值)        | 42.4 % (559/1319)  | **49.8 % (657/1319)** | + 7.4 pt   | **B**  |
| **Plausibly correct**<br>(数字或 XML 命中) | 42.9 % (566/1319)  | **50.4 % (665/1319)** | + 7.5 pt   | **B**  |
| 输出长度 avg / max                         | 90.9 / 257         | 92.2 / 257            | +1.3 tok   | ≈      |

1. **数值准确率** 新奖励把完全正确率提升了约 7 个百分点，这是只奖励 exact-match 的直接收益。

2. **格式合规率** 基本持平

   