# 当LLM出现幻觉时它想什么？

参考论文:

https://arxiv.org/pdf/2410.02707



在GPT这些模型在实际应用中存在一个重要的问题——幻觉（Hallucinations）。即它们可能生成看似合理但实际上错误的内容，涉及事实性错误、逻辑矛盾或不合常识的回答。

**一、什么是LLMs的幻觉？**

幻觉是指模型在生成文本时，输出了不正确或虚构的信息。这些错误可能是事实性错误、逻辑漏洞、偏见或者是违反常识的内容。 例如，当你问一个LLM：“世界上最高的山是什么？”模型可能正确地回答：“珠穆朗玛峰是世界上最高的山。”但有时候，它可能会回答：“乞力马扎罗山是世界上最高的山。”这种情况下，模型生成了一个事实性错误的回答，即产生了幻觉。

**二、LLMs内部知道它们何时犯错**

**1. 真实性信息集中在“确切答案词元上**

研究人员发现，LLMs在生成答案时，关于答案真假的信息主要集中在确切答案词元（exact answer tokens）上。所谓确切答案词元，是指在模型生成的答案中，直接给出关键信息的词。例如，在回答“中国首都在哪里？时，模型可能回答：“中国的首都在北京。”这里的“北京”就是确切答案词元。

**2. 利用探测分类器提高错误检测**

基于上述发现，研究人员训练了探测分类器（probing classifiers），专门对LLM在生成确切答案词元时的内部激活状态进行分析，以预测答案的正确性。他们的实验表明，针对确切答案词元的内部表示进行训练，可以显著提高错误检测的性能。

**3. 模型的内在表示包含更多信息**

即使LLM生成了错误的答案，其内部表示却可能已经包含了正确答案的信息。这意味着模型“知道”正确的答案，但在生成过程中由于各种原因（如训练策略、解码方法等）输出了错误的内容。 

**三、模型内外不一致的原因以及工程学上的意义**

模型的内部表示与其外部输出可能存在不一致性，这一发现揭示了LLMs在训练和生成过程中的一些深层次问题。 例子5：模型内部知道，但外部不说  假设我们问模型：“爱因斯坦提出的著名方程是什么？”模型回答：“爱因斯坦提出了相对论，但我不确定具体的方程。”然而，模型内部可能已经生成了“E=mc²”，但由于一些原因（可能是训练中学到的表达方式或者谨慎策略），没有在外部输出。 这个例子表明，我们可以尝试调整模型的生成策略，让其更充分地利用内部的正确信息，提供更准确的答案。

1. 优化模型的训练策略：在训练过程中，增加对正确答案词元的关注度，或者引入新的损失函数，强化模型对真实性的敏感性。
2. 改进解码方法：在生成过程中，结合探测分类器的反馈，实时调整生成路径，引导模型输出正确的答案。
3. 开发开放源码模型：由于利用模型内部激活状态需要白盒访问，开源的模型更有利于工程实践中的应用和改进。



**四、激活**

首先还是要普及一下“激活”的概念。

### 1、什么是激活（Activation）？

在神经网络（包括大型语言模型）中，**“激活”**是指神经元在接收到输入信号后输出的结果。可以把它想象成每个神经元的“兴奋程度”或“活跃程度”。

**通俗的比喻：**

- 把一个神经网络想象成一群决策者（神经元）在开会。
- 每个决策者接收到信息（输入），然后根据自己的判断标准，决定要不要“发言”或者“表态”。
- **激活**就是这个决策者的“发言力度”或“支持程度”，可能强烈支持，也可能不表态，或者反对。

在神经网络中，激活函数决定了神经元的输出（激活值）的范围。常见的激活函数有：

- 线性激活函数

  \- 特点：没有对输入进行任何非线性变换。

  \- 激活值范围：从负无穷到正无穷，激活值可以是任何实数。

- ReLU（Rectified Linear Unit）激活函数：

  \- 公式：f(x) = max(0, x)

  \- 特点：将输入小于0的部分截断为0，输入大于0的部分保持不变。

  \- 激活值范围：0到正无穷，可以超过1。

- Sigmoid激活函数：

  \- 公式：f(x) = 1 / (1 + e^{-x})

  \- 特点：将输入压缩到0到1之间。

  \- 激活值范围：0到1，不会超过1。

- Tanh激活函数：

  \- 公式：f(x) = (e^{x} - e^{-x}) / (e^{x} + e^{-x})

  \- 特点：将输入压缩到-1到1之间。

- GeLU（Gaussian Error Linear Unit）激活函数：

  \- 常用于Transformer模型。

  \- 特点：是一种平滑的非线性函数，没有严格的上下限。

  \- 激活值范围：理论上从负无穷到正无穷，但实际值集中在一定范围内。

### 2、激活有高低之分

- **高激活**：表示神经元对当前输入非常“感兴趣”或“敏感”，输出的数值较大。

- **低激活**：表示神经元对当前输入不太“感兴趣”，输出的数值较小，甚至为零。

  **在模型中，激活通常是一个数值，可以是正的、负的、或者在一定范围内变化。**

### 3、在模型生成答案时，激活的作用

当模型处理一个问题并生成答案时，它会对可能的候选答案进行评估。

- **每个可能的答案**（如“尼罗河”、“亚马逊河”）都会在模型的内部产生一系列的激活值。

- **激活值的大小**反映了模型对于某个答案的“信心”或“关注度”。

  **举例说明：**

- **问题**：”世界上最长的河流是什么？“

- **模型可能考虑的候选答案**：尼罗河、亚马逊河、长江、密西西比河等。

  

  **模型内部可能的激活情况：**

- **尼罗河**：激活值较高（例如0.8）

- **亚马逊河**：激活值也较高（例如0.75）

- **长江**：激活值较低（例如0.4）

- **密西西比河**：激活值很低（例如0.2）

  

- **解释：**

- 模型的内部激活状态显示，它认为“尼罗河”是一个很有可能的答案，“亚马逊河”也有可能。

- 最终，模型可能因为各种原因（如训练数据的偏差、解码策略等）选择了激活值略低的“亚马逊河”作为输出。

### 4、发现模型对“尼罗河”有高激活意味着什么？

当我们说**“发现模型实际上对于‘尼罗河’也有很高的激活”**，意思是：

- **模型的内部计算**已经将“尼罗河”视为一个强有力的候选答案，对其有很高的关注度或信心。
- **虽然最终输出了“亚马逊河”**，但从内部激活状态来看，“尼罗河”也是模型高度认可的答案之一。

### ** **

### **5、激活高但未输出的原因**

**那么，为什么模型没有输出激活值更高的“尼罗河”呢？**

可能的原因包括：

1. **解码策略的影响**：
   - 模型在生成答案时，使用了**概率采样**（如温度采样）或其他策略，导致并非总是选择激活值最高的词。
2. **训练数据的影响**：
   - 如果训练数据中，关于“亚马逊河是最长河流”的错误信息较多，模型可能受到误导。
3. **模型的不确定性**：
   - 模型在“尼罗河”和“亚马逊河”之间的激活值很接近，导致选择了激活值略低的“亚马逊河”作为输出。

### 6、激活的大小帮助我们理解模型的内部决策过程

通过分析模型内部的激活值，我们可以：

- **理解模型的信心分布**：知道模型对哪些候选答案有较高的信心。
- **发现潜在的正确答案**：即使模型输出了错误的答案，但其内部可能已经“想到”了正确的答案。

### 7、利用激活信息改进模型

如果我们知道模型对正确答案有高激活值，我们可以：

1. **调整解码策略**：
   - 修改模型的解码算法，让其更倾向于选择激活值最高的词。
2. **纠正模型输出**：
   - 利用内部激活信息，开发纠错机制，纠正模型的错误输出。
3. **训练模型更好地利用内部知识**：
   - 在训练过程中，强化模型对正确答案的偏好，降低错误答案的激活值。



**五、当确切答案的激活值最高时，模型仍可能出现幻觉的原因**

即使确切答案的激活值最高，模型仍可能输出错误的答案，即产生幻觉。原因包括但不限于如下几条。

**1. 激活值与最终输出之间的关系并非绝对直接：**

  \- 激活值：在模型的内部计算过程中，每个可能的候选词（或词元）都会有一个对应的激活值，表示模型对该词的“内部偏好”或“信心”。

  \- 解码过程：模型在生成最终输出时，会根据激活值以及解码策略（如贪心搜索、温度采样、Beam Search等）来选择下一个词元。

**2. 解码策略可能导致激活值最高的答案未被输出**

  \- 温度采样（Temperature Sampling）：在生成文本时，模型会根据激活值（通常是经过Softmax函数得到的概率分布）进行采样。如果温度参数较高，模型更倾向于探索，可能会选择激活值并非最高的词元。

  \- 随机性：即使激活值最高的词元在概率上占优势，但在采样过程中，仍有可能选择其他激活值较低的词元。

**3. 其它因素导致幻觉的产生**

  \- 训练数据的偏差：如果模型的训练数据中包含错误或误导性的信息，即使模型内部对正确答案有高激活值，可能仍受到错误信息的影响，最终输出错误答案。

  \- 模型的表达能力限制：模型可能缺乏足够的参数或结构来正确地生成准确的答案，导致即使内部有正确的信息，输出时仍出现幻觉。

  \- 上下文影响：模型的输出还受上下文和先前生成内容的影响，可能导致偏离正确答案。

   **总结：**

激活值最高的答案不一定会被输出，因为生成过程受到解码策略、随机性和其他因素的影响。

因此，即使确切答案的激活值最高，模型仍可能输出错误的答案，产生幻觉。

**六、解码中的几个的策略-贪心解码**

贪心解码只关注每一步的局部最优选择，可能导致生成重复的模式或循环，如 **A B A B**。

这种重复可能是因为某些词元在特定的上下文中总是具有最高的概率，导致模型反复选择它们。

**为了更直观地理解，让我们以实际句子生成为例。
**

### 示例场景：

**任务：** 让模型续写一句话，起始句是：

- **起始句：**“天气真好，我想去”

### 1. 使用贪心解码生成的结果

**贪心解码过程：**

- 时间步 1：

  - **输入：**“天气真好，我想去”
  - **模型预测：**概率最高的词是“散步”

- 时间步 2：

  - **输入：**“天气真好，我想去散步”
  - **模型预测：**概率最高的词是“。”

- 时间步 3：

  - **输入：**“天气真好，我想去散步。”
  - **模型预测：**概率最高的词是“我”

- 时间步 4：

  - **输入：**“天气真好，我想去散步。我”
  - **模型预测：**概率最高的词是“想”

- 时间步 5：

  - **输入：**“天气真好，我想去散步。我想”
  - **模型预测：**概率最高的词是“去”

- 时间步 6：

  - **输入：**“天气真好，我想去散步。我想去”

  - **模型预测：**概率最高的词是“散步”

    **生成的句子：**

- “天气真好，我想去散步。我想去散步。我想去散步。……”

###  

### **2. 分析**

- 出现了重复的模式“我想去散步”
  - 模型在句号后继续生成时，总是认为“我想去”后接“散步”概率最高，导致重复。
- 贪心解码没有考虑全局的上下文和内容的多样性
  - 只关注当前最可能的词，忽略了潜在的更丰富的表达。

### **3. 改进生成结果**

为了避免这种重复，可以采用其他解码策略，比如 **Top-k 采样** 或 **Top-p 采样**，引入一定的随机性和多样性。

**使用 Top-p 采样（p=0.9）生成的结果：**

- 时间步 1：

  - **输入：**“天气真好，我想去”
  - **模型预测：**概率最高的词是“散步”（0.4），其次是“公园”（0.35），再次是“旅行”（0.15），其他词概率较低。

- **Top-p 采样**在累计概率超过0.9时，选择候选词元集合：**“散步”、“公园”、“旅行”**。

- **随机采样可能选择“公园”**

  

- 时间步 2：

  - **输入：**“天气真好，我想去公园”
  - **模型预测：**概率较高的词是“赏花”、“散步”、“运动”。

- **随机采样可能选择“赏花”**

  

- 时间步 3：

  - **输入：**“天气真好，我想去公园赏花”
  - **模型预测：**概率较高的词是“。”、“，欣赏美景”、“，呼吸新鲜空气”。

- **随机采样可能选择“。”**

  **生成的句子：**

- “天气真好，我想去公园赏花。”

  

### 4. 改进的效果

- **避免了重复，生成了更丰富、更自然的句子**

- **引入了多样性，使得内容更加有趣**

  

### **5. A B A B 的问题总结**

- 代表模型在贪心解码下可能生成的重复序列，导致输出内容单调乏味。
- 出现这种情况是因为模型每次都选择最有可能的词，忽略了全局的连贯性和多样性。

###  

### 解决方法

- **引入随机性：**使用 Top-k 或 Top-p 采样，让模型有机会选择概率稍低但合理的词，增加生成内容的多样性。
- **调整解码策略：**采用组合解码策略，避免模型陷入重复循环。

### 6. 案例启示

- **贪心解码适用于需要高确定性的任务，但可能导致重复和缺乏创意。**
- **在自然语言生成中，为了生成更加自然、流畅的文本，常常需要在准确性和多样性之间取得平衡。**



***\*七、解码中的几个的策略-\*\*束搜索 Beam Search\*\**\***

## 1. 原理

是一种在解码过程中保留多个可能序列的策略。它在每个时间步保留得分最高的 **Beam Width（束宽）** 个部分序列，然后对这些序列进行扩展。它试图在计算资源允许的情况下找到更优的序列。

### 2. 特点

- 优点：
  - 比贪心搜索更有可能找到全局最优的序列。
  - 通过保留多个候选，可以避免陷入局部最优。
- 缺点：
  - 计算量比贪心搜索更大。
  - 可能导致输出较为通用，缺乏多样性。

###  

### 3. 示例

**任务：** 续写句子 “天气真好，我想去”

**设置：**

- **束宽（Beam Width）：** 3

- **模型在每个时间步提供的候选词及其得分（假设得分为对数概率的负值，得分越低越好）。**

  **时间步 1：**

- **输入：**“天气真好，我想去”

- **模型预测候选词：**

  1. **公园**（得分 0.5）

  2. **散步**（得分 0.6）

  3. **旅游**（得分 0.7）

  4. **购物**（得分 0.9）

     **因为束宽为3，我们保留得分最低的前三个词。在束搜索中，\**确实是保留得分最低的序列\**，这是因为得分被定义为负的对数概率（Negative Log-Likelihood，NLL）。由于对数概率是负值，得分越低，表示概率越高。**

     

- 序列1：“公园”（总得分 0.5）

- 序列2：“散步”（总得分 0.6）

- 序列3：“旅游”（总得分 0.7）

  **时间步 2：**

  对每个保留的序列进行扩展：

  **对于序列1：“……我想去公园”**

- **候选词：**

  1. **散步**（得分 0.4）→ 序列：“……公园散步”（总得分 0.5+0.4=0.9）

  2. **赏花**（得分 0.5）→ 序列：“……公园赏花”（总得分 0.5+0.5=1.0）

  3. **拍照**（得分 0.6）→ 序列：“……公园拍照”（总得分 0.5+0.6=1.1）

     **对于序列2：“……我想去散步”**

- **候选词：**

  1. **。**（句号，得分 0.3）→ 序列：“……散步。”（总得分 0.6+0.3=0.9）

  2. **，**（逗号，得分 0.5）→ 序列：“……散步，”（总得分 0.6+0.5=1.1）

  3. **和朋友**（得分 0.7）→ 序列：“……散步和朋友”（总得分 0.6+0.7=1.3）

     **对于序列3：“……我想去旅游”**

- **候选词：**

  1. **放松**（得分 0.6）→ 序列：“……旅游放松”（总得分 0.7+0.6=1.3）

  2. **拍照**（得分 0.7）→ 序列：“……旅游拍照”（总得分 0.7+0.7=1.4）

  3. **。**（句号，得分 0.5）→ 序列：“……旅游。”（总得分 0.7+0.5=1.2）

     **将所有新序列按照总得分排序，保留得分最低的3个序列：**

1. “……我想去公园散步”（总得分 0.9）

2. “……我想去散步。”（总得分 0.9）

3. “……我想去公园赏花”（总得分 1.0）

   **时间步 3：**

   继续对保留的序列进行扩展：

   **对于序列：“……我想去公园散步”**

- **候选词：**

  1. **。**（得分 0.3）→ 序列：“……公园散步。”（总得分 0.9+0.3=1.2）

  2. **，**（得分 0.5）→ 序列：“……公园散步，”（总得分 0.9+0.5=1.4）

     **对于序列：“……我想去散步。”**

- **句子已完结，不再扩展。**

  **对于序列：“……我想去公园赏花”**

- **候选词：**

  1. **。**（得分 0.3）→ 序列：“……公园赏花。”（总得分 1.0+0.3=1.3）

  2. **拍照**（得分 0.6）→ 序列：“……公园赏花拍照”（总得分 1.0+0.6=1.6）

     **保留得分最低的3个序列：**

1. “……我想去散步。”（总得分 0.9）*（已完成）*

2. “……我想去公园散步。”（总得分 1.2）

3. “……我想去公园赏花。”（总得分 1.3）

   **最终结果：**

- **得分最低的序列是：“天气真好，我想去散步。”（总得分 0.9）**

  **分析：**

- **束搜索通过考虑多个候选序列，避免了贪心搜索可能导致的次优选择。**

- **生成的句子自然且逻辑通顺，模型选择了得分最高的完整句子。**



## ***\*八、解码中的几个的策略-\******温度采样（Temperature Sampling）**



### 1. 原理

**温度采样**通过调整概率分布的“温度”参数 ( T )，改变模型对高低概率词的偏好，从而控制生成文本的随机性和多样性。

- 温度 ( T ) 的作用：
  - ( T < 1 )：降低随机性，模型更倾向于选择高概率词元。
  - ( T > 1 )：增加随机性，模型有更大概率选择低概率词元。

### 2. 示例

**任务：** 续写句子 “天气真好，我想去”

**场景设置：**

- **模型在某个时间步的原始概率分布（在不考虑具体数值的情况下）：**
  - **散步**（概率高）
  - **公园**（概率较高）
  - **海边**（概率中等）
  - **爬山**（概率较低）
  - **购物中心**（概率更低）

### 3. 使用不同的温度参数



#### a. 温度 ( T = 0.7 )（低温度）

**调整后的概率分布：**

- **高概率词的概率进一步增大，低概率词的概率降低。**

  **可能的输出：**

- **模型更可能选择“散步”或“公园”**

  **生成的句子：**

- “天气真好，我想去散步。”

  **分析：**

- **低温度使模型的输出更保守，选择最有可能的词。**

- **生成的句子较为常见，创造性低。**

#### b. 温度 ( T = 1.0 )（标准温度）

**调整后的概率分布与原始概率分布相同。**

**可能的输出：**

- **模型可能选择“散步”、“公园”或“海边”**

  **生成的句子：**

- “天气真好，我想去海边。”

  **分析：**

- **模型有适度的随机性，可能生成多种合理的句子。**

#### c. 温度 ( T = 1.5 )（高温度）

**调整后的概率分布：**

- **概率分布变得更平坦，低概率词的概率增加。**

  **可能的输出：**

- **模型有较大概率选择“爬山”或“购物中心”**

  **生成的句子：**

- “天气真好，我想去爬山。”

  **分析：**

- **高温度增加了输出的多样性，可能会生成更有创意的句子。**

- **但温度过高可能导致生成不符合逻辑的句子，如“天气真好，我想去购物中心。”（在好天气下去室内可能不符合常理）**



## ***\*九、解码中的几个的策略-\******Top-k 采样**



### 1. 原理

**Top-k 采样**在每个时间步只考虑概率排名前 ( k ) 的词元，其余词的概率设为零，然后重新归一化概率分布，再进行随机采样。

### 2. 示例

**任务：** 续写句子 “天气真好，我想去”

**模型在当前时间步的概率分布（假设）：**

1. **散步**（0.4）
2. **公园**（0.3）
3. **海边**（0.15）
4. **电影院**（0.05）
5. **购物中心**（0.04）
6. **图书馆**（0.03）
7. **健身房**（0.02）
8. **医院**（0.01）

### 3. 使用不同的 ( k ) 值



#### a. ( k = 3 )

**保留前三个词元：**

- **散步**

- **公园**

- **海边**

  **重新归一化后的概率：**

- **散步：0.4 / 0.85 ≈ 0.47**

- **公园：0.3 / 0.85 ≈ 0.35**

- **海边：0.15 / 0.85 ≈ 0.18**

  **可能的输出：**

- **随机选择“散步”、“公园”或“海边”**

  **生成的句子：**

- “天气真好，我想去公园。”

#### b. ( k = 5 )

**保留前五个词元：**

- **散步**

- **公园**

- **海边**

- **电影院**

- **购物中心**

  **重新归一化后的概率：**

- **散步：0.4 / 0.94 ≈ 0.43**

- **公园：0.3 / 0.94 ≈ 0.32**

- **海边：0.15 / 0.94 ≈ 0.16**

- **电影院：0.05 / 0.94 ≈ 0.053**

- **购物中心：0.04 / 0.94 ≈ 0.043**

  **可能的输出：**

- **有小概率会选择“电影院”或“购物中心”**

  **生成的句子：**

- “天气真好，我想去电影院。”

  **分析：**

- **较小的 ( k ) 值控制了随机性，确保生成合理的词。**

- **较大的 ( k ) 值增加了多样性，但可能引入不太合理的选择。**



## **十、\**\*\*解码中的几个的策略-\*\*束搜索\*\*\*\*\** Top-p（核）采样**

### 1. 原理

**Top-p 采样**（又称核采样）在每个时间步动态选择一组词元，使得这些词的累计概率超过阈值 ( p )，然后从中进行随机采样。

### 2. 示例

**任务：** 续写句子 “天气真好，我想去”

**模型在当前时间步的概率分布（按概率从高到低排序）：**

1. **散步**（0.4）→ 累计概率 0.4
2. **公园**（0.3）→ 累计概率 0.7
3. **海边**（0.15）→ 累计概率 0.85
4. **电影院**（0.05）→ 累计概率 0.90
5. **购物中心**（0.04）→ 累计概率 0.94
6. **图书馆**（0.03）→ 累计概率 0.97
7. **健身房**（0.02）→ 累计概率 0.99
8. **医院**（0.01）→ 累计概率 1.00

### 3. 使用不同的 ( p ) 值



#### a. ( p = 0.85 )

**候选词元集合：**

- 散步

- 公园

- 海边

  重新归一化后的概率：

- 散步：0.4 / 0.85 ≈ 0.47

- 公园：0.3 / 0.85 ≈ 0.35

- 海边：0.15 / 0.85 ≈ 0.18

  可能的输出：

- 随机选择“散步”、“公园”或“海边”

  生成的句子：

- “天气真好，我想去海边。”

#### b. ( p = 0.9 )

**候选词元集合：**

- 散步

- 公园

- 海边

- 电影院

  重新归一化后的概率：

- 散步：0.4 / 0.9 ≈ 0.44

- 公园：0.3 / 0.9 ≈ 0.33

- 海边：0.15 / 0.9 ≈ 0.17

- 电影院：0.05 / 0.9 ≈ 0.056

  **可能的输出：**

- **有小概率选择“电影院”**

  **生成的句子：**

- “天气真好，我想去电影院。”

  **分析：**

- Top-p 采样根据概率分布动态调整候选集合，比Top-k 采样更灵活。

- 当模型对前几个词的置信度很高时，候选集合小，输出更可靠。

- 当概率分布较平坦时，候选集合大，增加了随机性和多样性。



## **十一、组合策略和其他调整**



### 1. 禁止重复 n-gram

**原理：**

- 在生成过程中，禁止模型生成重复的 n-gram（如重复的短语），防止输出中出现重复。

### 示例

任务： 续写句子 “天气真好，我想去”

**问题：**

- 模型可能产生“……我想去公园，我想去公园。”的重复句子

  解决方案：

- 在解码过程中，如果发现生成的词会导致重复 n-gram，则降低其概率或将其设为零

  效果：

- 模型被迫选择其他合理的词，避免了重复

  生成的句子：

- “天气真好，我想去公园，感受大自然的气息。”

### 2. 长度惩罚

**原理：**

- 在解码时，对过短或过长的序列施加惩罚，控制生成内容的长度

  示例：

- 若模型倾向于生成过短的句子，长度惩罚可以鼓励其生成更完整的句子

  **生成的句子：**

- “天气真好，我想去公园散步，享受阳光明媚的下午。”



## **总结**

- 不同的解码策略可以影响模型生成文本的风格和质量。
- 贪心搜索简单但可能导致重复和缺乏多样性。
- 束搜索通过考虑多个候选序列，生成更优质的文本，但计算开销更大。
- 温度采样、Top-k 采样和 Top-p 采样通过引入随机性，增加了生成文本的多样性和创造性。
- 组合使用这些策略，并结合重复惩罚、长度惩罚等方法，可以进一步优化模型的输出，满足不同的需求。